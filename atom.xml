<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>单身程序员的小窝</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-02-16T17:51:50.433Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Jindong</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>robot maze</title>
    <link href="http://yoursite.com/2019/02/17/robot-maze/"/>
    <id>http://yoursite.com/2019/02/17/robot-maze/</id>
    <published>2019-02-16T17:41:54.000Z</published>
    <updated>2019-02-16T17:51:50.433Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Section-0-问题描述与完成项目流程"><a href="#Section-0-问题描述与完成项目流程" class="headerlink" title="Section 0 问题描述与完成项目流程"></a>Section 0 问题描述与完成项目流程</h1><h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p><img src="/2019/02/17/robot-maze/default.png" width="20%"></p><p>在该项目中，你将使用强化学习算法，实现一个自动走迷宫机器人。</p><ol><li>如上图所示，智能机器人显示在右上角。在我们的迷宫中，有陷阱（红色炸弹）及终点（蓝色的目标点）两种情景。机器人要尽量避开陷阱、尽快到达目的地。</li><li>小车可执行的动作包括：向上走 <code>u</code>、向右走 <code>r</code>、向下走 <code>d</code>、向左走 <code>l</code>。</li><li>执行不同的动作后，根据不同的情况会获得不同的奖励，具体而言，有以下几种情况。<ul><li>撞到墙壁：-10</li><li>走到终点：50</li><li>走到陷阱：-30</li><li>其余情况：-0.1</li></ul></li><li>我们需要通过修改 <code>robot.py</code> 中的代码，来实现一个 Q Learning 机器人，实现上述的目标。</li></ol><h2 id="2-完成项目流程"><a href="#2-完成项目流程" class="headerlink" title="2. 完成项目流程"></a>2. 完成项目流程</h2><ol><li>配置环境，使用 <code>envirnment.yml</code> 文件配置名为 <code>robot-env</code> 的 conda 环境，具体而言，你只需转到当前的目录，在命令行/终端中运行如下代码，稍作等待即可。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f envirnment.yml</span><br></pre></td></tr></table></figure></li></ol><p>安装完毕后，在命令行/终端中运行 <code>source activate robot-env</code>（Mac/Linux 系统）或 <code>activate robot-env</code>（Windows 系统）激活该环境。</p><ol><li>阅读 <code>main.ipynb</code> 中的指导完成项目，并根据指导修改对应的代码，生成、观察结果。</li><li>导出代码与报告，上传文件，提交审阅并优化。</li></ol><hr><hr><h1 id="Section-1-算法理解"><a href="#Section-1-算法理解" class="headerlink" title="Section 1 算法理解"></a>Section 1 算法理解</h1><h2 id="1-1-强化学习总览"><a href="#1-1-强化学习总览" class="headerlink" title="1. 1 强化学习总览"></a>1. 1 强化学习总览</h2><p>强化学习作为机器学习算法的一种，其模式也是让智能体在“训练”中学到“经验”，以实现给定的任务。但不同于监督学习与非监督学习，在强化学习的框架中，我们更侧重通过智能体与环境的<strong>交互</strong>来学习。通常在监督学习和非监督学习任务中，智能体往往需要通过给定的训练集，辅之以既定的训练目标（如最小化损失函数），通过给定的学习算法来实现这一目标。然而在强化学习中，智能体则是通过其与环境交互得到的奖励进行学习。这个环境可以是虚拟的（如虚拟的迷宫），也可以是真实的（自动驾驶汽车在真实道路上收集数据）。</p><p>在强化学习中有五个核心组成部分，它们分别是：<strong>环境（Environment）</strong>、<strong>智能体（Agent）</strong>、<strong>状态（State）</strong>、<strong>动作（Action）</strong>和<strong>奖励（Reward）</strong>。在某一时间节点 $t$：</p><ul><li>智能体在从环境中感知其所处的状态 $s_t$</li><li>智能体根据某些准则选择动作 $a_t$</li><li>环境根据智能体选择的动作，向智能体反馈奖励 $r_{t+1}$</li></ul><p>通过合理的学习算法，智能体将在这样的问题设置下，成功学到一个在状态 $s_t$ 选择动作 $a_t$ 的策略 $\pi (s_t) = a_t$。</p><hr><p><strong>问题 1</strong>：请参照如上的定义，描述出 “机器人走迷宫这个问题” 中强化学习五个组成部分对应的实际对象：</p><ul><li><strong>环境</strong> : 某一时间点<script type="math/tex">t</script></li><li><strong>状态</strong> : 从环境中获取其所出状态<script type="math/tex">s_{t}</script></li></ul><ul><li><strong>动作</strong> : 依据某些准则选择动作<script type="math/tex">a_{t}</script></li><li><strong>奖励</strong> : 依据智能体选择的动作反馈奖励<script type="math/tex">r_{t+1}</script></li></ul><script type="math/tex; mode=display">T(s^{'}, a, s) = P(s^{'}|a,s)</script><hr><h2 id="1-2-计算-Q-值"><a href="#1-2-计算-Q-值" class="headerlink" title="1.2 计算 Q 值"></a>1.2 计算 Q 值</h2><p>在我们的项目中，我们要实现基于 Q-Learning 的强化学习算法。Q-Learning 是一个值迭代（Value Iteration）算法。与策略迭代（Policy Iteration）算法不同，值迭代算法会计算每个”状态“或是”状态-动作“的值（Value）或是效用（Utility），然后在执行动作的时候，会设法最大化这个值。因此，对每个状态值的准确估计，是我们值迭代算法的核心。通常我们会考虑<strong>最大化动作的长期奖励</strong>，即不仅考虑当前动作带来的奖励，还会考虑动作长远的奖励。</p><p>在 Q-Learning 算法中，我们把这个长期奖励记为 Q 值，我们会考虑每个 ”状态-动作“ 的 Q 值，具体而言，它的计算公式为：</p><script type="math/tex; mode=display">q(s_{t},a) = R_{t+1} + \gamma \times\max_a q(a,s_{t+1})</script><p>也就是对于当前的“状态-动作” $(s<em>{t},a)$，我们考虑执行动作 $a$ 后环境给我们的奖励 $R</em>{t+1}$，以及执行动作 $a$ 到达 $s<em>{t+1}$后，执行任意动作能够获得的最大的Q值 $\max_a q(a,s</em>{t+1})$，$\gamma$ 为折扣因子。</p><p>不过一般地，我们使用更为保守地更新 Q 表的方法，即引入松弛变量 $alpha$，按如下的公式进行更新，使得 Q 表的迭代变化更为平缓。</p><script type="math/tex; mode=display">q(s_{t},a) = (1-\alpha) \times q(s_{t},a) + \alpha \times(R_{t+1} + \gamma \times\max_a q(a,s_{t+1}))</script><hr><p><img src="/2019/02/17/robot-maze/default2.png" width="20%"></p><p><strong>问题 2</strong>：根据已知条件求 $q(s_{t},a)$，在如下模板代码中的空格填入对应的数字即可。</p><p>已知：如上图，机器人位于 $s_1$，行动为 <code>u</code>，行动获得的奖励与题目的默认设置相同。在 $s_2$ 中执行各动作的 Q 值为：<code>u</code>: -24，<code>r</code>: -13，<code>d</code>: -0.29、<code>l</code>: +40，$\gamma$ 取0.9。</p><script type="math/tex; mode=display">\begin{align}q(s_{t},a) & = R_{t+1} + \gamma \times\max_a q(a,s_{t+1}) \\ & =(-0.1) + (0.9)*(40) \\ & =(35.9)\end{align}</script><hr><h2 id="1-3-如何选择动作"><a href="#1-3-如何选择动作" class="headerlink" title="1.3 如何选择动作"></a>1.3 如何选择动作</h2><p>在强化学习中，「探索-利用」问题是非常重要的问题。具体来说，根据上面的定义，我们会尽可能地让机器人在每次选择最优的决策，来最大化长期奖励。但是这样做有如下的弊端：</p><ol><li>在初步的学习中，我们的 Q 值会不准确，如果在这个时候都按照 Q 值来选择，那么会造成错误。</li><li>学习一段时间后，机器人的路线会相对固定，则机器人无法对环境进行有效的探索。</li></ol><p>因此我们需要一种办法，来解决如上的问题，增加机器人的探索。由此我们考虑使用 epsilon-greedy 算法，即在小车选择动作的时候，以一部分的概率随机选择动作，以一部分的概率按照最优的 Q 值选择动作。同时，这个选择随机动作的概率应当随着训练的过程逐步减小。</p><hr><p><strong>问题 3</strong>：在如下的代码块中，实现 epsilon-greedy 算法的逻辑，并运行测试代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line">actions = [<span class="string">'u'</span>,<span class="string">'r'</span>,<span class="string">'d'</span>,<span class="string">'l'</span>]</span><br><span class="line">qline = &#123;<span class="string">'u'</span>:<span class="number">1.2</span>, <span class="string">'r'</span>:<span class="number">-2.1</span>, <span class="string">'d'</span>:<span class="number">-24.5</span>, <span class="string">'l'</span>:<span class="number">27</span>&#125;</span><br><span class="line">epsilon = <span class="number">0.3</span> <span class="comment"># 以0.3的概率进行随机选择</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action</span><span class="params">(epsilon)</span>:</span></span><br><span class="line">    </span><br><span class="line">    action = <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">if</span> random.uniform(<span class="number">0</span>,<span class="number">1.0</span>) &lt;= epsilon: <span class="comment"># 以某一概率</span></span><br><span class="line">        action = random.choice(actions) <span class="comment"># 实现对动作的随机选择</span></span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        action = max(qline.items(), key=operator.itemgetter(<span class="number">1</span>))[<span class="number">0</span>] <span class="comment"># 否则选择具有最大 Q 值的动作</span></span><br><span class="line">    <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">result = <span class="string">''</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">    result += choose_action(epsilon)</span><br><span class="line"></span><br><span class="line">result</span><br></pre></td></tr></table></figure><pre><code>&#39;llldllllllldlllllllllrllllldllllluldllrrlllllllllllllllrlllldllllrlrllrlulllllulllrlldlllllrlllllllrlllllrllllllllllldlllludlu...&#39;</code></pre><hr><hr><h1 id="Section-2-代码实现"><a href="#Section-2-代码实现" class="headerlink" title="Section 2 代码实现"></a>Section 2 代码实现</h1><h2 id="2-1-Maze-类理解"><a href="#2-1-Maze-类理解" class="headerlink" title="2.1. Maze 类理解"></a>2.1. <code>Maze</code> 类理解</h2><p>我们首先引入了迷宫类 <code>Maze</code>，这是一个非常强大的函数，它能够根据你的要求随机创建一个迷宫，或者根据指定的文件，读入一个迷宫地图信息。</p><ol><li>使用 <code>Maze(&quot;file_name&quot;)</code> 根据指定文件创建迷宫，或者使用 <code>Maze(maze_size=(height,width))</code> 来随机生成一个迷宫。</li><li>使用 <code>trap_number</code> 参数，在创建迷宫的时候，设定迷宫中陷阱的数量。</li><li>直接键入迷宫变量的名字按回车，展示迷宫图像（如 <code>g=Maze(&quot;xx.txt&quot;)</code>，那么直接输入 <code>g</code> 即可。</li><li>建议生成的迷宫尺寸，长在 6~12 之间，宽在 10～12 之间。</li></ol><hr><p><strong>问题 4</strong>：在如下的代码块中，创建你的迷宫并展示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> Maze <span class="keyword">import</span> Maze</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'retina'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## todo: 创建迷宫并展示</span></span><br><span class="line">g = Maze(<span class="string">'test_world/maze_01.txt'</span>)</span><br><span class="line"></span><br><span class="line">g</span><br></pre></td></tr></table></figure><p><img src="/2019/02/17/robot-maze/output_13_0.png" alt="png"></p><pre><code>Maze of size (12, 12)</code></pre><hr><p>你可能已经注意到，在迷宫中我们已经默认放置了一个机器人。实际上，我们为迷宫配置了相应的 API，来帮助机器人的移动与感知。其中你随后会使用的两个 API 为 <code>maze.sense_robot()</code> 及 <code>maze.move_robot()</code>。</p><ol><li><code>maze.sense_robot()</code> 为一个无参数的函数，输出机器人在迷宫中目前的位置。</li><li><code>maze.move_robot(direction)</code> 对输入的移动方向，移动机器人，并返回对应动作的奖励值。</li></ol><hr><p><strong>问题 5</strong>：随机移动机器人，并记录下获得的奖励，展示出机器人最后的位置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">rewards = []</span><br><span class="line"></span><br><span class="line"><span class="comment">## 循环、随机移动机器人10次，记录下奖励</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    result = g.move_robot(random.choice(actions))</span><br><span class="line">    </span><br><span class="line">    rewards.append(result)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出机器人最后的位置</span></span><br><span class="line">print(g.sense_robot)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 打印迷宫，观察机器人位置</span></span><br><span class="line">g</span><br></pre></td></tr></table></figure><p><img src="/2019/02/17/robot-maze/output_15_0.png" alt="png"></p><pre><code>&lt;bound method Maze.sense_robot of Maze of size (12, 12)&gt;</code></pre><p><img src="/2019/02/17/robot-maze/output_15_2.png" alt="png"></p><pre><code>Maze of size (12, 12)</code></pre><h2 id="2-2-Robot-类实现"><a href="#2-2-Robot-类实现" class="headerlink" title="2.2. Robot 类实现"></a>2.2. <code>Robot</code> 类实现</h2><p><code>Robot</code> 类是我们需要重点实现的部分。在这个类中，我们需要实现诸多功能，以使得我们成功实现一个强化学习智能体。总体来说，之前我们是人为地在环境中移动了机器人，但是现在通过实现 <code>Robot</code> 这个类，机器人将会自己移动。通过实现学习函数，<code>Robot</code> 类将会学习到如何选择最优的动作，并且更新强化学习中对应的参数。</p><p>首先 <code>Robot</code> 有多个输入，其中 <code>alpha=0.5, gamma=0.9, epsilon0=0.5</code> 表征强化学习相关的各个参数的默认值，这些在之前你已经了解到，<code>Maze</code> 应为机器人所在迷宫对象。</p><p>随后观察 <code>Robot.update</code> 函数，它指明了在每次执行动作时，<code>Robot</code> 需要执行的程序。按照这些程序，各个函数的功能也就明了了。</p><p>最后你需要实现 <code>Robot.py</code> 代码中的8段代码，他们都在代码中以 <code>#TODO</code> 进行标注，你能轻松地找到他们。</p><hr><p><strong>问题 6</strong>：实现 <code>Robot.py</code> 中的8段代码，并运行如下代码检查效果（记得将 <code>maze</code> 变量修改为你创建迷宫的变量名）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> Robot <span class="keyword">import</span> Robot</span><br><span class="line">robot = Robot(g) <span class="comment"># 记得将 maze 变量修改为你创建迷宫的变量名</span></span><br><span class="line">robot.set_status(learning=<span class="keyword">True</span>,testing=<span class="keyword">False</span>)</span><br><span class="line">print(robot.update())</span><br><span class="line"></span><br><span class="line">g</span><br></pre></td></tr></table></figure><pre><code>(&#39;u&#39;, -0.1)</code></pre><p><img src="/2019/02/17/robot-maze/output_19_1.png" alt="png"></p><pre><code>Maze of size (12, 12)</code></pre><hr><h2 id="2-3-用-Runner-类训练-Robot"><a href="#2-3-用-Runner-类训练-Robot" class="headerlink" title="2.3 用 Runner 类训练 Robot"></a>2.3 用 <code>Runner</code> 类训练 Robot</h2><p>在实现了上述内容之后，我们就可以开始对我们 <code>Robot</code> 进行训练并调参了。我们为你准备了又一个非常棒的类 <code>Runner</code>，来实现整个训练过程及可视化。使用如下的代码，你可以成功对机器人进行训练。并且你会在当前文件夹中生成一个名为 <code>filename</code> 的视频，记录了整个训练的过程。通过观察该视频，你能够发现训练过程中的问题，并且优化你的代码及参数。</p><hr><p><strong>问题 7</strong>：尝试利用下列代码训练机器人，并进行调参。可选的参数包括：</p><ul><li>训练参数<ul><li>训练次数 <code>epoch</code></li></ul></li><li>机器人参数：<ul><li><code>epsilon0</code> (epsilon 初值)</li><li><code>epsilon</code>衰减（可以是线性、指数衰减，可以调整衰减的速度），你需要在 Robot.py 中调整</li><li><code>alpha</code></li><li><code>gamma</code></li></ul></li><li>迷宫参数:<ul><li>迷宫大小</li><li>迷宫中陷阱的数量</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 可选的参数：</span></span><br><span class="line">epoch = <span class="number">30</span></span><br><span class="line"></span><br><span class="line">epsilon0 = <span class="number">0.6</span></span><br><span class="line">alpha = <span class="number">0.5</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line"></span><br><span class="line">maze_size = (<span class="number">6</span>,<span class="number">8</span>)</span><br><span class="line">trap_number = <span class="number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> Runner <span class="keyword">import</span> Runner</span><br><span class="line"></span><br><span class="line">g = Maze(maze_size=maze_size,trap_number=trap_number)</span><br><span class="line">r = Robot(g,alpha=alpha, epsilon0=epsilon0, gamma=gamma)</span><br><span class="line">r.set_status(learning=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">runner = Runner(r, g)</span><br><span class="line">runner.run_training(epoch, display_direction=<span class="keyword">True</span>)</span><br><span class="line">runner.generate_movie(filename = <span class="string">"final1.mp4"</span>) <span class="comment"># 你可以注释该行代码，加快运行速度，不过你就无法观察到视频了。</span></span><br></pre></td></tr></table></figure><pre><code>Generate Movies: 100%|██████████| 892/892 [00:10&lt;00:00, 87.02it/s]</code></pre><hr><p>使用 <code>runner.plot_results()</code> 函数，能够打印机器人在训练过程中的一些参数信息。</p><ul><li>Success Times 代表机器人在训练过程中成功的累计次数，这应当是一个累积递增的图像。</li><li>Accumulated Rewards 代表机器人在每次训练 epoch 中，获得的累积奖励的值，这应当是一个逐步递增的图像。</li><li>Running Times per Epoch 代表在每次训练 epoch 中，小车训练的次数（到达终点就会停止该 epoch 转入下次训练），这应当是一个逐步递减的图像。</li></ul><hr><p><strong>问题 8</strong>：使用 <code>runner.plot_results()</code> 输出训练结果，根据该结果对你的机器人进行分析。</p><ul><li>指出你选用的参数如何，选用参数的原因。</li><li>建议你比较不同参数下机器人的训练的情况。</li><li>训练的结果是否满意，有何改进的计划。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">runner.plot_results()</span><br></pre></td></tr></table></figure><p><img src="/2019/02/17/robot-maze/output_25_0.png" alt="png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Section-0-问题描述与完成项目流程&quot;&gt;&lt;a href=&quot;#Section-0-问题描述与完成项目流程&quot; class=&quot;headerlink&quot; title=&quot;Section 0 问题描述与完成项目流程&quot;&gt;&lt;/a&gt;Section 0 问题描述与完成项目流程&lt;/
      
    
    </summary>
    
      <category term="强化学习" scheme="http://yoursite.com/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Reinforcement learning" scheme="http://yoursite.com/tags/Reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Explore Movie Dataset</title>
    <link href="http://yoursite.com/2019/02/16/Explore-Movie-Dataset/"/>
    <id>http://yoursite.com/2019/02/16/Explore-Movie-Dataset/</id>
    <published>2019-02-16T15:51:36.000Z</published>
    <updated>2019-02-16T15:52:11.776Z</updated>
    
    <content type="html"><![CDATA[<h2 id="探索电影数据集"><a href="#探索电影数据集" class="headerlink" title="探索电影数据集"></a>探索电影数据集</h2><p>在这个项目中，你将尝试使用所学的知识，使用 <code>NumPy</code>、<code>Pandas</code>、<code>matplotlib</code>、<code>seaborn</code> 库中的函数，来对电影数据集进行探索。</p><p>下载数据集：<br><a href="https://s3.cn-north-1.amazonaws.com.cn/static-documents/nd101/explore+dataset/tmdb-movies.csv" target="_blank" rel="noopener">TMDb电影数据</a></p><p>数据集各列名称的含义：</p><table><thead><tr><th>列名称</th><th>id</th><th>imdb_id</th><th>popularity</th><th>budget</th><th>revenue</th><th>original_title</th><th>cast</th><th>homepage</th><th>director</th><th>tagline</th><th>keywords</th><th>overview</th><th>runtime</th><th>genres</th><th>production_companies</th><th>release_date</th><th>vote_count</th><th>vote_average</th><th>release_year</th><th>budget_adj</th><th>revenue_adj</th></tr></thead><tbody> <tr><td>含义</td><td>编号</td><td>IMDB 编号</td><td>知名度</td><td>预算</td><td>票房</td><td>名称</td><td>主演</td><td>网站</td><td>导演</td><td>宣传词</td><td>关键词</td><td>简介</td><td>时常</td><td>类别</td><td>发行公司</td><td>发行日期</td><td>投票总数</td><td>投票均值</td><td>发行年份</td><td>预算（调整后）</td><td>票房（调整后）</td></tr></tbody></table><p><strong>请注意，你需要提交该报告导出的 <code>.html</code>、<code>.ipynb</code> 以及 <code>.py</code> 文件。</strong></p><hr><hr><h2 id="第一节-数据的导入与处理"><a href="#第一节-数据的导入与处理" class="headerlink" title="第一节 数据的导入与处理"></a>第一节 数据的导入与处理</h2><p>在这一部分，你需要编写代码，使用 Pandas 读取数据，并进行预处理。</p><p><strong>任务1.1：</strong> 导入库以及数据</p><ol><li>载入需要的库 <code>NumPy</code>、<code>Pandas</code>、<code>matplotlib</code>、<code>seaborn</code>。</li><li>利用 <code>Pandas</code> 库，读取 <code>tmdb-movies.csv</code> 中的数据，保存为 <code>movie_data</code>。</li></ol><p>提示：记得使用 notebook 中的魔法指令 <code>%matplotlib inline</code>，否则会导致你接下来无法打印出图像。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line">movie_data = pd.read_csv(<span class="string">'/Users/jindongwang/MachingLearning/AIPND-cn/tmdb-movies.csv'</span>)</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><hr><p><strong>任务1.2: </strong> 了解数据</p><p>你会接触到各种各样的数据表，因此在读取之后，我们有必要通过一些简单的方法，来了解我们数据表是什么样子的。</p><ol><li>获取数据表的行列，并打印。</li><li>使用 <code>.head()</code>、<code>.tail()</code>、<code>.sample()</code> 方法，观察、了解数据表的情况。</li><li>使用 <code>.dtypes</code> 属性，来查看各列数据的数据类型。</li><li>使用 <code>isnull()</code> 配合 <code>.any()</code> 等方法，来查看各列是否存在空值。</li><li>使用 <code>.describe()</code> 方法，看看数据表中数值型的数据是怎么分布的。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">movie_data.head()</span><br><span class="line">movie_data.tail()</span><br><span class="line">movie_data.sample()</span><br><span class="line"></span><br><span class="line">movie_data.dtypes</span><br><span class="line"></span><br><span class="line">movie_data.isnull().any()</span><br><span class="line">movie_data.describe()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>popularity</th>      <th>budget</th>      <th>revenue</th>      <th>runtime</th>      <th>vote_count</th>      <th>vote_average</th>      <th>release_year</th>      <th>budget_adj</th>      <th>revenue_adj</th>      <th>profit</th>    </tr>  </thead>  <tbody>    <tr>      <th>count</th>      <td>10820.000000</td>      <td>10820.000000</td>      <td>1.082000e+04</td>      <td>1.082000e+04</td>      <td>10820.000000</td>      <td>10820.000000</td>      <td>10820.000000</td>      <td>10820.000000</td>      <td>1.082000e+04</td>      <td>1.082000e+04</td>      <td>1.082000e+04</td>    </tr>    <tr>      <th>mean</th>      <td>66274.834381</td>      <td>0.647896</td>      <td>1.468256e+07</td>      <td>3.998479e+07</td>      <td>102.050370</td>      <td>218.179020</td>      <td>5.974270</td>      <td>2001.472828</td>      <td>1.758992e+07</td>      <td>5.153012e+07</td>      <td>2.530223e+07</td>    </tr>    <tr>      <th>std</th>      <td>92265.282197</td>      <td>1.001940</td>      <td>3.096573e+07</td>      <td>1.172250e+08</td>      <td>31.377214</td>      <td>576.706535</td>      <td>0.935026</td>      <td>12.630994</td>      <td>3.434136e+07</td>      <td>1.448923e+08</td>      <td>9.677916e+07</td>    </tr>    <tr>      <th>min</th>      <td>5.000000</td>      <td>0.000065</td>      <td>0.000000e+00</td>      <td>0.000000e+00</td>      <td>0.000000</td>      <td>10.000000</td>      <td>1.500000</td>      <td>1960.000000</td>      <td>0.000000e+00</td>      <td>0.000000e+00</td>      <td>-4.139124e+08</td>    </tr>    <tr>      <th>25%</th>      <td>10605.750000</td>      <td>0.207988</td>      <td>0.000000e+00</td>      <td>0.000000e+00</td>      <td>90.000000</td>      <td>17.000000</td>      <td>5.400000</td>      <td>1995.000000</td>      <td>0.000000e+00</td>      <td>0.000000e+00</td>      <td>0.000000e+00</td>    </tr>    <tr>      <th>50%</th>      <td>20695.500000</td>      <td>0.384556</td>      <td>0.000000e+00</td>      <td>0.000000e+00</td>      <td>99.000000</td>      <td>38.000000</td>      <td>6.000000</td>      <td>2006.000000</td>      <td>0.000000e+00</td>      <td>0.000000e+00</td>      <td>0.000000e+00</td>    </tr>    <tr>      <th>75%</th>      <td>75789.250000</td>      <td>0.715897</td>      <td>1.510000e+07</td>      <td>2.427926e+07</td>      <td>111.000000</td>      <td>146.000000</td>      <td>6.600000</td>      <td>2011.000000</td>      <td>2.093530e+07</td>      <td>3.388119e+07</td>      <td>9.333081e+06</td>    </tr>    <tr>      <th>max</th>      <td>417859.000000</td>      <td>32.985763</td>      <td>4.250000e+08</td>      <td>2.781506e+09</td>      <td>900.000000</td>      <td>9767.000000</td>      <td>9.200000</td>      <td>2015.000000</td>      <td>4.250000e+08</td>      <td>2.827124e+09</td>      <td>2.544506e+09</td>    </tr>  </tbody></table></div><hr><p><strong>任务1.3: </strong> 清理数据</p><p>在真实的工作场景中，数据处理往往是最为费时费力的环节。但是幸运的是，我们提供给大家的 tmdb 数据集非常的「干净」，不需要大家做特别多的数据清洗以及处理工作。在这一步中，你的核心的工作主要是对数据表中的空值进行处理。你可以使用 <code>.fillna()</code> 来填补空值，当然也可以使用 <code>.dropna()</code> 来丢弃数据表中包含空值的某些行或者列。</p><p>任务：使用适当的方法来清理空值，并将得到的数据保存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># movie_data.fillna(value=movie_data.mean(), inplace=True)</span></span><br><span class="line">movie_data.fillna(method=<span class="string">'bfill'</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">movie_data.dropna(axis=<span class="number">0</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">movie_data.info()</span><br></pre></td></tr></table></figure><pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;Int64Index: 10820 entries, 0 to 6181Data columns (total 22 columns):id                      10820 non-null int64imdb_id                 10820 non-null objectpopularity              10820 non-null float64budget                  10820 non-null int64revenue                 10820 non-null int64original_title          10820 non-null objectcast                    10820 non-null objecthomepage                10820 non-null objectdirector                10820 non-null objecttagline                 10820 non-null objectkeywords                10820 non-null objectoverview                10820 non-null objectruntime                 10820 non-null int64genres                  10820 non-null objectproduction_companies    10820 non-null objectrelease_date            10820 non-null objectvote_count              10820 non-null int64vote_average            10820 non-null float64release_year            10820 non-null int64budget_adj              10820 non-null float64revenue_adj             10820 non-null float64profit                  10820 non-null int64dtypes: float64(4), int64(7), object(11)memory usage: 1.9+ MB</code></pre><hr><hr><h2 id="第二节-根据指定要求读取数据"><a href="#第二节-根据指定要求读取数据" class="headerlink" title="第二节 根据指定要求读取数据"></a>第二节 根据指定要求读取数据</h2><p>相比 Excel 等数据分析软件，Pandas 的一大特长在于，能够轻松地基于复杂的逻辑选择合适的数据。因此，如何根据指定的要求，从数据表当获取适当的数据，是使用 Pandas 中非常重要的技能，也是本节重点考察大家的内容。</p><hr><p><strong>任务2.1: </strong> 简单读取</p><ol><li>读取数据表中名为 <code>id</code>、<code>popularity</code>、<code>budget</code>、<code>runtime</code>、<code>vote_average</code> 列的数据。</li><li>读取数据表中前1～20行以及48、49行的数据。</li><li>读取数据表中第50～60行的 <code>popularity</code> 那一列的数据。</li></ol><p>要求：每一个语句只能用一行代码实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">title_list = [<span class="string">'id'</span>, <span class="string">'popularity'</span>, <span class="string">'budget'</span>, <span class="string">'runtime'</span>, <span class="string">'vote_average'</span>]</span><br><span class="line">movie_data[title_list]</span><br><span class="line"></span><br><span class="line">movie_data.iloc[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">16</span>,<span class="number">17</span>,<span class="number">18</span>,<span class="number">19</span>,<span class="number">47</span>,<span class="number">48</span>]]</span><br><span class="line"><span class="comment">#单个我会 这组合起来就 。。</span></span><br><span class="line"></span><br><span class="line">movie_data[<span class="number">50</span>:<span class="number">61</span>][<span class="string">'popularity'</span>]</span><br></pre></td></tr></table></figure><pre><code>1387    6.0980276081    6.095293643     6.0524793912    6.01258413      5.984995644     5.94713614      5.9449274364    5.9445186190    5.9399273373    5.90335315      5.898400Name: popularity, dtype: float64</code></pre><hr><p><strong>任务2.2: </strong>逻辑读取（Logical Indexing）</p><ol><li>读取数据表中 <strong><code>popularity</code> 大于5</strong> 的所有数据。</li><li>读取数据表中 <strong><code>popularity</code> 大于5</strong> 的所有数据且<strong>发行年份在1996年之后</strong>的所有数据。</li></ol><p>提示：Pandas 中的逻辑运算符如 <code>&amp;</code>、<code>|</code>，分别代表<code>且</code>以及<code>或</code>。</p><p>要求：请使用 Logical Indexing实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">movie_data[movie_data[<span class="string">'popularity'</span>]&gt;<span class="number">5</span>]</span><br><span class="line">movie_data[(movie_data[<span class="string">'popularity'</span>]&gt;<span class="number">5</span>) &amp; (movie_data[<span class="string">'release_year'</span>]&gt;<span class="number">1996</span>)]</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>imdb_id</th>      <th>popularity</th>      <th>budget</th>      <th>revenue</th>      <th>original_title</th>      <th>cast</th>      <th>homepage</th>      <th>director</th>      <th>tagline</th>      <th>...</th>      <th>runtime</th>      <th>genres</th>      <th>production_companies</th>      <th>release_date</th>      <th>vote_count</th>      <th>vote_average</th>      <th>release_year</th>      <th>budget_adj</th>      <th>revenue_adj</th>      <th>profit</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>135397</td>      <td>tt0369610</td>      <td>32.985763</td>      <td>150000000</td>      <td>1513528810</td>      <td>Jurassic World</td>      <td>Chris Pratt|Bryce Dallas Howard|Irrfan Khan|Vi...</td>      <td>http://www.jurassicworld.com/</td>      <td>Colin Trevorrow</td>      <td>The park is open.</td>      <td>...</td>      <td>124</td>      <td>Action|Adventure|Science Fiction|Thriller</td>      <td>Universal Studios|Amblin Entertainment|Legenda...</td>      <td>6/9/15</td>      <td>5562</td>      <td>6.5</td>      <td>2015</td>      <td>1.379999e+08</td>      <td>1.392446e+09</td>      <td>1363528810</td>    </tr>    <tr>      <th>1</th>      <td>76341</td>      <td>tt1392190</td>      <td>28.419936</td>      <td>150000000</td>      <td>378436354</td>      <td>Mad Max: Fury Road</td>      <td>Tom Hardy|Charlize Theron|Hugh Keays-Byrne|Nic...</td>      <td>http://www.madmaxmovie.com/</td>      <td>George Miller</td>      <td>What a Lovely Day.</td>      <td>...</td>      <td>120</td>      <td>Action|Adventure|Science Fiction|Thriller</td>      <td>Village Roadshow Pictures|Kennedy Miller Produ...</td>      <td>5/13/15</td>      <td>6185</td>      <td>7.1</td>      <td>2015</td>      <td>1.379999e+08</td>      <td>3.481613e+08</td>      <td>228436354</td>    </tr>    <tr>      <th>629</th>      <td>157336</td>      <td>tt0816692</td>      <td>24.949134</td>      <td>165000000</td>      <td>621752480</td>      <td>Interstellar</td>      <td>Matthew McConaughey|Jessica Chastain|Anne Hath...</td>      <td>http://www.interstellarmovie.net/</td>      <td>Christopher Nolan</td>      <td>Mankind was born on Earth. It was never meant ...</td>      <td>...</td>      <td>169</td>      <td>Adventure|Drama|Science Fiction</td>      <td>Paramount Pictures|Legendary Pictures|Warner B...</td>      <td>11/5/14</td>      <td>6498</td>      <td>8.0</td>      <td>2014</td>      <td>1.519800e+08</td>      <td>5.726906e+08</td>      <td>456752480</td>    </tr>    <tr>      <th>630</th>      <td>118340</td>      <td>tt2015381</td>      <td>14.311205</td>      <td>170000000</td>      <td>773312399</td>      <td>Guardians of the Galaxy</td>      <td>Chris Pratt|Zoe Saldana|Dave Bautista|Vin Dies...</td>      <td>http://marvel.com/guardians</td>      <td>James Gunn</td>      <td>All heroes start somewhere.</td>      <td>...</td>      <td>121</td>      <td>Action|Science Fiction|Adventure</td>      <td>Marvel Studios|Moving Picture Company (MPC)|Bu...</td>      <td>7/30/14</td>      <td>5612</td>      <td>7.9</td>      <td>2014</td>      <td>1.565855e+08</td>      <td>7.122911e+08</td>      <td>603312399</td>    </tr>    <tr>      <th>2</th>      <td>262500</td>      <td>tt2908446</td>      <td>13.112507</td>      <td>110000000</td>      <td>295238201</td>      <td>Insurgent</td>      <td>Shailene Woodley|Theo James|Kate Winslet|Ansel...</td>      <td>http://www.thedivergentseries.movie/#insurgent</td>      <td>Robert Schwentke</td>      <td>One Choice Can Destroy You</td>      <td>...</td>      <td>119</td>      <td>Adventure|Science Fiction|Thriller</td>      <td>Summit Entertainment|Mandeville Films|Red Wago...</td>      <td>3/18/15</td>      <td>2480</td>      <td>6.3</td>      <td>2015</td>      <td>1.012000e+08</td>      <td>2.716190e+08</td>      <td>185238201</td>    </tr>    <tr>      <th>631</th>      <td>100402</td>      <td>tt1843866</td>      <td>12.971027</td>      <td>170000000</td>      <td>714766572</td>      <td>Captain America: The Winter Soldier</td>      <td>Chris Evans|Scarlett Johansson|Sebastian Stan|...</td>      <td>http://www.captainamericathewintersoldiermovie...</td>      <td>Joe Russo|Anthony Russo</td>      <td>In heroes we trust.</td>      <td>...</td>      <td>136</td>      <td>Action|Adventure|Science Fiction</td>      <td>Marvel Studios</td>      <td>3/20/14</td>      <td>3848</td>      <td>7.6</td>      <td>2014</td>      <td>1.565855e+08</td>      <td>6.583651e+08</td>      <td>544766572</td>    </tr>    <tr>      <th>632</th>      <td>245891</td>      <td>tt2911666</td>      <td>11.422751</td>      <td>20000000</td>      <td>78739897</td>      <td>John Wick</td>      <td>Keanu Reeves|Michael Nyqvist|Alfie Allen|Wille...</td>      <td>http://www.johnwickthemovie.com/</td>      <td>Chad Stahelski|David Leitch</td>      <td>Don't set him off.</td>      <td>...</td>      <td>101</td>      <td>Action|Thriller</td>      <td>Thunder Road Pictures|Warner Bros.|87Eleven|De...</td>      <td>10/22/14</td>      <td>2712</td>      <td>7.0</td>      <td>2014</td>      <td>1.842182e+07</td>      <td>7.252661e+07</td>      <td>58739897</td>    </tr>    <tr>      <th>3</th>      <td>140607</td>      <td>tt2488496</td>      <td>11.173104</td>      <td>200000000</td>      <td>2068178225</td>      <td>Star Wars: The Force Awakens</td>      <td>Harrison Ford|Mark Hamill|Carrie Fisher|Adam D...</td>      <td>http://www.starwars.com/films/star-wars-episod...</td>      <td>J.J. Abrams</td>      <td>Every generation has a story.</td>      <td>...</td>      <td>136</td>      <td>Action|Adventure|Science Fiction|Fantasy</td>      <td>Lucasfilm|Truenorth Productions|Bad Robot</td>      <td>12/15/15</td>      <td>5292</td>      <td>7.5</td>      <td>2015</td>      <td>1.839999e+08</td>      <td>1.902723e+09</td>      <td>1868178225</td>    </tr>    <tr>      <th>633</th>      <td>131631</td>      <td>tt1951265</td>      <td>10.739009</td>      <td>125000000</td>      <td>752100229</td>      <td>The Hunger Games: Mockingjay - Part 1</td>      <td>Jennifer Lawrence|Josh Hutcherson|Liam Hemswor...</td>      <td>http://www.thehungergames.movie/</td>      <td>Francis Lawrence</td>      <td>Fire burns brighter in the darkness</td>      <td>...</td>      <td>123</td>      <td>Science Fiction|Adventure|Thriller</td>      <td>Lionsgate|Color Force</td>      <td>11/18/14</td>      <td>3590</td>      <td>6.6</td>      <td>2014</td>      <td>1.151364e+08</td>      <td>6.927528e+08</td>      <td>627100229</td>    </tr>    <tr>      <th>634</th>      <td>122917</td>      <td>tt2310332</td>      <td>10.174599</td>      <td>250000000</td>      <td>955119788</td>      <td>The Hobbit: The Battle of the Five Armies</td>      <td>Martin Freeman|Ian McKellen|Richard Armitage|K...</td>      <td>http://www.thehobbit.com/</td>      <td>Peter Jackson</td>      <td>Witness the defining chapter of the Middle-Ear...</td>      <td>...</td>      <td>144</td>      <td>Adventure|Fantasy</td>      <td>WingNut Films|New Line Cinema|3Foot7|Metro-Gol...</td>      <td>12/10/14</td>      <td>3110</td>      <td>7.1</td>      <td>2014</td>      <td>2.302728e+08</td>      <td>8.797523e+08</td>      <td>705119788</td>    </tr>    <tr>      <th>1386</th>      <td>19995</td>      <td>tt0499549</td>      <td>9.432768</td>      <td>237000000</td>      <td>2781505847</td>      <td>Avatar</td>      <td>Sam Worthington|Zoe Saldana|Sigourney Weaver|S...</td>      <td>http://www.avatarmovie.com/</td>      <td>James Cameron</td>      <td>Enter the World of Pandora.</td>      <td>...</td>      <td>162</td>      <td>Action|Adventure|Fantasy|Science Fiction</td>      <td>Ingenious Film Partners|Twentieth Century Fox ...</td>      <td>12/10/09</td>      <td>8458</td>      <td>7.1</td>      <td>2009</td>      <td>2.408869e+08</td>      <td>2.827124e+09</td>      <td>2544505847</td>    </tr>    <tr>      <th>1919</th>      <td>27205</td>      <td>tt1375666</td>      <td>9.363643</td>      <td>160000000</td>      <td>825500000</td>      <td>Inception</td>      <td>Leonardo DiCaprio|Joseph Gordon-Levitt|Ellen P...</td>      <td>http://inceptionmovie.warnerbros.com/</td>      <td>Christopher Nolan</td>      <td>Your mind is the scene of the crime.</td>      <td>...</td>      <td>148</td>      <td>Action|Thriller|Science Fiction|Mystery|Adventure</td>      <td>Legendary Pictures|Warner Bros.|Syncopy</td>      <td>7/14/10</td>      <td>9767</td>      <td>7.9</td>      <td>2010</td>      <td>1.600000e+08</td>      <td>8.255000e+08</td>      <td>665500000</td>    </tr>    <tr>      <th>4</th>      <td>168259</td>      <td>tt2820852</td>      <td>9.335014</td>      <td>190000000</td>      <td>1506249360</td>      <td>Furious 7</td>      <td>Vin Diesel|Paul Walker|Jason Statham|Michelle ...</td>      <td>http://www.furious7.com/</td>      <td>James Wan</td>      <td>Vengeance Hits Home</td>      <td>...</td>      <td>137</td>      <td>Action|Crime|Thriller</td>      <td>Universal Pictures|Original Film|Media Rights ...</td>      <td>4/1/15</td>      <td>2947</td>      <td>7.3</td>      <td>2015</td>      <td>1.747999e+08</td>      <td>1.385749e+09</td>      <td>1316249360</td>    </tr>    <tr>      <th>5</th>      <td>281957</td>      <td>tt1663202</td>      <td>9.110700</td>      <td>135000000</td>      <td>532950503</td>      <td>The Revenant</td>      <td>Leonardo DiCaprio|Tom Hardy|Will Poulter|Domhn...</td>      <td>http://www.foxmovies.com/movies/the-revenant</td>      <td>Alejandro GonzÃ¡lez IÃ±Ã¡rritu</td>      <td>(n. One who has returned, as if from the dead.)</td>      <td>...</td>      <td>156</td>      <td>Western|Drama|Adventure|Thriller</td>      <td>Regency Enterprises|Appian Way|CatchPlay|Anony...</td>      <td>12/25/15</td>      <td>3929</td>      <td>7.2</td>      <td>2015</td>      <td>1.241999e+08</td>      <td>4.903142e+08</td>      <td>397950503</td>    </tr>    <tr>      <th>2409</th>      <td>550</td>      <td>tt0137523</td>      <td>8.947905</td>      <td>63000000</td>      <td>100853753</td>      <td>Fight Club</td>      <td>Edward Norton|Brad Pitt|Meat Loaf|Jared Leto|H...</td>      <td>http://www.foxmovies.com/movies/fight-club</td>      <td>David Fincher</td>      <td>How much can you know about yourself if you've...</td>      <td>...</td>      <td>139</td>      <td>Drama</td>      <td>Regency Enterprises|Fox 2000 Pictures|Taurus F...</td>      <td>10/14/99</td>      <td>5923</td>      <td>8.1</td>      <td>1999</td>      <td>8.247033e+07</td>      <td>1.320229e+08</td>      <td>37853753</td>    </tr>    <tr>      <th>635</th>      <td>177572</td>      <td>tt2245084</td>      <td>8.691294</td>      <td>165000000</td>      <td>652105443</td>      <td>Big Hero 6</td>      <td>Scott Adsit|Ryan Potter|Daniel Henney|T.J. Mil...</td>      <td>http://movies.disney.com/big-hero-6</td>      <td>Don Hall|Chris Williams</td>      <td>From the creators of Wreck-it Ralph and Frozen</td>      <td>...</td>      <td>102</td>      <td>Adventure|Family|Animation|Action|Comedy</td>      <td>Walt Disney Pictures|Walt Disney Animation Stu...</td>      <td>10/24/14</td>      <td>4185</td>      <td>7.8</td>      <td>2014</td>      <td>1.519800e+08</td>      <td>6.006485e+08</td>      <td>487105443</td>    </tr>    <tr>      <th>6</th>      <td>87101</td>      <td>tt1340138</td>      <td>8.654359</td>      <td>155000000</td>      <td>440603537</td>      <td>Terminator Genisys</td>      <td>Arnold Schwarzenegger|Jason Clarke|Emilia Clar...</td>      <td>http://www.terminatormovie.com/</td>      <td>Alan Taylor</td>      <td>Reset the future</td>      <td>...</td>      <td>125</td>      <td>Science Fiction|Action|Thriller|Adventure</td>      <td>Paramount Pictures|Skydance Productions</td>      <td>6/23/15</td>      <td>2598</td>      <td>5.8</td>      <td>2015</td>      <td>1.425999e+08</td>      <td>4.053551e+08</td>      <td>285603537</td>    </tr>    <tr>      <th>2633</th>      <td>120</td>      <td>tt0120737</td>      <td>8.575419</td>      <td>93000000</td>      <td>871368364</td>      <td>The Lord of the Rings: The Fellowship of the Ring</td>      <td>Elijah Wood|Ian McKellen|Viggo Mortensen|Liv T...</td>      <td>http://www.lordoftherings.net/</td>      <td>Peter Jackson</td>      <td>One ring to rule them all</td>      <td>...</td>      <td>178</td>      <td>Adventure|Fantasy|Action</td>      <td>WingNut Films|New Line Cinema|The Saul Zaentz ...</td>      <td>12/18/01</td>      <td>6079</td>      <td>7.8</td>      <td>2001</td>      <td>1.145284e+08</td>      <td>1.073080e+09</td>      <td>778368364</td>    </tr>    <tr>      <th>2875</th>      <td>155</td>      <td>tt0468569</td>      <td>8.466668</td>      <td>185000000</td>      <td>1001921825</td>      <td>The Dark Knight</td>      <td>Christian Bale|Michael Caine|Heath Ledger|Aaro...</td>      <td>http://thedarkknight.warnerbros.com/dvdsite/</td>      <td>Christopher Nolan</td>      <td>Why So Serious?</td>      <td>...</td>      <td>152</td>      <td>Drama|Action|Crime|Thriller</td>      <td>DC Comics|Legendary Pictures|Warner Bros.|Syncopy</td>      <td>7/16/08</td>      <td>8432</td>      <td>8.1</td>      <td>2008</td>      <td>1.873655e+08</td>      <td>1.014733e+09</td>      <td>816921825</td>    </tr>    <tr>      <th>3371</th>      <td>161337</td>      <td>tt2381375</td>      <td>8.411577</td>      <td>0</td>      <td>0</td>      <td>Underworld: Endless War</td>      <td>Trevor Devall|Brian Dobson|Paul Dobson|Laura H...</td>      <td>http://captainamerica.marvel.com/</td>      <td>Juno John Lee</td>      <td>When patriots become heroes</td>      <td>...</td>      <td>18</td>      <td>Action|Animation|Horror</td>      <td>Marvel Studios</td>      <td>10/19/11</td>      <td>21</td>      <td>5.9</td>      <td>2011</td>      <td>0.000000e+00</td>      <td>0.000000e+00</td>      <td>0</td>    </tr>    <tr>      <th>636</th>      <td>205596</td>      <td>tt2084970</td>      <td>8.110711</td>      <td>14000000</td>      <td>233555708</td>      <td>The Imitation Game</td>      <td>Benedict Cumberbatch|Keira Knightley|Matthew G...</td>      <td>http://theimitationgamemovie.com/</td>      <td>Morten Tyldum</td>      <td>The true enigma was the man who cracked the code.</td>      <td>...</td>      <td>113</td>      <td>History|Drama|Thriller|War</td>      <td>Black Bear Pictures|Bristol Automotive</td>      <td>11/14/14</td>      <td>3478</td>      <td>8.0</td>      <td>2014</td>      <td>1.289527e+07</td>      <td>2.151261e+08</td>      <td>219555708</td>    </tr>    <tr>      <th>3911</th>      <td>121</td>      <td>tt0167261</td>      <td>8.095275</td>      <td>79000000</td>      <td>926287400</td>      <td>The Lord of the Rings: The Two Towers</td>      <td>Elijah Wood|Ian McKellen|Viggo Mortensen|Liv T...</td>      <td>http://www.lordoftherings.net/</td>      <td>Peter Jackson</td>      <td>A New Power Is Rising.</td>      <td>...</td>      <td>179</td>      <td>Adventure|Fantasy|Action</td>      <td>WingNut Films|New Line Cinema|The Saul Zaentz ...</td>      <td>12/18/02</td>      <td>5114</td>      <td>7.8</td>      <td>2002</td>      <td>9.576865e+07</td>      <td>1.122902e+09</td>      <td>847287400</td>    </tr>    <tr>      <th>2634</th>      <td>671</td>      <td>tt0241527</td>      <td>8.021423</td>      <td>125000000</td>      <td>976475550</td>      <td>Harry Potter and the Philosopher's Stone</td>      <td>Daniel Radcliffe|Rupert Grint|Emma Watson|John...</td>      <td>http://harrypotter.warnerbros.com/harrypottera...</td>      <td>Chris Columbus</td>      <td>Let the Magic Begin.</td>      <td>...</td>      <td>152</td>      <td>Adventure|Fantasy|Family</td>      <td>1492 Pictures|Warner Bros.|Heyday Films</td>      <td>11/16/01</td>      <td>4265</td>      <td>7.2</td>      <td>2001</td>      <td>1.539360e+08</td>      <td>1.202518e+09</td>      <td>851475550</td>    </tr>    <tr>      <th>3372</th>      <td>1771</td>      <td>tt0458339</td>      <td>7.959228</td>      <td>140000000</td>      <td>370569774</td>      <td>Captain America: The First Avenger</td>      <td>Chris Evans|Hugo Weaving|Tommy Lee Jones|Hayle...</td>      <td>http://captainamerica.marvel.com/</td>      <td>Joe Johnston</td>      <td>When patriots become heroes</td>      <td>...</td>      <td>124</td>      <td>Action|Adventure|Science Fiction</td>      <td>Marvel Studios</td>      <td>7/22/11</td>      <td>5025</td>      <td>6.5</td>      <td>2011</td>      <td>1.357157e+08</td>      <td>3.592296e+08</td>      <td>230569774</td>    </tr>    <tr>      <th>2410</th>      <td>603</td>      <td>tt0133093</td>      <td>7.753899</td>      <td>63000000</td>      <td>463517383</td>      <td>The Matrix</td>      <td>Keanu Reeves|Laurence Fishburne|Carrie-Anne Mo...</td>      <td>http://www.warnerbros.com/matrix</td>      <td>Lilly Wachowski|Lana Wachowski</td>      <td>Welcome to the Real World.</td>      <td>...</td>      <td>136</td>      <td>Action|Science Fiction</td>      <td>Village Roadshow Pictures|Groucho II Film Part...</td>      <td>3/30/99</td>      <td>6351</td>      <td>7.8</td>      <td>1999</td>      <td>8.247033e+07</td>      <td>6.067687e+08</td>      <td>400517383</td>    </tr>    <tr>      <th>7</th>      <td>286217</td>      <td>tt3659388</td>      <td>7.667400</td>      <td>108000000</td>      <td>595380321</td>      <td>The Martian</td>      <td>Matt Damon|Jessica Chastain|Kristen Wiig|Jeff ...</td>      <td>http://www.foxmovies.com/movies/the-martian</td>      <td>Ridley Scott</td>      <td>Bring Him Home</td>      <td>...</td>      <td>141</td>      <td>Drama|Adventure|Science Fiction</td>      <td>Twentieth Century Fox Film Corporation|Scott F...</td>      <td>9/30/15</td>      <td>4572</td>      <td>7.6</td>      <td>2015</td>      <td>9.935996e+07</td>      <td>5.477497e+08</td>      <td>487380321</td>    </tr>    <tr>      <th>4361</th>      <td>24428</td>      <td>tt0848228</td>      <td>7.637767</td>      <td>220000000</td>      <td>1519557910</td>      <td>The Avengers</td>      <td>Robert Downey Jr.|Chris Evans|Mark Ruffalo|Chr...</td>      <td>http://marvel.com/avengers_movie/</td>      <td>Joss Whedon</td>      <td>Some assembly required.</td>      <td>...</td>      <td>143</td>      <td>Science Fiction|Action|Adventure</td>      <td>Marvel Studios</td>      <td>4/25/12</td>      <td>8903</td>      <td>7.3</td>      <td>2012</td>      <td>2.089437e+08</td>      <td>1.443191e+09</td>      <td>1299557910</td>    </tr>    <tr>      <th>8</th>      <td>211672</td>      <td>tt2293640</td>      <td>7.404165</td>      <td>74000000</td>      <td>1156730962</td>      <td>Minions</td>      <td>Sandra Bullock|Jon Hamm|Michael Keaton|Allison...</td>      <td>http://www.minionsmovie.com/</td>      <td>Kyle Balda|Pierre Coffin</td>      <td>Before Gru, they had a history of bad bosses</td>      <td>...</td>      <td>91</td>      <td>Family|Animation|Adventure|Comedy</td>      <td>Universal Pictures|Illumination Entertainment</td>      <td>6/17/15</td>      <td>2893</td>      <td>6.5</td>      <td>2015</td>      <td>6.807997e+07</td>      <td>1.064192e+09</td>      <td>1082730962</td>    </tr>    <tr>      <th>637</th>      <td>198663</td>      <td>tt1790864</td>      <td>7.137273</td>      <td>34000000</td>      <td>348319861</td>      <td>The Maze Runner</td>      <td>Dylan O'Brien|Ki Hong Lee|Kaya Scodelario|Aml ...</td>      <td>http://themazerunnermovie.com/</td>      <td>Wes Ball</td>      <td>Run - Remember - Survive</td>      <td>...</td>      <td>113</td>      <td>Action|Mystery|Science Fiction|Thriller</td>      <td>Ingenious Media|Twentieth Century Fox Film Cor...</td>      <td>9/10/14</td>      <td>3425</td>      <td>7.0</td>      <td>2014</td>      <td>3.131710e+07</td>      <td>3.208343e+08</td>      <td>314319861</td>    </tr>    <tr>      <th>4949</th>      <td>122</td>      <td>tt0167260</td>      <td>7.122455</td>      <td>94000000</td>      <td>1118888979</td>      <td>The Lord of the Rings: The Return of the King</td>      <td>Elijah Wood|Ian McKellen|Viggo Mortensen|Liv T...</td>      <td>http://www.lordoftherings.net</td>      <td>Peter Jackson</td>      <td>The eye of the enemy is moving.</td>      <td>...</td>      <td>201</td>      <td>Adventure|Fantasy|Action</td>      <td>WingNut Films|New Line Cinema</td>      <td>12/1/03</td>      <td>5636</td>      <td>7.9</td>      <td>2003</td>      <td>1.114231e+08</td>      <td>1.326278e+09</td>      <td>1024888979</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>3912</th>      <td>672</td>      <td>tt0295297</td>      <td>6.012584</td>      <td>100000000</td>      <td>876688482</td>      <td>Harry Potter and the Chamber of Secrets</td>      <td>Daniel Radcliffe|Rupert Grint|Emma Watson|Kenn...</td>      <td>http://www.iceagemovies.com/films/ice-age</td>      <td>Chris Columbus</td>      <td>Hogwarts is back in session.</td>      <td>...</td>      <td>161</td>      <td>Adventure|Fantasy|Family</td>      <td>1492 Pictures|Warner Bros.|Heyday Films|MIRACL...</td>      <td>11/13/02</td>      <td>3458</td>      <td>7.2</td>      <td>2002</td>      <td>1.212261e+08</td>      <td>1.062776e+09</td>      <td>776688482</td>    </tr>    <tr>      <th>13</th>      <td>257344</td>      <td>tt2120120</td>      <td>5.984995</td>      <td>88000000</td>      <td>243637091</td>      <td>Pixels</td>      <td>Adam Sandler|Michelle Monaghan|Peter Dinklage|...</td>      <td>http://www.pixels-movie.com/</td>      <td>Chris Columbus</td>      <td>Game On.</td>      <td>...</td>      <td>105</td>      <td>Action|Comedy|Science Fiction</td>      <td>Columbia Pictures|Happy Madison Productions</td>      <td>7/16/15</td>      <td>1575</td>      <td>5.8</td>      <td>2015</td>      <td>8.095996e+07</td>      <td>2.241460e+08</td>      <td>155637091</td>    </tr>    <tr>      <th>644</th>      <td>240832</td>      <td>tt2872732</td>      <td>5.947136</td>      <td>40000000</td>      <td>463360063</td>      <td>Lucy</td>      <td>Scarlett Johansson|Morgan Freeman|Choi Min-sik...</td>      <td>http://lucymovie.com/</td>      <td>Luc Besson</td>      <td>The average person uses 10% of their brain cap...</td>      <td>...</td>      <td>89</td>      <td>Action|Science Fiction</td>      <td>Universal Pictures|TF1 Films Production|Canal+...</td>      <td>7/14/14</td>      <td>3699</td>      <td>6.3</td>      <td>2014</td>      <td>3.684364e+07</td>      <td>4.267968e+08</td>      <td>423360063</td>    </tr>    <tr>      <th>14</th>      <td>99861</td>      <td>tt2395427</td>      <td>5.944927</td>      <td>280000000</td>      <td>1405035767</td>      <td>Avengers: Age of Ultron</td>      <td>Robert Downey Jr.|Chris Hemsworth|Mark Ruffalo...</td>      <td>http://marvel.com/movies/movie/193/avengers_ag...</td>      <td>Joss Whedon</td>      <td>A New Age Has Come.</td>      <td>...</td>      <td>141</td>      <td>Action|Adventure|Science Fiction</td>      <td>Marvel Studios|Prime Focus|Revolution Sun Studios</td>      <td>4/22/15</td>      <td>4304</td>      <td>7.4</td>      <td>2015</td>      <td>2.575999e+08</td>      <td>1.292632e+09</td>      <td>1125035767</td>    </tr>    <tr>      <th>4364</th>      <td>68718</td>      <td>tt1853728</td>      <td>5.944518</td>      <td>100000000</td>      <td>425368238</td>      <td>Django Unchained</td>      <td>Jamie Foxx|Christoph Waltz|Leonardo DiCaprio|K...</td>      <td>http://unchainedmovie.com/</td>      <td>Quentin Tarantino</td>      <td>Life, liberty and the pursuit of vengeance.</td>      <td>...</td>      <td>165</td>      <td>Drama|Western</td>      <td>Columbia Pictures|The Weinstein Company</td>      <td>12/25/12</td>      <td>7375</td>      <td>7.7</td>      <td>2012</td>      <td>9.497443e+07</td>      <td>4.039911e+08</td>      <td>325368238</td>    </tr>    <tr>      <th>6190</th>      <td>674</td>      <td>tt0330373</td>      <td>5.939927</td>      <td>150000000</td>      <td>895921036</td>      <td>Harry Potter and the Goblet of Fire</td>      <td>Daniel Radcliffe|Rupert Grint|Emma Watson|Ralp...</td>      <td>http://harrypotter.warnerbros.com/</td>      <td>Mike Newell</td>      <td>Dark And Difficult Times Lie Ahead.</td>      <td>...</td>      <td>157</td>      <td>Adventure|Fantasy|Family</td>      <td>Patalex IV Productions Limited|Warner Bros.|He...</td>      <td>11/5/05</td>      <td>3406</td>      <td>7.3</td>      <td>2005</td>      <td>1.674845e+08</td>      <td>1.000353e+09</td>      <td>745921036</td>    </tr>    <tr>      <th>3373</th>      <td>64690</td>      <td>tt0780504</td>      <td>5.903353</td>      <td>15000000</td>      <td>76175166</td>      <td>Drive</td>      <td>Ryan Gosling|Carey Mulligan|Christina Hendrick...</td>      <td>http://www.harrypotter.com</td>      <td>Nicolas Winding Refn</td>      <td>There are no clean getaways.</td>      <td>...</td>      <td>100</td>      <td>Drama|Action|Thriller|Crime</td>      <td>Bold Films|Marc Platt Productions|Odd Lot Ente...</td>      <td>1/10/11</td>      <td>2347</td>      <td>7.3</td>      <td>2011</td>      <td>1.454097e+07</td>      <td>7.384406e+07</td>      <td>61175166</td>    </tr>    <tr>      <th>15</th>      <td>273248</td>      <td>tt3460252</td>      <td>5.898400</td>      <td>44000000</td>      <td>155760117</td>      <td>The Hateful Eight</td>      <td>Samuel L. Jackson|Kurt Russell|Jennifer Jason ...</td>      <td>http://thehatefuleight.com/</td>      <td>Quentin Tarantino</td>      <td>No one comes up here without a damn good reason.</td>      <td>...</td>      <td>167</td>      <td>Crime|Drama|Mystery|Western</td>      <td>Double Feature Films|The Weinstein Company|Fil...</td>      <td>12/25/15</td>      <td>2389</td>      <td>7.4</td>      <td>2015</td>      <td>4.047998e+07</td>      <td>1.432992e+08</td>      <td>111760117</td>    </tr>    <tr>      <th>6554</th>      <td>834</td>      <td>tt0401855</td>      <td>5.838503</td>      <td>50000000</td>      <td>111340801</td>      <td>Underworld: Evolution</td>      <td>Kate Beckinsale|Scott Speedman|Tony Curran|Sha...</td>      <td>http://www.sonypictures.com/movies/underworlda...</td>      <td>Len Wiseman</td>      <td>My God. Brother, what have you done?</td>      <td>...</td>      <td>106</td>      <td>Fantasy|Action|Science Fiction|Thriller</td>      <td>Lakeshore Entertainment|Screen Gems</td>      <td>1/12/06</td>      <td>1015</td>      <td>6.3</td>      <td>2006</td>      <td>5.408346e+07</td>      <td>1.204339e+08</td>      <td>61340801</td>    </tr>    <tr>      <th>6962</th>      <td>673</td>      <td>tt0304141</td>      <td>5.827781</td>      <td>130000000</td>      <td>789804554</td>      <td>Harry Potter and the Prisoner of Azkaban</td>      <td>Daniel Radcliffe|Rupert Grint|Emma Watson|Gary...</td>      <td>http://www.eternalsunshine.com</td>      <td>Alfonso CuarÃ³n</td>      <td>Something wicked this way comes.</td>      <td>...</td>      <td>141</td>      <td>Adventure|Fantasy|Family</td>      <td>1492 Pictures|Warner Bros.|Heyday Films|P of A...</td>      <td>5/31/04</td>      <td>3550</td>      <td>7.4</td>      <td>2004</td>      <td>1.500779e+08</td>      <td>9.117862e+08</td>      <td>659804554</td>    </tr>    <tr>      <th>1388</th>      <td>12437</td>      <td>tt0834001</td>      <td>5.806897</td>      <td>35000000</td>      <td>91327197</td>      <td>Underworld: Rise of the Lycans</td>      <td>Bill Nighy|Michael Sheen|Rhona Mitra|Shane Bro...</td>      <td>http://harrypotter.warnerbros.com/harrypottera...</td>      <td>Patrick Tatopoulos</td>      <td>Every war has a beginning.</td>      <td>...</td>      <td>92</td>      <td>Fantasy|Action|Adventure|Science Fiction|Thriller</td>      <td>Lakeshore Entertainment|Screen Gems|Sketch Fil...</td>      <td>1/22/09</td>      <td>979</td>      <td>6.2</td>      <td>2009</td>      <td>3.557402e+07</td>      <td>9.282500e+07</td>      <td>56327197</td>    </tr>    <tr>      <th>645</th>      <td>98566</td>      <td>tt1291150</td>      <td>5.787396</td>      <td>125000000</td>      <td>477200000</td>      <td>Teenage Mutant Ninja Turtles</td>      <td>Megan Fox|Will Arnett|William Fichtner|Alan Ri...</td>      <td>http://www.teenagemutantninjaturtlesmovie.com</td>      <td>Jonathan Liebesman</td>      <td>Mysterious. Dangerous. Reptilious. You've neve...</td>      <td>...</td>      <td>101</td>      <td>Science Fiction|Action|Adventure|Fantasy|Comedy</td>      <td>Paramount Pictures|Nickelodeon Movies|Platinum...</td>      <td>8/7/14</td>      <td>1836</td>      <td>5.8</td>      <td>2014</td>      <td>1.151364e+08</td>      <td>4.395446e+08</td>      <td>352200000</td>    </tr>    <tr>      <th>16</th>      <td>260346</td>      <td>tt2446042</td>      <td>5.749758</td>      <td>48000000</td>      <td>325771424</td>      <td>Taken 3</td>      <td>Liam Neeson|Forest Whitaker|Maggie Grace|Famke...</td>      <td>http://www.taken3movie.com/</td>      <td>Olivier Megaton</td>      <td>It Ends Here</td>      <td>...</td>      <td>109</td>      <td>Crime|Action|Thriller</td>      <td>Twentieth Century Fox Film Corporation|M6 Film...</td>      <td>1/1/15</td>      <td>1578</td>      <td>6.1</td>      <td>2015</td>      <td>4.415998e+07</td>      <td>2.997096e+08</td>      <td>277771424</td>    </tr>    <tr>      <th>3374</th>      <td>12445</td>      <td>tt1201607</td>      <td>5.711315</td>      <td>125000000</td>      <td>1327817822</td>      <td>Harry Potter and the Deathly Hallows: Part 2</td>      <td>Daniel Radcliffe|Rupert Grint|Emma Watson|Alan...</td>      <td>http://www.harrypotter.com</td>      <td>David Yates</td>      <td>It all ends here.</td>      <td>...</td>      <td>130</td>      <td>Adventure|Family|Fantasy</td>      <td>Warner Bros.|Heyday Films|Moving Picture Compa...</td>      <td>7/7/11</td>      <td>3750</td>      <td>7.7</td>      <td>2011</td>      <td>1.211748e+08</td>      <td>1.287184e+09</td>      <td>1202817822</td>    </tr>    <tr>      <th>1920</th>      <td>10138</td>      <td>tt1228705</td>      <td>5.704860</td>      <td>200000000</td>      <td>623933331</td>      <td>Iron Man 2</td>      <td>Robert Downey Jr.|Gwyneth Paltrow|Don Cheadle|...</td>      <td>http://www.ironmanmovie.com/</td>      <td>Jon Favreau</td>      <td>It's not the armor that makes the hero, but th...</td>      <td>...</td>      <td>124</td>      <td>Adventure|Action|Science Fiction</td>      <td>Marvel Studios</td>      <td>4/28/10</td>      <td>4920</td>      <td>6.6</td>      <td>2010</td>      <td>2.000000e+08</td>      <td>6.239333e+08</td>      <td>423933331</td>    </tr>    <tr>      <th>646</th>      <td>225886</td>      <td>tt1956620</td>      <td>5.701683</td>      <td>40000000</td>      <td>126069509</td>      <td>Sex Tape</td>      <td>Cameron Diaz|Jason Segel|Rob Corddry|Ellie Kem...</td>      <td>http://nightcrawlerfilm.com/</td>      <td>Jake Kasdan</td>      <td>A movie about a movie they don't want you to see.</td>      <td>...</td>      <td>97</td>      <td>Comedy</td>      <td>Escape Artists|Media Rights Capital|Sony Pictu...</td>      <td>7/17/14</td>      <td>1150</td>      <td>5.3</td>      <td>2014</td>      <td>3.684364e+07</td>      <td>1.161215e+08</td>      <td>86069509</td>    </tr>    <tr>      <th>2876</th>      <td>10681</td>      <td>tt0910970</td>      <td>5.678119</td>      <td>180000000</td>      <td>521311860</td>      <td>WALLÂ·E</td>      <td>Ben Burtt|Elissa Knight|Jeff Garlin|Fred Willa...</td>      <td>http://disney.go.com/disneypictures/wall-e/</td>      <td>Andrew Stanton</td>      <td>An adventure beyond the ordinar-E.</td>      <td>...</td>      <td>98</td>      <td>Animation|Family</td>      <td>Walt Disney Pictures|Pixar Animation Studios</td>      <td>6/22/08</td>      <td>4209</td>      <td>7.6</td>      <td>2008</td>      <td>1.823016e+08</td>      <td>5.279777e+08</td>      <td>341311860</td>    </tr>    <tr>      <th>4365</th>      <td>37724</td>      <td>tt1074638</td>      <td>5.603587</td>      <td>200000000</td>      <td>1108561013</td>      <td>Skyfall</td>      <td>Daniel Craig|Judi Dench|Javier Bardem|Ralph Fi...</td>      <td>http://www.skyfall-movie.com</td>      <td>Sam Mendes</td>      <td>Think on your sins.</td>      <td>...</td>      <td>143</td>      <td>Action|Adventure|Thriller</td>      <td>Columbia Pictures</td>      <td>10/25/12</td>      <td>6137</td>      <td>6.8</td>      <td>2012</td>      <td>1.899489e+08</td>      <td>1.052849e+09</td>      <td>908561013</td>    </tr>    <tr>      <th>17</th>      <td>102899</td>      <td>tt0478970</td>      <td>5.573184</td>      <td>130000000</td>      <td>518602163</td>      <td>Ant-Man</td>      <td>Paul Rudd|Michael Douglas|Evangeline Lilly|Cor...</td>      <td>http://marvel.com/movies/movie/180/ant-man</td>      <td>Peyton Reed</td>      <td>Heroes Don't Get Any Bigger</td>      <td>...</td>      <td>115</td>      <td>Science Fiction|Action|Adventure</td>      <td>Marvel Studios</td>      <td>7/14/15</td>      <td>3779</td>      <td>7.0</td>      <td>2015</td>      <td>1.195999e+08</td>      <td>4.771138e+08</td>      <td>388602163</td>    </tr>    <tr>      <th>1921</th>      <td>12155</td>      <td>tt1014759</td>      <td>5.572950</td>      <td>200000000</td>      <td>1025467110</td>      <td>Alice in Wonderland</td>      <td>Mia Wasikowska|Johnny Depp|Anne Hathaway|Helen...</td>      <td>http://disney.go.com/wonderland/</td>      <td>Tim Burton</td>      <td>You're invited to a very important date.</td>      <td>...</td>      <td>108</td>      <td>Family|Fantasy|Adventure</td>      <td>Walt Disney Pictures|Team Todd|Tim Burton Prod...</td>      <td>3/3/10</td>      <td>2853</td>      <td>6.3</td>      <td>2010</td>      <td>2.000000e+08</td>      <td>1.025467e+09</td>      <td>825467110</td>    </tr>    <tr>      <th>18</th>      <td>150689</td>      <td>tt1661199</td>      <td>5.556818</td>      <td>95000000</td>      <td>542351353</td>      <td>Cinderella</td>      <td>Lily James|Cate Blanchett|Richard Madden|Helen...</td>      <td>http://www.thehungergames.movie/</td>      <td>Kenneth Branagh</td>      <td>Midnight is just the beginning.</td>      <td>...</td>      <td>112</td>      <td>Romance|Fantasy|Family|Drama</td>      <td>Walt Disney Pictures|Genre Films|Beagle Pug Fi...</td>      <td>3/12/15</td>      <td>1495</td>      <td>6.8</td>      <td>2015</td>      <td>8.739996e+07</td>      <td>4.989630e+08</td>      <td>447351353</td>    </tr>    <tr>      <th>647</th>      <td>242582</td>      <td>tt2872718</td>      <td>5.522641</td>      <td>8500000</td>      <td>38697217</td>      <td>Nightcrawler</td>      <td>Jake Gyllenhaal|Rene Russo|Riz Ahmed|Bill Paxt...</td>      <td>http://nightcrawlerfilm.com/</td>      <td>Dan Gilroy</td>      <td>The city shines brightest at night</td>      <td>...</td>      <td>117</td>      <td>Crime|Drama|Thriller</td>      <td>Bold Films|Sierra / Affinity</td>      <td>10/23/14</td>      <td>2087</td>      <td>7.6</td>      <td>2014</td>      <td>7.829274e+06</td>      <td>3.564366e+07</td>      <td>30197217</td>    </tr>    <tr>      <th>19</th>      <td>131634</td>      <td>tt1951266</td>      <td>5.476958</td>      <td>160000000</td>      <td>650523427</td>      <td>The Hunger Games: Mockingjay - Part 2</td>      <td>Jennifer Lawrence|Josh Hutcherson|Liam Hemswor...</td>      <td>http://www.thehungergames.movie/</td>      <td>Francis Lawrence</td>      <td>The fire will burn forever.</td>      <td>...</td>      <td>136</td>      <td>War|Adventure|Science Fiction</td>      <td>Studio Babelsberg|StudioCanal|Lionsgate|Walt D...</td>      <td>11/18/15</td>      <td>2380</td>      <td>6.5</td>      <td>2015</td>      <td>1.471999e+08</td>      <td>5.984813e+08</td>      <td>490523427</td>    </tr>    <tr>      <th>20</th>      <td>158852</td>      <td>tt1964418</td>      <td>5.462138</td>      <td>190000000</td>      <td>209035668</td>      <td>Tomorrowland</td>      <td>Britt Robertson|George Clooney|Raffey Cassidy|...</td>      <td>http://movies.disney.com/tomorrowland</td>      <td>Brad Bird</td>      <td>Imagine a world where nothing is impossible.</td>      <td>...</td>      <td>130</td>      <td>Action|Family|Science Fiction|Adventure|Mystery</td>      <td>Walt Disney Pictures|Babieka|A113</td>      <td>5/19/15</td>      <td>1899</td>      <td>6.2</td>      <td>2015</td>      <td>1.747999e+08</td>      <td>1.923127e+08</td>      <td>19035668</td>    </tr>    <tr>      <th>6191</th>      <td>272</td>      <td>tt0372784</td>      <td>5.400826</td>      <td>150000000</td>      <td>374218673</td>      <td>Batman Begins</td>      <td>Christian Bale|Michael Caine|Liam Neeson|Katie...</td>      <td>http://www2.warnerbros.com/batmanbegins/index....</td>      <td>Christopher Nolan</td>      <td>Evil fears the knight.</td>      <td>...</td>      <td>140</td>      <td>Action|Crime|Drama</td>      <td>DC Comics|Legendary Pictures|Warner Bros.|DC E...</td>      <td>6/14/05</td>      <td>4914</td>      <td>7.3</td>      <td>2005</td>      <td>1.674845e+08</td>      <td>4.178388e+08</td>      <td>224218673</td>    </tr>    <tr>      <th>21</th>      <td>307081</td>      <td>tt1798684</td>      <td>5.337064</td>      <td>30000000</td>      <td>91709827</td>      <td>Southpaw</td>      <td>Jake Gyllenhaal|Rachel McAdams|Forest Whitaker...</td>      <td>http://www.sanandreasmovie.com/</td>      <td>Antoine Fuqua</td>      <td>Believe in Hope.</td>      <td>...</td>      <td>123</td>      <td>Action|Drama</td>      <td>Escape Artists|Riche-Ludwig Productions</td>      <td>6/15/15</td>      <td>1386</td>      <td>7.3</td>      <td>2015</td>      <td>2.759999e+07</td>      <td>8.437300e+07</td>      <td>61709827</td>    </tr>    <tr>      <th>1922</th>      <td>44214</td>      <td>tt0947798</td>      <td>5.293180</td>      <td>13000000</td>      <td>327803731</td>      <td>Black Swan</td>      <td>Natalie Portman|Mila Kunis|Vincent Cassel|Barb...</td>      <td>http://www.foxsearchlight.com/blackswan/</td>      <td>Darren Aronofsky</td>      <td>In the era of personal branding, the scariest ...</td>      <td>...</td>      <td>108</td>      <td>Drama|Mystery|Thriller</td>      <td>Fox Searchlight Pictures|Dune Entertainment|Pr...</td>      <td>12/2/10</td>      <td>2597</td>      <td>7.1</td>      <td>2010</td>      <td>1.300000e+07</td>      <td>3.278037e+08</td>      <td>314803731</td>    </tr>    <tr>      <th>5423</th>      <td>49047</td>      <td>tt1454468</td>      <td>5.242753</td>      <td>105000000</td>      <td>716392705</td>      <td>Gravity</td>      <td>Sandra Bullock|George Clooney|Ed Harris|Orto I...</td>      <td>http://gravitymovie.warnerbros.com/</td>      <td>Alfonso CuarÃ³n</td>      <td>Don't Let Go</td>      <td>...</td>      <td>91</td>      <td>Science Fiction|Thriller|Drama</td>      <td>Warner Bros.|Heyday Films|Esperanto Filmoj</td>      <td>9/27/13</td>      <td>3775</td>      <td>7.4</td>      <td>2013</td>      <td>9.828350e+07</td>      <td>6.705675e+08</td>      <td>611392705</td>    </tr>    <tr>      <th>5424</th>      <td>76338</td>      <td>tt1981115</td>      <td>5.111900</td>      <td>170000000</td>      <td>479765000</td>      <td>Thor: The Dark World</td>      <td>Chris Hemsworth|Natalie Portman|Tom Hiddleston...</td>      <td>http://marvel.com/thor</td>      <td>Alan Taylor</td>      <td>Delve into the darkness</td>      <td>...</td>      <td>112</td>      <td>Action|Adventure|Fantasy</td>      <td>Marvel Studios</td>      <td>10/29/13</td>      <td>3025</td>      <td>6.8</td>      <td>2013</td>      <td>1.591257e+08</td>      <td>4.490760e+08</td>      <td>309765000</td>    </tr>    <tr>      <th>1389</th>      <td>767</td>      <td>tt0417741</td>      <td>5.076472</td>      <td>250000000</td>      <td>933959197</td>      <td>Harry Potter and the Half-Blood Prince</td>      <td>Daniel Radcliffe|Rupert Grint|Emma Watson|Tom ...</td>      <td>http://harrypotter.warnerbros.com/harrypottera...</td>      <td>David Yates</td>      <td>Dark Secrets Revealed</td>      <td>...</td>      <td>153</td>      <td>Adventure|Fantasy|Family</td>      <td>Warner Bros.|Heyday Films</td>      <td>7/7/09</td>      <td>3220</td>      <td>7.3</td>      <td>2009</td>      <td>2.541001e+08</td>      <td>9.492765e+08</td>      <td>683959197</td>    </tr>  </tbody></table><p>78 rows × 22 columns</p></div><hr><p><strong>任务2.3: </strong>分组读取</p><ol><li>对 <code>release_year</code> 进行分组，使用 <a href="http://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.core.groupby.DataFrameGroupBy.agg.html" target="_blank" rel="noopener"><code>.agg</code></a> 获得 <code>revenue</code> 的均值。</li><li>对 <code>director</code> 进行分组，使用 <a href="http://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.core.groupby.DataFrameGroupBy.agg.html" target="_blank" rel="noopener"><code>.agg</code></a> 获得 <code>popularity</code> 的均值，从高到低排列。</li></ol><p>要求：使用 <code>Groupby</code> 命令实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">movie_data.groupby(<span class="string">'release_year'</span>)[<span class="string">'revenue'</span>].agg(np.mean)</span><br><span class="line">movie_data.groupby(<span class="string">'director'</span>)[<span class="string">'popularity'</span>].agg(np.mean).sort_values(ascending=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><pre><code>directorColin Trevorrow                     16.696886Joe Russo|Anthony Russo             12.971027Chad Stahelski|David Leitch         11.422751Don Hall|Chris Williams              8.691294Juno John Lee                        8.411577Kyle Balda|Pierre Coffin             7.404165Alan Taylor                          6.883129Peter Richardson                     6.668990Pete Docter                          6.326804Christopher Nolan                    6.195521Alex Garland                         6.118847Patrick Tatopoulos                   5.806897Wes Ball                             5.553082Dan Gilroy                           5.522641Lilly Wachowski|Lana Wachowski       5.331930James Gunn                           5.225378Bob Peterson|Pete Docter             4.908902J.J. Abrams                          4.800957Alejandro GonzÃ¡lez IÃ±Ã¡rritu       4.793536Roger Allers|Rob Minkoff             4.782688Damien Chazelle                      4.780419Morten Tyldum                        4.485181George Miller                        4.450001Francis Lawrence                     4.437604Len Wiseman                          4.380968Quentin Tarantino                    4.187272David Yates                          4.163195Evan Goldberg|Seth Rogen             3.989231John Lasseter|Joe Ranft              3.941265Chris Buck|Jennifer Lee              3.918739                                      ...    Venus Keung Kwok-Man|Wong Jing       0.004433Connor McGuire|Colin McGuire         0.004323Shashank Khaitan                     0.004282Alex Holdridge                       0.004196Karen Disher|Guy Moore               0.004010Kristopher Belman                    0.003933Eldar Ryazanov                       0.003659Laurent Malaquais                    0.003611James Erskine                        0.003504Walter Carvalho|Sandra Werneck       0.003461Katie Graham|Andrew Matthews         0.003066Nicolas Castro                       0.002922David Higby                          0.002838Marv Newland                         0.002757David Thorpe                         0.002599Oliver Irving                        0.002514Barbara Schroeder                    0.002460Charles B. Pierce                    0.002381Norman Buckley                       0.002262Beth McCarthy-Miller|Rob Ashford     0.002165Amol Palekar                         0.001983Sarah Burns|Ken Burns                0.001783Enrico Oldoini                       0.001567Russ Malkin|David Alexanian          0.001531Nacho G. Velilla                     0.001499Stephen Cragg                        0.001423Jean-Xavier de Lestrade              0.001315Zana Briski|Ross Kauffman            0.001117Dibakar Banerjee                     0.001115Pascal Thomas                        0.000973Name: popularity, Length: 5056, dtype: float64</code></pre><hr><hr><h2 id="第三节-绘图与可视化"><a href="#第三节-绘图与可视化" class="headerlink" title="第三节 绘图与可视化"></a>第三节 绘图与可视化</h2><p>接着你要尝试对你的数据进行图像的绘制以及可视化。这一节最重要的是，你能够选择合适的图像，对特定的可视化目标进行可视化。所谓可视化的目标，是你希望从可视化的过程中，观察到怎样的信息以及变化。例如，观察票房随着时间的变化、哪个导演最受欢迎等。</p><table><thead><tr><th>可视化的目标</th><th>可以使用的图像</th></tr></thead><tbody> <tr><td>表示某一属性数据的分布</td><td>饼图、直方图、散点图</td></tr> <tr><td>表示某一属性数据随着某一个变量变化</td><td>条形图、折线图、热力图</td></tr> <tr><td>比较多个属性的数据之间的关系</td><td>散点图、小提琴图、堆积条形图、堆积折线图</td></tr></tbody></table><p>在这个部分，你需要根据题目中问题，选择适当的可视化图像进行绘制，并进行相应的分析。对于选做题，他们具有一定的难度，你可以尝试挑战一下～</p><p><strong>任务3.1：</strong>对 <code>popularity</code> 最高的20名电影绘制其 <code>popularity</code> 值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">movie_data.sort_values(by=<span class="string">'popularity'</span>, ascending=<span class="keyword">False</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">plt.xlabel(<span class="string">'popularity'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'original_title'</span>)</span><br><span class="line">plt.barh(movie_data[<span class="string">'original_title'</span>][:<span class="number">20</span>], width=movie_data[<span class="string">'popularity'</span>][:<span class="number">20</span>], align=<span class="string">'center'</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;BarContainer object of 20 artists&gt;</code></pre><p><img src="/2019/02/16/Explore-Movie-Dataset/output_19_1.png" alt="png"></p><hr><p><strong>任务3.2：</strong>分析电影净利润（票房-成本）随着年份变化的情况，并简单进行分析。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">movie_data[<span class="string">'profit'</span>] = movie_data[<span class="string">'revenue'</span>] - movie_data[<span class="string">'budget'</span>]</span><br><span class="line">pd.Series.value_counts(movie_data[<span class="string">'release_year'</span>])</span><br><span class="line">data = movie_data.groupby(by=<span class="string">'release_year'</span>)[<span class="string">'profit'</span>].agg([np.mean,np.std])</span><br><span class="line">x = list(data.index)</span><br><span class="line">y = data[<span class="string">'mean'</span>]</span><br><span class="line">plt.xlabel(<span class="string">'release_year'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'profit'</span>)</span><br><span class="line">plt.errorbar(x, y)</span><br></pre></td></tr></table></figure><pre><code>&lt;ErrorbarContainer object of 3 artists&gt;</code></pre><p><img src="/2019/02/16/Explore-Movie-Dataset/output_21_1.png" alt="png"></p><hr><p><strong>[选做]任务3.3：</strong>选择最多产的10位导演（电影数量最多的），绘制他们排行前3的三部电影的票房情况，并简要进行分析。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tmp = movie_data[<span class="string">'director'</span>].str.split(<span class="string">'|'</span>, expand=<span class="keyword">True</span>).stack().reset_index(level=<span class="number">1</span>, drop=<span class="keyword">True</span>).rename(<span class="string">'director'</span>)</span><br><span class="line">movie_data_split = movie_data[[<span class="string">'original_title'</span>, <span class="string">'revenue'</span>]].join(tmp)</span><br><span class="line"></span><br><span class="line">movie_data_split_direction = list(movie_data_split[<span class="string">'director'</span>].value_counts().isin([<span class="number">45</span>,<span class="number">34</span>,<span class="number">31</span>,<span class="number">30</span>,<span class="number">23</span>,<span class="number">22</span>,<span class="number">21</span>,<span class="number">20</span>]))</span><br><span class="line">movie_data_split_direction</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)&lt;ipython-input-74-a6f6663f8cbb&gt; in &lt;module&gt;()      2 movie_data_split = movie_data[[&#39;original_title&#39;, &#39;revenue&#39;]].join(tmp)      3 ----&gt; 4 movie_data_split_direction = list(movie_data_split[&#39;director&#39;].value_counts().isin([45,34,31,30,23,22,21,20]).index())      5 movie_data_split_directionTypeError: &#39;Index&#39; object is not callable</code></pre><hr><p><strong>[选做]任务3.4：</strong>分析1968年~2015年六月电影的数量的变化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><hr><p><strong>[选做]任务3.5：</strong>分析1968年~2015年六月电影 <code>Comedy</code> 和 <code>Drama</code> 两类电影的数量的变化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>注意: 当你写完了所有的代码，并且回答了所有的问题。你就可以把你的 iPython Notebook 导出成 HTML 文件。你可以在菜单栏，这样导出<strong>File -&gt; Download as -&gt; HTML (.html)、Python (.py)</strong> 把导出的 HTML、python文件 和这个 iPython notebook 一起提交给审阅者。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;探索电影数据集&quot;&gt;&lt;a href=&quot;#探索电影数据集&quot; class=&quot;headerlink&quot; title=&quot;探索电影数据集&quot;&gt;&lt;/a&gt;探索电影数据集&lt;/h2&gt;&lt;p&gt;在这个项目中，你将尝试使用所学的知识，使用 &lt;code&gt;NumPy&lt;/code&gt;、&lt;code&gt;Pan
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Linear Algebra</title>
    <link href="http://yoursite.com/2019/02/16/Linear-Algebra/"/>
    <id>http://yoursite.com/2019/02/16/Linear-Algebra/</id>
    <published>2019-02-16T15:37:33.000Z</published>
    <updated>2019-02-16T15:42:47.803Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性代数：机器学习背后的优化原理"><a href="#线性代数：机器学习背后的优化原理" class="headerlink" title="线性代数：机器学习背后的优化原理"></a>线性代数：机器学习背后的优化原理</h1><p>线性代数作为数学的一个分支，广泛应用于科学和工程中，掌握好线性代数对于理解和从事机器学习算法相关工作是很有必要的，尤其对于深度学习算法而言。因此，这个项目会从浅入深更好的帮助你学习与积累一些跟人工智能强相关的线性代数的知识。</p><p>本项目内容理论知识部分参考<a href="https://book.douban.com/subject/27087503/" target="_blank" rel="noopener">《DeepLearning》又名花书</a>第二章，希望大家支持正版购买图书。</p><p>若项目中的题目有困难没完成也没关系，我们鼓励你带着问题提交项目，评审人会给予你诸多帮助。</p><p>所有选做题都可以不做，不影响项目通过。如果你做了，那么项目评审会帮你批改，也会因为选做部分做错而判定为不通过。</p><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们将讲解常用的线性代数知识，而学员需使用numpy来实现这些知识点（当然也可以自己写算法实现），还需要使用matplotlib完成规定图像习题，当然，本项目用到的python代码(或numpy的使用)课程中并未完全教授，所以需要学员对相应操作进行学习与查询，这在我们往后的人工智能学习之旅中是必不可少的一个技能，请大家珍惜此项目的练习机会。</p><p>当然，这里提供官方的<a href="https://docs.scipy.org/doc/numpy/user/quickstart.html#" target="_blank" rel="noopener">numpy Quickstart</a>来帮助你更好的完成项目。</p><p>本项目还需要使用LaTeX公式，以下两个链接供学习与使用：</p><p><a href="https://www.authorea.com/users/77723/articles/110898-how-to-write-mathematical-equations-expressions-and-symbols-with-latex-a-cheatsheet" target="_blank" rel="noopener">Latex cheatsheet</a></p><p><a href="http://www.personal.ceu.hu/tex/cookbook.html#inline" target="_blank" rel="noopener">aTeX Cookbook</a></p><p>首先，导入你所需的软件包。一般我们建议在工程开头导入<strong>所有</strong>需要的软件包。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> import相关库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><h2 id="1、标量，向量，矩阵，张量"><a href="#1、标量，向量，矩阵，张量" class="headerlink" title="1、标量，向量，矩阵，张量"></a>1、标量，向量，矩阵，张量</h2><p><strong>首先，让我们回顾下基本的定义：</strong></p><ul><li><p>标量（scalar）：形式而言，一个标量是一个单独的数，常用斜体的小写变量名称来表示。<em>v</em></p></li><li><p>向量（vector）：形式而言，一个向量是一列有序数，常用粗体的小写变量名称表示<strong>v</strong>，或者上面标记剪头$\vec{v}$ </p></li><li><p>矩阵（matrix）：形式而言，一个矩阵是一个二维数组，常用大写变量名称表示A，表示内部的元素则会使用$A_{i,j}$</p></li><li><p>张量（tensor）：形式而言，一个张量是一个多维数组，常用粗体的大写字母变量名称表示<strong>T</strong>，表示内部的元素则会使用$A_{i,j,z}$ 等等</p></li></ul><p>用图片直观的显示区别如下<br><img src="/2019/02/16/Linear-Algebra/diff.png" width="500"></p><p><strong>接下来让我们回顾下基本的运算：</strong></p><ul><li><p>加法<br><img src="/2019/02/16/Linear-Algebra/add.png" width="500"></p></li><li><p>标量乘法<br><img src="/2019/02/16/Linear-Algebra/scmu.png" width="400"></p></li><li><p>转置<br><img src="/2019/02/16/Linear-Algebra/trans.png" width="370"></p></li><li><p>矩阵向量乘法（内积，人工智能中常见的拼写：matrix product 或者 dot product）<br><img src="/2019/02/16/Linear-Algebra/mul.png" width="570"></p></li></ul><p><strong>线性方程组：</strong></p><p>由矩阵乘法也演变出了我们最常见的线性方程组，已知矩阵与未知向量的乘积，等于另一个已知向量，通过此方程组可求解那个未知向量，一般写为x，具体如下表示。<br>等式左侧可以这么来理解：<br><img src="/2019/02/16/Linear-Algebra/axb.png" width="400"><br>列为具体的矩阵来看：</p><script type="math/tex; mode=display">\begin{bmatrix}    A_{1,1} & A_{1,2} & \cdots & A_{1,n} \\\\    A_{2,1} & A_{2,2} & \cdots & A_{2,n} \\\\    \cdots & \cdots & \cdots & \cdots \\\\    A_{m,1} & A_{m,2} & \cdots & A_{m,n}\end{bmatrix}\times\begin{bmatrix}    x_1 \\\\    x_2 \\\\    \cdots \\\\    x_n\end{bmatrix}=\begin{bmatrix}    b_1 \\\\    b_2 \\\\    \cdots \\\\    b_m\end{bmatrix}</script><p>或者更简单的表示为</p><script type="math/tex; mode=display">Ax=b</script><p>既然有未知数，那么自然需要求解未知数，而我们的未知数需要满足所有方程，也不是一直都有解的，下面来列我们二维矩阵所组成的方程解的情况,若两条线平行不存在焦点，那么说明没有一个$x_1$, $x_2$同时满足两个方程，则此方程组无解，同理，若相交，则有一个解，若完全相等，则有无穷个解。<br><img src="/2019/02/16/Linear-Algebra/axbsolu.png" width="570"></p><h3 id="1-1、基本运算并绘图"><a href="#1-1、基本运算并绘图" class="headerlink" title="1.1、基本运算并绘图"></a>1.1、基本运算并绘图</h3><p>例题 $\vec{v}$ + $\vec{w}$</p><p>$\hspace{1cm}\vec{v} = \begin{bmatrix} 1\ 1\end{bmatrix}$</p><p>$\hspace{1cm}\vec{w} = \begin{bmatrix} -2\ 2\end{bmatrix}$</p><p>结果需要先使用numpy计算向量运算结果，并用LaTeX公式表示：</p><p>$\hspace{1cm}\vec{v}+\vec{w} = \begin{bmatrix} -1\ 3\end{bmatrix}$</p><p>并使用matlibplot绘制出(图表颜色样式不要求)</p><p><img src="/2019/02/16/Linear-Algebra/add_e.png" width="300"></p><h4 id="1-1-1"><a href="#1-1-1" class="headerlink" title="1.1.1"></a>1.1.1</h4><p><strong>根据上面例题展示，计算并绘制  $2\vec{v}$ - $\vec{w}$  的结果</strong></p><p>$\hspace{1cm}\vec{v} = \begin{bmatrix} 4\ 1\end{bmatrix}$</p><p>$\hspace{1cm}\vec{w} = \begin{bmatrix} -1\ 2\end{bmatrix}$</p><p>$\hspace{1cm}\vec{v}+\vec{w} = \begin{bmatrix} -1\ 3\end{bmatrix}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.1.1 TODO：</span></span><br><span class="line">a = <span class="number">2</span></span><br><span class="line">v = np.array([<span class="number">4</span>,<span class="number">1</span>])</span><br><span class="line">av = v*a</span><br><span class="line">w = np.array([<span class="number">-1</span>,<span class="number">2</span>])</span><br><span class="line">v_2_w = <span class="number">2</span> * v - w</span><br><span class="line"></span><br><span class="line">ax = plt.axes()</span><br><span class="line">ax.arrow(<span class="number">0</span>, <span class="number">0</span>, *av, color=<span class="string">'b'</span>, linewidth=<span class="number">2.0</span>, head_width=<span class="number">0.20</span>, head_length=<span class="number">0.25</span>)</span><br><span class="line">ax.arrow(v_2_w[<span class="number">0</span>], v_2_w[<span class="number">1</span>], *w, color=<span class="string">'r'</span>, linewidth=<span class="number">2.0</span>, head_width=<span class="number">0.20</span>, head_length=<span class="number">0.25</span>)</span><br><span class="line">ax.arrow(<span class="number">0</span>, <span class="number">0</span>, *v_2_w, color=<span class="string">'k'</span>, linewidth=<span class="number">2.0</span>, head_width=<span class="number">0.20</span>, head_length=<span class="number">0.25</span>)</span><br><span class="line">plt.xlim(<span class="number">-1</span>,<span class="number">10</span>)</span><br><span class="line">major_xticks = np.arange(<span class="number">-1</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">plt.ylim(<span class="number">-1</span>,<span class="number">4</span>)</span><br><span class="line">major_yticks = np.arange(<span class="number">-1</span>, <span class="number">10</span>)</span><br><span class="line">ax.set_yticks(major_yticks)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.grid(b=<span class="keyword">True</span>, which=<span class="string">'major'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/02/16/Linear-Algebra/output_6_0.png" alt="png"></p><script type="math/tex; mode=display">\begin{bmatrix}    4\\\\    1\end{bmatrix}\times\begin{bmatrix}    2\end{bmatrix}-\begin{bmatrix}    -1\\\\    2\end{bmatrix}=\begin{bmatrix}    9\\\\    0\end{bmatrix}</script><p>例题，方程组求解：</p><script type="math/tex; mode=display">\begin{cases}y = 2x + 1\\\\y = 6x - 2\end{cases}</script><p>用matplotlib绘制图表（图表样式不要求）<br><img src="/2019/02/16/Linear-Algebra/2equ_solu.png" width="300"><br>由上可知此方程组有且仅有一个解</p><p>需使用numpy（或自写算法）计算该解的结果,并用LaTeX公式表示出来(结果可以用小数或者分数展示)</p><script type="math/tex; mode=display">\begin{cases}x = \frac{3}{4} \\\\y = \frac{5}{2}\end{cases}</script><h4 id="1-1-2"><a href="#1-1-2" class="headerlink" title="1.1.2"></a>1.1.2</h4><p><strong>根据上面例题展示，绘制方程组，说明是否有解是否为唯一解，若有解需计算出方程组的解</strong></p><script type="math/tex; mode=display">\begin{cases}y = 2x + 1\\\\y = \frac{1}{10}x+6\end{cases}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.1.2 TODO</span></span><br><span class="line">X = np.mat([[<span class="number">-1</span>,<span class="number">2</span>],[<span class="number">-1</span>,<span class="number">0.1</span>]])</span><br><span class="line">y = np.mat([<span class="number">-1</span>,<span class="number">-6</span>]).T</span><br><span class="line">x = np.arange(<span class="number">-10</span>,<span class="number">10</span>)</span><br><span class="line">plt.plot(<span class="number">2</span>*x+<span class="number">1</span>)</span><br><span class="line">plt.plot(<span class="number">0.1</span>*x+<span class="number">6</span>)</span><br><span class="line">plt.show</span><br><span class="line"></span><br><span class="line">r = np.linalg.solve(X,y)</span><br><span class="line">print(r)</span><br><span class="line"><span class="comment">#有唯一解 函数相交</span></span><br></pre></td></tr></table></figure><pre><code>[[6.26315789] [2.63157895]]</code></pre><p><img src="/2019/02/16/Linear-Algebra/output_9_1.png" alt="png"></p><h3 id="1-2、说明题"><a href="#1-2、说明题" class="headerlink" title="1.2、说明题"></a>1.2、说明题</h3><h4 id="1-2-1"><a href="#1-2-1" class="headerlink" title="1.2.1"></a>1.2.1</h4><p><strong>使用numpy（或自写算法）说明$(AB)^{\text{T}} = B^\text{T}A^\text{T}$</strong></p><p><strong>其中</strong></p><script type="math/tex; mode=display">A=\begin{bmatrix}    21 & 7 \\\\    15 & 42 \\\\    9 & 6\end{bmatrix}, B=\begin{bmatrix}    4 \\\\    33\end{bmatrix}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.2.1 TODO</span></span><br><span class="line">A = np.array([[<span class="number">21</span>,<span class="number">7</span>],[<span class="number">15</span>,<span class="number">42</span>],[<span class="number">9</span>,<span class="number">6</span>]],dtype=int)</span><br><span class="line">B = np.array([<span class="number">4</span>,<span class="number">33</span>],dtype=int)</span><br><span class="line"></span><br><span class="line">display(A.dot(B).T)</span><br><span class="line">display(B.T.dot(A.T))</span><br><span class="line"><span class="comment">#结果证明(AB)T=BTAT</span></span><br></pre></td></tr></table></figure><pre><code>array([ 315, 1446,  234])array([ 315, 1446,  234])</code></pre><h4 id="1-2-2"><a href="#1-2-2" class="headerlink" title="1.2.2"></a>1.2.2</h4><p><strong>使用numpy（或自写算法）说明  $A ( B + C ) = AB + AC$ </strong></p><p><strong>其中</strong></p><script type="math/tex; mode=display">A=\begin{bmatrix}    9 & 3 \\\\    8 & 4 \\\\    7 & 6\end{bmatrix}, B=\begin{bmatrix}    5 \\\\    2\end{bmatrix}, C=\begin{bmatrix}    5 \\\\    7\end{bmatrix}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.2.2 TODO</span></span><br><span class="line">A = np.array([[<span class="number">9</span>,<span class="number">3</span>],[<span class="number">8</span>,<span class="number">4</span>],[<span class="number">7</span>,<span class="number">6</span>]])</span><br><span class="line">B = np.array([[<span class="number">5</span>,<span class="number">2</span>]])</span><br><span class="line">C = np.array([[<span class="number">5</span>,<span class="number">7</span>]])</span><br><span class="line"></span><br><span class="line">display(A*(B+C))</span><br><span class="line">display(A*B+A*C)</span><br><span class="line"><span class="comment"># 据结果A(B+C)=AB+AC</span></span><br></pre></td></tr></table></figure><pre><code>array([[90, 27],       [80, 36],       [70, 54]])array([[90, 27],       [80, 36],       [70, 54]])</code></pre><h2 id="2、特殊矩阵"><a href="#2、特殊矩阵" class="headerlink" title="2、特殊矩阵"></a>2、特殊矩阵</h2><ul><li>单位矩阵</li></ul><p>如果选取任意一个向量和某矩阵相乘，该向量都不会改变，我们将这种保持n维向量不变的矩阵记为单位矩阵$I_n$</p><ul><li>逆矩阵</li></ul><p>如果存在一个矩阵，使$A^{-1} A = I_n$，那么$A^{-1}$就是A的逆矩阵。</p><ul><li>对角矩阵</li></ul><p>如果一个矩阵只有主对角线上还有非零元素，其他位置都是零，这个矩阵就是对角矩阵</p><ul><li>对称矩阵</li></ul><p>如果一个矩阵的转置是和它自己相等的矩阵，即$A=A^{T}$，那么这个矩阵就是对称矩阵</p><ul><li>正交矩阵</li></ul><p>行向量和列向量是分别标准正交(90度)的方阵，即$A^{T}A = AA^{T} = I_n$，又即$A^{-1} = A^{T}$，那么这种方阵就是正交矩阵</p><h3 id="2-1、证明题"><a href="#2-1、证明题" class="headerlink" title="2.1、证明题"></a>2.1、证明题</h3><p>通过LaTeX公式，结合上面所述概念，假设$A^{-1}$存在的情况下，证明$Ax=b$的解$x={A}^{-1}{b}$</p><p>回答：</p><p>因为$A^{-1}$存在 所以 等式两边同乘 $A^{-1}$           $A^{-1}Ax = A^{-1}b$</p><p>又因为$A^{-1} A = I_n$  所以 等式为 $I_nx = A^{-1}b$</p><p>同时 $I_n$为单位矩阵所以 $x = A^{-1}b$</p><p>$a^2$</p><h3 id="2-2、-计算题"><a href="#2-2、-计算题" class="headerlink" title="2.2、 计算题"></a>2.2、 计算题</h3><h4 id="2-2-1"><a href="#2-2-1" class="headerlink" title="2.2.1"></a>2.2.1</h4><p>通过numpy计算，再次验证2.1证明题</p><script type="math/tex; mode=display">\begin{cases}y = 2x + 1\\\\y = \frac{1}{10}x+6\end{cases}</script><p>并用LaTeX公式写出$A^{-1}$是多少（小数分数皆可）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.2.1 TODO</span></span><br><span class="line">x = np.mat([[<span class="number">-1</span>,<span class="number">2</span>],[<span class="number">-1</span>,<span class="number">0.1</span>]])</span><br><span class="line">y = np.mat([<span class="number">-1</span>,<span class="number">-6</span>]).T</span><br><span class="line">r = np.linalg.solve(x,y)</span><br></pre></td></tr></table></figure><h4 id="2-2-2"><a href="#2-2-2" class="headerlink" title="2.2.2"></a>2.2.2</h4><p>1、请用numpy（或自写算法）实现一个6x6的对角矩阵，矩阵的对角线由3至8（含8）组成。</p><p>2、计算第一问生成的对角矩阵与向量$[6,7,1,2,5,9]^{T}$的乘积</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.2.2 TODO</span></span><br><span class="line">diagonal = np.diag(np.arange(<span class="number">3</span>,<span class="number">9</span>))</span><br><span class="line">vector = np.array([<span class="number">6</span>,<span class="number">7</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">9</span>])</span><br><span class="line">diagonal.dot(vector.T)</span><br></pre></td></tr></table></figure><pre><code>array([18, 28,  5, 12, 35, 72])</code></pre><h2 id="3、迹运算"><a href="#3、迹运算" class="headerlink" title="3、迹运算"></a>3、迹运算</h2><p>迹运算返回的是矩阵对角元素的和，如图所示<br><img src="/2019/02/16/Linear-Algebra/matrix.png" width="360"><br>写成数学公式为：</p><script type="math/tex; mode=display">\large Tr(A) = \sum_{i}A_{i,i}</script><p><strong>说明题：</strong></p><p>使用numpy验证</p><script type="math/tex; mode=display">\large Tr(ABC) = Tr(CAB) = Tr(BCA)</script><p>其中</p><script type="math/tex; mode=display">A=\begin{bmatrix}    7 & 6 \\\\    29 & 3\end{bmatrix}</script><script type="math/tex; mode=display">B=\begin{bmatrix}    2 & -8 \\\\    9 & 10\end{bmatrix}</script><script type="math/tex; mode=display">C=\begin{bmatrix}    2 & 17 \\\\    1 & 5\end{bmatrix}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3 TODO</span></span><br><span class="line">A = np.array([[<span class="number">7</span>,<span class="number">6</span>],[<span class="number">29</span>,<span class="number">3</span>]])</span><br><span class="line">B = np.array([[<span class="number">2</span>,<span class="number">-8</span>],[<span class="number">9</span>,<span class="number">10</span>]])</span><br><span class="line">C = np.array([[<span class="number">2</span>,<span class="number">17</span>],[<span class="number">1</span>,<span class="number">5</span>]])</span><br><span class="line">ABC = A.dot(B).dot(C)</span><br><span class="line">CAB = C.dot(A).dot(B)</span><br><span class="line">BCA = B.dot(C).dot(A)</span><br><span class="line">print(<span class="string">'ABC:&#123;0&#125;,CAB:&#123;1&#125;,BCA:&#123;2&#125;'</span>.format(np.trace(ABC),np.trace(CAB),np.trace(BCA)))</span><br></pre></td></tr></table></figure><pre><code>ABC:575,CAB:575,BCA:575</code></pre><h2 id="4、衡量向量以及矩阵的大小：范数与条件数"><a href="#4、衡量向量以及矩阵的大小：范数与条件数" class="headerlink" title="4、衡量向量以及矩阵的大小：范数与条件数"></a>4、衡量向量以及矩阵的大小：范数与条件数</h2><h3 id="范数的定义"><a href="#范数的定义" class="headerlink" title="范数的定义"></a>范数的定义</h3><p>在线性代数等数学分支中，范数（Norm）是一个函数，其给予某向量空间（或矩阵）中的每个向量以长度或称之为大小。对于零向量，其长度为零。直观的说，向量或矩阵的范数越大，则我们可以说这个向量或矩阵也就越大。有时范数有很多更为常见的叫法，如绝对值其实便是一维向量空间中实数或复数的范数，范数的一般化定义：设$p\ge 1$，p-norm用以下来表示</p><script type="math/tex; mode=display">\large {\Vert x \Vert}_{p} =  \lgroup {\sum_{i}{\vert x_i \vert}^p }\rgroup ^{\frac{1}{p}}</script><p>此处，当p=1时，我们称之曼哈顿范数(Manhattan Norm)。其来源是曼哈顿的出租车司机在四四方方的曼哈顿街道中从一点到另一点所需要走过的距离。也即我们所要讨论的L1范数。其表示某个向量中所有元素绝对值的和。 而当p=2时，则是我们最为常见的Euclidean norm。也称为Euclidean distance，中文叫欧几里得范数，也即我们要讨论的L2范数，他也经常被用来衡量向量的大小。 而当p=0时，严格的说此时p已不算是范数了，L0范数是指向量中非0的元素的个数，但很多人仍然称之为L0范数（Zero norm零范数）。 这三个范数有很多非常有意思的特征，尤其是在机器学习中的正则化（Regularization）以及稀疏编码（Sparse Coding）有非常有趣的应用，这个在进阶课程可以做更深入的了解。</p><p><strong>L0 范数</strong></p><script type="math/tex; mode=display">\large \Vert x \Vert = \sqrt[0]{\sum_i x_i^0} = \#(i|x_i \neq0)</script><p><strong>L1 范数</strong></p><script type="math/tex; mode=display">\large {\Vert x \Vert}_{1} =  \lgroup {\sum_{i}{\vert x_i \vert} }\rgroup</script><p><strong>L2 范数</strong></p><script type="math/tex; mode=display">\large {\Vert x \Vert}_{2} =  \lgroup {\sum_{i}{\vert x_i \vert}^2 }\rgroup ^{\frac{1}{2}}</script><p>另外这里还存在特例：<br> 当 $ p -&gt; \infty $ 时，我们称之为 $ L^{\infty} $范数，也被称为“maximum norm（max范数）”，这个范数表示向量中具有最大幅度的元素的绝对值：</p><script type="math/tex; mode=display">\large {\Vert x \Vert}^{\infty} =  \max_{i}{\vert x_i \vert}</script><p><a href="http://t.cn/RINHvvt" target="_blank" rel="noopener">以上资料部分参考wiki</a></p><h3 id="4-1、计算向量的范数"><a href="#4-1、计算向量的范数" class="headerlink" title="4.1、计算向量的范数"></a>4.1、计算向量的范数</h3><p>编写一个函数来计算一下向量的各种范数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO 实现这里向量范数计算的函数，要求可以计算p = 0,1,2,3 ... 无穷 情况下的范数</span></span><br><span class="line"></span><br><span class="line"><span class="string">""" 计算向量的范数</span></span><br><span class="line"><span class="string">    参数</span></span><br><span class="line"><span class="string">        x: 向量 numpy数组 或者list数组</span></span><br><span class="line"><span class="string">        p: 范数的阶，int型整数或者None</span></span><br><span class="line"><span class="string">        infty: 是否计算max范数，bool型变量，True的时候表示计算max范数，False的时候计算p范数</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    返回</span></span><br><span class="line"><span class="string">        向量的范数，float类型数值</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    hint:</span></span><br><span class="line"><span class="string">        1.你需要首先判断infty是True or False, 然后判断p 是否为零</span></span><br><span class="line"><span class="string">        2.注意int类型变量在计算时候需要规整为float类型</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_Norm</span><span class="params">(x, p = <span class="number">2</span>, infty = False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> infty == <span class="keyword">False</span>:</span><br><span class="line">        answer = np.linalg.norm(x,float(p))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        answer = np.linalg.norm(x,np.Inf)</span><br><span class="line">    <span class="keyword">return</span> answer</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%run -i -e test.py LinearRegressionTestCase.test_calc_Norm</span><br></pre></td></tr></table></figure><pre><code>.----------------------------------------------------------------------Ran 1 test in 0.004sOK</code></pre><h3 id="4-2、计算矩阵的范数"><a href="#4-2、计算矩阵的范数" class="headerlink" title="4.2、计算矩阵的范数"></a>4.2、计算矩阵的范数</h3><p>我们也需要衡量矩阵的大小，对于矩阵大小的衡量在很多优化问题中是非常重要的。而在深度学习中，最常见的做法是使用Frobenius 范数(Frobenius norm)，也称作矩阵的F范数，其定义如下：</p><script type="math/tex; mode=display">\large {\Vert A \Vert}_{F} =  \sqrt {\sum_{i,j}{\vert A_{i,j} \vert}^2 }</script><p>我们这里继续来计算一下F范数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO 实现这里矩阵Frobenius范数计算的函数</span></span><br><span class="line"></span><br><span class="line"><span class="string">""" 计算向量的范数</span></span><br><span class="line"><span class="string">    参数</span></span><br><span class="line"><span class="string">        A: 给定的任意二维矩阵 list或者numpy数组形式</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    返回</span></span><br><span class="line"><span class="string">        矩阵的Frobenius范数，float类型数值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_Frobenius_Norm</span><span class="params">(A)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.linalg.norm(A,<span class="string">'fro'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%run -i -e test.py LinearRegressionTestCase.test_calc_Frobenius_Norm</span><br></pre></td></tr></table></figure><pre><code>.----------------------------------------------------------------------Ran 1 test in 0.001sOK</code></pre><h3 id="4-3、计算矩阵的条件数"><a href="#4-3、计算矩阵的条件数" class="headerlink" title="4.3、计算矩阵的条件数"></a>4.3、计算矩阵的条件数</h3><p>矩阵的条件数(condition number)是矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，我们这里为了简化条件，这里只考虑矩阵是奇异矩阵的时候，如何计算以及理解条件数(condition number):</p><p>当矩阵A为奇异矩阵的时候，condition number为无限大；当矩阵A非奇异的时候，我们定义condition number如下：</p><script type="math/tex; mode=display">\large \kappa{(A)} =  {\Vert A \Vert}_F {\Vert A^{-1} \Vert}_F</script><p><a href="https://zh.wikipedia.org/wiki/%E9%9D%9E%E5%A5%87%E5%BC%82%E6%96%B9%E9%98%B5" target="_blank" rel="noopener">奇异矩阵，非奇异矩阵</a></p><p>计算矩阵的条件数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" 计算矩阵的条件数</span></span><br><span class="line"><span class="string">    参数</span></span><br><span class="line"><span class="string">        A: 给定的任意二维矩阵 list或者numpy数组形式</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    返回</span></span><br><span class="line"><span class="string">        矩阵的condition number,</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_Condition_Number</span><span class="params">(A)</span>:</span></span><br><span class="line">    A = np.mat(A)</span><br><span class="line">    <span class="keyword">return</span> np.dot(np.linalg.norm(A,<span class="string">'fro'</span>),np.linalg.norm(A.I,<span class="string">'fro'</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%run -i -e test.py LinearRegressionTestCase.test_calc_Condition_Number</span><br></pre></td></tr></table></figure><pre><code>/opt/anaconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py:68: PendingDeprecationWarning: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray.  return matrix(data, dtype=dtype, copy=False).----------------------------------------------------------------------Ran 1 test in 0.017sOK</code></pre><h3 id="选做-4-4、条件数的理解与应用"><a href="#选做-4-4、条件数的理解与应用" class="headerlink" title="(选做)4.4、条件数的理解与应用"></a>(选做)4.4、条件数的理解与应用</h3><p>a. 有如下两个2*2的非奇异矩阵A和B:</p><p>$ A = \begin{bmatrix}<br>     1   &amp;2 \<br>     3   &amp;4 \<br>\end{bmatrix} $ </p><p>$ B = \begin{bmatrix}<br>     1   &amp;2 \<br>     2   &amp;4.0001 \<br>\end{bmatrix}<br>$</p><p>计算condition number(A), condition number(B);</p><p>b. 根据上面构造的矩阵A,B分别计算线性系统方程组的解$w$:</p><p>   A $ \begin{bmatrix}w<em>{a1}\w</em>{a2}\ \end{bmatrix} $ = $ \begin{bmatrix}1\2\ \end{bmatrix} $, </p><p>   B $ \begin{bmatrix}w<em>{b1}\w</em>{b2}\ \end{bmatrix} $ = $ \begin{bmatrix}1\2\ \end{bmatrix} $,</p><p>   A $ \begin{bmatrix}w<em>{a1}\w</em>{a2}\ \end{bmatrix} $ = $ \begin{bmatrix}{1.0001}\{2.0001}\ \end{bmatrix} $, </p><p>   B $ \begin{bmatrix}w<em>{b1}\w</em>{b2}\ \end{bmatrix} $ = $ \begin{bmatrix}{1.0001}\{2.0001}\ \end{bmatrix} $.</p><p>c. 计算完成之后，比较condition number大小与线性系统稳定性之间的关系，并且给出规律性的总结；</p><p>d. <strong>阅读与思考</strong>: 考虑更为通用的一种情况，我们计算一个典型的线性回归系统: </p><script type="math/tex; mode=display">Xw = b</script><p>可以简单推导得出其闭式解为：$ w=(X^TX)^{−1}X^Tb $ ，如果 $X^TX$可逆</p><p>推导过程： </p><p>1.等式两边乘以$X^T$</p><script type="math/tex; mode=display">X^TXw = X^Tb</script><p>2.等式两边乘以$(X^TX)^{-1}$</p><script type="math/tex; mode=display">(X^TX)^{-1}X^TXw = (X^TX)^{−1}X^Tb</script><p>3.因为$A^{-1}A = I$，两边约去即可得：</p><script type="math/tex; mode=display">w=(X^TX)^{−1}X^Tb</script><p>当我们需要拟合的数据X满足数据量远远小于特征数目的时候，也就是X矩阵的行数 &lt;&lt; X矩阵的列数的时候，因为$X^TX$不是奇异矩阵，此时方程组不存在闭式解；那么我们该如何重新构造$X^TX$，使得该闭式解成立？</p><p>hint1. 单位矩阵的condition number是最低的，是最为稳定的；</p><p>hint2. 如果要使得该系统存在闭式解，那么就必须使得求逆运算是可以进行的，也就是说重新构造的$X^TX$必须是可逆的方阵；</p><p>hint3. 重新构造的方式可以是在$X^TX$基础上进行加或者减或者乘除相关矩阵的操作；</p><p>一种可行的方式就是：</p><script type="math/tex; mode=display">w = (X^TX+\lambda I)^{−1}X^Tb</script><p>实际上我们最为常用的<a href="http://scikit-learn.org/stable/modules/linear_model.html" target="_blank" rel="noopener">Ridge Regression</a>和 L2范数以及condition number之间某种程度上是可以相互推导的：</p><p>首先，Ridge Regression的损失函数为：</p><script type="math/tex; mode=display">J_w = min({\Vert Xw -y \Vert}^2 + \alpha {\Vert w \Vert}^2)</script><p>我们展开这个损失函数：</p><script type="math/tex; mode=display">{\Vert Xw -y \Vert}^2 + \alpha {\Vert w \Vert}^2  =  (Xw -y)^T (Xw-y) + \alpha w^Tw</script><p>由于这里是一个凸函数，我们令导数等于零，即为最小值的解，求导可得：</p><script type="math/tex; mode=display">X^T (Xw-y) + \alpha w = 0</script><p>整理即可得到：</p><script type="math/tex; mode=display">w = (X^TX+\lambda I)^{−1}X^Tb</script><h2 id="5、SVD"><a href="#5、SVD" class="headerlink" title="5、SVD"></a>5、SVD</h2><p><a href="https://en.wikipedia.org/wiki/Singular-value_decomposition" target="_blank" rel="noopener">SVD</a>是Singular value decomposition的缩写，称为奇异值分解，是分解矩阵的一种方式，会将矩阵分解为奇异向量（singular vector）和奇异值（singular value），分解的意义其实很明确，就是想将一个很大很复杂的矩阵，用更小更简单的几个子矩阵的相乘来表示，这些小矩阵描述的是矩阵的重要的特性。</p><p>那么SVD具体的数学表达是什么呢？</p><p>假设有一个矩阵C，我们可以将矩阵C分解为三个矩阵的乘积：<br><img src="/2019/02/16/Linear-Algebra/svd.png" width="480"></p><script type="math/tex; mode=display">\large C = UDV^{T}</script><p>如果C是一个m x n的矩阵，那么U是一个m x m的矩阵，D是一个m x n的矩阵，V是一个n x n的矩阵，这些小矩阵并不是普普通通的矩阵，U和V都定义为正交矩阵，而D定义为对角矩阵。</p><p>SVD最常用的做法就是用来进行特征的降维以及矩阵的低秩重构，例如这里分别取矩阵U、D、VT的前k列，如图示中的白色部分，然后重新计算新的C矩阵，即为k维度下的矩阵重构，这种方法被广泛应用于自然语言处理<a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis" target="_blank" rel="noopener">LSA</a>、推荐系统<a href="https://tech.meituan.com/deep_understanding_of_ffm_principles_and_practices.html" target="_blank" rel="noopener">SVD++,FM,FFM</a>等领域，如有兴趣可以继续参考链接相关资料。<br><img src="/2019/02/16/Linear-Algebra/svd_decompostion.png" width="480"></p><p>具体计算UDV的算法不是我们这个项目的关键，我们只需使用numpy得出结果即可，下面的习题，将会带你体会SVD的某一应用场景。</p><p>提示：我们会需要使用<a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.linalg.html" target="_blank" rel="noopener">numpy.linalg</a>相关函数。</p><h3 id="5-1、使用numpy去计算任意矩阵的奇异值分解："><a href="#5-1、使用numpy去计算任意矩阵的奇异值分解：" class="headerlink" title="5.1、使用numpy去计算任意矩阵的奇异值分解："></a>5.1、使用numpy去计算任意矩阵的奇异值分解：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" 计算任意矩阵的奇异值分解</span></span><br><span class="line"><span class="string">    参数</span></span><br><span class="line"><span class="string">        A: 给定的任意二维矩阵 list或者numpy数组形式 </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    返回</span></span><br><span class="line"><span class="string">        使用numpy.linalg相关函数，直接返回分解之后的矩阵U,D,V</span></span><br><span class="line"><span class="string">        （可以尝试一下使用np.shape一下分解出来的U，D，VT，会发现维度跟我们上面讲解所描述的不同，</span></span><br><span class="line"><span class="string">        暂时不用管他直接返回np求解出的U，D，VT即可）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_svd</span><span class="params">(A)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.linalg.svd(A)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%run -i -e test.py LinearRegressionTestCase.test_calc_svd</span><br></pre></td></tr></table></figure><pre><code>.----------------------------------------------------------------------Ran 1 test in 0.004sOK</code></pre><h3 id="选做-5-2、利用奇异值分解对矩阵进行降维"><a href="#选做-5-2、利用奇异值分解对矩阵进行降维" class="headerlink" title="(选做) 5.2、利用奇异值分解对矩阵进行降维"></a>(选做) 5.2、利用奇异值分解对矩阵进行降维</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO 利用SVD进行对于矩阵进行降维</span></span><br><span class="line"></span><br><span class="line"><span class="string">""" 利用SVD进行对于矩阵进行降维</span></span><br><span class="line"><span class="string">    参数</span></span><br><span class="line"><span class="string">        A: 给定的任意二维矩阵 list或者numpy数组形式 shape为(m,n)</span></span><br><span class="line"><span class="string">        topk: 降维的维度 (m,n) -&gt; (m,topk)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    返回</span></span><br><span class="line"><span class="string">        降维后的矩阵 (m, topk)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    hint</span></span><br><span class="line"><span class="string">    1. 对角矩阵D存在一个较为明显的特性，就是D的对角线元素是递减的，这些元素实际上是衡量了所分解的矩阵U,V的列向量的重要性</span></span><br><span class="line"><span class="string">    2. 因此我们常说的svd降维就是利用选取的前topk大的对角线矩阵元素进行构造新的降维矩阵</span></span><br><span class="line"><span class="string">    3. U的按照前topk截取的列向量 * topk截取的对角矩阵 即为新的降维后的矩阵</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_svd_decompostion</span><span class="params">(A, topk = <span class="number">2</span>)</span>:</span></span><br><span class="line">    U,Sig,V = np.linalg.svd(A)</span><br><span class="line">    Sig[<span class="number">1</span>] = topk</span><br><span class="line">    <span class="keyword">return</span> Sig</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%run -i -e test.py LinearRegressionTestCase.test_calc_svd_decompostion</span><br></pre></td></tr></table></figure><pre><code>.----------------------------------------------------------------------Ran 1 test in 0.004sOK</code></pre><h3 id="选做-5-3、利用奇异值分解对矩阵进行降维后重构"><a href="#选做-5-3、利用奇异值分解对矩阵进行降维后重构" class="headerlink" title="(选做) 5.3、利用奇异值分解对矩阵进行降维后重构"></a>(选做) 5.3、利用奇异值分解对矩阵进行降维后重构</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" 利用SVD进行对于矩阵进行降维</span></span><br><span class="line"><span class="string">    参数</span></span><br><span class="line"><span class="string">        A: 给定的任意二维矩阵 list或者numpy数组形式 shape为(m,n)</span></span><br><span class="line"><span class="string">        topk: 降维的维度 (m,n) -&gt; (m,topk)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    返回</span></span><br><span class="line"><span class="string">        降维重构后的矩阵 (m, n)</span></span><br><span class="line"><span class="string">    hint</span></span><br><span class="line"><span class="string">        这里除了降维矩阵外，另外一个较为常见的应用就是对矩阵进行重构，具体的做法类似前面的思路</span></span><br><span class="line"><span class="string">        1. 选取对应的U，D，V的topk向量</span></span><br><span class="line"><span class="string">        2. U的按照前topk截取的列向量 * topk截取的对角矩阵 * V^T按照前topk截取的行向量(注意这里是V的转置,因为分解得到的是V^T)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_svd_reconsitution</span><span class="params">(A, topk = <span class="number">2</span>)</span>:</span></span><br><span class="line">    A = np.array(A)</span><br><span class="line">    U,Sig,V = np.linalg.svd(A)</span><br><span class="line">    Sig[<span class="number">1</span>] = topk</span><br><span class="line">    </span><br><span class="line">    S = np.zeros((<span class="number">5</span>,<span class="number">8</span>))</span><br><span class="line">    S[:<span class="number">5</span>,:<span class="number">5</span>] = np.diag(Sig)</span><br><span class="line">    A_conv = np.dot(np.dot(A.T,U),S)</span><br><span class="line">    A = np.dot(np.dot(U,S),V)</span><br><span class="line">    <span class="keyword">return</span> A</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%run -i -e test.py LinearRegressionTestCase.test_calc_svd_reconsitution</span><br></pre></td></tr></table></figure><pre><code>.----------------------------------------------------------------------Ran 1 test in 0.001sOK</code></pre><h3 id="选做-5-4、计算不同降维大小重构矩阵的Frobenius范数损失"><a href="#选做-5-4、计算不同降维大小重构矩阵的Frobenius范数损失" class="headerlink" title="(选做) 5.4、计算不同降维大小重构矩阵的Frobenius范数损失"></a>(选做) 5.4、计算不同降维大小重构矩阵的Frobenius范数损失</h3><p>定义矩阵$A$以及使用SVD降维（降维大小为k)分解后的重构矩阵$A_k$，则这里的F范数损失定义如下：</p><script type="math/tex; mode=display">\Large Loss_{F} = {\Vert A - A_k \Vert}_F</script><p>这里需要编码求出对于给定的矩阵A 分别在不同的降维幅度下重构后的F范数损失，并且作出损失大小随着降维大小的变化图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 不要修改这里！</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston  </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline </span><br><span class="line">A = load_boston()[<span class="string">'data'</span>]  <span class="comment"># 载入boston house 数据集</span></span><br><span class="line">print(A.shape)</span><br></pre></td></tr></table></figure><pre><code>(506, 13)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">loss_hist = []</span><br><span class="line"><span class="keyword">for</span> topk <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">13</span>):</span><br><span class="line">    <span class="comment"># 5.4 TODO </span></span><br><span class="line">    <span class="comment">### 1.计算相应的SVD topk降维后的重构矩阵，需实现calc_svd_reconsitution</span></span><br><span class="line">    <span class="comment">### 2.计算对应的F范数损失，并存储loss放入loss_hist列表中</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 画出F损失随着降维大小的变化图</span></span><br><span class="line"><span class="comment">### x坐标为对应的降维大小，y坐标为对应的F损失</span></span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">13</span>),loss_hist,<span class="string">'r--'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'decomposition size'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'F Loss'</span>)</span><br></pre></td></tr></table></figure><pre><code>  File &quot;&lt;ipython-input-24-309fc4d50cf4&gt;&quot;, line 10    plt.plot(range(1,13),loss_hist,&#39;r&#39;)      ^IndentationError: expected an indented block</code></pre><h3 id="5-5、SVD的有趣应用"><a href="#5-5、SVD的有趣应用" class="headerlink" title="5.5、SVD的有趣应用"></a>5.5、SVD的有趣应用</h3><p>为了这个习题我准备了两张图，参见项目文件夹下的test_girl.jpg和test_boy.jpeg，自选一张，你需要</p><ul><li>需要使用 <code>PIL</code> 加载你所选择的图像（<a href="https://pillow.readthedocs.io/en/latest/reference/Image.html" target="_blank" rel="noopener">文档</a>）,所以记得导入需要的包（模块）</li><li>使用Image的<a href="https://pillow.readthedocs.io/en/latest/reference/Image.html#PIL.Image.Image.convert" target="_blank" rel="noopener">convert方法</a>将图像变为灰度图</li><li>将convert后的结果转换成np.array,需用到<a href="https://pillow.readthedocs.io/en/latest/reference/Image.html#PIL.Image.Image.getdata" target="_blank" rel="noopener">Image.getdata方法</a>来读取图片每个pixel的数据，特别注意一下，对于彩色的图来说，即使我们转为了灰度图，但每一个pixel还是由RGB三个维度组成，所以在getdata时，band需要设定为某一个颜色index，比如band = 0，这样只用R这个维度的数据。用这个方法来保证图片的每个pixel只占有一个单元的空间。</li><li>因为我们转np.array时破坏了原有图形的样子，变成了一个一维数据，我们要将转换后的np.array恢复到图片应有的size，转换后，可以shape确认下是否与最开始转出的灰度图的size一致，注意图的size是（宽，高），而宽对应array.shape的应该是列，而高对应的是行，别弄反了。</li><li>使用上方实现的calc_svd函数计算上一步计算出的np.array数据，赋值给变量：U,D,VT</li><li>打印出U,D,VT的shape形状，尤其注意观察D的shape</li><li>在U，VT，D变量成功实现的情况下，运行测试程序看效果</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5.5 TODO</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">im = Image.open(<span class="string">'test_girl.jpg'</span>).convert(<span class="string">'LA'</span>)</span><br><span class="line">im_array = np.array(im.getdata(band=<span class="number">0</span>))</span><br><span class="line">im_array = im_array.reshape(im.size[<span class="number">1</span>],im.size[<span class="number">0</span>])</span><br><span class="line">U,D,VT = calc_svd(im_array)</span><br><span class="line">display(U.shape)</span><br><span class="line">display(D.shape)</span><br><span class="line">display(VT.shape)</span><br></pre></td></tr></table></figure><pre><code>(600, 600)(600,)(750, 750)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#请在U，D，V变量完成的情况下调用此测试程序，不要修改此处</span></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> i,topk <span class="keyword">in</span> enumerate([<span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">50</span>]):</span><br><span class="line">    reconstimg = np.matrix(U[:, :topk]) * np.diag(D[:topk]) * np.matrix(VT[:topk, :])</span><br><span class="line">    plt.subplot(<span class="number">231</span>+i)</span><br><span class="line">    plt.imshow(reconstimg, cmap=<span class="string">'gray'</span>)</span><br><span class="line">    title = <span class="string">"n = %s"</span> % ((i+<span class="number">1</span>)*<span class="number">5</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/02/16/Linear-Algebra/output_51_0.png" alt="png"></p><p>相关继续深入学习的资料：</p><ol><li><a href="http://freemind.pluskid.org/series/mlopt/" target="_blank" rel="noopener">机器学习与优化</a></li><li><a href="https://www.zhihu.com/question/40043805/answer/138429562" target="_blank" rel="noopener">PCA与SVD的区别</a></li><li><a href="https://www.cnblogs.com/pinard/p/6251584.html" target="_blank" rel="noopener">SVD在降维中的应用</a></li><li><a href="https://blog.csdn.net/pipisorry/article/details/42560331" target="_blank" rel="noopener">SVD在自然语言处理中的应用</a></li><li><a href="https://tech.meituan.com/deep_understanding_of_ffm_principles_and_practices.html" target="_blank" rel="noopener">SVD在推荐系统中的应用</a></li><li><a href="https://web.stanford.edu/~hastie/ElemStatLearn//" target="_blank" rel="noopener">《Elements of Statistical Learning》Trevor Hastie, Robert Tibshirani, and Jerome Friedman</a></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;线性代数：机器学习背后的优化原理&quot;&gt;&lt;a href=&quot;#线性代数：机器学习背后的优化原理&quot; class=&quot;headerlink&quot; title=&quot;线性代数：机器学习背后的优化原理&quot;&gt;&lt;/a&gt;线性代数：机器学习背后的优化原理&lt;/h1&gt;&lt;p&gt;线性代数作为数学的一个分支，
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>image classifier project</title>
    <link href="http://yoursite.com/2019/02/16/image-classifier-project/"/>
    <id>http://yoursite.com/2019/02/16/image-classifier-project/</id>
    <published>2019-02-16T15:35:55.000Z</published>
    <updated>2019-02-16T15:49:40.440Z</updated>
    
    <content type="html"><![CDATA[<h1 id="开发-AI-应用"><a href="#开发-AI-应用" class="headerlink" title="开发 AI 应用"></a>开发 AI 应用</h1><p>未来，AI 算法在日常生活中的应用将越来越广泛。例如，你可能想要在智能手机应用中包含图像分类器。为此，在整个应用架构中，你将使用一个用成百上千个图像训练过的深度学习模型。未来的软件开发很大一部分将是使用这些模型作为应用的常用部分。</p><p>在此项目中，你将训练一个图像分类器来识别不同的花卉品种。可以想象有这么一款手机应用，当你对着花卉拍摄时，它能够告诉你这朵花的名称。在实际操作中，你会训练此分类器，然后导出它以用在你的应用中。我们将使用<a href="http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html" target="_blank" rel="noopener">此数据集</a>，其中包含 102 个花卉类别。你可以在下面查看几个示例。<br><img src="/2019/02/16/image-classifier-project/Flowers.png" alt="png"></p><p>该项目分为多个步骤：</p><ul><li>加载和预处理图像数据集</li><li>用数据集训练图像分类器</li><li>使用训练的分类器预测图像内容</li></ul><p>我们将指导你完成每一步，你将用 Python 实现这些步骤。</p><p>完成此项目后，你将拥有一个可以用任何带标签图像的数据集进行训练的应用。你的网络将学习花卉，并成为一个命令行应用。但是，你对新技能的应用取决于你的想象力和构建数据集的精力。例如，想象有一款应用能够拍摄汽车，告诉你汽车的制造商和型号，然后查询关于该汽车的信息。构建你自己的数据集并开发一款新型应用吧。</p><p>首先，导入你所需的软件包。建议在代码开头导入所有软件包。当你创建此 notebook 时，如果发现你需要导入某个软件包，确保在开头导入该软件包。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Imports here</span></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'retina'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms, models</span><br></pre></td></tr></table></figure><h2 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h2><p>在此项目中，你将使用 <code>torchvision</code> 加载数据（<a href="http://pytorch.org/docs/master/torchvision/transforms.html#" target="_blank" rel="noopener">文档</a>）。数据应该和此 notebook 一起包含在内，否则你可以<a href="https://s3.amazonaws.com/content.udacity-data.com/nd089/flower_data.tar.gz" target="_blank" rel="noopener">在此处下载数据</a>。数据集分成了三部分：训练集、验证集和测试集。对于训练集，你需要变换数据，例如随机缩放、剪裁和翻转。这样有助于网络泛化，并带来更好的效果。你还需要确保将输入数据的大小调整为 224x224 像素，因为预训练的网络需要这么做。</p><p>验证集和测试集用于衡量模型对尚未见过的数据的预测效果。对此步骤，你不需要进行任何缩放或旋转变换，但是需要将图像剪裁到合适的大小。</p><p>对于所有三个数据集，你都需要将均值和标准差标准化到网络期望的结果。均值为 <code>[0.485, 0.456, 0.406]</code>，标准差为 <code>[0.229, 0.224, 0.225]</code>。这样使得每个颜色通道的值位于 -1 到 1 之间，而不是 0 到 1 之间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_dir = <span class="string">'train'</span></span><br><span class="line">valid_dir = <span class="string">'valid'</span></span><br><span class="line">test_dir = <span class="string">'test'</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Define your transforms for the training, validation, and testing sets</span></span><br><span class="line">data_dir = <span class="string">'flowers/'</span></span><br><span class="line">data_transforms = transforms.Compose([</span><br><span class="line">    transforms.RandomRotation(<span class="number">30</span>),</span><br><span class="line">    transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                        [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">])</span><br><span class="line">test_transforms = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">255</span>),</span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                        [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Load the datasets with ImageFolder</span></span><br><span class="line">image_datasets = datasets.ImageFolder(data_dir + train_dir, transform=data_transforms)</span><br><span class="line">image_datasets_test = datasets.ImageFolder(data_dir + test_dir, transform=test_transforms)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Using the image datasets and the trainforms, define the dataloaders</span></span><br><span class="line">dataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=<span class="number">64</span>, shuffle=<span class="keyword">True</span>)</span><br><span class="line">test_dataloaders = torch.utils.data.DataLoader(image_datasets_test, batch_size=<span class="number">32</span>)</span><br></pre></td></tr></table></figure><h3 id="标签映射"><a href="#标签映射" class="headerlink" title="标签映射"></a>标签映射</h3><p>你还需要加载从类别标签到类别名称的映射。你可以在文件 <code>cat_to_name.json</code> 中找到此映射。它是一个 JSON 对象，可以使用 <a href="https://docs.python.org/2/library/json.html" target="_blank" rel="noopener"><code>json</code> 模块</a>读取它。这样可以获得一个从整数编码的类别到实际花卉名称的映射字典。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'cat_to_name.json'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    cat_to_name = json.load(f)</span><br></pre></td></tr></table></figure><h1 id="构建和训练分类器"><a href="#构建和训练分类器" class="headerlink" title="构建和训练分类器"></a>构建和训练分类器</h1><p>数据准备好后，就开始构建和训练分类器了。和往常一样，你应该使用 <code>torchvision.models</code> 中的某个预训练模型获取图像特征。使用这些特征构建和训练新的前馈分类器。</p><p>这部分将由你来完成。如果你想与他人讨论这部分，欢迎与你的同学讨论！你还可以在论坛上提问或在工作时间内咨询我们的课程经理和助教导师。</p><p>请参阅<a href="https://review.udacity.com/#!/rubrics/1663/view" target="_blank" rel="noopener">审阅标准</a>，了解如何成功地完成此部分。你需要执行以下操作：</p><ul><li>加载<a href="http://pytorch.org/docs/master/torchvision/models.html" target="_blank" rel="noopener">预训练的网络</a>（如果你需要一个起点，推荐使用 VGG 网络，它简单易用）</li><li>使用 ReLU 激活函数和丢弃定义新的未训练前馈网络作为分类器</li><li>使用反向传播训练分类器层，并使用预训练的网络获取特征</li><li>跟踪验证集的损失和准确率，以确定最佳超参数</li></ul><p>我们在下面为你留了一个空的单元格，但是你可以使用多个单元格。建议将问题拆分为更小的部分，并单独运行。检查确保每部分都达到预期效果，然后再完成下个部分。你可能会发现，当你实现每部分时，可能需要回去修改之前的代码，这很正常！</p><p>训练时，确保仅更新前馈网络的权重。如果一切构建正确的话，验证准确率应该能够超过 70%。确保尝试不同的超参数（学习速率、分类器中的单元、周期等），寻找最佳模型。保存这些超参数并用作项目下个部分的默认值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Build and train your network</span></span><br><span class="line">model = models.vgg16(pretrained=<span class="keyword">True</span>)</span><br><span class="line">model</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">classifier = nn.Sequential(OrderedDict([</span><br><span class="line">    (<span class="string">'fc1'</span>, nn.Linear(<span class="number">25088</span>,<span class="number">1024</span>)),</span><br><span class="line">    (<span class="string">'relu'</span>, nn.ReLU()),</span><br><span class="line">    (<span class="string">'fc2'</span>, nn.Linear(<span class="number">1024</span>,<span class="number">102</span>)),</span><br><span class="line">    (<span class="string">'output'</span>, nn.LogSoftmax(dim=<span class="number">1</span>))</span><br><span class="line">]))</span><br><span class="line"></span><br><span class="line">model.classifier = classifier</span><br><span class="line"></span><br><span class="line">criterion = nn.NLLLoss()</span><br><span class="line">optimizer = optim.Adam(model.classifier.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><pre><code>Downloading: &quot;https://download.pytorch.org/models/vgg16-397923af.pth&quot; to /root/.torch/models/vgg16-397923af.pth100%|██████████| 553433881/553433881 [00:34&lt;00:00, 16147158.40it/s]</code></pre><h2 id="测试网络"><a href="#测试网络" class="headerlink" title="测试网络"></a>测试网络</h2><p>建议使用网络在训练或验证过程中从未见过的测试数据测试训练的网络。这样，可以很好地判断模型预测全新图像的效果。用网络预测测试图像，并测量准确率，就像验证过程一样。如果模型训练良好的话，你应该能够达到大约 70% 的准确率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Do validation on the test set</span></span><br><span class="line">epochs = <span class="number">3</span></span><br><span class="line">print_every = <span class="number">40</span></span><br><span class="line">steps = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">model.to(<span class="string">'cuda'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    running_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> li, (images, labels) <span class="keyword">in</span> enumerate(dataloaders):</span><br><span class="line">        steps += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        images, labels = images.to(<span class="string">'cuda'</span>), labels.to(<span class="string">'cuda'</span>)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        outputs = model.forward(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> print_every % steps == <span class="number">0</span>:</span><br><span class="line">            model.eval()</span><br><span class="line">            accuracy = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> ii, (images, labels) <span class="keyword">in</span> enumerate(test_dataloaders):</span><br><span class="line">                </span><br><span class="line">                images, labels = images.to(<span class="string">'cuda'</span>), labels.to(<span class="string">'cuda'</span>)</span><br><span class="line">                predicted = model(images)</span><br><span class="line">                equality = (labels == predicted.max(<span class="number">1</span>)[<span class="number">1</span>])</span><br><span class="line">                accuracy += equality.type_as(torch.FloatTensor()).mean()</span><br><span class="line">            print(<span class="string">"Epoch: &#123;&#125;/&#123;&#125;"</span>.format(i+<span class="number">1</span>, epochs),</span><br><span class="line">                  <span class="string">"Loss: &#123;:.4f&#125;"</span>.format(running_loss/print_every),</span><br><span class="line">                  <span class="string">"Test accuracy: &#123;:.4f&#125;"</span>.format(accuracy/(ii+<span class="number">1</span>)))</span><br><span class="line">            model.train()</span><br><span class="line">            steps = <span class="number">0</span></span><br><span class="line">            running_loss = <span class="number">0</span></span><br></pre></td></tr></table></figure><pre><code>Epoch: 1/3 Loss: 0.1159 Test accuracy: 0.0613Epoch: 1/3 Loss: 0.1666 Test accuracy: 0.0481Epoch: 1/3 Loss: 0.1464 Test accuracy: 0.1146Epoch: 1/3 Loss: 0.1557 Test accuracy: 0.1070Epoch: 1/3 Loss: 0.1142 Test accuracy: 0.1154Epoch: 1/3 Loss: 0.1290 Test accuracy: 0.1619Epoch: 1/3 Loss: 0.1171 Test accuracy: 0.2007Epoch: 1/3 Loss: 0.1080 Test accuracy: 0.1863Epoch: 1/3 Loss: 0.1133 Test accuracy: 0.2224Epoch: 1/3 Loss: 0.1019 Test accuracy: 0.2841Epoch: 1/3 Loss: 0.0866 Test accuracy: 0.3153Epoch: 1/3 Loss: 0.0825 Test accuracy: 0.3322Epoch: 1/3 Loss: 0.0917 Test accuracy: 0.3346Epoch: 1/3 Loss: 0.0864 Test accuracy: 0.3531Epoch: 1/3 Loss: 0.0782 Test accuracy: 0.3771Epoch: 1/3 Loss: 0.0754 Test accuracy: 0.3847Epoch: 1/3 Loss: 0.0702 Test accuracy: 0.3875Epoch: 1/3 Loss: 0.0790 Test accuracy: 0.3971Epoch: 1/3 Loss: 0.0797 Test accuracy: 0.4152Epoch: 1/3 Loss: 0.0707 Test accuracy: 0.4292Epoch: 1/3 Loss: 0.0616 Test accuracy: 0.4248Epoch: 1/3 Loss: 0.0624 Test accuracy: 0.4429Epoch: 1/3 Loss: 0.0766 Test accuracy: 0.4593Epoch: 1/3 Loss: 0.0594 Test accuracy: 0.4701Epoch: 1/3 Loss: 0.0612 Test accuracy: 0.4953Epoch: 1/3 Loss: 0.0680 Test accuracy: 0.4989Epoch: 1/3 Loss: 0.0605 Test accuracy: 0.5077Epoch: 1/3 Loss: 0.0618 Test accuracy: 0.5153Epoch: 1/3 Loss: 0.0606 Test accuracy: 0.5157Epoch: 1/3 Loss: 0.0542 Test accuracy: 0.5493Epoch: 1/3 Loss: 0.0562 Test accuracy: 0.5385Epoch: 1/3 Loss: 0.0484 Test accuracy: 0.5389Epoch: 1/3 Loss: 0.0390 Test accuracy: 0.5638Epoch: 1/3 Loss: 0.0559 Test accuracy: 0.5786Epoch: 1/3 Loss: 0.0445 Test accuracy: 0.5758Epoch: 1/3 Loss: 0.0487 Test accuracy: 0.5682Epoch: 1/3 Loss: 0.0436 Test accuracy: 0.5819Epoch: 1/3 Loss: 0.0566 Test accuracy: 0.5731Epoch: 1/3 Loss: 0.0488 Test accuracy: 0.5879Epoch: 1/3 Loss: 0.0469 Test accuracy: 0.6104Epoch: 1/3 Loss: 0.0519 Test accuracy: 0.6116Epoch: 1/3 Loss: 0.0413 Test accuracy: 0.6144Epoch: 1/3 Loss: 0.0532 Test accuracy: 0.6300Epoch: 1/3 Loss: 0.0380 Test accuracy: 0.6480Epoch: 1/3 Loss: 0.0592 Test accuracy: 0.6673Epoch: 1/3 Loss: 0.0497 Test accuracy: 0.6805Epoch: 1/3 Loss: 0.0449 Test accuracy: 0.6925Epoch: 1/3 Loss: 0.0509 Test accuracy: 0.6969Epoch: 1/3 Loss: 0.0479 Test accuracy: 0.6933Epoch: 1/3 Loss: 0.0409 Test accuracy: 0.6912Epoch: 1/3 Loss: 0.0470 Test accuracy: 0.6952Epoch: 1/3 Loss: 0.0369 Test accuracy: 0.6832Epoch: 1/3 Loss: 0.0432 Test accuracy: 0.6696Epoch: 1/3 Loss: 0.0372 Test accuracy: 0.6821Epoch: 1/3 Loss: 0.0347 Test accuracy: 0.6781Epoch: 1/3 Loss: 0.0302 Test accuracy: 0.6777Epoch: 1/3 Loss: 0.0433 Test accuracy: 0.6757Epoch: 1/3 Loss: 0.0433 Test accuracy: 0.6869Epoch: 1/3 Loss: 0.0454 Test accuracy: 0.7022Epoch: 1/3 Loss: 0.0380 Test accuracy: 0.7130Epoch: 1/3 Loss: 0.0339 Test accuracy: 0.7154Epoch: 1/3 Loss: 0.0346 Test accuracy: 0.7094Epoch: 1/3 Loss: 0.0343 Test accuracy: 0.6914Epoch: 1/3 Loss: 0.0469 Test accuracy: 0.6998Epoch: 1/3 Loss: 0.0324 Test accuracy: 0.7118Epoch: 1/3 Loss: 0.0347 Test accuracy: 0.7130Epoch: 1/3 Loss: 0.0314 Test accuracy: 0.7174Epoch: 1/3 Loss: 0.0408 Test accuracy: 0.7222Epoch: 1/3 Loss: 0.0355 Test accuracy: 0.7382Epoch: 1/3 Loss: 0.0421 Test accuracy: 0.7603Epoch: 1/3 Loss: 0.0414 Test accuracy: 0.7747Epoch: 1/3 Loss: 0.0332 Test accuracy: 0.7771Epoch: 1/3 Loss: 0.0318 Test accuracy: 0.7755Epoch: 1/3 Loss: 0.0281 Test accuracy: 0.7666Epoch: 1/3 Loss: 0.0430 Test accuracy: 0.7718Epoch: 1/3 Loss: 0.0384 Test accuracy: 0.7690Epoch: 1/3 Loss: 0.0350 Test accuracy: 0.7718Epoch: 1/3 Loss: 0.0353 Test accuracy: 0.7582Epoch: 1/3 Loss: 0.0390 Test accuracy: 0.7618Epoch: 1/3 Loss: 0.0271 Test accuracy: 0.7522Epoch: 1/3 Loss: 0.0253 Test accuracy: 0.7594Epoch: 1/3 Loss: 0.0417 Test accuracy: 0.7622Epoch: 1/3 Loss: 0.0258 Test accuracy: 0.7590Epoch: 1/3 Loss: 0.0358 Test accuracy: 0.7590Epoch: 1/3 Loss: 0.0314 Test accuracy: 0.7566Epoch: 1/3 Loss: 0.0260 Test accuracy: 0.7386Epoch: 1/3 Loss: 0.0384 Test accuracy: 0.7278Epoch: 1/3 Loss: 0.0281 Test accuracy: 0.7218Epoch: 1/3 Loss: 0.0323 Test accuracy: 0.7126Epoch: 1/3 Loss: 0.0287 Test accuracy: 0.7077Epoch: 1/3 Loss: 0.0287 Test accuracy: 0.7142Epoch: 1/3 Loss: 0.0206 Test accuracy: 0.7355Epoch: 1/3 Loss: 0.0277 Test accuracy: 0.7523Epoch: 1/3 Loss: 0.0314 Test accuracy: 0.7667Epoch: 1/3 Loss: 0.0339 Test accuracy: 0.7823Epoch: 1/3 Loss: 0.0322 Test accuracy: 0.7967Epoch: 1/3 Loss: 0.0248 Test accuracy: 0.8064Epoch: 1/3 Loss: 0.0267 Test accuracy: 0.7995Epoch: 1/3 Loss: 0.0368 Test accuracy: 0.7815Epoch: 1/3 Loss: 0.0378 Test accuracy: 0.7747Epoch: 1/3 Loss: 0.0312 Test accuracy: 0.7567Epoch: 1/3 Loss: 0.0221 Test accuracy: 0.7519Epoch: 1/3 Loss: 0.0260 Test accuracy: 0.7386Epoch: 2/3 Loss: 0.0254 Test accuracy: 0.7378Epoch: 2/3 Loss: 0.0358 Test accuracy: 0.7514Epoch: 2/3 Loss: 0.0297 Test accuracy: 0.7574Epoch: 2/3 Loss: 0.0246 Test accuracy: 0.7510Epoch: 2/3 Loss: 0.0277 Test accuracy: 0.7602Epoch: 2/3 Loss: 0.0300 Test accuracy: 0.7723Epoch: 2/3 Loss: 0.0331 Test accuracy: 0.7803Epoch: 2/3 Loss: 0.0217 Test accuracy: 0.7739Epoch: 2/3 Loss: 0.0230 Test accuracy: 0.7727Epoch: 2/3 Loss: 0.0229 Test accuracy: 0.7655Epoch: 2/3 Loss: 0.0195 Test accuracy: 0.7583Epoch: 2/3 Loss: 0.0281 Test accuracy: 0.7571Epoch: 2/3 Loss: 0.0306 Test accuracy: 0.7655Epoch: 2/3 Loss: 0.0194 Test accuracy: 0.7819Epoch: 2/3 Loss: 0.0213 Test accuracy: 0.7916Epoch: 2/3 Loss: 0.0231 Test accuracy: 0.7976Epoch: 2/3 Loss: 0.0173 Test accuracy: 0.7916Epoch: 2/3 Loss: 0.0161 Test accuracy: 0.7856Epoch: 2/3 Loss: 0.0144 Test accuracy: 0.7771Epoch: 2/3 Loss: 0.0289 Test accuracy: 0.7739Epoch: 2/3 Loss: 0.0204 Test accuracy: 0.7695Epoch: 2/3 Loss: 0.0260 Test accuracy: 0.7815Epoch: 2/3 Loss: 0.0271 Test accuracy: 0.8031Epoch: 2/3 Loss: 0.0237 Test accuracy: 0.8120Epoch: 2/3 Loss: 0.0228 Test accuracy: 0.8084Epoch: 2/3 Loss: 0.0238 Test accuracy: 0.8084Epoch: 2/3 Loss: 0.0208 Test accuracy: 0.7980Epoch: 2/3 Loss: 0.0237 Test accuracy: 0.7919Epoch: 2/3 Loss: 0.0181 Test accuracy: 0.7976Epoch: 2/3 Loss: 0.0361 Test accuracy: 0.7996Epoch: 2/3 Loss: 0.0228 Test accuracy: 0.7828Epoch: 2/3 Loss: 0.0252 Test accuracy: 0.7852Epoch: 2/3 Loss: 0.0246 Test accuracy: 0.7880Epoch: 2/3 Loss: 0.0157 Test accuracy: 0.7856Epoch: 2/3 Loss: 0.0221 Test accuracy: 0.7847Epoch: 2/3 Loss: 0.0241 Test accuracy: 0.7919Epoch: 2/3 Loss: 0.0220 Test accuracy: 0.7959Epoch: 2/3 Loss: 0.0158 Test accuracy: 0.8055Epoch: 2/3 Loss: 0.0233 Test accuracy: 0.8164Epoch: 2/3 Loss: 0.0267 Test accuracy: 0.8304Epoch: 2/3 Loss: 0.0331 Test accuracy: 0.8324Epoch: 2/3 Loss: 0.0232 Test accuracy: 0.8312Epoch: 2/3 Loss: 0.0169 Test accuracy: 0.8132Epoch: 2/3 Loss: 0.0201 Test accuracy: 0.8036Epoch: 2/3 Loss: 0.0197 Test accuracy: 0.7827Epoch: 2/3 Loss: 0.0181 Test accuracy: 0.7899Epoch: 2/3 Loss: 0.0277 Test accuracy: 0.8043Epoch: 2/3 Loss: 0.0124 Test accuracy: 0.8176Epoch: 2/3 Loss: 0.0211 Test accuracy: 0.8236Epoch: 2/3 Loss: 0.0203 Test accuracy: 0.8284Epoch: 2/3 Loss: 0.0177 Test accuracy: 0.8357Epoch: 2/3 Loss: 0.0196 Test accuracy: 0.8272Epoch: 2/3 Loss: 0.0251 Test accuracy: 0.8236Epoch: 2/3 Loss: 0.0212 Test accuracy: 0.8140Epoch: 2/3 Loss: 0.0180 Test accuracy: 0.8056Epoch: 2/3 Loss: 0.0213 Test accuracy: 0.7984Epoch: 2/3 Loss: 0.0292 Test accuracy: 0.7964Epoch: 2/3 Loss: 0.0195 Test accuracy: 0.8060Epoch: 2/3 Loss: 0.0317 Test accuracy: 0.8036Epoch: 2/3 Loss: 0.0245 Test accuracy: 0.8168Epoch: 2/3 Loss: 0.0229 Test accuracy: 0.8144Epoch: 2/3 Loss: 0.0170 Test accuracy: 0.8168Epoch: 2/3 Loss: 0.0243 Test accuracy: 0.8048Epoch: 2/3 Loss: 0.0174 Test accuracy: 0.8068Epoch: 2/3 Loss: 0.0205 Test accuracy: 0.8008Epoch: 2/3 Loss: 0.0250 Test accuracy: 0.8056Epoch: 2/3 Loss: 0.0168 Test accuracy: 0.8104Epoch: 2/3 Loss: 0.0174 Test accuracy: 0.8116Epoch: 2/3 Loss: 0.0151 Test accuracy: 0.8104Epoch: 2/3 Loss: 0.0247 Test accuracy: 0.8056Epoch: 2/3 Loss: 0.0254 Test accuracy: 0.8092Epoch: 2/3 Loss: 0.0323 Test accuracy: 0.8272Epoch: 2/3 Loss: 0.0241 Test accuracy: 0.8332Epoch: 2/3 Loss: 0.0187 Test accuracy: 0.8369Epoch: 2/3 Loss: 0.0249 Test accuracy: 0.8372Epoch: 2/3 Loss: 0.0256 Test accuracy: 0.8396Epoch: 2/3 Loss: 0.0196 Test accuracy: 0.8280Epoch: 2/3 Loss: 0.0201 Test accuracy: 0.8280Epoch: 2/3 Loss: 0.0243 Test accuracy: 0.8324Epoch: 2/3 Loss: 0.0303 Test accuracy: 0.8280Epoch: 2/3 Loss: 0.0196 Test accuracy: 0.8300Epoch: 2/3 Loss: 0.0188 Test accuracy: 0.8132Epoch: 2/3 Loss: 0.0207 Test accuracy: 0.8152Epoch: 2/3 Loss: 0.0300 Test accuracy: 0.8068Epoch: 2/3 Loss: 0.0179 Test accuracy: 0.8080Epoch: 2/3 Loss: 0.0353 Test accuracy: 0.8068Epoch: 2/3 Loss: 0.0162 Test accuracy: 0.8236Epoch: 2/3 Loss: 0.0130 Test accuracy: 0.8256Epoch: 2/3 Loss: 0.0252 Test accuracy: 0.8256Epoch: 2/3 Loss: 0.0199 Test accuracy: 0.8284Epoch: 2/3 Loss: 0.0217 Test accuracy: 0.8360Epoch: 2/3 Loss: 0.0212 Test accuracy: 0.8492Epoch: 2/3 Loss: 0.0222 Test accuracy: 0.8516Epoch: 2/3 Loss: 0.0216 Test accuracy: 0.8492Epoch: 2/3 Loss: 0.0164 Test accuracy: 0.8528Epoch: 2/3 Loss: 0.0171 Test accuracy: 0.8504Epoch: 2/3 Loss: 0.0170 Test accuracy: 0.8464Epoch: 2/3 Loss: 0.0197 Test accuracy: 0.8464Epoch: 2/3 Loss: 0.0249 Test accuracy: 0.8404Epoch: 2/3 Loss: 0.0329 Test accuracy: 0.8348Epoch: 3/3 Loss: 0.0149 Test accuracy: 0.8364Epoch: 3/3 Loss: 0.0172 Test accuracy: 0.8316Epoch: 3/3 Loss: 0.0240 Test accuracy: 0.8292Epoch: 3/3 Loss: 0.0231 Test accuracy: 0.8228Epoch: 3/3 Loss: 0.0250 Test accuracy: 0.8312Epoch: 3/3 Loss: 0.0110 Test accuracy: 0.8312Epoch: 3/3 Loss: 0.0179 Test accuracy: 0.8300Epoch: 3/3 Loss: 0.0157 Test accuracy: 0.8296Epoch: 3/3 Loss: 0.0201 Test accuracy: 0.8345Epoch: 3/3 Loss: 0.0181 Test accuracy: 0.8308Epoch: 3/3 Loss: 0.0270 Test accuracy: 0.8284Epoch: 3/3 Loss: 0.0233 Test accuracy: 0.8264Epoch: 3/3 Loss: 0.0161 Test accuracy: 0.8268Epoch: 3/3 Loss: 0.0189 Test accuracy: 0.8172Epoch: 3/3 Loss: 0.0272 Test accuracy: 0.8152Epoch: 3/3 Loss: 0.0241 Test accuracy: 0.8107Epoch: 3/3 Loss: 0.0175 Test accuracy: 0.8119Epoch: 3/3 Loss: 0.0187 Test accuracy: 0.8167Epoch: 3/3 Loss: 0.0137 Test accuracy: 0.8320Epoch: 3/3 Loss: 0.0210 Test accuracy: 0.8344Epoch: 3/3 Loss: 0.0156 Test accuracy: 0.8400Epoch: 3/3 Loss: 0.0179 Test accuracy: 0.8380Epoch: 3/3 Loss: 0.0126 Test accuracy: 0.8320Epoch: 3/3 Loss: 0.0169 Test accuracy: 0.8236Epoch: 3/3 Loss: 0.0161 Test accuracy: 0.8304Epoch: 3/3 Loss: 0.0160 Test accuracy: 0.8376Epoch: 3/3 Loss: 0.0187 Test accuracy: 0.8412Epoch: 3/3 Loss: 0.0231 Test accuracy: 0.8448Epoch: 3/3 Loss: 0.0206 Test accuracy: 0.8393Epoch: 3/3 Loss: 0.0248 Test accuracy: 0.8332Epoch: 3/3 Loss: 0.0169 Test accuracy: 0.8429Epoch: 3/3 Loss: 0.0158 Test accuracy: 0.8364Epoch: 3/3 Loss: 0.0172 Test accuracy: 0.8320Epoch: 3/3 Loss: 0.0199 Test accuracy: 0.8320Epoch: 3/3 Loss: 0.0167 Test accuracy: 0.8352Epoch: 3/3 Loss: 0.0220 Test accuracy: 0.8340Epoch: 3/3 Loss: 0.0151 Test accuracy: 0.8316Epoch: 3/3 Loss: 0.0198 Test accuracy: 0.8316Epoch: 3/3 Loss: 0.0119 Test accuracy: 0.8376Epoch: 3/3 Loss: 0.0177 Test accuracy: 0.8352Epoch: 3/3 Loss: 0.0188 Test accuracy: 0.8308Epoch: 3/3 Loss: 0.0187 Test accuracy: 0.8332Epoch: 3/3 Loss: 0.0193 Test accuracy: 0.8360Epoch: 3/3 Loss: 0.0135 Test accuracy: 0.8356Epoch: 3/3 Loss: 0.0173 Test accuracy: 0.8380Epoch: 3/3 Loss: 0.0154 Test accuracy: 0.8356Epoch: 3/3 Loss: 0.0187 Test accuracy: 0.8340Epoch: 3/3 Loss: 0.0203 Test accuracy: 0.8360Epoch: 3/3 Loss: 0.0242 Test accuracy: 0.8396Epoch: 3/3 Loss: 0.0175 Test accuracy: 0.8260Epoch: 3/3 Loss: 0.0140 Test accuracy: 0.8212Epoch: 3/3 Loss: 0.0127 Test accuracy: 0.8116Epoch: 3/3 Loss: 0.0303 Test accuracy: 0.8056Epoch: 3/3 Loss: 0.0125 Test accuracy: 0.7996Epoch: 3/3 Loss: 0.0148 Test accuracy: 0.8020Epoch: 3/3 Loss: 0.0168 Test accuracy: 0.8080Epoch: 3/3 Loss: 0.0129 Test accuracy: 0.8120Epoch: 3/3 Loss: 0.0202 Test accuracy: 0.8156Epoch: 3/3 Loss: 0.0230 Test accuracy: 0.8144Epoch: 3/3 Loss: 0.0111 Test accuracy: 0.8180Epoch: 3/3 Loss: 0.0242 Test accuracy: 0.8184Epoch: 3/3 Loss: 0.0158 Test accuracy: 0.8256Epoch: 3/3 Loss: 0.0155 Test accuracy: 0.8184Epoch: 3/3 Loss: 0.0133 Test accuracy: 0.8228Epoch: 3/3 Loss: 0.0142 Test accuracy: 0.8216Epoch: 3/3 Loss: 0.0189 Test accuracy: 0.8300Epoch: 3/3 Loss: 0.0184 Test accuracy: 0.8481Epoch: 3/3 Loss: 0.0163 Test accuracy: 0.8505Epoch: 3/3 Loss: 0.0066 Test accuracy: 0.8444Epoch: 3/3 Loss: 0.0141 Test accuracy: 0.8364Epoch: 3/3 Loss: 0.0192 Test accuracy: 0.8260Epoch: 3/3 Loss: 0.0196 Test accuracy: 0.8248Epoch: 3/3 Loss: 0.0177 Test accuracy: 0.8264Epoch: 3/3 Loss: 0.0183 Test accuracy: 0.8227Epoch: 3/3 Loss: 0.0220 Test accuracy: 0.8315Epoch: 3/3 Loss: 0.0182 Test accuracy: 0.8391Epoch: 3/3 Loss: 0.0240 Test accuracy: 0.8391Epoch: 3/3 Loss: 0.0275 Test accuracy: 0.8503Epoch: 3/3 Loss: 0.0233 Test accuracy: 0.8463Epoch: 3/3 Loss: 0.0228 Test accuracy: 0.8463Epoch: 3/3 Loss: 0.0137 Test accuracy: 0.8379Epoch: 3/3 Loss: 0.0231 Test accuracy: 0.8376Epoch: 3/3 Loss: 0.0134 Test accuracy: 0.8308Epoch: 3/3 Loss: 0.0292 Test accuracy: 0.8324Epoch: 3/3 Loss: 0.0212 Test accuracy: 0.8389Epoch: 3/3 Loss: 0.0101 Test accuracy: 0.8405Epoch: 3/3 Loss: 0.0216 Test accuracy: 0.8372Epoch: 3/3 Loss: 0.0202 Test accuracy: 0.8396Epoch: 3/3 Loss: 0.0227 Test accuracy: 0.8420Epoch: 3/3 Loss: 0.0194 Test accuracy: 0.8400Epoch: 3/3 Loss: 0.0221 Test accuracy: 0.8400Epoch: 3/3 Loss: 0.0159 Test accuracy: 0.8420Epoch: 3/3 Loss: 0.0198 Test accuracy: 0.8444Epoch: 3/3 Loss: 0.0214 Test accuracy: 0.8372Epoch: 3/3 Loss: 0.0144 Test accuracy: 0.8324Epoch: 3/3 Loss: 0.0148 Test accuracy: 0.8228Epoch: 3/3 Loss: 0.0187 Test accuracy: 0.8296Epoch: 3/3 Loss: 0.0175 Test accuracy: 0.8312Epoch: 3/3 Loss: 0.0175 Test accuracy: 0.8300Epoch: 3/3 Loss: 0.0280 Test accuracy: 0.8340Epoch: 3/3 Loss: 0.0187 Test accuracy: 0.8412Epoch: 3/3 Loss: 0.0215 Test accuracy: 0.8501Epoch: 3/3 Loss: 0.0249 Test accuracy: 0.8537</code></pre><h2 id="保存检查点"><a href="#保存检查点" class="headerlink" title="保存检查点"></a>保存检查点</h2><p>训练好网络后，保存模型，以便稍后加载它并进行预测。你可能还需要保存其他内容，例如从类别到索引的映射，索引是从某个图像数据集中获取的：<code>image_datasets[&#39;train&#39;].class_to_idx</code>。你可以将其作为属性附加到模型上，这样稍后推理会更轻松。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">注意，稍后你需要完全重新构建模型，以便用模型进行推理。确保在检查点中包含你所需的任何信息。如果你想加载模型并继续训练，则需要保存周期数量和优化器状态 `optimizer.state_dict`。你可能需要在下面的下个部分使用训练的模型，因此建议立即保存它。</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">```python</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Save the checkpoint </span></span><br><span class="line"><span class="comment"># model_dict = image_datasets.class_to_idx</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">'checkpoint.pth'</span>)</span><br></pre></td></tr></table></figure><h2 id="加载检查点"><a href="#加载检查点" class="headerlink" title="加载检查点"></a>加载检查点</h2><p>此刻，建议写一个可以加载检查点并重新构建模型的函数。这样的话，你可以回到此项目并继续完善它，而不用重新训练网络。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Write a function that loads a checkpoint and rebuilds the model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_checkpoint</span><span class="params">(filepath)</span>:</span></span><br><span class="line">    model_res = torch.load(filepath)</span><br><span class="line">    model.load_state_dict(model_res)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = load_checkpoint(<span class="string">'checkpoint.pth'</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure><pre><code>VGG(  (features): Sequential(    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (1): ReLU(inplace)    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (3): ReLU(inplace)    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (6): ReLU(inplace)    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (8): ReLU(inplace)    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (11): ReLU(inplace)    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (13): ReLU(inplace)    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (15): ReLU(inplace)    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (18): ReLU(inplace)    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (20): ReLU(inplace)    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (22): ReLU(inplace)    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (25): ReLU(inplace)    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (27): ReLU(inplace)    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (29): ReLU(inplace)    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (classifier): Sequential(    (fc1): Linear(in_features=25088, out_features=1024, bias=True)    (relu): ReLU()    (fc2): Linear(in_features=1024, out_features=102, bias=True)    (output): LogSoftmax()  ))</code></pre><h1 id="类别推理"><a href="#类别推理" class="headerlink" title="类别推理"></a>类别推理</h1><p>现在，你需要写一个使用训练的网络进行推理的函数。即你将向网络中传入一个图像，并预测图像中的花卉类别。写一个叫做 <code>predict</code> 的函数，该函数会接受图像和模型，然后返回概率在前 $K$ 的类别及其概率。应该如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(image_path, model, topk=<span class="number">5</span>)</span>:</span></span><br><span class="line">    <span class="string">''' Predict the class (or classes) of an image using a trained deep learning model.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    model.eval()</span><br><span class="line">    </span><br><span class="line">    dataiter = iter(valid_dataloaders)</span><br><span class="line">    images, labels = dataiter.next()</span><br><span class="line">    outputs = model(Variable(images))</span><br><span class="line">    </span><br><span class="line">    _,predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> _,predicted</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the code to predict the class from an image file</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">image_path = data_dir + valid_dir</span><br><span class="line">probs, classes = predict(image_path, model)</span><br><span class="line">print(probs)</span><br><span class="line">print(classes)</span><br><span class="line"><span class="comment"># &gt; [ 0.01558163  0.01541934  0.01452626  0.01443549  0.01407339]</span></span><br><span class="line"><span class="comment"># &gt; ['70', '3', '45', '62', '55']</span></span><br></pre></td></tr></table></figure><pre><code>tensor([-5.6533e-01, -1.0756e+00, -3.3086e-02, -6.0575e-01, -1.6227e+00,        -1.2889e+00, -9.1778e-01, -1.0155e-01, -9.0205e-03, -2.6388e-02,        -1.1171e-01, -2.5127e-05, -8.6765e-06, -1.2372e-05, -3.3515e-03,        -4.3414e-04, -2.2719e-02, -3.1610e-02, -1.4661e-01, -6.8172e-03,        -3.0326e-03, -4.6300e-04, -5.8895e-01, -7.0555e-03, -4.4683e-03,        -1.2083e-06, -7.6587e-02, -2.2083e-05, -7.6721e-05, -5.5256e-02,        -9.8522e-01, -1.8399e-01])tensor([  0,  95,   0,  27,  99,   0,  49,   0,  94,   1,   1,   1,          2,   2,   2,   2,   2,   2,   3,   3,   3,   3,   5,   4,          4,   4,   4,   4,   4,   5,  40,   5])</code></pre><p>首先，你需要处理输入图像，使其可以用于你的网络。</p><h2 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h2><p>你需要使用 <code>PIL</code> 加载图像（<a href="https://pillow.readthedocs.io/en/latest/reference/Image.html" target="_blank" rel="noopener">文档</a>）。建议写一个函数来处理图像，使图像可以作为模型的输入。该函数应该按照训练的相同方式处理图像。</p><p>首先，调整图像大小，使最小的边为 256 像素，并保持宽高比。为此，可以使用 <a href="http://pillow.readthedocs.io/en/3.1.x/reference/Image.html#PIL.Image.Image.thumbnail" target="_blank" rel="noopener"><code>thumbnail</code></a> 或 <a href="http://pillow.readthedocs.io/en/3.1.x/reference/Image.html#PIL.Image.Image.thumbnail" target="_blank" rel="noopener"><code>resize</code></a> 方法。然后，你需要从图像的中心裁剪出 224x224 的部分。</p><p>图像的颜色通道通常编码为整数 0-255，但是该模型要求值为浮点数 0-1。你需要变换值。使用 Numpy 数组最简单，你可以从 PIL 图像中获取，例如 <code>np_image = np.array(pil_image)</code>。</p><p>和之前一样，网络要求图像按照特定的方式标准化。均值应标准化为 <code>[0.485, 0.456, 0.406]</code>，标准差应标准化为 <code>[0.229, 0.224, 0.225]</code>。你需要用每个颜色通道减去均值，然后除以标准差。</p><p>最后，PyTorch 要求颜色通道为第一个维度，但是在 PIL 图像和 Numpy 数组中是第三个维度。你可以使用 <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ndarray.transpose.html" target="_blank" rel="noopener"><code>ndarray.transpose</code></a>对维度重新排序。颜色通道必须是第一个维度，并保持另外两个维度的顺序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_image</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="string">''' Scales, crops, and normalizes a PIL image for a PyTorch model,</span></span><br><span class="line"><span class="string">        returns an Numpy array</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    valid_transforms = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">255</span>),</span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                        [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">])</span><br><span class="line">    image_datasets_valid = datasets.ImageFolder(data_dir + valid_dir, transform=valid_transforms)</span><br><span class="line">    </span><br><span class="line">    valid_dataloaders = torch.utils.data.DataLoader(image_datasets_valid, batch_size=<span class="number">32</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Process a PIL image for use in a PyTorch model</span></span><br></pre></td></tr></table></figure><p>要检查你的项目，可以使用以下函数来转换 PyTorch 张量并将其显示在  notebook 中。如果 <code>process_image</code> 函数可行，用该函数运行输出应该会返回原始图像（但是剪裁掉的部分除外）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(image, ax=None, title=None)</span>:</span></span><br><span class="line">    <span class="string">"""Imshow for Tensor."""</span></span><br><span class="line">    <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        fig, ax = plt.subplots()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># PyTorch tensors assume the color channel is the first dimension</span></span><br><span class="line">    <span class="comment"># but matplotlib assumes is the third dimension</span></span><br><span class="line">    image = image.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Undo preprocessing</span></span><br><span class="line">    mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">    std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    image = std * image + mean</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Image needs to be clipped between 0 and 1 or it looks like noise when displayed</span></span><br><span class="line">    image = np.clip(image, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    ax.imshow(image)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ax</span><br></pre></td></tr></table></figure><h2 id="类别预测"><a href="#类别预测" class="headerlink" title="类别预测"></a>类别预测</h2><p>可以获得格式正确的图像后 </p><p>要获得前 $K$ 个值，在张量中使用 <a href="http://pytorch.org/docs/master/torch.html#torch.topk" target="_blank" rel="noopener"><code>x.topk(k)</code></a>。该函数会返回前 <code>k</code> 个概率和对应的类别索引。你需要使用  <code>class_to_idx</code>（希望你将其添加到了模型中）将这些索引转换为实际类别标签，或者从用来加载数据的<a href="https://pytorch.org/docs/master/torchvision/datasets.html?highlight=imagefolder#torchvision.datasets.ImageFolder" target="_blank" rel="noopener"> <code>ImageFolder</code></a>进行转换。确保颠倒字典</p><p>同样，此方法应该接受图像路径和模型检查点，并返回概率和类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">probs, classes = predict(image_path, model)</span><br><span class="line">print(probs)</span><br><span class="line">print(classes)</span><br><span class="line">&gt; [ <span class="number">0.01558163</span>  <span class="number">0.01541934</span>  <span class="number">0.01452626</span>  <span class="number">0.01443549</span>  <span class="number">0.01407339</span>]</span><br><span class="line">&gt; [<span class="string">'70'</span>, <span class="string">'3'</span>, <span class="string">'45'</span>, <span class="string">'62'</span>, <span class="string">'55'</span>]</span><br></pre></td></tr></table></figure><h2 id="检查运行状况"><a href="#检查运行状况" class="headerlink" title="检查运行状况"></a>检查运行状况</h2><p>你已经可以使用训练的模型做出预测，现在检查模型的性能如何。即使测试准确率很高，始终有必要检查是否存在明显的错误。使用 <code>matplotlib</code> 将前 5 个类别的概率以及输入图像绘制为条形图，应该如下所示：</p><p><img src="/2019/02/16/image-classifier-project/inference_example.png" width="300px"></p><p>你可以使用 <code>cat_to_name.json</code> 文件（应该之前已经在 notebook 中加载该文件）将类别整数编码转换为实际花卉名称。要将 PyTorch 张量显示为图像，请使用定义如下的 <code>imshow</code> 函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Display an image along with the top 5 classes</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;开发-AI-应用&quot;&gt;&lt;a href=&quot;#开发-AI-应用&quot; class=&quot;headerlink&quot; title=&quot;开发 AI 应用&quot;&gt;&lt;/a&gt;开发 AI 应用&lt;/h1&gt;&lt;p&gt;未来，AI 算法在日常生活中的应用将越来越广泛。例如，你可能想要在智能手机应用中包含图像分类
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>强化学习(二) 蒙特卡洛</title>
    <link href="http://yoursite.com/2019/02/16/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B/"/>
    <id>http://yoursite.com/2019/02/16/蒙特卡洛/</id>
    <published>2019-02-16T15:21:21.000Z</published>
    <updated>2019-02-16T15:26:44.506Z</updated>
    
    <content type="html"><![CDATA[<h1 id="迷你项目：蒙特卡洛方法"><a href="#迷你项目：蒙特卡洛方法" class="headerlink" title="迷你项目：蒙特卡洛方法"></a>迷你项目：蒙特卡洛方法</h1><p>在此 notebook 中，你将自己编写很多蒙特卡洛 (MC) 算法的实现。</p><p>虽然我们提供了一些起始代码，但是你可以删掉这些提示并从头编写代码。</p><h3 id="第-0-部分：探索-BlackjackEnv"><a href="#第-0-部分：探索-BlackjackEnv" class="headerlink" title="第 0 部分：探索 BlackjackEnv"></a>第 0 部分：探索 BlackjackEnv</h3><p>请使用以下代码单元格创建 <a href="https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py" target="_blank" rel="noopener">Blackjack</a> 环境的实例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">'Blackjack-v0'</span>)</span><br></pre></td></tr></table></figure><p>每个状态都是包含以下三个元素的 3 元组：</p><ul><li>玩家的当前点数之和 $\in {0, 1, \ldots, 31}$，</li><li>庄家朝上的牌点数之和  $\in {1, \ldots, 10}$，及</li><li>玩家是否有能使用的王牌（<code>no</code> $=0$、<code>yes</code> $=1$）。</li></ul><p>智能体可以执行两个潜在动作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">STICK = <span class="number">0</span></span><br><span class="line">HIT = <span class="number">1</span></span><br></pre></td></tr></table></figure><p>通过运行以下代码单元格进行验证。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(env.observation_space)</span><br><span class="line">print(env.action_space)</span><br></pre></td></tr></table></figure><pre><code>Tuple(Discrete(32), Discrete(11), Discrete(2))Discrete(2)</code></pre><p>执行以下代码单元格以按照随机策略玩二十一点。</p><p>（<em>代码当前会玩三次二十一点——你可以随意修改该数字，或者多次运行该单元格。该单元格旨在让你体验当智能体与环境互动时返回的输出结果。</em>）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    state = env.reset()</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        print(state)</span><br><span class="line">        action = env.action_space.sample()</span><br><span class="line">        state, reward, done, info = env.step(action)</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            print(<span class="string">'End game! Reward: '</span>, reward)</span><br><span class="line">            print(<span class="string">'You won :)\n'</span>) <span class="keyword">if</span> reward &gt; <span class="number">0</span> <span class="keyword">else</span> print(<span class="string">'You lost :(\n'</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure><pre><code>(12, 8, False)End game! Reward:  -1.0You lost :((19, 10, True)(19, 10, False)(21, 10, False)End game! Reward:  1.0You won :)(17, 9, False)End game! Reward:  -1You lost :((10, 7, False)(12, 7, False)(13, 7, False)(15, 7, False)End game! Reward:  -1You lost :((21, 10, True)(21, 10, False)End game! Reward:  -1You lost :(</code></pre><h3 id="第-1-部分：MC-预测-状态值"><a href="#第-1-部分：MC-预测-状态值" class="headerlink" title="第 1 部分：MC 预测 - 状态值"></a>第 1 部分：MC 预测 - 状态值</h3><p>在此部分，你将自己编写 MC 预测的实现（用于估算状态值函数）。</p><p>我们首先将研究以下策略：如果点数之和超过 18，玩家将始终停止出牌。函数  <code>generate_episode_from_limit</code> 会根据该策略抽取一个阶段。 </p><p>该函数会接收以下<strong>输入</strong>：</p><ul><li><code>bj_env</code>：这是 OpenAI Gym 的 Blackjack 环境的实例。</li></ul><p>它会返回以下<strong>输出</strong>：</p><ul><li><code>episode</code>：这是一个（状态、动作、奖励）元组列表，对应的是 $(S<em>0, A_0, R_1, \ldots, S</em>{T-1}, A<em>{T-1}, R</em>{T})$， 其中 $T$ 是最终时间步。具体而言，<code>episode[i]</code> 返回 $(S<em>i, A_i, R</em>{i+1})$， <code>episode[i][0]</code>、<code>episode[i][1]</code>和 <code>episode[i][2]</code> 分别返回 $S<em>i$, $A_i$和 $R</em>{i+1}$。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_episode_from_limit</span><span class="params">(bj_env)</span>:</span></span><br><span class="line">    episode = []</span><br><span class="line">    state = bj_env.reset()</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        action = <span class="number">0</span> <span class="keyword">if</span> state[<span class="number">0</span>] &gt; <span class="number">18</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        next_state, reward, done, info = bj_env.step(action)</span><br><span class="line">        episode.append((state, action, reward))</span><br><span class="line">        state = next_state</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> episode</span><br></pre></td></tr></table></figure><p>执行以下代码单元格以按照该策略玩二十一点。 </p><p>（<em>代码当前会玩三次二十一点——你可以随意修改该数字，或者多次运行该单元格。该单元格旨在让你熟悉  <code>generate_episode_from_limit</code> 函数的输出结果。</em>）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    print(generate_episode_from_limit(env))</span><br></pre></td></tr></table></figure><pre><code>[((21, 6, True), 0, 1.0)][((20, 10, False), 0, 1.0)][((12, 10, False), 1, -1)]</code></pre><p>现在你已经准备好自己编写 MC 预测的实现了。你可以选择实现首次经历或所有经历 MC 预测；对于 Blackjack 环境，这两种技巧是对等的。</p><p>你的算法将有四个参数：</p><ul><li><code>env</code>：这是 OpenAI Gym 环境的实例。</li><li><code>num_episodes</code>：这是通过智能体-环境互动生成的阶段次数。</li><li><code>generate_episode</code>：这是返回互动阶段的函数。</li><li><code>gamma</code>：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：<code>1</code>。</li></ul><p>该算法会返回以下输出结果：</p><ul><li><code>V</code>：这是一个字典，其中 <code>V[s]</code> 是状态 <code>s</code> 的估算值。例如，如果代码返回以下输出结果：<br>{(4, 7, False): -0.38775510204081631, (18, 6, False): -0.58434296365330851, (13, 2, False): -0.43409090909090908, (6, 7, False): -0.3783783783783784, …<br>则状态 <code>(4, 7, False)</code> 的值估算为  <code>-0.38775510204081631</code>。</li></ul><p>如果你不知道如何在 Python 中使用 <code>defaultdict</code>，建议查看<a href="https://www.accelebrate.com/blog/using-defaultdict-python/" target="_blank" rel="noopener">此源代码</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mc_prediction_v</span><span class="params">(env, num_episodes, generate_episode, gamma=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="comment"># initialize empty dictionary of lists</span></span><br><span class="line">    returns = defaultdict(list)</span><br><span class="line">    <span class="comment"># loop over episodes</span></span><br><span class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(<span class="number">1</span>, num_episodes+<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># monitor progress</span></span><br><span class="line">        <span class="keyword">if</span> i_episode % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"\rEpisode &#123;&#125;/&#123;&#125;."</span>.format(i_episode, num_episodes), end=<span class="string">""</span>)</span><br><span class="line">            sys.stdout.flush()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">## <span class="doctag">TODO:</span> complete the function</span></span><br><span class="line">        episode = generate_episode(env)</span><br><span class="line">        states, action, rewards = zip(*episode)</span><br><span class="line">        discounts = np.array([gamma**i <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rewards)+<span class="number">1</span>)])</span><br><span class="line">        <span class="keyword">for</span> i, state <span class="keyword">in</span> enumerate(states):</span><br><span class="line">            returns[state].append(sum(rewards[i:]*discounts[:-(<span class="number">1</span>+i)]))</span><br><span class="line">    V = &#123;k: np.mean(v) <span class="keyword">for</span> k, v <span class="keyword">in</span> returns.items()&#125;</span><br><span class="line">    <span class="keyword">return</span> V</span><br></pre></td></tr></table></figure><p>使用以下单元格计算并绘制状态值函数估算值。 (<em>用于绘制值函数的代码来自<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/lib/plotting.py" target="_blank" rel="noopener">此源代码</a>，并且稍作了修改。</em>）</p><p>要检查你的实现是否正确，应将以下图与解决方案 notebook <strong>Monte_Carlo_Solution.ipynb</strong> 中的对应图进行比较。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> plot_utils <span class="keyword">import</span> plot_blackjack_values</span><br><span class="line"></span><br><span class="line"><span class="comment"># obtain the value function</span></span><br><span class="line">V = mc_prediction_v(env, <span class="number">500000</span>, generate_episode_from_limit)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the value function</span></span><br><span class="line">plot_blackjack_values(V)</span><br></pre></td></tr></table></figure><pre><code>Episode 500000/500000.&lt;matplotlib.figure.Figure at 0x7f88aaf03e80&gt;</code></pre><h3 id="第-2-部分：MC-预测-动作值"><a href="#第-2-部分：MC-预测-动作值" class="headerlink" title="第 2 部分：MC 预测 - 动作值"></a>第 2 部分：MC 预测 - 动作值</h3><p>在此部分，你将自己编写 MC 预测的实现（用于估算动作值函数）。  </p><p>我们首先将研究以下策略：如果点数之和超过 18，玩家将<em>几乎</em>始终停止出牌。具体而言，如果点数之和大于 18，她选择动作 <code>STICK</code> 的概率是 80%；如果点数之和不大于 18，她选择动作  <code>HIT</code> 的概率是 80%。函数 <code>generate_episode_from_limit_stochastic</code> 会根据该策略抽取一个阶段。 </p><p>该函数会接收以下<strong>输入</strong>：</p><ul><li><code>bj_env</code>：这是 OpenAI Gym 的 Blackjack 环境的实例。</li></ul><p>该算法会返回以下<strong>输出结果</strong>：</p><ul><li><code>episode</code>: 这是一个（状态、动作、奖励）元组列表，对应的是 $(S<em>0, A_0, R_1, \ldots, S</em>{T-1}, A<em>{T-1}, R</em>{T})$， 其中 $T$ 是最终时间步。具体而言，<code>episode[i]</code> 返回 $(S<em>i, A_i, R</em>{i+1})$， <code>episode[i][0]</code>、<code>episode[i][1]</code>和 <code>episode[i][2]</code> 分别返回 $S<em>i$, $A_i$和 $R</em>{i+1}$。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_episode_from_limit_stochastic</span><span class="params">(bj_env)</span>:</span></span><br><span class="line">    episode = []</span><br><span class="line">    state = bj_env.reset()</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        probs = [<span class="number">0.8</span>, <span class="number">0.2</span>] <span class="keyword">if</span> state[<span class="number">0</span>] &gt; <span class="number">18</span> <span class="keyword">else</span> [<span class="number">0.2</span>, <span class="number">0.8</span>]</span><br><span class="line">        action = np.random.choice(np.arange(<span class="number">2</span>), p=probs)</span><br><span class="line">        next_state, reward, done, info = bj_env.step(action)</span><br><span class="line">        episode.append((state, action, reward))</span><br><span class="line">        state = next_state</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> episode</span><br></pre></td></tr></table></figure><p>现在你已经准备好自己编写 MC 预测的实现了。你可以选择实现首次经历或所有经历 MC 预测；对于 Blackjack 环境，这两种技巧是对等的。</p><p>你的算法将有四个参数：</p><ul><li><code>env</code>: 这是 OpenAI Gym 环境的实例。</li><li><code>num_episodes</code>：这是通过智能体-环境互动生成的阶段次数。</li><li><code>generate_episode</code>：这是返回互动阶段的函数。</li><li><code>gamma</code>：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：<code>1</code>。</li></ul><p>该算法会返回以下输出结果：</p><ul><li><code>Q</code>：这是一个字典（一维数组），其中 <code>Q[s][a]</code> 是状态 <code>s</code> 和动作 <code>a</code> 对应的估算动作值。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mc_prediction_q</span><span class="params">(env, num_episodes, generate_episode, gamma=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="comment"># initialize empty dictionaries of arrays</span></span><br><span class="line">    returns_sum = defaultdict(<span class="keyword">lambda</span>: np.zeros(env.action_space.n))</span><br><span class="line">    N = defaultdict(<span class="keyword">lambda</span>: np.zeros(env.action_space.n))</span><br><span class="line">    Q = defaultdict(<span class="keyword">lambda</span>: np.zeros(env.action_space.n))</span><br><span class="line">    <span class="comment"># loop over episodes</span></span><br><span class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(<span class="number">1</span>, num_episodes+<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># monitor progress</span></span><br><span class="line">        <span class="keyword">if</span> i_episode % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"\rEpisode &#123;&#125;/&#123;&#125;."</span>.format(i_episode, num_episodes), end=<span class="string">""</span>)</span><br><span class="line">            sys.stdout.flush()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">## <span class="doctag">TODO:</span> complete the function</span></span><br><span class="line">        episode = generate_episode(env)</span><br><span class="line">        states, actions, rewards = zip(*episode)</span><br><span class="line">        discounts = np.array([gamma**i <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rewards)+<span class="number">1</span>)])</span><br><span class="line">        <span class="keyword">for</span> i, state <span class="keyword">in</span> enumerate(states):</span><br><span class="line">            returns_sum[state][actions[i]] += sum(rewards[i:]*discounts[:-(<span class="number">1</span>+i)])</span><br><span class="line">            N[state][actions[i]] += <span class="number">1</span></span><br><span class="line">            Q[state][actions[i]] = returns_sum[state][actions[i]] / N[state][actions[i]]</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> Q</span><br></pre></td></tr></table></figure><p>请使用以下单元格获取动作值函数估值 $Q$。我们还绘制了相应的状态值函数。</p><p>要检查你的实现是否正确，应将以下图与解决方案 notebook <strong>Monte_Carlo_Solution.ipynb</strong> 中的对应图进行比较。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># obtain the action-value function</span></span><br><span class="line">Q = mc_prediction_q(env, <span class="number">500000</span>, generate_episode_from_limit_stochastic)</span><br><span class="line"></span><br><span class="line"><span class="comment"># obtain the state-value function</span></span><br><span class="line">V_to_plot = dict((k,(k[<span class="number">0</span>]&gt;<span class="number">18</span>)*(np.dot([<span class="number">0.8</span>, <span class="number">0.2</span>],v)) + (k[<span class="number">0</span>]&lt;=<span class="number">18</span>)*(np.dot([<span class="number">0.2</span>, <span class="number">0.8</span>],v))) \</span><br><span class="line">         <span class="keyword">for</span> k, v <span class="keyword">in</span> Q.items())</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the state-value function</span></span><br><span class="line">plot_blackjack_values(V_to_plot)</span><br></pre></td></tr></table></figure><pre><code>Episode 500000/500000.</code></pre><p><img src="/2019/02/16/蒙特卡洛/output_23_1.png" alt="png"></p><h3 id="第-3-部分：MC-控制-GLIE"><a href="#第-3-部分：MC-控制-GLIE" class="headerlink" title="第 3 部分：MC 控制 - GLIE"></a>第 3 部分：MC 控制 - GLIE</h3><p>在此部分，你将自己编写常量-$\alpha$ MC 控制的实现。</p><p>你的算法将有四个参数：</p><ul><li><code>env</code>: 这是 OpenAI Gym 环境的实例。</li><li><code>num_episodes</code>：这是通过智能体-环境互动生成的阶段次数。</li><li><code>generate_episode</code>：这是返回互动阶段的函数。</li><li><code>gamma</code>：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：<code>1</code>。</li></ul><p>该算法会返回以下输出结果：</p><ul><li><p><code>Q</code>：这是一个字典（一维数组），其中 <code>Q[s][a]</code> 是状态 <code>s</code> 和动作 <code>a</code> 对应的估算动作值。</p></li><li><p><code>policy</code>：这是一个字典，其中 <code>policy[s]</code> 会返回智能体在观察状态 <code>s</code> 之后选择的动作。</p></li></ul><p>（<em>你可以随意定义其他函数，以帮助你整理代码。</em>）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_episode_from_Q</span><span class="params">(env, Q, epsilon, nA)</span>:</span></span><br><span class="line">    episode = []</span><br><span class="line">    state = env.reset()</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        action = np.random.choice(np.arange(nA), p=get_probs(Q[state], epsilon, nA)) \</span><br><span class="line">                                    <span class="keyword">if</span> state <span class="keyword">in</span> Q <span class="keyword">else</span> env.action_space.sample()</span><br><span class="line">        next_state, reward, done, info = env.step(action)</span><br><span class="line">        episode.append((state, action, reward))</span><br><span class="line">        state = next_state</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> episode</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_probs</span><span class="params">(Q_s, epsilon, nA)</span>:</span></span><br><span class="line">    <span class="string">""" obtains the action probabilities corresponding to epsilon-greedy policy """</span></span><br><span class="line">    policy_s = np.ones(nA) * epsilon / nA</span><br><span class="line">    best_a = np.argmax(Q_s)</span><br><span class="line">    policy_s[best_a] = <span class="number">1</span> - epsilon + (epsilon / nA)</span><br><span class="line">    <span class="keyword">return</span> policy_s</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_Q_GLIE</span><span class="params">(env, episode, Q, N, gamma)</span>:</span></span><br><span class="line">    <span class="string">""" updates the action-value function estimate using the most recent episode """</span></span><br><span class="line">    states, actions, rewards = zip(*episode)</span><br><span class="line">    <span class="comment"># prepare for discounting</span></span><br><span class="line">    discounts = np.array([gamma**i <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rewards)+<span class="number">1</span>)])</span><br><span class="line">    <span class="keyword">for</span> i, state <span class="keyword">in</span> enumerate(states):</span><br><span class="line">        old_Q = Q[state][actions[i]] </span><br><span class="line">        old_N = N[state][actions[i]]</span><br><span class="line">        Q[state][actions[i]] = old_Q + (sum(rewards[i:]*discounts[:-(<span class="number">1</span>+i)]) - old_Q)/(old_N+<span class="number">1</span>)</span><br><span class="line">        N[state][actions[i]] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> Q, N</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mc_control_GLIE</span><span class="params">(env, num_episodes, gamma=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    nA = env.action_space.n</span><br><span class="line">    <span class="comment"># initialize empty dictionaries of arrays</span></span><br><span class="line">    Q = defaultdict(<span class="keyword">lambda</span>: np.zeros(nA))</span><br><span class="line">    N = defaultdict(<span class="keyword">lambda</span>: np.zeros(nA))</span><br><span class="line">    <span class="comment"># loop over episodes</span></span><br><span class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(<span class="number">1</span>, num_episodes+<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># monitor progress</span></span><br><span class="line">        <span class="keyword">if</span> i_episode % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"\rEpisode &#123;&#125;/&#123;&#125;."</span>.format(i_episode, num_episodes), end=<span class="string">""</span>)</span><br><span class="line">            sys.stdout.flush()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">## <span class="doctag">TODO:</span> complete the function</span></span><br><span class="line">        epsilon = <span class="number">1.0</span>/((i_episode/<span class="number">8000</span>)+<span class="number">1</span>)</span><br><span class="line">        episode = generate_episode_from_Q(env, Q, epsilon, nA)</span><br><span class="line">        Q, N = update_Q_GLIE(env, episode, Q, N, gamma)</span><br><span class="line">    <span class="comment"># determine the policy corresponding to the final action-value function estimate</span></span><br><span class="line">    policy = dict((k,np.argmax(v)) <span class="keyword">for</span> k, v <span class="keyword">in</span> Q.items())</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> policy, Q</span><br></pre></td></tr></table></figure><p>通过以下单元格获取估算的最优策略和动作值函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># obtain the estimated optimal policy and action-value function</span></span><br><span class="line">policy_glie, Q_glie = mc_control_GLIE(env, <span class="number">500000</span>)</span><br></pre></td></tr></table></figure><pre><code>Episode 500000/500000.</code></pre><p>接着，我们将绘制相应的状态值函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># obtain the state-value function</span></span><br><span class="line">V_glie = dict((k,np.max(v)) <span class="keyword">for</span> k, v <span class="keyword">in</span> Q_glie.items())</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the state-value function</span></span><br><span class="line">plot_blackjack_values(V_glie)</span><br></pre></td></tr></table></figure><p><img src="/2019/02/16/蒙特卡洛/output_30_0.png" alt="png"></p><p>最后，我们将可视化估算为最优策略的策略。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> plot_utils <span class="keyword">import</span> plot_policy</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the policy</span></span><br><span class="line">plot_policy(policy_glie)</span><br></pre></td></tr></table></figure><p><img src="/2019/02/16/蒙特卡洛/output_32_0.png" alt="png"></p><p><strong>真</strong>最优策略 $\pi_*$ 可以在该<a href="http://go.udacity.com/rl-textbook" target="_blank" rel="noopener">教科书</a>的第 82 页找到（下文也提供了）。请将你的最终估算值与最优策略进行比较——它们能够有多接近？如果你对算法的效果不满意，请花时间调整 $\epsilon$ 的衰减率和/或使该算法运行更多个阶段，以获得更好的结果。</p><p><img src="/2019/02/16/蒙特卡洛/optimal.png" alt="True Optimal Policy"></p><h3 id="第-4-部分：MC-控制-常量-alpha"><a href="#第-4-部分：MC-控制-常量-alpha" class="headerlink" title="第 4 部分：MC 控制 - 常量-$\alpha$"></a>第 4 部分：MC 控制 - 常量-$\alpha$</h3><p>在此部分，你将自己编写常量-$\alpha$ MC 控制的实现。  </p><p>你的算法将有三个参数：</p><ul><li><code>env</code>: 这是 OpenAI Gym 环境的实例。</li><li><code>num_episodes</code>：这是通过智能体-环境互动生成的阶段次数。</li><li><code>generate_episode</code>：这是返回互动阶段的函数。</li><li><code>alpha</code>：这是更新步骤的步长参数。</li><li><code>gamma</code>：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：<code>1</code>。</li></ul><p>该算法会返回以下输出结果：</p><ul><li><code>Q</code>：这是一个字典（一维数组），其中 <code>Q[s][a]</code> 是状态 <code>s</code> 和动作 <code>a</code> 对应的估算动作值。</li></ul><ul><li><code>policy</code>：这是一个字典，其中 <code>policy[s]</code> 会返回智能体在观察状态 <code>s</code> 之后选择的动作。</li></ul><p>（<em>你可以随意定义其他函数，以帮助你整理代码。</em>）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mc_control_alpha</span><span class="params">(env, num_episodes, alpha, gamma=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    nA = env.action_space.n</span><br><span class="line">    <span class="comment"># initialize empty dictionary of arrays</span></span><br><span class="line">    Q = defaultdict(<span class="keyword">lambda</span>: np.zeros(nA))</span><br><span class="line">    <span class="comment"># loop over episodes</span></span><br><span class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(<span class="number">1</span>, num_episodes+<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># monitor progress</span></span><br><span class="line">        <span class="keyword">if</span> i_episode % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"\rEpisode &#123;&#125;/&#123;&#125;."</span>.format(i_episode, num_episodes), end=<span class="string">""</span>)</span><br><span class="line">            sys.stdout.flush()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">## <span class="doctag">TODO:</span> complete the function</span></span><br><span class="line">        epsilon = <span class="number">1.0</span>/((i_episode/<span class="number">8000</span>)+<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># generate an episode by following epsilon-greedy policy</span></span><br><span class="line">        episode = generate_episode_from_Q(env, Q, epsilon, nA)</span><br><span class="line">        states, actions, rewards = zip(*episode)</span><br><span class="line">        <span class="comment"># prepare for discounting</span></span><br><span class="line">        discounts = np.array([gamma**i <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rewards)+<span class="number">1</span>)])</span><br><span class="line">        <span class="keyword">for</span> i, state <span class="keyword">in</span> enumerate(states):</span><br><span class="line">            old_Q = Q[state][actions[i]] </span><br><span class="line">            Q[state][actions[i]] = old_Q + alpha*(sum(rewards[i:]*discounts[:-(<span class="number">1</span>+i)]) - old_Q)</span><br><span class="line">    policy = dict((k,np.argmax(v)) <span class="keyword">for</span> k, v <span class="keyword">in</span> Q.items())</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> policy, Q</span><br></pre></td></tr></table></figure><p>通过以下单元格获得估算的最优策略和动作值函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># obtain the estimated optimal policy and action-value function</span></span><br><span class="line">policy_alpha, Q_alpha = mc_control_alpha(env, <span class="number">500000</span>, <span class="number">0.008</span>)</span><br></pre></td></tr></table></figure><pre><code>Episode 500000/500000.</code></pre><p>接着，我们将绘制相应的状态值函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># obtain the state-value function</span></span><br><span class="line">V_alpha = dict((k,np.max(v)) <span class="keyword">for</span> k, v <span class="keyword">in</span> Q_alpha.items())</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the state-value function</span></span><br><span class="line">plot_blackjack_values(V_alpha)</span><br></pre></td></tr></table></figure><p><img src="/2019/02/16/蒙特卡洛/output_38_0.png" alt="png"></p><p>最后，我们将可视化估算为最优策略的策略。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot the policy</span></span><br><span class="line">plot_policy(policy_alpha)</span><br></pre></td></tr></table></figure><p><img src="/2019/02/16/蒙特卡洛/output_40_0.png" alt="png"></p><p><strong>真</strong>最优策略 $\pi_*$ 可以在该<a href="http://go.udacity.com/rl-textbook" target="_blank" rel="noopener">教科书</a>的第 82 页找到（下文也提供了）。请将你的最终估算值与最优策略进行比较——它们能够有多接近？如果你对算法的效果不满意，请花时间调整 $\epsilon$ 的衰减率和/或使该算法运行更多个阶段，以获得更好的结果。</p><p><img src="/2019/02/16/蒙特卡洛/optimal.png" alt="True Optimal Policy"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;迷你项目：蒙特卡洛方法&quot;&gt;&lt;a href=&quot;#迷你项目：蒙特卡洛方法&quot; class=&quot;headerlink&quot; title=&quot;迷你项目：蒙特卡洛方法&quot;&gt;&lt;/a&gt;迷你项目：蒙特卡洛方法&lt;/h1&gt;&lt;p&gt;在此 notebook 中，你将自己编写很多蒙特卡洛 (MC) 算法
      
    
    </summary>
    
      <category term="强化学习" scheme="http://yoursite.com/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Reinforcement learning" scheme="http://yoursite.com/tags/Reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>强化学习(一) 动态规划</title>
    <link href="http://yoursite.com/2019/02/16/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    <id>http://yoursite.com/2019/02/16/动态规划/</id>
    <published>2019-02-16T11:19:24.000Z</published>
    <updated>2019-02-16T15:19:10.252Z</updated>
    
    <content type="html"><![CDATA[<h1 id="迷你项目：动态规划"><a href="#迷你项目：动态规划" class="headerlink" title="迷你项目：动态规划"></a>迷你项目：动态规划</h1><p>在此 notebook 中，你将自己编写很多经典动态规划算法的实现。</p><p>虽然我们提供了一些起始代码，但是你可以删掉这些提示并从头编写代码。</p><h3 id="第-0-部分：探索-FrozenLakeEnv"><a href="#第-0-部分：探索-FrozenLakeEnv" class="headerlink" title="第 0 部分：探索 FrozenLakeEnv"></a>第 0 部分：探索 FrozenLakeEnv</h3><p>请使用以下代码单元格创建 <a href="https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py" target="_blank" rel="noopener">FrozenLake</a> 环境的实例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> frozenlake <span class="keyword">import</span> FrozenLakeEnv</span><br><span class="line"></span><br><span class="line">env = FrozenLakeEnv()</span><br></pre></td></tr></table></figure><p>智能体将会在 $4 \times 4$ 网格世界中移动，状态编号如下所示：</p><p>[[ 0  1  2  3]<br> [ 4  5  6  7]<br> [ 8  9 10 11]<br> [12 13 14 15]]</p><p>智能体可以执行 4 个潜在动作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">LEFT = <span class="number">0</span></span><br><span class="line">DOWN = <span class="number">1</span></span><br><span class="line">RIGHT = <span class="number">2</span></span><br><span class="line">UP = <span class="number">3</span></span><br></pre></td></tr></table></figure><p>因此，$\mathcal{S}^+ = {0, 1, \ldots, 15}$ 以及 $\mathcal{A} = {0, 1, 2, 3}$。请通过运行以下代码单元格验证这一点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print the state space and action space</span></span><br><span class="line">print(env.observation_space)</span><br><span class="line">print(env.action_space)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print the total number of states and actions</span></span><br><span class="line">print(env.nS)</span><br><span class="line">print(env.nA)</span><br></pre></td></tr></table></figure><pre><code>Discrete(16)Discrete(4)164</code></pre><p>动态规划假设智能体完全了解 MDP。我们已经修改了 <code>frozenlake.py</code> 文件以使智能体能够访问一步动态特性。 </p><p>请执行以下代码单元格以返回特定状态和动作对应的一步动态特性。具体而言，当智能体在网格世界中以状态 1 向左移动时，<code>env.P[1][0]</code> 会返回每个潜在奖励的概率和下一个状态。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.P[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>[(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]</code></pre><p>每个条目的格式如下所示</p><p>prob, next_state, reward, done</p><p>其中：</p><ul><li><code>prob</code> 详细说明了相应的  (<code>next_state</code>, <code>reward</code>) 对的条件概率，以及</li><li>如果 <code>next_state</code> 是终止状态，则 <code>done</code> 是 <code>True</code> ，否则是 <code>False</code>。</li></ul><p>因此，我们可以按照以下方式解析 <code>env.P[1][0]</code>：</p><script type="math/tex; mode=display">\mathbb{P}(S_{t+1}=s',R_{t+1}=r|S_t=1,A_t=0) = \begin{cases}               \frac{1}{3} \text{ if } s'=1, r=0\\               \frac{1}{3} \text{ if } s'=0, r=0\\               \frac{1}{3} \text{ if } s'=5, r=0\\               0 \text{ else}            \end{cases}</script><p>你可以随意更改上述代码单元格，以探索在其他（状态、动作）对下环境的行为是怎样的。</p><h3 id="第-1-部分：迭代策略评估"><a href="#第-1-部分：迭代策略评估" class="headerlink" title="第 1 部分：迭代策略评估"></a>第 1 部分：迭代策略评估</h3><p>在此部分，你将自己编写迭代策略评估的实现。</p><p>你的算法应该有四个<strong>输入</strong>参数：</p><ul><li><code>env</code>：这是 OpenAI Gym 环境的实例，其中 <code>env.P</code> 会返回一步动态特性。</li><li><code>policy</code>：这是一个二维 numpy 数组，其中 <code>policy.shape[0]</code> 等于状态数量 (<code>env.nS</code>) ， <code>policy.shape[1]</code> 等于动作数量 (<code>env.nA</code>) 。<code>policy[s][a]</code>  返回智能体在状态 <code>s</code> 时根据该策略选择动作 <code>a</code> 的概率。</li><li><code>gamma</code>：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：<code>1</code>。</li><li><code>theta</code>：这是一个非常小的正数，用于判断估算值是否足够地收敛于真值函数 (默认值为：<code>1e-8</code>）。</li></ul><p>该算法会返回以下<strong>输出结果</strong>：</p><ul><li><code>V</code>：这是一个一维numpy数组，其中 <code>V.shape[0]</code> 等于状态数量 (<code>env.nS</code>)。<code>V[s]</code> 包含状态 <code>s</code> 在输入策略下的估算值。</li></ul><p>请完成以下代码单元格中的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">policy_evaluation</span><span class="params">(env, policy, gamma=<span class="number">1</span>, theta=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    V = np.zeros(env.nS)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## <span class="doctag">TODO:</span> complete the function</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        delta = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> range(env.nS):</span><br><span class="line">            Vs = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> a, action_prob <span class="keyword">in</span> enumerate(policy[s]):</span><br><span class="line">                <span class="keyword">for</span> prob, next_state, reward, done <span class="keyword">in</span> env.P[s][a]:</span><br><span class="line">                    Vs += action_prob * prob  * (reward + gamma * V[next_state])</span><br><span class="line">            delta = max(delta, np.abs(V[s]-Vs))</span><br><span class="line">            V[s] = Vs</span><br><span class="line">        <span class="keyword">if</span> delta &lt; theta:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> V</span><br></pre></td></tr></table></figure><p>我们将评估等概率随机策略  $\pi$，其中对于所有 $s\in\mathcal{S}$ 和 $a\in\mathcal{A}(s)$ ，$\pi(a|s) = \frac{1}{|\mathcal{A}(s)|}$。  </p><p>请使用以下代码单元格在变量 <code>random_policy</code>中指定该策略。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">random_policy = np.ones([env.nS, env.nA]) / env.nA</span><br></pre></td></tr></table></figure><p>运行下个代码单元格以评估等概率随机策略并可视化输出结果。状态值函数已调整形状，以匹配网格世界的形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> plot_utils <span class="keyword">import</span> plot_values</span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate the policy </span></span><br><span class="line">V = policy_evaluation(env, random_policy)</span><br><span class="line"></span><br><span class="line">plot_values(V)</span><br></pre></td></tr></table></figure><p><img src="/2019/02/16/动态规划/output_17_0.png" alt="png"></p><p>运行以下代码单元格以测试你的函数。如果代码单元格返回 <strong>PASSED</strong>，则表明你正确地实现了该函数！ </p><p><strong>注意：</strong>为了确保结果准确，确保你的 <code>policy_evaluation</code> 函数满足上文列出的要求（具有四个输入、一个输出，并且没有更改输入参数的默认值）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> check_test</span><br><span class="line"></span><br><span class="line">check_test.run_check(<span class="string">'policy_evaluation_check'</span>, policy_evaluation)</span><br></pre></td></tr></table></figure><p><strong><span style="color: green;">PASSED</span></strong></p><h3 id="第-2-部分：通过-v-pi-获取-q-pi"><a href="#第-2-部分：通过-v-pi-获取-q-pi" class="headerlink" title="第 2 部分：通过 $v\pi$ 获取 $q\pi$"></a>第 2 部分：通过 $v<em>\pi$ 获取 $q</em>\pi$</h3><p>在此部分，你将编写一个函数，该函数的输入是状态值函数估值以及一些状态 $s\in\mathcal{S}$。它会返回输入状态 $s\in\mathcal{S}$ 对应的<strong>动作值函数中的行</strong>。即你的函数应同时接受输入 $v<em>\pi$ 和 $s$，并针对所有 $a\in\mathcal{A}(s)$ 返回 $q</em>\pi(s,a)$。</p><p>你的算法应该有四个<strong>输入</strong>参数：</p><ul><li><code>env</code>：这是 OpenAI Gym 环境的实例，其中 <code>env.P</code> 会返回一步动态特性。</li><li><code>V</code>：这是一个一维 numpy 数组，其中 <code>V.shape[0]</code> 等于状态数量 (<code>env.nS</code>)。<code>V[s]</code> 包含状态 <code>s</code> 的估值。</li><li><code>s</code>：这是环境中的状态对应的整数。它应该是在 <code>0</code> 到 <code>(env.nS)-1</code>（含）之间的值。</li><li><code>gamma</code>：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：<code>1</code>。</li></ul><p>该算法会返回以下<strong>输出结果</strong>：</p><ul><li><code>q</code>：这是一个一维 numpy 数组，其中 <code>q.shape[0]</code> 等于动作数量 (<code>env.nA</code>)。<code>q[a]</code> 包含状态 <code>s</code> 和动作 <code>a</code> 的（估算）值。</li></ul><p>请完成以下代码单元格中的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_from_v</span><span class="params">(env, V, s, gamma=<span class="number">1</span>)</span>:</span></span><br><span class="line">    q = np.zeros(env.nA)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## <span class="doctag">TODO:</span> complete the function</span></span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> range(env.nA):</span><br><span class="line">        <span class="keyword">for</span> prob, next_state, reward, done <span class="keyword">in</span>  env.P[s][a]:</span><br><span class="line">            q[a] += prob * (reward + gamma * V[next_state])</span><br><span class="line">    <span class="keyword">return</span> q</span><br></pre></td></tr></table></figure><p>请运行以下代码单元格以输出上述状态值函数对应的动作值函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Q = np.zeros([env.nS, env.nA])</span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> range(env.nS):</span><br><span class="line">    Q[s] = q_from_v(env, V, s)</span><br><span class="line">print(<span class="string">"Action-Value Function:"</span>)</span><br><span class="line">print(Q)</span><br></pre></td></tr></table></figure><pre><code>Action-Value Function:[[ 0.0147094   0.01393978  0.01393978  0.01317015] [ 0.00852356  0.01163091  0.0108613   0.01550788] [ 0.02444514  0.02095298  0.02406033  0.01435346] [ 0.01047649  0.01047649  0.00698432  0.01396865] [ 0.02166487  0.01701828  0.01624865  0.01006281] [ 0.          0.          0.          0.        ] [ 0.05433538  0.04735105  0.05433538  0.00698432] [ 0.          0.          0.          0.        ] [ 0.01701828  0.04099204  0.03480619  0.04640826] [ 0.07020885  0.11755991  0.10595784  0.05895312] [ 0.18940421  0.17582037  0.16001424  0.04297382] [ 0.          0.          0.          0.        ] [ 0.          0.          0.          0.        ] [ 0.08799677  0.20503718  0.23442716  0.17582037] [ 0.25238823  0.53837051  0.52711478  0.43929118] [ 0.          0.          0.          0.        ]]</code></pre><p>运行以下代码单元格以测试你的函数。如果代码单元格返回 <strong>PASSED</strong>，则表明你正确地实现了该函数！ </p><p><strong>注意：</strong>为了确保结果准确，确保 <code>q_from_v</code> 函数满足上文列出的要求（具有四个输入、一个输出，并且没有更改输入参数的默认值）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">check_test.run_check(<span class="string">'q_from_v_check'</span>, q_from_v)</span><br></pre></td></tr></table></figure><p><strong><span style="color: green;">PASSED</span></strong></p><h3 id="第-3-部分：策略改进"><a href="#第-3-部分：策略改进" class="headerlink" title="第 3 部分：策略改进"></a>第 3 部分：策略改进</h3><p>在此部分，你将自己编写策略改进实现。 </p><p>你的算法应该有三个<strong>输入</strong>参数：</p><ul><li><code>env</code>：这是 OpenAI Gym 环境的实例，其中 <code>env.P</code> 会返回一步动态特性。</li><li><code>V</code>：这是一个一维 numpy 数组，其中 <code>V.shape[0]</code> 等于状态数量 (<code>env.nS</code>)。<code>V[s]</code> 包含状态 <code>s</code> 的估值。</li><li><code>gamma</code>：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：<code>1</code>。</li></ul><p>该算法会返回以下<strong>输出结果</strong>：</p><ul><li><code>policy</code>：这是一个二维 numpy 数组，其中 <code>policy.shape[0]</code> 等于状态数量 (<code>env.nS</code>) ， <code>policy.shape[1]</code> 等于动作数量 (<code>env.nA</code>) 。<code>policy[s][a]</code>  返回智能体在状态 <code>s</code> 时根据该策略选择动作 <code>a</code> 的概率。</li></ul><p>请完成以下代码单元格中的函数。建议你使用你在上文实现的 <code>q_from_v</code> 函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">policy_improvement</span><span class="params">(env, V, gamma=<span class="number">1</span>)</span>:</span></span><br><span class="line">    policy = np.zeros([env.nS, env.nA]) / env.nA</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## <span class="doctag">TODO:</span> complete the function</span></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> range(env.nS):</span><br><span class="line">        q = q_from_v(env, V, s, gamma)</span><br><span class="line">        </span><br><span class="line"><span class="comment">#         best_a = np.argwhere(q==np.max(q)).flatten()</span></span><br><span class="line"><span class="comment">#         policy[s] = np.sum([np.eye(env.nA)[i] for i in best_a], axis=0)/len(best_a)</span></span><br><span class="line">        policy[s][np.argmax(q)] = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> policy</span><br></pre></td></tr></table></figure><p>运行以下代码单元格以测试你的函数。如果代码单元格返回 <strong>PASSED</strong>，则表明你正确地实现了该函数！ </p><p><strong>注意：</strong>为了确保结果准确，确保 <code>policy_improvement</code> 函数满足上文列出的要求（具有三个输入、一个输出，并且没有更改输入参数的默认值）。</p><p>在继续转到该 notebook 的下个部分之前，强烈建议你参阅 <strong>Dynamic_Programming_Solution.ipynb</strong> 中的解决方案。该函数有很多正确的实现方式！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">check_test.run_check(<span class="string">'policy_improvement_check'</span>, policy_improvement)</span><br></pre></td></tr></table></figure><p><strong><span style="color: green;">PASSED</span></strong></p><h3 id="第-4-部分：策略迭代"><a href="#第-4-部分：策略迭代" class="headerlink" title="第 4 部分：策略迭代"></a>第 4 部分：策略迭代</h3><p>在此部分，你将自己编写策略迭代的实现。该算法会返回最优策略，以及相应的状态值函数。</p><p>你的算法应该有三个<strong>输入</strong>参数：</p><ul><li><code>env</code>：这是 OpenAI Gym 环境的实例，其中 <code>env.P</code> 会返回一步动态特性。</li><li><code>gamma</code>：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：<code>1</code>。</li><li><code>theta</code>：这是一个非常小的正数，用于判断策略评估步骤是否足够地收敛于真值函数 (默认值为：<code>1e-8</code>）。</li></ul><p>该算法会返回以下<strong>输出结果</strong>：</p><ul><li><code>policy</code>：这是一个二维 numpy 数组，其中 <code>policy.shape[0]</code> 等于状态数量 (<code>env.nS</code>) ， <code>policy.shape[1]</code> 等于动作数量 (<code>env.nA</code>) 。<code>policy[s][a]</code>  返回智能体在状态 <code>s</code> 时根据该策略选择动作 <code>a</code> 的概率。</li><li><code>V</code>：这是一个一维 numpy 数组，其中 <code>V.shape[0]</code> 等于状态数量 (<code>env.nS</code>)。<code>V[s]</code> 包含状态 <code>s</code> 的估值。</li></ul><p>请完成以下代码单元格中的函数。强烈建议你使用你在上文实现的 <code>policy_evaluation</code> 和 <code>policy_improvement</code> 函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">policy_iteration</span><span class="params">(env, gamma=<span class="number">1</span>, theta=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    policy = np.ones([env.nS, env.nA]) / env.nA</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        V = policy_evaluation(env, policy, gamma, theta)</span><br><span class="line">        new_policy = policy_improvement(env, V)</span><br><span class="line">    </span><br><span class="line">        <span class="comment">## <span class="doctag">TODO:</span> complete the function</span></span><br><span class="line">        <span class="keyword">if</span> np.max(abs(policy_evaluation(env, policy) - policy_evaluation(env, new_policy))) &lt; theta*<span class="number">1e2</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">        policy = copy.copy(new_policy)</span><br><span class="line">    <span class="keyword">return</span> policy, V</span><br></pre></td></tr></table></figure><p>运行下个代码单元格以解决该 MDP 并可视化输出结果。最优状态值函数已调整形状，以匹配网格世界的形状。</p><p><strong>将该最优状态值函数与此 notebook 第 1 部分的状态值函数进行比较</strong>。<em>最优状态值函数一直都大于或等于等概率随机策略的状态值函数吗？</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># obtain the optimal policy and optimal state-value function</span></span><br><span class="line">policy_pi, V_pi = policy_iteration(env)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print the optimal policy</span></span><br><span class="line">print(<span class="string">"\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):"</span>)</span><br><span class="line">print(policy_pi,<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">plot_values(V_pi)</span><br></pre></td></tr></table></figure><pre><code>Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):[[ 1.  0.  0.  0.] [ 0.  0.  0.  1.] [ 0.  0.  0.  1.] [ 0.  0.  0.  1.] [ 1.  0.  0.  0.] [ 1.  0.  0.  0.] [ 1.  0.  0.  0.] [ 1.  0.  0.  0.] [ 0.  0.  0.  1.] [ 0.  1.  0.  0.] [ 1.  0.  0.  0.] [ 1.  0.  0.  0.] [ 1.  0.  0.  0.] [ 0.  0.  1.  0.] [ 0.  1.  0.  0.] [ 1.  0.  0.  0.]] </code></pre><p><img src="/2019/02/16/动态规划/output_33_1.png" alt="png"></p><p>运行以下代码单元格以测试你的函数。如果代码单元格返回 <strong>PASSED</strong>，则表明你正确地实现了该函数！ </p><p><strong>注意：</strong>为了确保结果准确，确保 <code>policy_iteratio</code> 函数满足上文列出的要求（具有三个输入、两个输出，并且没有更改输入参数的默认值）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">check_test.run_check(<span class="string">'policy_iteration_check'</span>, policy_iteration)</span><br></pre></td></tr></table></figure><p><strong><span style="color: green;">PASSED</span></strong></p><h3 id="第-5-部分：截断策略迭代"><a href="#第-5-部分：截断策略迭代" class="headerlink" title="第 5 部分：截断策略迭代"></a>第 5 部分：截断策略迭代</h3><p>在此部分，你将自己编写截断策略迭代的实现。  </p><p>首先，你将实现截断策略评估。你的算法应该有五个<strong>输入</strong>参数：</p><ul><li><code>env</code>：这是 OpenAI Gym 环境的实例，其中 <code>env.P</code> 会返回一步动态特性。</li><li><code>policy</code>：这是一个二维 numpy 数组，其中 <code>policy.shape[0]</code> 等于状态数量 (<code>env.nS</code>) ， <code>policy.shape[1]</code> 等于动作数量 (<code>env.nA</code>) 。<code>policy[s][a]</code>  返回智能体在状态 <code>s</code> 时根据该策略选择动作 <code>a</code> 的概率。</li><li><code>V</code>：这是一个一维 numpy 数组，其中 <code>V.shape[0]</code> 等于状态数量 (<code>env.nS</code>)。<code>V[s]</code> 包含状态 <code>s</code> 的估值。</li><li><code>max_it</code>：这是一个正整数，对应的是经历状态空间的次数（默认值为：<code>1</code>）。</li><li><code>gamma</code>：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：<code>1</code>。</li></ul><p>该算法会返回以下<strong>输出结果</strong>：</p><ul><li><code>V</code>：这是一个一维 numpy 数组，其中 <code>V.shape[0]</code> 等于状态数量 (<code>env.nS</code>)。<code>V[s]</code> 包含状态 <code>s</code> 的估值。</li></ul><p>请完成以下代码单元格中的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">truncated_policy_evaluation</span><span class="params">(env, policy, V, max_it=<span class="number">1</span>, gamma=<span class="number">1</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">## <span class="doctag">TODO:</span> complete the function</span></span><br><span class="line">    counter = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> counter &lt; max_it:</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> range(env.nS):</span><br><span class="line">            v = <span class="number">0</span></span><br><span class="line">            q = q_from_v(env, V, s, gamma)</span><br><span class="line">            <span class="keyword">for</span> a, action_prob <span class="keyword">in</span> enumerate(policy[s]):</span><br><span class="line">                v += action_prob * q[a]</span><br><span class="line">            V[s] = v</span><br><span class="line">        counter += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> V</span><br></pre></td></tr></table></figure><p>接着，你将实现截断策略迭代。你的算法应该接受四个<strong>输入</strong>参数：</p><ul><li><code>env</code>：这是 OpenAI Gym 环境的实例，其中 <code>env.P</code> 会返回一步动态特性。</li><li><code>max_it</code>：这是一个正整数，对应的是经历状态空间的次数（默认值为：<code>1</code>）。</li><li><code>gamma</code>：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：<code>1</code>。</li><li><code>theta</code>：这是一个非常小的正整数，用作停止条件（默认值为：<code>1e-8</code>）。</li></ul><p>该算法会返回以下<strong>输出结果</strong>：</p><ul><li><code>policy</code>：这是一个二维 numpy 数组，其中 <code>policy.shape[0]</code> 等于状态数量 (<code>env.nS</code>) ， <code>policy.shape[1]</code> 等于动作数量 (<code>env.nA</code>) 。<code>policy[s][a]</code>  返回智能体在状态 <code>s</code> 时根据该策略选择动作 <code>a</code> 的概率。</li><li><code>V</code>：这是一个一维 numpy 数组，其中 <code>V.shape[0]</code> 等于状态数量 (<code>env.nS</code>)。<code>V[s]</code> 包含状态 <code>s</code> 的估值。</li></ul><p>请完成以下代码单元格中的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">truncated_policy_iteration</span><span class="params">(env, max_it=<span class="number">1</span>, gamma=<span class="number">1</span>, theta=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    V = np.zeros(env.nS)</span><br><span class="line">    policy = np.zeros([env.nS, env.nA]) / env.nA</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## <span class="doctag">TODO:</span> complete the function</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        policy = policy_improvement(env, V)</span><br><span class="line">        old_V = copy.copy(V)</span><br><span class="line">        V = policy_evaluation(env, policy, gamma=<span class="number">1</span>, theta=<span class="number">1e-8</span>)</span><br><span class="line">        <span class="keyword">if</span> max(abs(V - old_V)) &lt; theta:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> policy, V</span><br></pre></td></tr></table></figure><p>运行下个代码单元格以解决该 MDP 并可视化输出结果。状态值函数已调整形状，以匹配网格世界的形状。</p><p>请实验不同的 <code>max_it</code> 参数值。始终都能获得最优状态值函数吗？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">policy_tpi, V_tpi = truncated_policy_iteration(env, max_it=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print the optimal policy</span></span><br><span class="line">print(<span class="string">"\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):"</span>)</span><br><span class="line">print(policy_tpi,<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the optimal state-value function</span></span><br><span class="line">plot_values(V_tpi)</span><br></pre></td></tr></table></figure><pre><code>Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):[[ 1.  0.  0.  0.] [ 0.  0.  0.  1.] [ 0.  0.  0.  1.] [ 0.  0.  0.  1.] [ 1.  0.  0.  0.] [ 1.  0.  0.  0.] [ 1.  0.  0.  0.] [ 1.  0.  0.  0.] [ 0.  0.  0.  1.] [ 0.  1.  0.  0.] [ 1.  0.  0.  0.] [ 1.  0.  0.  0.] [ 1.  0.  0.  0.] [ 0.  0.  1.  0.] [ 0.  1.  0.  0.] [ 1.  0.  0.  0.]] </code></pre><p><img src="/2019/02/16/动态规划/output_41_1.png" alt="png"></p><p>运行以下代码单元格以测试你的函数。如果代码单元格返回 <strong>PASSED</strong>，则表明你正确地实现了该函数！ </p><p><strong>注意：</strong>为了确保结果准确，确保 <code>truncated_policy_iteration</code> 函数满足上文列出的要求（具有四个输入、两个输出，并且没有更改输入参数的默认值）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">check_test.run_check(<span class="string">'truncated_policy_iteration_check'</span>, truncated_policy_iteration)</span><br></pre></td></tr></table></figure><p><strong><span style="color: green;">PASSED</span></strong></p><h3 id="第-6-部分：值迭代"><a href="#第-6-部分：值迭代" class="headerlink" title="第 6 部分：值迭代"></a>第 6 部分：值迭代</h3><p>在此部分，你将自己编写值迭代的实现。</p><p>你的算法应该接受三个输入参数：</p><ul><li><code>env</code>：这是 OpenAI Gym 环境的实例，其中 <code>env.P</code> 会返回一步动态特性。</li><li><code>gamma</code>：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：<code>1</code>。 </li><li><code>theta</code>：这是一个非常小的正整数，用作停止条件（默认值为：<code>1e-8</code>）。</li></ul><p>该算法会返回以下<strong>输出结果</strong>：</p><ul><li><code>policy</code>：这是一个二维 numpy 数组，其中 <code>policy.shape[0]</code> 等于状态数量 (<code>env.nS</code>) ， <code>policy.shape[1]</code> 等于动作数量 (<code>env.nA</code>) 。<code>policy[s][a]</code>  返回智能体在状态 <code>s</code> 时根据该策略选择动作 <code>a</code> 的概率。</li><li><code>V</code>：这是一个一维 numpy 数组，其中 <code>V.shape[0]</code> 等于状态数量 (<code>env.nS</code>)。<code>V[s]</code> 包含状态 <code>s</code> 的估值。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">value_iteration</span><span class="params">(env, gamma=<span class="number">1</span>, theta=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    V = np.zeros(env.nS)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## <span class="doctag">TODO:</span> complete the function</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        delta = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> range(env.nS):</span><br><span class="line">            v = copy.copy(V[s])</span><br><span class="line">            V[s] = max(q_from_v(env, V, s, gamma))</span><br><span class="line">            delta = max(delta, abs(v-V[s]))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> delta &lt; theta:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    policy = policy_improvement(env, V, gamma)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> policy, V</span><br></pre></td></tr></table></figure><p>运行下个代码单元格以解决该 MDP 并可视化输出结果。状态值函数已调整形状，以匹配网格世界的形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">policy_vi, V_vi = value_iteration(env)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print the optimal policy</span></span><br><span class="line">print(<span class="string">"\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):"</span>)</span><br><span class="line">print(policy_vi,<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the optimal state-value function</span></span><br><span class="line">plot_values(V_vi)</span><br></pre></td></tr></table></figure><pre><code>Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):[[ 1.  0.  0.  0.] [ 0.  0.  0.  1.] [ 0.  0.  0.  1.] [ 0.  0.  0.  1.] [ 1.  0.  0.  0.] [ 1.  0.  0.  0.] [ 1.  0.  0.  0.] [ 1.  0.  0.  0.] [ 0.  0.  0.  1.] [ 0.  1.  0.  0.] [ 1.  0.  0.  0.] [ 1.  0.  0.  0.] [ 1.  0.  0.  0.] [ 0.  0.  1.  0.] [ 0.  1.  0.  0.] [ 1.  0.  0.  0.]] </code></pre><p><img src="/2019/02/16/动态规划/output_47_1.png" alt="png"></p><p>运行以下代码单元格以测试你的函数。如果代码单元格返回 <strong>PASSED</strong>，则表明你正确地实现了该函数！ </p><p><strong>注意：</strong>为了确保结果准确，确保 <code>truncated_policy_iteration</code> 函数满足上文列出的要求（具有三个输入、两个输出，并且没有更改输入参数的默认值）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">check_test.run_check(<span class="string">'value_iteration_check'</span>, value_iteration)</span><br></pre></td></tr></table></figure><p><strong><span style="color: green;">PASSED</span></strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;迷你项目：动态规划&quot;&gt;&lt;a href=&quot;#迷你项目：动态规划&quot; class=&quot;headerlink&quot; title=&quot;迷你项目：动态规划&quot;&gt;&lt;/a&gt;迷你项目：动态规划&lt;/h1&gt;&lt;p&gt;在此 notebook 中，你将自己编写很多经典动态规划算法的实现。&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
      <category term="强化学习" scheme="http://yoursite.com/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Reinforcement learning" scheme="http://yoursite.com/tags/Reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Customer Setments</title>
    <link href="http://yoursite.com/2019/02/16/Customer-Setments/"/>
    <id>http://yoursite.com/2019/02/16/Customer-Setments/</id>
    <published>2019-02-16T11:16:05.000Z</published>
    <updated>2019-02-16T11:16:49.399Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习纳米学位"><a href="#机器学习纳米学位" class="headerlink" title="机器学习纳米学位"></a>机器学习纳米学位</h1><h2 id="非监督学习"><a href="#非监督学习" class="headerlink" title="非监督学习"></a>非监督学习</h2><h2 id="项目-3-创建用户分类"><a href="#项目-3-创建用户分类" class="headerlink" title="项目 3: 创建用户分类"></a>项目 3: 创建用户分类</h2><p>欢迎来到机器学习工程师纳米学位的第三个项目！在这个 notebook 文件中，有些模板代码已经提供给你，但你还需要实现更多的功能来完成这个项目。除非有明确要求，你无须修改任何已给出的代码。以<strong>‘练习’</strong>开始的标题表示接下来的代码部分中有你必须要实现的功能。每一部分都会有详细的指导，需要实现的部分也会在注释中以 <strong>‘TODO’</strong> 标出。请仔细阅读所有的提示！</p><p>除了实现代码外，你还<strong>必须</strong>回答一些与项目和你的实现有关的问题。每一个需要你回答的问题都会以<strong>‘问题 X’</strong>为标题。请仔细阅读每个问题，并且在问题后的<strong>‘回答’</strong>文字框中写出完整的答案。我们将根据你对问题的回答和撰写代码所实现的功能来对你提交的项目进行评分。</p><blockquote><p><strong>提示：</strong>Code 和 Markdown 区域可通过 <strong>Shift + Enter</strong> 快捷键运行。此外，Markdown 可以通过双击进入编辑模式。</p></blockquote><h2 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h2><p>在这个项目中，你将分析一个数据集的内在结构，这个数据集包含很多客户真对不同类型产品的年度采购额（用<strong>金额</strong>表示）。这个项目的任务之一是如何最好地描述一个批发商不同种类顾客之间的差异。这样做将能够使得批发商能够更好的组织他们的物流服务以满足每个客户的需求。</p><p>这个项目的数据集能够在<a href="https://archive.ics.uci.edu/ml/datasets/Wholesale+customers" target="_blank" rel="noopener">UCI机器学习信息库</a>中找到.因为这个项目的目的，分析将不会包括 ‘Channel’ 和 ‘Region’ 这两个特征——重点集中在6个记录的客户购买的产品类别上。</p><p>运行下面的的代码单元以载入整个客户数据集和一些这个项目需要的 Python 库。如果你的数据集载入成功，你将看到后面输出数据集的大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查你的Python版本</span></span><br><span class="line"><span class="keyword">from</span> sys <span class="keyword">import</span> version_info</span><br><span class="line"><span class="keyword">if</span> version_info.major != <span class="number">3</span>:</span><br><span class="line">    <span class="keyword">raise</span> Exception(<span class="string">'请使用Python 3.x 来完成此项目'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入这个项目需要的库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> visuals <span class="keyword">as</span> vs</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display <span class="comment"># 使得我们可以对DataFrame使用display()函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置以内联的形式显示matplotlib绘制的图片（在notebook中显示更美观）</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment"># 高分辨率显示</span></span><br><span class="line">%config InlineBackend.figure_format=<span class="string">'retina'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入整个客户数据集</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    data = pd.read_csv(<span class="string">"customers.csv"</span>)</span><br><span class="line">    data.drop([<span class="string">'Region'</span>, <span class="string">'Channel'</span>], axis = <span class="number">1</span>, inplace = <span class="keyword">True</span>)</span><br><span class="line">    print(<span class="string">"Wholesale customers dataset has &#123;&#125; samples with &#123;&#125; features each."</span>.format(*data.shape))</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">"Dataset could not be loaded. Is the dataset missing?"</span>)</span><br></pre></td></tr></table></figure><pre><code>Wholesale customers dataset has 440 samples with 6 features each.</code></pre><h2 id="分析数据"><a href="#分析数据" class="headerlink" title="分析数据"></a>分析数据</h2><p>在这部分，你将开始分析数据，通过可视化和代码来理解每一个特征和其他特征的联系。你会看到关于数据集的统计描述，考虑每一个属性的相关性，然后从数据集中选择若干个样本数据点，你将在整个项目中一直跟踪研究这几个数据点。</p><p>运行下面的代码单元给出数据集的一个统计描述。注意这个数据集包含了6个重要的产品类型：<strong>‘Fresh’</strong>, <strong>‘Milk’</strong>, <strong>‘Grocery’</strong>, <strong>‘Frozen’</strong>, <strong>‘Detergents_Paper’</strong>和 <strong>‘Delicatessen’</strong>。想一下这里每一个类型代表你会购买什么样的产品。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示数据集的一个描述</span></span><br><span class="line">display(data.describe())</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Fresh</th>      <th>Milk</th>      <th>Grocery</th>      <th>Frozen</th>      <th>Detergents_Paper</th>      <th>Delicatessen</th>    </tr>  </thead>  <tbody>    <tr>      <th>count</th>      <td>440.000000</td>      <td>440.000000</td>      <td>440.000000</td>      <td>440.000000</td>      <td>440.000000</td>      <td>440.000000</td>    </tr>    <tr>      <th>mean</th>      <td>12000.297727</td>      <td>5796.265909</td>      <td>7951.277273</td>      <td>3071.931818</td>      <td>2881.493182</td>      <td>1524.870455</td>    </tr>    <tr>      <th>std</th>      <td>12647.328865</td>      <td>7380.377175</td>      <td>9503.162829</td>      <td>4854.673333</td>      <td>4767.854448</td>      <td>2820.105937</td>    </tr>    <tr>      <th>min</th>      <td>3.000000</td>      <td>55.000000</td>      <td>3.000000</td>      <td>25.000000</td>      <td>3.000000</td>      <td>3.000000</td>    </tr>    <tr>      <th>25%</th>      <td>3127.750000</td>      <td>1533.000000</td>      <td>2153.000000</td>      <td>742.250000</td>      <td>256.750000</td>      <td>408.250000</td>    </tr>    <tr>      <th>50%</th>      <td>8504.000000</td>      <td>3627.000000</td>      <td>4755.500000</td>      <td>1526.000000</td>      <td>816.500000</td>      <td>965.500000</td>    </tr>    <tr>      <th>75%</th>      <td>16933.750000</td>      <td>7190.250000</td>      <td>10655.750000</td>      <td>3554.250000</td>      <td>3922.000000</td>      <td>1820.250000</td>    </tr>    <tr>      <th>max</th>      <td>112151.000000</td>      <td>73498.000000</td>      <td>92780.000000</td>      <td>60869.000000</td>      <td>40827.000000</td>      <td>47943.000000</td>    </tr>  </tbody></table></div><h3 id="练习-选择样本"><a href="#练习-选择样本" class="headerlink" title="练习: 选择样本"></a>练习: 选择样本</h3><p>为了对客户有一个更好的了解，并且了解代表他们的数据将会在这个分析过程中如何变换。最好是选择几个样本数据点，并且更为详细地分析它们。在下面的代码单元中，选择<strong>三个</strong>索引加入到索引列表<code>indices</code>中，这三个索引代表你要追踪的客户。我们建议你不断尝试，直到找到三个明显不同的客户。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO：从数据集中选择三个你希望抽样的数据点的索引</span></span><br><span class="line">indices = [<span class="number">1</span>, <span class="number">14</span>, <span class="number">168</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为选择的样本建立一个DataFrame</span></span><br><span class="line">samples = pd.DataFrame(data.loc[indices], columns = data.keys()).reset_index(drop = <span class="keyword">True</span>)</span><br><span class="line">print(<span class="string">"Chosen samples of wholesale customers dataset:"</span>)</span><br><span class="line">display(samples)</span><br></pre></td></tr></table></figure><pre><code>Chosen samples of wholesale customers dataset:</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Fresh</th>      <th>Milk</th>      <th>Grocery</th>      <th>Frozen</th>      <th>Detergents_Paper</th>      <th>Delicatessen</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>7057</td>      <td>9810</td>      <td>9568</td>      <td>1762</td>      <td>3293</td>      <td>1776</td>    </tr>    <tr>      <th>1</th>      <td>24653</td>      <td>9465</td>      <td>12091</td>      <td>294</td>      <td>5058</td>      <td>2168</td>    </tr>    <tr>      <th>2</th>      <td>5809</td>      <td>735</td>      <td>803</td>      <td>1393</td>      <td>79</td>      <td>429</td>    </tr>  </tbody></table></div><h3 id="问题-1"><a href="#问题-1" class="headerlink" title="问题 1"></a>问题 1</h3><p>在你看来你选择的这三个样本点分别代表什么类型的企业（客户）？对每一个你选择的样本客户，通过它在每一种产品类型上的花费与数据集的统计描述进行比较，给出你做上述判断的理由。</p><p><strong>提示：</strong> 企业的类型包括超市、咖啡馆、零售商以及其他。注意不要使用具体企业的名字，比如说在描述一个餐饮业客户时，你不能使用麦当劳。</p><p><strong>回答:第一个可能是咖啡厅 Milk 和 Grocery的购买需求高于平均值 第二个可能是餐厅 Fresh Milk Grocery Detergents_Paper Delicatessen都高于平均值 第三个可能是生鲜超市 Fresh 和 Frozen的需求要大些</strong></p><h3 id="练习-特征相关性"><a href="#练习-特征相关性" class="headerlink" title="练习: 特征相关性"></a>练习: 特征相关性</h3><p>一个有趣的想法是，考虑这六个类别中的一个（或者多个）产品类别，是否对于理解客户的购买行为具有实际的相关性。也就是说，当用户购买了一定数量的某一类产品，我们是否能够确定他们必然会成比例地购买另一种类的产品。有一个简单的方法可以检测相关性：我们用移除了某一个特征之后的数据集来构建一个监督学习（回归）模型，然后用这个模型去预测那个被移除的特征，再对这个预测结果进行评分，看看预测结果如何。</p><p>在下面的代码单元中，你需要实现以下的功能：</p><ul><li>使用 <code>DataFrame.drop</code> 函数移除数据集中你选择的不需要的特征，并将移除后的结果赋值给 <code>new_data</code> 。</li><li>使用 <code>sklearn.model_selection.train_test_split</code> 将数据集分割成训练集和测试集。<ul><li>使用移除的特征作为你的目标标签。设置 <code>test_size</code> 为 <code>0.25</code> 并设置一个 <code>random_state</code> 。</li></ul></li></ul><ul><li>导入一个 DecisionTreeRegressor （决策树回归器），设置一个 <code>random_state</code>，然后用训练集训练它。</li><li>使用回归器的 <code>score</code> 函数输出模型在测试集上的预测得分。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO：为DataFrame创建一个副本，用'drop'函数丢弃一个特征# TODO： </span></span><br><span class="line">new_data = data.drop(<span class="string">'Detergents_Paper'</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：使用给定的特征作为目标，将数据分割成训练集和测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(new_data, data[<span class="string">'Detergents_Paper'</span>], test_size=<span class="number">0.25</span>, random_state=<span class="number">40</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：创建一个DecisionTreeRegressor（决策树回归器）并在训练集上训练它</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line">regressor = DecisionTreeRegressor(random_state=<span class="number">0</span>)</span><br><span class="line">regressor.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># TODO：输出在测试集上的预测得分</span></span><br><span class="line">score = regressor.score(X_test, y_test)</span><br><span class="line">print(score)</span><br></pre></td></tr></table></figure><pre><code>0.7879942418323117</code></pre><h3 id="问题-2"><a href="#问题-2" class="headerlink" title="问题 2"></a>问题 2</h3><p>你尝试预测哪一个特征？预测的得分是多少？这个特征对于区分用户的消费习惯来说必要吗？为什么？<br><strong>提示：</strong> 决定系数（coefficient of determination），$R^2$ 结果在0到1之间，1表示完美拟合，一个负的 $R^2$ 表示模型不能够拟合数据。</p><p><strong>回答:尝试预测Detergents_Paper 得分为0.787 不必要 该特征与其他特征有一定相关性 决定系数较高 可通过其他特征预测</strong></p><h3 id="可视化特征分布"><a href="#可视化特征分布" class="headerlink" title="可视化特征分布"></a>可视化特征分布</h3><p>为了能够对这个数据集有一个更好的理解，我们可以对数据集中的每一个产品特征构建一个散布矩阵（scatter matrix）。如果你发现你在上面尝试预测的特征对于区分一个特定的用户来说是必须的，那么这个特征和其它的特征可能不会在下面的散射矩阵中显示任何关系。相反的，如果你认为这个特征对于识别一个特定的客户是没有作用的，那么通过散布矩阵可以看出在这个数据特征和其它特征中有关联性。运行下面的代码以创建一个散布矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于数据中的每一对特征构造一个散布矩阵</span></span><br><span class="line">pd.scatter_matrix(data, alpha = <span class="number">0.3</span>, figsize = (<span class="number">14</span>,<span class="number">8</span>), diagonal = <span class="string">'kde'</span>);</span><br></pre></td></tr></table></figure><pre><code>C:\ProgramData\Anaconda3\lib\site-packages\ipykernel_launcher.py:2: FutureWarning: pandas.scatter_matrix is deprecated, use pandas.plotting.scatter_matrix instead</code></pre><p><img src="/2019/02/16/Customer-Setments/output_17_1.png" alt="png"></p><h3 id="问题-3"><a href="#问题-3" class="headerlink" title="问题 3"></a>问题 3</h3><p>这里是否存在一些特征他们彼此之间存在一定程度相关性？如果有请列出。这个结果是验证了还是否认了你尝试预测的那个特征的相关性？这些特征的数据是怎么分布的？</p><p><strong>提示：</strong> 这些数据是正态分布（normally distributed）的吗？大多数的数据点分布在哪？</p><p><strong>回答:特征之间存在一定相关性 在矩阵对角线出现正偏态分布</strong></p><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>在这个部分，你将通过在数据上做一个合适的缩放，并检测异常点（你可以选择性移除）将数据预处理成一个更好的代表客户的形式。预处理数据是保证你在分析中能够得到显著且有意义的结果的重要环节。</p><h3 id="练习-特征缩放"><a href="#练习-特征缩放" class="headerlink" title="练习: 特征缩放"></a>练习: 特征缩放</h3><p>如果数据不是正态分布的，尤其是数据的平均数和中位数相差很大的时候（表示数据非常歪斜）。这时候通常用一个<a href="https://github.com/czcbangkai/translations/blob/master/use_of_logarithms_in_economics/use_of_logarithms_in_economics.pdf" target="_blank" rel="noopener">非线性的缩放</a>是很合适的，<a href="http://econbrowser.com/archives/2014/02/use-of-logarithms-in-economics" target="_blank" rel="noopener">（英文原文）</a> — 尤其是对于金融数据。一种实现这个缩放的方法是使用 <a href="http://scipy.github.io/devdocs/generated/scipy.stats.boxcox.html" target="_blank" rel="noopener">Box-Cox 变换</a>，这个方法能够计算出能够最佳减小数据倾斜的指数变换方法。一个比较简单的并且在大多数情况下都适用的方法是使用自然对数。</p><p>在下面的代码单元中，你将需要实现以下功能：</p><ul><li>使用 <code>np.log</code> 函数在数据 <code>data</code> 上做一个对数缩放，然后将它的副本（不改变原始data的值）赋值给 <code>log_data</code>。 </li><li>使用 <code>np.log</code> 函数在样本数据 <code>samples</code> 上做一个对数缩放，然后将它的副本赋值给 <code>log_samples</code>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO：使用自然对数缩放数据</span></span><br><span class="line">log_data = np.log(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：使用自然对数缩放样本数据</span></span><br><span class="line">log_samples = np.log(samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为每一对新产生的特征制作一个散射矩阵</span></span><br><span class="line">pd.scatter_matrix(log_data, alpha = <span class="number">0.3</span>, figsize = (<span class="number">14</span>,<span class="number">8</span>), diagonal = <span class="string">'kde'</span>);</span><br></pre></td></tr></table></figure><pre><code>C:\ProgramData\Anaconda3\lib\site-packages\ipykernel_launcher.py:8: FutureWarning: pandas.scatter_matrix is deprecated, use pandas.plotting.scatter_matrix instead</code></pre><p><img src="/2019/02/16/Customer-Setments/output_22_1.png" alt="png"></p><h3 id="观察"><a href="#观察" class="headerlink" title="观察"></a>观察</h3><p>在使用了一个自然对数的缩放之后，数据的各个特征会显得更加的正态分布。对于任意的你以前发现有相关关系的特征对，观察他们的相关关系是否还是存在的（并且尝试观察，他们的相关关系相比原来是变强了还是变弱了）。</p><p>运行下面的代码以观察样本数据在进行了自然对数转换之后如何改变了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 展示经过对数变换后的样本数据</span></span><br><span class="line">display(log_samples)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Fresh</th>      <th>Milk</th>      <th>Grocery</th>      <th>Frozen</th>      <th>Detergents_Paper</th>      <th>Delicatessen</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>8.861775</td>      <td>9.191158</td>      <td>9.166179</td>      <td>7.474205</td>      <td>8.099554</td>      <td>7.482119</td>    </tr>    <tr>      <th>1</th>      <td>10.112654</td>      <td>9.155356</td>      <td>9.400217</td>      <td>5.683580</td>      <td>8.528726</td>      <td>7.681560</td>    </tr>    <tr>      <th>2</th>      <td>8.667164</td>      <td>6.599870</td>      <td>6.688355</td>      <td>7.239215</td>      <td>4.369448</td>      <td>6.061457</td>    </tr>  </tbody></table></div><h3 id="练习-异常值检测"><a href="#练习-异常值检测" class="headerlink" title="练习: 异常值检测"></a>练习: 异常值检测</h3><p>对于任何的分析，在数据预处理的过程中检测数据中的异常值都是非常重要的一步。异常值的出现会使得把这些值考虑进去后结果出现倾斜。这里有很多关于怎样定义什么是数据集中的异常值的经验法则。这里我们将使用<a href="http://datapigtechnologies.com/blog/index.php/highlighting-outliers-in-your-data-with-the-tukey-method/" target="_blank" rel="noopener"> Tukey 的定义异常值的方法</a>：一个异常阶（outlier step）被定义成1.5倍的四分位距（interquartile range，IQR）。一个数据点如果某个特征包含在该特征的 IQR 之外的特征，那么该数据点被认定为异常点。</p><p>在下面的代码单元中，你需要完成下面的功能：</p><ul><li>将指定特征的 25th 分位点的值分配给 <code>Q1</code> 。使用 <code>np.percentile</code> 来完成这个功能。</li><li>将指定特征的 75th 分位点的值分配给 <code>Q3</code> 。同样的，使用 <code>np.percentile</code> 来完成这个功能。</li><li>将指定特征的异常阶的计算结果赋值给 <code>step</code>。</li><li>选择性地通过将索引添加到 <code>outliers</code> 列表中，以移除异常值。</li></ul><p><strong>注意：</strong> 如果你选择移除异常值，请保证你选择的样本点不在这些移除的点当中！<br>一旦你完成了这些功能，数据集将存储在 <code>good_data</code> 中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于每一个特征，找到值异常高或者是异常低的数据点</span></span><br><span class="line"><span class="keyword">for</span> feature <span class="keyword">in</span> log_data.keys():</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> 计算给定特征的Q1（数据的25th分位点）</span></span><br><span class="line">    Q1 = np.percentile(log_data[feature], <span class="number">25</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> 计算给定特征的Q3（数据的75th分位点）</span></span><br><span class="line">    Q3 = np.percentile(log_data[feature], <span class="number">75</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> 使用四分位范围计算异常阶（1.5倍的四分位距）</span></span><br><span class="line">    step = (Q3 - Q1) * <span class="number">1.5</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 显示异常点</span></span><br><span class="line">    print(<span class="string">"Data points considered outliers for the feature '&#123;&#125;':"</span>.format(feature))</span><br><span class="line">    display(log_data[~((log_data[feature] &gt;= Q1 - step) &amp; (log_data[feature] &lt;= Q3 + step))])</span><br><span class="line">    </span><br><span class="line"><span class="comment"># TODO(可选): 选择你希望移除的数据点的索引</span></span><br><span class="line">outliers  = [<span class="number">65</span>,<span class="number">66</span>,<span class="number">75</span>,<span class="number">128</span>,<span class="number">154</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下代码会移除outliers中索引的数据点, 并储存在good_data中</span></span><br><span class="line">good_data = log_data.drop(log_data.index[outliers]).reset_index(drop = <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>Data points considered outliers for the feature &#39;Fresh&#39;:</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Fresh</th>      <th>Milk</th>      <th>Grocery</th>      <th>Frozen</th>      <th>Detergents_Paper</th>      <th>Delicatessen</th>    </tr>  </thead>  <tbody>    <tr>      <th>65</th>      <td>4.442651</td>      <td>9.950323</td>      <td>10.732651</td>      <td>3.583519</td>      <td>10.095388</td>      <td>7.260523</td>    </tr>    <tr>      <th>66</th>      <td>2.197225</td>      <td>7.335634</td>      <td>8.911530</td>      <td>5.164786</td>      <td>8.151333</td>      <td>3.295837</td>    </tr>    <tr>      <th>81</th>      <td>5.389072</td>      <td>9.163249</td>      <td>9.575192</td>      <td>5.645447</td>      <td>8.964184</td>      <td>5.049856</td>    </tr>    <tr>      <th>95</th>      <td>1.098612</td>      <td>7.979339</td>      <td>8.740657</td>      <td>6.086775</td>      <td>5.407172</td>      <td>6.563856</td>    </tr>    <tr>      <th>96</th>      <td>3.135494</td>      <td>7.869402</td>      <td>9.001839</td>      <td>4.976734</td>      <td>8.262043</td>      <td>5.379897</td>    </tr>    <tr>      <th>128</th>      <td>4.941642</td>      <td>9.087834</td>      <td>8.248791</td>      <td>4.955827</td>      <td>6.967909</td>      <td>1.098612</td>    </tr>    <tr>      <th>171</th>      <td>5.298317</td>      <td>10.160530</td>      <td>9.894245</td>      <td>6.478510</td>      <td>9.079434</td>      <td>8.740337</td>    </tr>    <tr>      <th>193</th>      <td>5.192957</td>      <td>8.156223</td>      <td>9.917982</td>      <td>6.865891</td>      <td>8.633731</td>      <td>6.501290</td>    </tr>    <tr>      <th>218</th>      <td>2.890372</td>      <td>8.923191</td>      <td>9.629380</td>      <td>7.158514</td>      <td>8.475746</td>      <td>8.759669</td>    </tr>    <tr>      <th>304</th>      <td>5.081404</td>      <td>8.917311</td>      <td>10.117510</td>      <td>6.424869</td>      <td>9.374413</td>      <td>7.787382</td>    </tr>    <tr>      <th>305</th>      <td>5.493061</td>      <td>9.468001</td>      <td>9.088399</td>      <td>6.683361</td>      <td>8.271037</td>      <td>5.351858</td>    </tr>    <tr>      <th>338</th>      <td>1.098612</td>      <td>5.808142</td>      <td>8.856661</td>      <td>9.655090</td>      <td>2.708050</td>      <td>6.309918</td>    </tr>    <tr>      <th>353</th>      <td>4.762174</td>      <td>8.742574</td>      <td>9.961898</td>      <td>5.429346</td>      <td>9.069007</td>      <td>7.013016</td>    </tr>    <tr>      <th>355</th>      <td>5.247024</td>      <td>6.588926</td>      <td>7.606885</td>      <td>5.501258</td>      <td>5.214936</td>      <td>4.844187</td>    </tr>    <tr>      <th>357</th>      <td>3.610918</td>      <td>7.150701</td>      <td>10.011086</td>      <td>4.919981</td>      <td>8.816853</td>      <td>4.700480</td>    </tr>    <tr>      <th>412</th>      <td>4.574711</td>      <td>8.190077</td>      <td>9.425452</td>      <td>4.584967</td>      <td>7.996317</td>      <td>4.127134</td>    </tr>  </tbody></table></div><pre><code>Data points considered outliers for the feature &#39;Milk&#39;:</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Fresh</th>      <th>Milk</th>      <th>Grocery</th>      <th>Frozen</th>      <th>Detergents_Paper</th>      <th>Delicatessen</th>    </tr>  </thead>  <tbody>    <tr>      <th>86</th>      <td>10.039983</td>      <td>11.205013</td>      <td>10.377047</td>      <td>6.894670</td>      <td>9.906981</td>      <td>6.805723</td>    </tr>    <tr>      <th>98</th>      <td>6.220590</td>      <td>4.718499</td>      <td>6.656727</td>      <td>6.796824</td>      <td>4.025352</td>      <td>4.882802</td>    </tr>    <tr>      <th>154</th>      <td>6.432940</td>      <td>4.007333</td>      <td>4.919981</td>      <td>4.317488</td>      <td>1.945910</td>      <td>2.079442</td>    </tr>    <tr>      <th>356</th>      <td>10.029503</td>      <td>4.897840</td>      <td>5.384495</td>      <td>8.057377</td>      <td>2.197225</td>      <td>6.306275</td>    </tr>  </tbody></table></div><pre><code>Data points considered outliers for the feature &#39;Grocery&#39;:</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Fresh</th>      <th>Milk</th>      <th>Grocery</th>      <th>Frozen</th>      <th>Detergents_Paper</th>      <th>Delicatessen</th>    </tr>  </thead>  <tbody>    <tr>      <th>75</th>      <td>9.923192</td>      <td>7.036148</td>      <td>1.098612</td>      <td>8.390949</td>      <td>1.098612</td>      <td>6.882437</td>    </tr>    <tr>      <th>154</th>      <td>6.432940</td>      <td>4.007333</td>      <td>4.919981</td>      <td>4.317488</td>      <td>1.945910</td>      <td>2.079442</td>    </tr>  </tbody></table></div><pre><code>Data points considered outliers for the feature &#39;Frozen&#39;:</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Fresh</th>      <th>Milk</th>      <th>Grocery</th>      <th>Frozen</th>      <th>Detergents_Paper</th>      <th>Delicatessen</th>    </tr>  </thead>  <tbody>    <tr>      <th>38</th>      <td>8.431853</td>      <td>9.663261</td>      <td>9.723703</td>      <td>3.496508</td>      <td>8.847360</td>      <td>6.070738</td>    </tr>    <tr>      <th>57</th>      <td>8.597297</td>      <td>9.203618</td>      <td>9.257892</td>      <td>3.637586</td>      <td>8.932213</td>      <td>7.156177</td>    </tr>    <tr>      <th>65</th>      <td>4.442651</td>      <td>9.950323</td>      <td>10.732651</td>      <td>3.583519</td>      <td>10.095388</td>      <td>7.260523</td>    </tr>    <tr>      <th>145</th>      <td>10.000569</td>      <td>9.034080</td>      <td>10.457143</td>      <td>3.737670</td>      <td>9.440738</td>      <td>8.396155</td>    </tr>    <tr>      <th>175</th>      <td>7.759187</td>      <td>8.967632</td>      <td>9.382106</td>      <td>3.951244</td>      <td>8.341887</td>      <td>7.436617</td>    </tr>    <tr>      <th>264</th>      <td>6.978214</td>      <td>9.177714</td>      <td>9.645041</td>      <td>4.110874</td>      <td>8.696176</td>      <td>7.142827</td>    </tr>    <tr>      <th>325</th>      <td>10.395650</td>      <td>9.728181</td>      <td>9.519735</td>      <td>11.016479</td>      <td>7.148346</td>      <td>8.632128</td>    </tr>    <tr>      <th>420</th>      <td>8.402007</td>      <td>8.569026</td>      <td>9.490015</td>      <td>3.218876</td>      <td>8.827321</td>      <td>7.239215</td>    </tr>    <tr>      <th>429</th>      <td>9.060331</td>      <td>7.467371</td>      <td>8.183118</td>      <td>3.850148</td>      <td>4.430817</td>      <td>7.824446</td>    </tr>    <tr>      <th>439</th>      <td>7.932721</td>      <td>7.437206</td>      <td>7.828038</td>      <td>4.174387</td>      <td>6.167516</td>      <td>3.951244</td>    </tr>  </tbody></table></div><pre><code>Data points considered outliers for the feature &#39;Detergents_Paper&#39;:</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Fresh</th>      <th>Milk</th>      <th>Grocery</th>      <th>Frozen</th>      <th>Detergents_Paper</th>      <th>Delicatessen</th>    </tr>  </thead>  <tbody>    <tr>      <th>75</th>      <td>9.923192</td>      <td>7.036148</td>      <td>1.098612</td>      <td>8.390949</td>      <td>1.098612</td>      <td>6.882437</td>    </tr>    <tr>      <th>161</th>      <td>9.428190</td>      <td>6.291569</td>      <td>5.645447</td>      <td>6.995766</td>      <td>1.098612</td>      <td>7.711101</td>    </tr>  </tbody></table></div><pre><code>Data points considered outliers for the feature &#39;Delicatessen&#39;:</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Fresh</th>      <th>Milk</th>      <th>Grocery</th>      <th>Frozen</th>      <th>Detergents_Paper</th>      <th>Delicatessen</th>    </tr>  </thead>  <tbody>    <tr>      <th>66</th>      <td>2.197225</td>      <td>7.335634</td>      <td>8.911530</td>      <td>5.164786</td>      <td>8.151333</td>      <td>3.295837</td>    </tr>    <tr>      <th>109</th>      <td>7.248504</td>      <td>9.724899</td>      <td>10.274568</td>      <td>6.511745</td>      <td>6.728629</td>      <td>1.098612</td>    </tr>    <tr>      <th>128</th>      <td>4.941642</td>      <td>9.087834</td>      <td>8.248791</td>      <td>4.955827</td>      <td>6.967909</td>      <td>1.098612</td>    </tr>    <tr>      <th>137</th>      <td>8.034955</td>      <td>8.997147</td>      <td>9.021840</td>      <td>6.493754</td>      <td>6.580639</td>      <td>3.583519</td>    </tr>    <tr>      <th>142</th>      <td>10.519646</td>      <td>8.875147</td>      <td>9.018332</td>      <td>8.004700</td>      <td>2.995732</td>      <td>1.098612</td>    </tr>    <tr>      <th>154</th>      <td>6.432940</td>      <td>4.007333</td>      <td>4.919981</td>      <td>4.317488</td>      <td>1.945910</td>      <td>2.079442</td>    </tr>    <tr>      <th>183</th>      <td>10.514529</td>      <td>10.690808</td>      <td>9.911952</td>      <td>10.505999</td>      <td>5.476464</td>      <td>10.777768</td>    </tr>    <tr>      <th>184</th>      <td>5.789960</td>      <td>6.822197</td>      <td>8.457443</td>      <td>4.304065</td>      <td>5.811141</td>      <td>2.397895</td>    </tr>    <tr>      <th>187</th>      <td>7.798933</td>      <td>8.987447</td>      <td>9.192075</td>      <td>8.743372</td>      <td>8.148735</td>      <td>1.098612</td>    </tr>    <tr>      <th>203</th>      <td>6.368187</td>      <td>6.529419</td>      <td>7.703459</td>      <td>6.150603</td>      <td>6.860664</td>      <td>2.890372</td>    </tr>    <tr>      <th>233</th>      <td>6.871091</td>      <td>8.513988</td>      <td>8.106515</td>      <td>6.842683</td>      <td>6.013715</td>      <td>1.945910</td>    </tr>    <tr>      <th>285</th>      <td>10.602965</td>      <td>6.461468</td>      <td>8.188689</td>      <td>6.948897</td>      <td>6.077642</td>      <td>2.890372</td>    </tr>    <tr>      <th>289</th>      <td>10.663966</td>      <td>5.655992</td>      <td>6.154858</td>      <td>7.235619</td>      <td>3.465736</td>      <td>3.091042</td>    </tr>    <tr>      <th>343</th>      <td>7.431892</td>      <td>8.848509</td>      <td>10.177932</td>      <td>7.283448</td>      <td>9.646593</td>      <td>3.610918</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.boxplot(data=data, x=<span class="string">'Fresh'</span>, orient=<span class="string">'v'</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x256682c49b0&gt;</code></pre><p><img src="/2019/02/16/Customer-Setments/output_27_1.png" alt="png"></p><h3 id="问题-4"><a href="#问题-4" class="headerlink" title="问题 4"></a>问题 4</h3><p>请列出所有在多于一个特征下被看作是异常的数据点。这些点应该被从数据集中移除吗？为什么？把你认为需要移除的数据点全部加入到到 <code>outliers</code> 变量中。</p><p><strong>回答:不应该 共有48个异常值 对于本样本数据一共有400左右样本 占比过大 如果全部移除会对结果有较大影响 会造成数据损失 因此 判断有两个以上异常值的数据再进行移除 所以移除65,66,75,128,154 这5个样本</strong></p><h2 id="特征转换"><a href="#特征转换" class="headerlink" title="特征转换"></a>特征转换</h2><p>在这个部分中你将使用主成分分析（PCA）来分析批发商客户数据的内在结构。由于使用PCA在一个数据集上会计算出最大化方差的维度，我们将找出哪一个特征组合能够最好的描绘客户。</p><h3 id="练习-主成分分析（PCA）"><a href="#练习-主成分分析（PCA）" class="headerlink" title="练习: 主成分分析（PCA）"></a>练习: 主成分分析（PCA）</h3><p>既然数据被缩放到一个更加正态分布的范围中并且我们也移除了需要移除的异常点，我们现在就能够在 <code>good_data</code> 上使用PCA算法以发现数据的哪一个维度能够最大化特征的方差。除了找到这些维度，PCA 也将报告每一个维度的解释方差比（explained variance ratio）—这个数据有多少方差能够用这个单独的维度来解释。注意 PCA 的一个组成部分（维度）能够被看做这个空间中的一个新的“特征”，但是它是原来数据中的特征构成的。</p><p>在下面的代码单元中，你将要实现下面的功能：</p><ul><li>导入 <code>sklearn.decomposition.PCA</code> 并且将 <code>good_data</code> 用 PCA 并且使用6个维度进行拟合后的结果保存到 <code>pca</code> 中。</li><li>使用 <code>pca.transform</code> 将 <code>log_samples</code> 进行转换，并将结果存储到 <code>pca_samples</code> 中。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO：通过在good data上进行PCA，将其转换成6个维度</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA(n_components=<span class="number">6</span>)</span><br><span class="line">pca.fit(good_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：使用上面的PCA拟合将变换施加在log_samples上</span></span><br><span class="line">pca_samples = pca.transform(good_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成PCA的结果图</span></span><br><span class="line">pca_results = vs.pca_results(good_data, pca)</span><br></pre></td></tr></table></figure><p><img src="/2019/02/16/Customer-Setments/output_32_0.png" alt="png"></p><h3 id="问题-5"><a href="#问题-5" class="headerlink" title="问题 5"></a>问题 5</h3><p>数据的第一个和第二个主成分<strong>总共</strong>表示了多少的方差？ 前四个主成分呢？使用上面提供的可视化图像，从用户花费的角度来讨论前四个主要成分中每个主成分代表的消费行为并给出你做出判断的理由。</p><p><strong>提示：</strong></p><ul><li>对每个主成分中的特征分析权重的正负和大小。</li><li>结合每个主成分权重的正负讨论消费行为。</li><li>某一特定维度上的正向增长对应正权特征的增长和负权特征的减少。增长和减少的速率和每个特征的权重相关。<a href="https://onlinecourses.science.psu.edu/stat505/node/54" target="_blank" rel="noopener">参考资料：Interpretation of the Principal Components</a></li></ul><p><strong>回答:0.72 0.93 第一个 咖啡厅 最大的负权特征是洗涤类商品 第二个 超市 生鲜冻品和鱼的权值最大 第三个 鱼类零售店 鱼的权值最大 第四个 熟食店 熟食的权值占比最大</strong></p><h3 id="观察-1"><a href="#观察-1" class="headerlink" title="观察"></a>观察</h3><p>运行下面的代码，查看经过对数转换的样本数据在进行一个6个维度的主成分分析（PCA）之后会如何改变。观察样本数据的前四个维度的数值。考虑这和你初始对样本点的解释是否一致。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 展示经过PCA转换的sample log-data</span></span><br><span class="line">display(pd.DataFrame(np.round(pca_samples, <span class="number">4</span>), columns = pca_results.index.values))</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Dimension 1</th>      <th>Dimension 2</th>      <th>Dimension 3</th>      <th>Dimension 4</th>      <th>Dimension 5</th>      <th>Dimension 6</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>-1.7580</td>      <td>0.0097</td>      <td>-0.9590</td>      <td>-1.6824</td>      <td>0.2680</td>      <td>-0.3891</td>    </tr>    <tr>      <th>1</th>      <td>-1.7887</td>      <td>-0.8123</td>      <td>0.2315</td>      <td>-0.0036</td>      <td>0.1194</td>      <td>-0.2106</td>    </tr>    <tr>      <th>2</th>      <td>-1.8834</td>      <td>-1.5991</td>      <td>1.3204</td>      <td>-0.5432</td>      <td>-0.3934</td>      <td>-0.3117</td>    </tr>    <tr>      <th>3</th>      <td>1.1553</td>      <td>-1.4052</td>      <td>0.5422</td>      <td>0.4127</td>      <td>-0.6865</td>      <td>0.6409</td>    </tr>    <tr>      <th>4</th>      <td>-0.7848</td>      <td>-2.3943</td>      <td>0.4798</td>      <td>-0.3483</td>      <td>-0.3191</td>      <td>0.0613</td>    </tr>    <tr>      <th>5</th>      <td>-1.0850</td>      <td>-0.3243</td>      <td>-0.2635</td>      <td>-0.8812</td>      <td>0.1862</td>      <td>-0.5347</td>    </tr>    <tr>      <th>6</th>      <td>-1.1286</td>      <td>0.2629</td>      <td>-1.3162</td>      <td>-0.5369</td>      <td>-0.4836</td>      <td>0.1097</td>    </tr>    <tr>      <th>7</th>      <td>-1.5672</td>      <td>-0.9010</td>      <td>0.3684</td>      <td>-0.2682</td>      <td>-0.4571</td>      <td>0.1526</td>    </tr>    <tr>      <th>8</th>      <td>-0.8636</td>      <td>0.6650</td>      <td>-0.5376</td>      <td>-0.7922</td>      <td>-0.1551</td>      <td>0.0344</td>    </tr>    <tr>      <th>9</th>      <td>-2.8734</td>      <td>-0.6774</td>      <td>0.1330</td>      <td>-0.1802</td>      <td>-0.0250</td>      <td>0.1224</td>    </tr>    <tr>      <th>10</th>      <td>-2.0887</td>      <td>-0.7006</td>      <td>0.8537</td>      <td>1.0105</td>      <td>-0.5587</td>      <td>0.2495</td>    </tr>    <tr>      <th>11</th>      <td>1.0120</td>      <td>-0.0103</td>      <td>-0.7516</td>      <td>-0.0545</td>      <td>-0.4333</td>      <td>0.6602</td>    </tr>    <tr>      <th>12</th>      <td>-2.2406</td>      <td>-1.2419</td>      <td>-1.0729</td>      <td>-1.9589</td>      <td>0.2160</td>      <td>-0.1782</td>    </tr>    <tr>      <th>13</th>      <td>-1.8891</td>      <td>-1.3001</td>      <td>-1.1945</td>      <td>0.9689</td>      <td>-0.2426</td>      <td>0.2970</td>    </tr>    <tr>      <th>14</th>      <td>-2.3388</td>      <td>-0.9013</td>      <td>-1.1515</td>      <td>-1.6713</td>      <td>-0.0485</td>      <td>-0.0739</td>    </tr>    <tr>      <th>15</th>      <td>0.4258</td>      <td>0.8803</td>      <td>-1.2189</td>      <td>-0.7945</td>      <td>-0.7319</td>      <td>0.3868</td>    </tr>    <tr>      <th>16</th>      <td>-2.7939</td>      <td>2.0377</td>      <td>0.3420</td>      <td>-1.2847</td>      <td>0.1457</td>      <td>-0.1353</td>    </tr>    <tr>      <th>17</th>      <td>0.2575</td>      <td>-0.5179</td>      <td>1.1702</td>      <td>-1.5806</td>      <td>0.4159</td>      <td>-0.5328</td>    </tr>    <tr>      <th>18</th>      <td>-1.3906</td>      <td>-1.8004</td>      <td>0.0301</td>      <td>-0.3807</td>      <td>-0.2116</td>      <td>0.1467</td>    </tr>    <tr>      <th>19</th>      <td>-0.9992</td>      <td>0.4720</td>      <td>-0.9332</td>      <td>-0.1723</td>      <td>-0.4229</td>      <td>0.5269</td>    </tr>    <tr>      <th>20</th>      <td>-0.8375</td>      <td>-1.0765</td>      <td>-0.3684</td>      <td>-0.8111</td>      <td>-0.5111</td>      <td>-0.3040</td>    </tr>    <tr>      <th>21</th>      <td>1.7467</td>      <td>0.1939</td>      <td>0.2753</td>      <td>0.6012</td>      <td>-0.7470</td>      <td>0.1974</td>    </tr>    <tr>      <th>22</th>      <td>-0.1419</td>      <td>-2.7722</td>      <td>0.3293</td>      <td>0.3928</td>      <td>-1.3904</td>      <td>0.2012</td>    </tr>    <tr>      <th>23</th>      <td>-2.8096</td>      <td>-3.6459</td>      <td>1.0567</td>      <td>-0.5186</td>      <td>0.6999</td>      <td>-0.1811</td>    </tr>    <tr>      <th>24</th>      <td>-2.0709</td>      <td>-2.4853</td>      <td>0.2692</td>      <td>-0.4013</td>      <td>-0.1917</td>      <td>0.1027</td>    </tr>    <tr>      <th>25</th>      <td>-1.2292</td>      <td>1.5540</td>      <td>-3.2462</td>      <td>0.0043</td>      <td>0.1124</td>      <td>-0.0697</td>    </tr>    <tr>      <th>26</th>      <td>1.9083</td>      <td>-0.3765</td>      <td>0.1924</td>      <td>0.1502</td>      <td>-0.3852</td>      <td>0.5367</td>    </tr>    <tr>      <th>27</th>      <td>2.4162</td>      <td>0.6069</td>      <td>-0.7652</td>      <td>-1.3209</td>      <td>0.1614</td>      <td>0.8089</td>    </tr>    <tr>      <th>28</th>      <td>-3.5695</td>      <td>-0.9977</td>      <td>0.9477</td>      <td>-0.5400</td>      <td>0.2579</td>      <td>0.0323</td>    </tr>    <tr>      <th>29</th>      <td>0.5684</td>      <td>-1.0850</td>      <td>-1.4044</td>      <td>-0.5784</td>      <td>-0.6738</td>      <td>-0.2157</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>405</th>      <td>-0.4777</td>      <td>-0.3472</td>      <td>0.3116</td>      <td>-0.3929</td>      <td>-0.9402</td>      <td>0.1142</td>    </tr>    <tr>      <th>406</th>      <td>0.7424</td>      <td>0.0059</td>      <td>2.1018</td>      <td>-0.9815</td>      <td>0.2461</td>      <td>-0.0371</td>    </tr>    <tr>      <th>407</th>      <td>-2.1528</td>      <td>5.3859</td>      <td>0.0930</td>      <td>0.4023</td>      <td>0.3577</td>      <td>0.3111</td>    </tr>    <tr>      <th>408</th>      <td>-0.0741</td>      <td>-1.6911</td>      <td>1.6461</td>      <td>1.4172</td>      <td>0.0587</td>      <td>0.1461</td>    </tr>    <tr>      <th>409</th>      <td>0.5554</td>      <td>-0.0029</td>      <td>-0.2440</td>      <td>1.6316</td>      <td>-0.4586</td>      <td>-0.0161</td>    </tr>    <tr>      <th>410</th>      <td>-1.5972</td>      <td>-0.8047</td>      <td>0.1483</td>      <td>-0.0839</td>      <td>-0.3191</td>      <td>-0.0512</td>    </tr>    <tr>      <th>411</th>      <td>-2.5495</td>      <td>0.1090</td>      <td>-0.1921</td>      <td>-0.0073</td>      <td>-0.0074</td>      <td>-0.3327</td>    </tr>    <tr>      <th>412</th>      <td>-1.9218</td>      <td>0.5424</td>      <td>-0.4014</td>      <td>-0.8837</td>      <td>-0.1168</td>      <td>0.1580</td>    </tr>    <tr>      <th>413</th>      <td>-3.2940</td>      <td>2.4621</td>      <td>0.3317</td>      <td>-0.9146</td>      <td>0.1175</td>      <td>0.1444</td>    </tr>    <tr>      <th>414</th>      <td>-0.3359</td>      <td>-0.0856</td>      <td>-0.1970</td>      <td>-1.0176</td>      <td>-0.6091</td>      <td>-0.7771</td>    </tr>    <tr>      <th>415</th>      <td>-3.0266</td>      <td>1.8034</td>      <td>-1.1358</td>      <td>-2.9577</td>      <td>-0.4264</td>      <td>0.1528</td>    </tr>    <tr>      <th>416</th>      <td>-1.4571</td>      <td>-1.0316</td>      <td>-0.5676</td>      <td>-0.6119</td>      <td>-0.4132</td>      <td>0.1330</td>    </tr>    <tr>      <th>417</th>      <td>0.4422</td>      <td>-0.7142</td>      <td>-0.9356</td>      <td>-0.9923</td>      <td>-0.7925</td>      <td>0.4116</td>    </tr>    <tr>      <th>418</th>      <td>-0.4191</td>      <td>-0.4595</td>      <td>-1.0590</td>      <td>-0.2384</td>      <td>-0.2853</td>      <td>-0.1660</td>    </tr>    <tr>      <th>419</th>      <td>-1.0698</td>      <td>0.0957</td>      <td>-1.8679</td>      <td>0.3247</td>      <td>-0.2282</td>      <td>0.6291</td>    </tr>    <tr>      <th>420</th>      <td>2.3699</td>      <td>-1.7726</td>      <td>1.3282</td>      <td>0.7617</td>      <td>0.4672</td>      <td>0.1593</td>    </tr>    <tr>      <th>421</th>      <td>-2.0740</td>      <td>-1.5983</td>      <td>-0.0683</td>      <td>0.4013</td>      <td>-0.0483</td>      <td>0.0983</td>    </tr>    <tr>      <th>422</th>      <td>0.4545</td>      <td>-2.6564</td>      <td>0.0980</td>      <td>1.1628</td>      <td>1.4384</td>      <td>-0.5162</td>    </tr>    <tr>      <th>423</th>      <td>-0.1223</td>      <td>0.6924</td>      <td>0.0667</td>      <td>0.9488</td>      <td>0.6363</td>      <td>-0.2967</td>    </tr>    <tr>      <th>424</th>      <td>1.4269</td>      <td>1.2099</td>      <td>-0.1030</td>      <td>-3.9222</td>      <td>0.6257</td>      <td>0.5211</td>    </tr>    <tr>      <th>425</th>      <td>-0.0856</td>      <td>0.4483</td>      <td>1.0450</td>      <td>-1.3292</td>      <td>1.1732</td>      <td>1.1231</td>    </tr>    <tr>      <th>426</th>      <td>-0.2111</td>      <td>-1.6998</td>      <td>0.8104</td>      <td>1.4239</td>      <td>-0.0610</td>      <td>-0.2023</td>    </tr>    <tr>      <th>427</th>      <td>0.1304</td>      <td>0.5643</td>      <td>-1.9278</td>      <td>-1.1452</td>      <td>-0.7829</td>      <td>0.4971</td>    </tr>    <tr>      <th>428</th>      <td>0.9382</td>      <td>0.6387</td>      <td>1.3840</td>      <td>-0.3231</td>      <td>-0.0505</td>      <td>-0.7708</td>    </tr>    <tr>      <th>429</th>      <td>-1.0055</td>      <td>-0.3825</td>      <td>-1.0855</td>      <td>-0.6019</td>      <td>-0.2346</td>      <td>0.1880</td>    </tr>    <tr>      <th>430</th>      <td>0.6448</td>      <td>-2.8583</td>      <td>0.6377</td>      <td>0.5879</td>      <td>1.9515</td>      <td>0.7170</td>    </tr>    <tr>      <th>431</th>      <td>3.1848</td>      <td>-1.9448</td>      <td>0.2677</td>      <td>-0.6799</td>      <td>-0.2663</td>      <td>-0.5194</td>    </tr>    <tr>      <th>432</th>      <td>-3.7425</td>      <td>-0.8561</td>      <td>-0.9885</td>      <td>-0.8879</td>      <td>0.0503</td>      <td>0.2058</td>    </tr>    <tr>      <th>433</th>      <td>1.6691</td>      <td>-0.3980</td>      <td>0.5161</td>      <td>-1.3189</td>      <td>0.0913</td>      <td>0.0056</td>    </tr>    <tr>      <th>434</th>      <td>0.7390</td>      <td>3.6914</td>      <td>-2.0335</td>      <td>-0.9927</td>      <td>0.3109</td>      <td>-0.1734</td>    </tr>  </tbody></table><p>435 rows × 6 columns</p></div><h3 id="练习：降维"><a href="#练习：降维" class="headerlink" title="练习：降维"></a>练习：降维</h3><p>当使用主成分分析的时候，一个主要的目的是减少数据的维度，这实际上降低了问题的复杂度。当然降维也是需要一定代价的：更少的维度能够表示的数据中的总方差更少。因为这个，<strong>累计解释方差比（cumulative explained variance ratio）</strong>对于我们确定这个问题需要多少维度非常重要。另外，如果大部分的方差都能够通过两个或者是三个维度进行表示的话，降维之后的数据能够被可视化。</p><p>在下面的代码单元中，你将实现下面的功能：</p><ul><li>将 <code>good_data</code> 用两个维度的PCA进行拟合，并将结果存储到 <code>pca</code> 中去。</li><li>使用 <code>pca.transform</code> 将 <code>good_data</code> 进行转换，并将结果存储在 <code>reduced_data</code> 中。</li><li>使用 <code>pca.transform</code> 将 <code>log_samples</code> 进行转换，并将结果存储在 <code>pca_samples</code> 中。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO：通过在good data上进行PCA，将其转换成两个维度</span></span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pca.fit(good_data)</span><br><span class="line"><span class="comment"># TODO：使用上面训练的PCA将good data进行转换</span></span><br><span class="line">reduced_data = pca.transform(good_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：使用上面训练的PCA将log_samples进行转换</span></span><br><span class="line">pca_samples = pca.transform(log_samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为降维后的数据创建一个DataFrame</span></span><br><span class="line">reduced_data = pd.DataFrame(reduced_data, columns = [<span class="string">'Dimension 1'</span>, <span class="string">'Dimension 2'</span>])</span><br></pre></td></tr></table></figure><h3 id="观察-2"><a href="#观察-2" class="headerlink" title="观察"></a>观察</h3><p>运行以下代码观察当仅仅使用两个维度进行 PCA 转换后，这个对数样本数据将怎样变化。观察这里的结果与一个使用六个维度的 PCA 转换相比较时，前两维的数值是保持不变的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 展示经过两个维度的PCA转换之后的样本log-data</span></span><br><span class="line">display(pd.DataFrame(np.round(pca_samples, <span class="number">4</span>), columns = [<span class="string">'Dimension 1'</span>, <span class="string">'Dimension 2'</span>]))</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Dimension 1</th>      <th>Dimension 2</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>-1.7887</td>      <td>-0.8123</td>    </tr>    <tr>      <th>1</th>      <td>-2.3388</td>      <td>-0.9013</td>    </tr>    <tr>      <th>2</th>      <td>3.2785</td>      <td>0.9078</td>    </tr>  </tbody></table></div><h2 id="可视化一个双标图（Biplot）"><a href="#可视化一个双标图（Biplot）" class="headerlink" title="可视化一个双标图（Biplot）"></a>可视化一个双标图（Biplot）</h2><p>双标图是一个散点图，每个数据点的位置由它所在主成分的分数确定。坐标系是主成分（这里是 <code>Dimension 1</code> 和 <code>Dimension 2</code>）。此外，双标图还展示出初始特征在主成分上的投影。一个双标图可以帮助我们理解降维后的数据，发现主成分和初始特征之间的关系。</p><p>运行下面的代码来创建一个降维后数据的双标图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化双标图</span></span><br><span class="line">vs.biplot(good_data, reduced_data, pca)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x256683adc88&gt;</code></pre><p><img src="/2019/02/16/Customer-Setments/output_42_1.png" alt="png"></p><h3 id="观察-3"><a href="#观察-3" class="headerlink" title="观察"></a>观察</h3><p>一旦我们有了原始特征的投影（红色箭头），就能更加容易的理解散点图每个数据点的相对位置。</p><p>在这个双标图中，哪些初始特征与第一个主成分有强关联？哪些初始特征与第二个主成分相关联？你观察到的是否与之前得到的 pca_results 图相符？</p><h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><p>在这个部分，你讲选择使用 K-Means 聚类算法或者是高斯混合模型聚类算法以发现数据中隐藏的客户分类。然后，你将从簇中恢复一些特定的关键数据点，通过将它们转换回原始的维度和规模，从而理解他们的含义。</p><h3 id="问题-6"><a href="#问题-6" class="headerlink" title="问题 6"></a>问题 6</h3><p>使用 K-Means 聚类算法的优点是什么？使用高斯混合模型聚类算法的优点是什么？基于你现在对客户数据的观察结果，你选用了这两个算法中的哪一个，为什么？</p><p><strong>回答:K-Means优点是：计算速度快、时间短，易解释，聚类效果还不错；但缺点主要是需要提前确定K值，对异常值极度敏感。 高斯混合模型聚类算法的优点是聚类输出的信息量更大，理论上可以拟合任何连续的概率密度函数。 我会选高斯混合模型，因为两种算法聚类得分差异很小，且GMM能输出数据点属于某一类别的概率，因此输出的信息丰富程度大大高于K-means算法</strong></p><h3 id="练习-创建聚类"><a href="#练习-创建聚类" class="headerlink" title="练习: 创建聚类"></a>练习: 创建聚类</h3><p>针对不同情况，有些问题你需要的聚类数目可能是已知的。但是在聚类数目不作为一个<strong>先验</strong>知道的情况下，我们并不能够保证某个聚类的数目对这个数据是最优的，因为我们对于数据的结构（如果存在的话）是不清楚的。但是，我们可以通过计算每一个簇中点的<strong>轮廓系数</strong>来衡量聚类的质量。数据点的<a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html" target="_blank" rel="noopener">轮廓系数</a>衡量了它与分配给他的簇的相似度，这个值范围在-1（不相似）到1（相似）。<strong>平均</strong>轮廓系数为我们提供了一种简单地度量聚类质量的方法。</p><p>在接下来的代码单元中，你将实现下列功能：</p><ul><li>在 <code>reduced_data</code> 上使用一个聚类算法，并将结果赋值到 <code>clusterer</code>，需要设置  <code>random_state</code> 使得结果可以复现。</li><li>使用 <code>clusterer.predict</code> 预测 <code>reduced_data</code> 中的每一个点的簇，并将结果赋值到 <code>preds</code>。</li><li>使用算法的某个属性值找到聚类中心，并将它们赋值到 <code>centers</code>。</li><li>预测 <code>pca_samples</code> 中的每一个样本点的类别并将结果赋值到 <code>sample_preds</code>。</li><li>导入 <code>sklearn.metrics.silhouette_score</code> 包并计算 <code>reduced_data</code> 相对于 <code>preds</code> 的轮廓系数。<ul><li>将轮廓系数赋值给 <code>score</code> 并输出结果。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO：在降维后的数据上使用你选择的聚类算法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.mixture <span class="keyword">import</span> GaussianMixture</span><br><span class="line">clusterer = GaussianMixture(n_components=<span class="number">2</span>, random_state=<span class="number">40</span>)</span><br><span class="line">clusterer.fit(reduced_data)</span><br><span class="line"><span class="comment"># TODO：预测每一个点的簇</span></span><br><span class="line">preds = clusterer.predict(reduced_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：找到聚类中心</span></span><br><span class="line">centers = clusterer.means_</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：预测在每一个转换后的样本点的类</span></span><br><span class="line">sample_preds = clusterer.predict(pca_samples)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_score</span><br><span class="line"><span class="comment"># TODO：计算选择的类别的平均轮廓系数（mean silhouette coefficient）</span></span><br><span class="line">score = silhouette_score(reduced_data, preds)</span><br><span class="line">print(score)</span><br></pre></td></tr></table></figure><pre><code>0.4219168464626149</code></pre><h3 id="问题-7"><a href="#问题-7" class="headerlink" title="问题 7"></a>问题 7</h3><p>汇报你尝试的不同的聚类数对应的轮廓系数。在这些当中哪一个聚类的数目能够得到最佳的轮廓系数？</p><p><strong>回答:聚类为2时 0.421 聚类为3时 0.375 聚类为4时0.248 所以聚类为2时效果最佳</strong></p><h3 id="聚类可视化"><a href="#聚类可视化" class="headerlink" title="聚类可视化"></a>聚类可视化</h3><p>一旦你选好了通过上面的评价函数得到的算法的最佳聚类数目，你就能够通过使用下面的代码块可视化来得到的结果。作为实验，你可以试着调整你的聚类算法的聚类的数量来看一下不同的可视化结果。但是你提供的最终的可视化图像必须和你选择的最优聚类数目一致。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从已有的实现中展示聚类的结果</span></span><br><span class="line">vs.cluster_results(reduced_data, preds, centers, pca_samples)</span><br></pre></td></tr></table></figure><p><img src="/2019/02/16/Customer-Setments/output_52_0.png" alt="png"></p><h3 id="练习-数据恢复"><a href="#练习-数据恢复" class="headerlink" title="练习: 数据恢复"></a>练习: 数据恢复</h3><p>上面的可视化图像中提供的每一个聚类都有一个中心点。这些中心（或者叫平均点）并不是数据中真实存在的点，但是是所有预测在这个簇中的数据点的平均。对于创建客户分类的问题，一个簇的中心对应于那个分类的平均用户。因为这个数据现在进行了降维并缩放到一定的范围，我们可以通过施加一个反向的转换恢复这个点所代表的用户的花费。</p><p>在下面的代码单元中，你将实现下列的功能：</p><ul><li>使用 <code>pca.inverse_transform</code> 将 <code>centers</code> 反向转换，并将结果存储在 <code>log_centers</code> 中。</li><li>使用 <code>np.log</code> 的反函数 <code>np.exp</code> 反向转换 <code>log_centers</code> 并将结果存储到 <code>true_centers</code> 中。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO：反向转换中心点</span></span><br><span class="line">log_centers = pca.inverse_transform(centers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：对中心点做指数转换</span></span><br><span class="line">true_centers = np.exp(log_centers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示真实的中心点</span></span><br><span class="line">segments = [<span class="string">'Segment &#123;&#125;'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(centers))]</span><br><span class="line">true_centers = pd.DataFrame(np.round(true_centers), columns = data.keys())</span><br><span class="line">true_centers.index = segments</span><br><span class="line">display(true_centers)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Fresh</th>      <th>Milk</th>      <th>Grocery</th>      <th>Frozen</th>      <th>Detergents_Paper</th>      <th>Delicatessen</th>    </tr>  </thead>  <tbody>    <tr>      <th>Segment 0</th>      <td>3552.0</td>      <td>7837.0</td>      <td>12219.0</td>      <td>870.0</td>      <td>4696.0</td>      <td>962.0</td>    </tr>    <tr>      <th>Segment 1</th>      <td>8953.0</td>      <td>2114.0</td>      <td>2765.0</td>      <td>2075.0</td>      <td>353.0</td>      <td>732.0</td>    </tr>  </tbody></table></div><h3 id="问题-8"><a href="#问题-8" class="headerlink" title="问题 8"></a>问题 8</h3><p>考虑上面的代表性数据点在每一个产品类型的花费总数，你认为这些客户分类代表了哪类客户？为什么？需要参考在项目最开始得到的统计值来给出理由。</p><p><strong>提示：</strong> 一个被分到<code>&#39;Cluster X&#39;</code>的客户最好被用 <code>&#39;Segment X&#39;</code>中的特征集来标识的企业类型表示。</p><p><strong>回答:Segment 0 代表餐厅 食品类出售比重较大 Segment 1 代表超市 基本符合超市出售商品特征</strong></p><h3 id="问题-9"><a href="#问题-9" class="headerlink" title="问题 9"></a>问题 9</h3><p>对于每一个样本点<strong>问题 8 </strong>中的哪一个分类能够最好的表示它？你之前对样本的预测和现在的结果相符吗？</p><p>运行下面的代码单元以找到每一个样本点被预测到哪一个簇中去。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示预测结果</span></span><br><span class="line"><span class="keyword">for</span> i, pred <span class="keyword">in</span> enumerate(sample_preds):</span><br><span class="line">    print(<span class="string">"Sample point"</span>, i, <span class="string">"predicted to be in Cluster"</span>, pred)</span><br></pre></td></tr></table></figure><pre><code>Sample point 0 predicted to be in Cluster 0Sample point 1 predicted to be in Cluster 0Sample point 2 predicted to be in Cluster 1</code></pre><p><strong>回答:cluster0 结果不太相符 原数据分类比较细致 这里我们只是使用了2个簇 </strong></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在最后一部分中，你要学习如何使用已经被分类的数据。首先，你要考虑不同组的客户<strong>客户分类</strong>，针对不同的派送策略受到的影响会有什么不同。其次，你要考虑到，每一个客户都被打上了标签（客户属于哪一个分类）可以给客户数据提供一个多一个特征。最后，你会把客户分类与一个数据中的隐藏变量做比较，看一下这个分类是否辨识了特定的关系。</p><h3 id="问题-10"><a href="#问题-10" class="headerlink" title="问题 10"></a>问题 10</h3><p>在对他们的服务或者是产品做细微的改变的时候，公司经常会使用 <a href="https://en.wikipedia.org/wiki/A/B_testing" target="_blank" rel="noopener">A/B tests </a>以确定这些改变会对客户产生积极作用还是消极作用。这个批发商希望考虑将他的派送服务从每周5天变为每周3天，但是他只会对他客户当中对此有积极反馈的客户采用。这个批发商应该如何利用客户分类来知道哪些客户对它的这个派送策略的改变有积极的反馈，如果有的话？你需要给出在这个情形下A/B 测试具体的实现方法，以及最终得出结论的依据是什么？</p><p><strong>提示：</strong> 我们能假设这个改变对所有的客户影响都一致吗？我们怎样才能够确定它对于哪个类型的客户影响最大？</p><p><strong>回答：对两组客户分别为A1;A2,B1;B2 A1参照组 A2实验组 分别应用每周5天和每周3天的服务 分别记录每组的周均销售额 如果A2组的销售额较高 则说明该组对该类客户组有积极作用 然后对A1 A2组 使用新的策略否则保持不变 同理B1,B2采取同样的方式</strong></p><h3 id="问题-11"><a href="#问题-11" class="headerlink" title="问题 11"></a>问题 11</h3><p>通过聚类技术，我们能够将原有的没有标记的数据集中的附加结构分析出来。因为每一个客户都有一个最佳的划分（取决于你选择使用的聚类算法），我们可以把用户分类作为数据的一个<a href="https://en.wikipedia.org/wiki/Feature_learning#Unsupervised_feature_learning" target="_blank" rel="noopener">工程特征</a>。假设批发商最近迎来十位新顾客，并且他已经为每位顾客每个产品类别年度采购额进行了预估。进行了这些估算之后，批发商该如何运用它的预估和非监督学习的结果来对这十个新的客户进行更好的预测？</p><p><strong>提示</strong>：在下面的代码单元中，我们提供了一个已经做好聚类的数据（聚类结果为数据中的cluster属性），我们将在这个数据集上做一个小实验。尝试运行下面的代码看看我们尝试预测‘Region’的时候，如果存在聚类特征’cluster’与不存在相比对最终的得分会有什么影响？这对你有什么启发？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取包含聚类结果的数据</span></span><br><span class="line">cluster_data = pd.read_csv(<span class="string">"cluster.csv"</span>)</span><br><span class="line">y = cluster_data[<span class="string">'Region'</span>]</span><br><span class="line">X = cluster_data.drop([<span class="string">'Region'</span>], axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">24</span>)</span><br><span class="line"></span><br><span class="line">clf = RandomForestClassifier(random_state=<span class="number">24</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">score_with_cluster = clf.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除cluster特征</span></span><br><span class="line">X_train = X_train.copy()</span><br><span class="line">X_train.drop([<span class="string">'cluster'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">X_test = X_test.copy()</span><br><span class="line">X_test.drop([<span class="string">'cluster'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">score_no_cluster = clf.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"不使用cluster特征的得分: %.4f"</span>%score_no_cluster)</span><br><span class="line">print(<span class="string">"使用cluster特征的得分: %.4f"</span>%score_with_cluster)</span><br></pre></td></tr></table></figure><pre><code>不使用cluster特征的得分: 0.6437使用cluster特征的得分: 0.6667C:\ProgramData\Anaconda3\lib\site-packages\sklearn\ensemble\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.  from numpy.core.umath_tests import inner1d</code></pre><p><strong>回答：cluster特征对用户的影响较小 但使用caluster有益于精度提高</strong></p><h3 id="可视化内在的分布"><a href="#可视化内在的分布" class="headerlink" title="可视化内在的分布"></a>可视化内在的分布</h3><p>在这个项目的开始，我们讨论了从数据集中移除 <code>&#39;Channel&#39;</code> 和 <code>&#39;Region&#39;</code> 特征，这样在分析过程中我们就会着重分析用户产品类别。通过重新引入 <code>Channel</code> 这个特征到数据集中，并施加和原来数据集同样的 PCA 变换的时候我们将能够发现数据集产生一个有趣的结构。</p><p>运行下面的代码单元以查看哪一个数据点在降维的空间中被标记为 <code>&#39;HoReCa&#39;</code> (旅馆/餐馆/咖啡厅)或者 <code>&#39;Retail&#39;</code>。另外，你将发现样本点在图中被圈了出来，用以显示他们的标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据‘Channel‘数据显示聚类的结果</span></span><br><span class="line">vs.channel_results(reduced_data, outliers, pca_samples)</span><br></pre></td></tr></table></figure><p><img src="/2019/02/16/Customer-Setments/output_67_0.png" alt="png"></p><h3 id="问题-12"><a href="#问题-12" class="headerlink" title="问题 12"></a>问题 12</h3><p>你选择的聚类算法和聚类点的数目，与内在的旅馆/餐馆/咖啡店和零售商的分布相比，有足够好吗？根据这个分布有没有哪个簇能够刚好划分成’零售商’或者是’旅馆/饭店/咖啡馆’？你觉得这个分类和前面你对于用户分类的定义是一致的吗？</p><p><strong>回答: 基本一致 零售类也属于超市的范围 所有都一样因为特征不多 很多分类比较模糊</strong></p><blockquote><p><strong>注意</strong>: 当你写完了所有的代码，并且回答了所有的问题。你就可以把你的 iPython Notebook 导出成 HTML 文件。你可以在菜单栏，这样导出<strong>File -&gt; Download as -&gt; HTML (.html)</strong>把这个 HTML 和这个 iPython notebook 一起做为你的作业提交。  </p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;机器学习纳米学位&quot;&gt;&lt;a href=&quot;#机器学习纳米学位&quot; class=&quot;headerlink&quot; title=&quot;机器学习纳米学位&quot;&gt;&lt;/a&gt;机器学习纳米学位&lt;/h1&gt;&lt;h2 id=&quot;非监督学习&quot;&gt;&lt;a href=&quot;#非监督学习&quot; class=&quot;headerlink
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>finding donors</title>
    <link href="http://yoursite.com/2019/02/16/finding-donors/"/>
    <id>http://yoursite.com/2019/02/16/finding-donors/</id>
    <published>2019-02-16T11:13:44.000Z</published>
    <updated>2019-02-16T11:14:39.171Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习纳米学位"><a href="#机器学习纳米学位" class="headerlink" title="机器学习纳米学位"></a>机器学习纳米学位</h1><h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><h2 id="项目2-为CharityML寻找捐献者"><a href="#项目2-为CharityML寻找捐献者" class="headerlink" title="项目2: 为CharityML寻找捐献者"></a>项目2: 为<em>CharityML</em>寻找捐献者</h2><p>欢迎来到机器学习工程师纳米学位的第二个项目！在此文件中，有些示例代码已经提供给你，但你还需要实现更多的功能让项目成功运行。除非有明确要求，你无须修改任何已给出的代码。以<strong>‘练习’</strong>开始的标题表示接下来的代码部分中有你必须要实现的功能。每一部分都会有详细的指导，需要实现的部分也会在注释中以’TODO’标出。请仔细阅读所有的提示！</p><p>除了实现代码外，你还必须回答一些与项目和你的实现有关的问题。每一个需要你回答的问题都会以<strong>‘问题 X’</strong>为标题。请仔细阅读每个问题，并且在问题后的<strong>‘回答’</strong>文字框中写出完整的答案。我们将根据你对问题的回答和撰写代码所实现的功能来对你提交的项目进行评分。</p><blockquote><p><strong>提示：</strong>Code 和 Markdown 区域可通过<strong>Shift + Enter</strong>快捷键运行。此外，Markdown可以通过双击进入编辑模式。</p></blockquote><h2 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h2><p>在这个项目中，你将使用1994年美国人口普查收集的数据，选用几个监督学习算法以准确地建模被调查者的收入。然后，你将根据初步结果从中选择出最佳的候选算法，并进一步优化该算法以最好地建模这些数据。你的目标是建立一个能够准确地预测被调查者年收入是否超过50000美元的模型。这种类型的任务会出现在那些依赖于捐款而存在的非营利性组织。了解人群的收入情况可以帮助一个非营利性的机构更好地了解他们要多大的捐赠，或是否他们应该接触这些人。虽然我们很难直接从公开的资源中推断出一个人的一般收入阶层，但是我们可以（也正是我们将要做的）从其他的一些公开的可获得的资源中获得一些特征从而推断出该值。</p><p>这个项目的数据集来自<a href="https://archive.ics.uci.edu/ml/datasets/Census+Income" target="_blank" rel="noopener">UCI机器学习知识库</a>。这个数据集是由Ron Kohavi和Barry Becker在发表文章<em>“Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid”</em>之后捐赠的，你可以在Ron Kohavi提供的<a href="https://www.aaai.org/Papers/KDD/1996/KDD96-033.pdf" target="_blank" rel="noopener">在线版本</a>中找到这个文章。我们在这里探索的数据集相比于原有的数据集有一些小小的改变，比如说移除了特征<code>&#39;fnlwgt&#39;</code> 以及一些遗失的或者是格式不正确的记录。</p><hr><h2 id="探索数据"><a href="#探索数据" class="headerlink" title="探索数据"></a>探索数据</h2><p>运行下面的代码单元以载入需要的Python库并导入人口普查数据。注意数据集的最后一列<code>&#39;income&#39;</code>将是我们需要预测的列（表示被调查者的年收入会大于或者是最多50,000美元），人口普查数据中的每一列都将是关于被调查者的特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为这个项目导入需要的库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display <span class="comment"># 允许为DataFrame使用display()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入附加的可视化代码visuals.py</span></span><br><span class="line"><span class="keyword">import</span> visuals <span class="keyword">as</span> vs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为notebook提供更加漂亮的可视化</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入人口普查数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">"census.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 成功 - 显示第一条记录</span></span><br><span class="line">display(data.head(n=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><div><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>age</th>      <th>workclass</th>      <th>education_level</th>      <th>education-num</th>      <th>marital-status</th>      <th>occupation</th>      <th>relationship</th>      <th>race</th>      <th>sex</th>      <th>capital-gain</th>      <th>capital-loss</th>      <th>hours-per-week</th>      <th>native-country</th>      <th>income</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>39</td>      <td>State-gov</td>      <td>Bachelors</td>      <td>13.0</td>      <td>Never-married</td>      <td>Adm-clerical</td>      <td>Not-in-family</td>      <td>White</td>      <td>Male</td>      <td>2174.0</td>      <td>0.0</td>      <td>40.0</td>      <td>United-States</td>      <td>&lt;=50K</td>    </tr>  </tbody></table></div><h3 id="练习：数据探索"><a href="#练习：数据探索" class="headerlink" title="练习：数据探索"></a>练习：数据探索</h3><p>首先我们对数据集进行一个粗略的探索，我们将看看每一个类别里会有多少被调查者？并且告诉我们这些里面多大比例是年收入大于50,000美元的。在下面的代码单元中，你将需要计算以下量：</p><ul><li>总的记录数量，<code>&#39;n_records&#39;</code></li><li>年收入大于50,000美元的人数，<code>&#39;n_greater_50k&#39;</code>.</li><li>年收入最多为50,000美元的人数 <code>&#39;n_at_most_50k&#39;</code>.</li><li>年收入大于50,000美元的人所占的比例， <code>&#39;greater_percent&#39;</code>.</li></ul><p><strong>提示：</strong> 您可能需要查看上面的生成的表，以了解<code>&#39;income&#39;</code>条目的格式是什么样的。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO：总的记录数</span></span><br><span class="line">n_records = data.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：被调查者的收入大于$50,000的人数</span></span><br><span class="line">n_greater_50k = len(data[data.income == <span class="string">'&gt;50K'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：被调查者的收入最多为$50,000的人数</span></span><br><span class="line">n_at_most_50k = len(data[data.income == <span class="string">'&lt;=50K'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：被调查者收入大于$50,000所占的比例</span></span><br><span class="line">greater_percent = float(n_greater_50k) / n_records * <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Total number of records: &#123;&#125;"</span>.format(n_records))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Individuals making more than $50,000: &#123;&#125;"</span>.format(n_greater_50k))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Individuals making at most $50,000: &#123;&#125;"</span>.format(n_at_most_50k))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Percentage of individuals making more than $50,000: &#123;:.2f&#125;%"</span>.format(greater_percent))</span><br></pre></td></tr></table></figure><pre><code>Total number of records: 45222Individuals making more than $50,000: 11208Individuals making at most $50,000: 34014Percentage of individuals making more than $50,000: 24.78%</code></pre><hr><h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><p>在数据能够被作为输入提供给机器学习算法之前，它经常需要被清洗，格式化，和重新组织 - 这通常被叫做<strong>预处理</strong>。幸运的是，对于这个数据集，没有我们必须处理的无效或丢失的条目，然而，由于某一些特征存在的特性我们必须进行一定的调整。这个预处理都可以极大地帮助我们提升几乎所有的学习算法的结果和预测能力。</p><h3 id="获得特征和标签"><a href="#获得特征和标签" class="headerlink" title="获得特征和标签"></a>获得特征和标签</h3><p><code>income</code> 列是我们需要的标签，记录一个人的年收入是否高于50K。 因此我们应该把他从数据中剥离出来，单独存放。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将数据切分成特征和对应的标签</span></span><br><span class="line">income_raw = data[<span class="string">'income'</span>]</span><br><span class="line">features_raw = data.drop(<span class="string">'income'</span>, axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="转换倾斜的连续特征"><a href="#转换倾斜的连续特征" class="headerlink" title="转换倾斜的连续特征"></a>转换倾斜的连续特征</h3><p>一个数据集有时可能包含至少一个靠近某个数字的特征，但有时也会有一些相对来说存在极大值或者极小值的不平凡分布的的特征。算法对这种分布的数据会十分敏感，并且如果这种数据没有能够很好地规一化处理会使得算法表现不佳。在人口普查数据集的两个特征符合这个描述：’<code>capital-gain&#39;</code>和<code>&#39;capital-loss&#39;</code>。</p><p>运行下面的代码单元以创建一个关于这两个特征的条形图。请注意当前的值的范围和它们是如何分布的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化 'capital-gain'和'capital-loss' 两个特征</span></span><br><span class="line">vs.distribution(features_raw)</span><br></pre></td></tr></table></figure><p><img src="/2019/02/16/finding-donors/output_10_0.png" alt="png"></p><p>对于高度倾斜分布的特征如<code>&#39;capital-gain&#39;</code>和<code>&#39;capital-loss&#39;</code>，常见的做法是对数据施加一个<a href="https://en.wikipedia.org/wiki/Data_transformation_(statistics)" target="_blank" rel="noopener">对数转换</a>，将数据转换成对数，这样非常大和非常小的值不会对学习算法产生负面的影响。并且使用对数变换显著降低了由于异常值所造成的数据范围异常。但是在应用这个变换时必须小心：因为0的对数是没有定义的，所以我们必须先将数据处理成一个比0稍微大一点的数以成功完成对数转换。</p><p>运行下面的代码单元来执行数据的转换和可视化结果。再次，注意值的范围和它们是如何分布的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于倾斜的数据使用Log转换</span></span><br><span class="line">skewed = [<span class="string">'capital-gain'</span>, <span class="string">'capital-loss'</span>]</span><br><span class="line">features_raw[skewed] = data[skewed].apply(<span class="keyword">lambda</span> x: np.log(x + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化对数转换后 'capital-gain'和'capital-loss' 两个特征</span></span><br><span class="line">vs.distribution(features_raw, transformed = <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="/2019/02/16/finding-donors/output_12_0.png" alt="png"></p><h3 id="规一化数字特征"><a href="#规一化数字特征" class="headerlink" title="规一化数字特征"></a>规一化数字特征</h3><p>除了对于高度倾斜的特征施加转换，对数值特征施加一些形式的缩放通常会是一个好的习惯。在数据上面施加一个缩放并不会改变数据分布的形式（比如上面说的’capital-gain’ or ‘capital-loss’）；但是，规一化保证了每一个特征在使用监督学习器的时候能够被平等的对待。注意一旦使用了缩放，观察数据的原始形式不再具有它本来的意义了，就像下面的例子展示的。</p><p>运行下面的代码单元来规一化每一个数字特征。我们将使用<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html" target="_blank" rel="noopener"><code>sklearn.preprocessing.MinMaxScaler</code></a>来完成这个任务。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化一个 scaler，并将它施加到特征上</span></span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">numerical = [<span class="string">'age'</span>, <span class="string">'education-num'</span>, <span class="string">'capital-gain'</span>, <span class="string">'capital-loss'</span>, <span class="string">'hours-per-week'</span>]</span><br><span class="line">features_raw[numerical] = scaler.fit_transform(data[numerical])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示一个经过缩放的样例记录</span></span><br><span class="line">display(features_raw.head(n = <span class="number">5</span>))</span><br></pre></td></tr></table></figure><div><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>age</th>      <th>workclass</th>      <th>education_level</th>      <th>education-num</th>      <th>marital-status</th>      <th>occupation</th>      <th>relationship</th>      <th>race</th>      <th>sex</th>      <th>capital-gain</th>      <th>capital-loss</th>      <th>hours-per-week</th>      <th>native-country</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0.301370</td>      <td>State-gov</td>      <td>Bachelors</td>      <td>0.800000</td>      <td>Never-married</td>      <td>Adm-clerical</td>      <td>Not-in-family</td>      <td>White</td>      <td>Male</td>      <td>0.02174</td>      <td>0.0</td>      <td>0.397959</td>      <td>United-States</td>    </tr>    <tr>      <th>1</th>      <td>0.452055</td>      <td>Self-emp-not-inc</td>      <td>Bachelors</td>      <td>0.800000</td>      <td>Married-civ-spouse</td>      <td>Exec-managerial</td>      <td>Husband</td>      <td>White</td>      <td>Male</td>      <td>0.00000</td>      <td>0.0</td>      <td>0.122449</td>      <td>United-States</td>    </tr>    <tr>      <th>2</th>      <td>0.287671</td>      <td>Private</td>      <td>HS-grad</td>      <td>0.533333</td>      <td>Divorced</td>      <td>Handlers-cleaners</td>      <td>Not-in-family</td>      <td>White</td>      <td>Male</td>      <td>0.00000</td>      <td>0.0</td>      <td>0.397959</td>      <td>United-States</td>    </tr>    <tr>      <th>3</th>      <td>0.493151</td>      <td>Private</td>      <td>11th</td>      <td>0.400000</td>      <td>Married-civ-spouse</td>      <td>Handlers-cleaners</td>      <td>Husband</td>      <td>Black</td>      <td>Male</td>      <td>0.00000</td>      <td>0.0</td>      <td>0.397959</td>      <td>United-States</td>    </tr>    <tr>      <th>4</th>      <td>0.150685</td>      <td>Private</td>      <td>Bachelors</td>      <td>0.800000</td>      <td>Married-civ-spouse</td>      <td>Prof-specialty</td>      <td>Wife</td>      <td>Black</td>      <td>Female</td>      <td>0.00000</td>      <td>0.0</td>      <td>0.397959</td>      <td>Cuba</td>    </tr>  </tbody></table></div><h3 id="练习：数据预处理"><a href="#练习：数据预处理" class="headerlink" title="练习：数据预处理"></a>练习：数据预处理</h3><p>从上面的<strong>数据探索</strong>中的表中，我们可以看到有几个属性的每一条记录都是非数字的。通常情况下，学习算法期望输入是数字的，这要求非数字的特征（称为类别变量）被转换。转换类别变量的一种流行的方法是使用<strong>独热编码</strong>方案。独热编码为每一个非数字特征的每一个可能的类别创建一个<em>“虚拟”</em>变量。例如，假设<code>someFeature</code>有三个可能的取值<code>A</code>，<code>B</code>或者<code>C</code>，。我们将把这个特征编码成<code>someFeature_A</code>, <code>someFeature_B</code>和<code>someFeature_C</code>.</p><div class="table-container"><table><thead><tr><th style="text-align:center">特征X</th><th></th><th style="text-align:center">特征X_A</th><th style="text-align:center">特征X_B</th><th style="text-align:center">特征X_C</th></tr></thead><tbody><tr><td style="text-align:center">B</td><td></td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">C</td><td>——&gt; 独热编码 ——&gt;</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">A</td><td></td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr></tbody></table></div><p>此外，对于非数字的特征，我们需要将非数字的标签<code>&#39;income&#39;</code>转换成数值以保证学习算法能够正常工作。因为这个标签只有两种可能的类别（”&lt;=50K”和”&gt;50K”），我们不必要使用独热编码，可以直接将他们编码分别成两个类<code>0</code>和<code>1</code>，在下面的代码单元中你将实现以下功能：</p><ul><li>使用<a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies" target="_blank" rel="noopener"><code>pandas.get_dummies()</code></a>对<code>&#39;features_raw&#39;</code>数据来施加一个独热编码。</li><li>将目标标签<code>&#39;income_raw&#39;</code>转换成数字项。<ul><li>将”&lt;=50K”转换成<code>0</code>；将”&gt;50K”转换成<code>1</code>。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO：使用pandas.get_dummies()对'features_raw'数据进行独热编码</span></span><br><span class="line">features = pd.get_dummies(features_raw)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：将'income_raw'编码成数字值</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line">income = pd.Series(preprocessing.LabelEncoder().fit_transform(income_raw))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印经过独热编码之后的特征数量</span></span><br><span class="line">encoded = list(features.columns)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"&#123;&#125; total features after one-hot encoding."</span>.format(len(encoded)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除下面一行的注释以观察编码的特征名字</span></span><br><span class="line"><span class="keyword">print</span> (encoded)</span><br></pre></td></tr></table></figure><pre><code>103 total features after one-hot encoding.[&#39;age&#39;, &#39;education-num&#39;, &#39;capital-gain&#39;, &#39;capital-loss&#39;, &#39;hours-per-week&#39;, &#39;workclass_ Federal-gov&#39;, &#39;workclass_ Local-gov&#39;, &#39;workclass_ Private&#39;, &#39;workclass_ Self-emp-inc&#39;, &#39;workclass_ Self-emp-not-inc&#39;, &#39;workclass_ State-gov&#39;, &#39;workclass_ Without-pay&#39;, &#39;education_level_ 10th&#39;, &#39;education_level_ 11th&#39;, &#39;education_level_ 12th&#39;, &#39;education_level_ 1st-4th&#39;, &#39;education_level_ 5th-6th&#39;, &#39;education_level_ 7th-8th&#39;, &#39;education_level_ 9th&#39;, &#39;education_level_ Assoc-acdm&#39;, &#39;education_level_ Assoc-voc&#39;, &#39;education_level_ Bachelors&#39;, &#39;education_level_ Doctorate&#39;, &#39;education_level_ HS-grad&#39;, &#39;education_level_ Masters&#39;, &#39;education_level_ Preschool&#39;, &#39;education_level_ Prof-school&#39;, &#39;education_level_ Some-college&#39;, &#39;marital-status_ Divorced&#39;, &#39;marital-status_ Married-AF-spouse&#39;, &#39;marital-status_ Married-civ-spouse&#39;, &#39;marital-status_ Married-spouse-absent&#39;, &#39;marital-status_ Never-married&#39;, &#39;marital-status_ Separated&#39;, &#39;marital-status_ Widowed&#39;, &#39;occupation_ Adm-clerical&#39;, &#39;occupation_ Armed-Forces&#39;, &#39;occupation_ Craft-repair&#39;, &#39;occupation_ Exec-managerial&#39;, &#39;occupation_ Farming-fishing&#39;, &#39;occupation_ Handlers-cleaners&#39;, &#39;occupation_ Machine-op-inspct&#39;, &#39;occupation_ Other-service&#39;, &#39;occupation_ Priv-house-serv&#39;, &#39;occupation_ Prof-specialty&#39;, &#39;occupation_ Protective-serv&#39;, &#39;occupation_ Sales&#39;, &#39;occupation_ Tech-support&#39;, &#39;occupation_ Transport-moving&#39;, &#39;relationship_ Husband&#39;, &#39;relationship_ Not-in-family&#39;, &#39;relationship_ Other-relative&#39;, &#39;relationship_ Own-child&#39;, &#39;relationship_ Unmarried&#39;, &#39;relationship_ Wife&#39;, &#39;race_ Amer-Indian-Eskimo&#39;, &#39;race_ Asian-Pac-Islander&#39;, &#39;race_ Black&#39;, &#39;race_ Other&#39;, &#39;race_ White&#39;, &#39;sex_ Female&#39;, &#39;sex_ Male&#39;, &#39;native-country_ Cambodia&#39;, &#39;native-country_ Canada&#39;, &#39;native-country_ China&#39;, &#39;native-country_ Columbia&#39;, &#39;native-country_ Cuba&#39;, &#39;native-country_ Dominican-Republic&#39;, &#39;native-country_ Ecuador&#39;, &#39;native-country_ El-Salvador&#39;, &#39;native-country_ England&#39;, &#39;native-country_ France&#39;, &#39;native-country_ Germany&#39;, &#39;native-country_ Greece&#39;, &#39;native-country_ Guatemala&#39;, &#39;native-country_ Haiti&#39;, &#39;native-country_ Holand-Netherlands&#39;, &#39;native-country_ Honduras&#39;, &#39;native-country_ Hong&#39;, &#39;native-country_ Hungary&#39;, &#39;native-country_ India&#39;, &#39;native-country_ Iran&#39;, &#39;native-country_ Ireland&#39;, &#39;native-country_ Italy&#39;, &#39;native-country_ Jamaica&#39;, &#39;native-country_ Japan&#39;, &#39;native-country_ Laos&#39;, &#39;native-country_ Mexico&#39;, &#39;native-country_ Nicaragua&#39;, &#39;native-country_ Outlying-US(Guam-USVI-etc)&#39;, &#39;native-country_ Peru&#39;, &#39;native-country_ Philippines&#39;, &#39;native-country_ Poland&#39;, &#39;native-country_ Portugal&#39;, &#39;native-country_ Puerto-Rico&#39;, &#39;native-country_ Scotland&#39;, &#39;native-country_ South&#39;, &#39;native-country_ Taiwan&#39;, &#39;native-country_ Thailand&#39;, &#39;native-country_ Trinadad&amp;Tobago&#39;, &#39;native-country_ United-States&#39;, &#39;native-country_ Vietnam&#39;, &#39;native-country_ Yugoslavia&#39;]</code></pre><h3 id="混洗和切分数据"><a href="#混洗和切分数据" class="headerlink" title="混洗和切分数据"></a>混洗和切分数据</h3><p>现在所有的 <em>类别变量</em> 已被转换成数值特征，而且所有的数值特征已被规一化。和我们一般情况下做的一样，我们现在将数据（包括特征和它们的标签）切分成训练和测试集。其中80%的数据将用于训练和20%的数据用于测试。然后再进一步把训练数据分为训练集和验证集，用来选择和优化模型。</p><p>运行下面的代码单元来完成切分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 train_test_split</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将'features'和'income'数据切分成训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(features, income, test_size = <span class="number">0.2</span>, random_state = <span class="number">0</span>,</span><br><span class="line">                                                    stratify = income)</span><br><span class="line"><span class="comment"># 将'X_train'和'y_train'进一步切分为训练集和验证集</span></span><br><span class="line">X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>,</span><br><span class="line">                                                    stratify = y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示切分的结果</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Training set has &#123;&#125; samples."</span>.format(X_train.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Validation set has &#123;&#125; samples."</span>.format(X_val.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Testing set has &#123;&#125; samples."</span>.format(X_test.shape[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><pre><code>Training set has 28941 samples.Validation set has 7236 samples.Testing set has 9045 samples.</code></pre><hr><h2 id="评价模型性能"><a href="#评价模型性能" class="headerlink" title="评价模型性能"></a>评价模型性能</h2><p>在这一部分中，我们将尝试四种不同的算法，并确定哪一个能够最好地建模数据。四种算法包含一个<em>天真的预测器</em> 和三个你选择的监督学习器。</p><h3 id="评价方法和朴素的预测器"><a href="#评价方法和朴素的预测器" class="headerlink" title="评价方法和朴素的预测器"></a>评价方法和朴素的预测器</h3><p><em>CharityML</em>通过他们的研究人员知道被调查者的年收入大于$50,000最有可能向他们捐款。因为这个原因<em>CharityML</em>对于准确预测谁能够获得$50,000以上收入尤其有兴趣。这样看起来使用<strong>准确率</strong>作为评价模型的标准是合适的。另外，把<em>没有</em>收入大于$50,000的人识别成年收入大于$50,000对于<em>CharityML</em>来说是有害的，因为他想要找到的是有意愿捐款的用户。这样，我们期望的模型具有准确预测那些能够年收入大于$50,000的能力比模型去<strong>查全</strong>这些被调查者<em>更重要</em>。我们能够使用<strong>F-beta score</strong>作为评价指标，这样能够同时考虑查准率和查全率：</p><script type="math/tex; mode=display">F_{\beta} = (1 + \beta^2) \cdot \frac{precision \cdot recall}{\left( \beta^2 \cdot precision \right) + recall}</script><p>尤其是，当 $\beta = 0.5$ 的时候更多的强调查准率，这叫做<strong>F$_{0.5}$ score</strong> （或者为了简单叫做F-score）。</p><h3 id="问题-1-天真的预测器的性能"><a href="#问题-1-天真的预测器的性能" class="headerlink" title="问题 1 - 天真的预测器的性能"></a>问题 1 - 天真的预测器的性能</h3><p>通过查看收入超过和不超过 $50,000 的人数，我们能发现多数被调查者年收入没有超过 $50,000。如果我们简单地预测说<em>“这个人的收入没有超过 $50,000”</em>，我们就可以得到一个 准确率超过 50% 的预测。这样我们甚至不用看数据就能做到一个准确率超过 50%。这样一个预测被称作是天真的。通常对数据使用一个<em>天真的预测器</em>是十分重要的，这样能够帮助建立一个模型表现是否好的基准。 使用下面的代码单元计算天真的预测器的相关性能。将你的计算结果赋值给<code>&#39;accuracy&#39;</code>, <code>‘precision’</code>, <code>‘recall’</code> 和 <code>&#39;fscore&#39;</code>，这些值会在后面被使用，请注意这里不能使用scikit-learn，你需要根据公式自己实现相关计算。</p><p><em>如果我们选择一个无论什么情况都预测被调查者年收入大于 $50,000 的模型，那么这个模型在<strong>验证集上</strong>的准确率，查准率，查全率和 F-score是多少？</em>  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#不能使用scikit-learn，你需要根据公式自己实现相关计算。</span></span><br><span class="line">TP = float(len(y_val[y_val == <span class="number">1</span>]))</span><br><span class="line">FP = float(len(y_val[y_val == <span class="number">0</span>]))</span><br><span class="line">FN = <span class="number">0</span></span><br><span class="line"><span class="comment">#TODO： 计算准确率</span></span><br><span class="line">accuracy = float(TP)/len(y_val)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO： 计算查准率 Precision</span></span><br><span class="line">precision = TP/(TP+FP)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO： 计算查全率 Recall</span></span><br><span class="line">recall = TP/(TP+FN)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO： 使用上面的公式，设置beta=0.5，计算F-score</span></span><br><span class="line">fscore = (<span class="number">1</span>+<span class="number">0.5</span>**<span class="number">2</span>)*((precision*recall)) / (<span class="number">0.5</span>**<span class="number">2</span>*precision+recall)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Naive Predictor on validation data: \n \</span></span><br><span class="line"><span class="string">    Accuracy score: &#123;:.4f&#125; \n \</span></span><br><span class="line"><span class="string">    Precision: &#123;:.4f&#125; \n \</span></span><br><span class="line"><span class="string">    Recall: &#123;:.4f&#125; \n \</span></span><br><span class="line"><span class="string">    F-score: &#123;:.4f&#125;"</span>.format(accuracy, precision, recall, fscore))</span><br></pre></td></tr></table></figure><pre><code>Naive Predictor on validation data:      Accuracy score: 0.2478      Precision: 0.2478      Recall: 1.0000      F-score: 0.2917</code></pre><h2 id="监督学习模型"><a href="#监督学习模型" class="headerlink" title="监督学习模型"></a>监督学习模型</h2><h3 id="问题-2-模型应用"><a href="#问题-2-模型应用" class="headerlink" title="问题 2 - 模型应用"></a>问题 2 - 模型应用</h3><p>你能够在 <a href="http://scikit-learn.org/stable/supervised_learning.html" target="_blank" rel="noopener"><code>scikit-learn</code></a> 中选择以下监督学习模型</p><ul><li>高斯朴素贝叶斯 (GaussianNB)</li><li>决策树 (DecisionTree)</li><li>集成方法 (Bagging, AdaBoost, Random Forest, Gradient Boosting)</li><li>K近邻 (K Nearest Neighbors)</li><li>随机梯度下降分类器 (SGDC)</li><li>支撑向量机 (SVM)</li><li>Logistic回归（LogisticRegression）</li></ul><p>从上面的监督学习模型中选择三个适合我们这个问题的模型，并回答相应问题。</p><h3 id="模型1"><a href="#模型1" class="headerlink" title="模型1"></a>模型1</h3><p><strong>模型名称</strong></p><p>回答：SVM</p><p><strong>描述一个该模型在真实世界的一个应用场景。（你需要为此做点研究，并给出你的引用出处）</strong></p><p>回答：人类行为认知的辨别，根据图像判断人物在做什么</p><p><strong>这个模型的优势是什么？他什么情况下表现最好？</strong></p><p>回答：丰富的核函数可以灵活解决回归问题和分类问题，当特征大于样本数且样本总数较小的时候表现优异</p><p><strong>这个模型的缺点是什么？什么条件下它表现很差？</strong></p><p>回答：当需要计算大量样本时丰富的核函数因为没有通用标准计算量大 且表现平平</p><p><strong>根据我们当前数据集的特点，为什么这个模型适合这个问题。</strong></p><p>回答：当前数据集含有大量特征 且样本总数适中 而且svm丰富的核函数可以很好的满足该项目需求</p><h3 id="模型2"><a href="#模型2" class="headerlink" title="模型2"></a>模型2</h3><p><strong>模型名称</strong></p><p>回答：Random Forest</p><p><strong>描述一个该模型在真实世界的一个应用场景。（你需要为此做点研究，并给出你的引用出处）</strong></p><p>回答：随机森林分类器对土地覆盖进行分类</p><p><strong>这个模型的优势是什么？他什么情况下表现最好？</strong></p><p>回答：简单直观 便于理解 提前归一化 以及处理缺失值 可以解决任何类型的数据集 且不需要对数据预处理 通过网格搜索选择超参数 高泛化能力 当特征和样本数量比列保持平衡时表现优异</p><p><strong>这个模型的缺点是什么？什么条件下它表现很差？</strong></p><p>回答：容易过拟合 当某叶子结点发生变化时整体结构也发生变化 且当特征的样本比例不平衡的时候容易出现偏向 </p><p><strong>根据我们当前数据集的特点，为什么这个模型适合这个问题。</strong></p><p>回答：特征唯独高 评估各个特征重要性 小范围噪声不会过拟合</p><h3 id="模型3"><a href="#模型3" class="headerlink" title="模型3"></a>模型3</h3><p><strong>模型名称</strong></p><p>回答：K近邻 (K Nearest Neighbors)</p><p><strong>描述一个该模型在真实世界的一个应用场景。（你需要为此做点研究，并给出你的引用出处）</strong></p><p>回答：使用k近邻法估计和绘制森林林分密度，体积和覆盖类型</p><p><strong>这个模型的优势是什么？他什么情况下表现最好？</strong></p><p>回答：思想简单 容易理解 聚类效果较优</p><p><strong>这个模型的缺点是什么？什么条件下它表现很差？</strong></p><p>回答：对异常值敏感 提前判断K值 局部最优 复杂度高不易控制 迭代次数较多</p><p><strong>根据我们当前数据集的特点，为什么这个模型适合这个问题。</strong></p><p>回答：该项目数据适中且属于二分类为题</p><h3 id="练习-创建一个训练和预测的流水线"><a href="#练习-创建一个训练和预测的流水线" class="headerlink" title="练习 - 创建一个训练和预测的流水线"></a>练习 - 创建一个训练和预测的流水线</h3><p>为了正确评估你选择的每一个模型的性能，创建一个能够帮助你快速有效地使用不同大小的训练集并在验证集上做预测的训练和验证的流水线是十分重要的。<br>你在这里实现的功能将会在接下来的部分中被用到。在下面的代码单元中，你将实现以下功能：</p><ul><li>从<a href="http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics" target="_blank" rel="noopener"><code>sklearn.metrics</code></a>中导入<code>fbeta_score</code>和<code>accuracy_score</code>。</li><li>用训练集拟合学习器，并记录训练时间。</li><li>对训练集的前300个数据点和验证集进行预测并记录预测时间。</li><li>计算预测训练集的前300个数据点的准确率和F-score。</li><li>计算预测验证集的准确率和F-score。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO：从sklearn中导入两个评价指标 - fbeta_score和accuracy_score</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> fbeta_score, accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_predict</span><span class="params">(learner, sample_size, X_train, y_train, X_val, y_val)</span>:</span> </span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    inputs:</span></span><br><span class="line"><span class="string">       - learner: the learning algorithm to be trained and predicted on</span></span><br><span class="line"><span class="string">       - sample_size: the size of samples (number) to be drawn from training set</span></span><br><span class="line"><span class="string">       - X_train: features training set</span></span><br><span class="line"><span class="string">       - y_train: income training set</span></span><br><span class="line"><span class="string">       - X_val: features validation set</span></span><br><span class="line"><span class="string">       - y_val: income validation set</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    results = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># TODO：使用sample_size大小的训练数据来拟合学习器</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Fit the learner to the training data using slicing with 'sample_size'</span></span><br><span class="line">    start = time() <span class="comment"># 获得程序开始时间</span></span><br><span class="line">    learner = learner.fit(X_train[:sample_size], y_train[:sample_size])</span><br><span class="line">    end = time() <span class="comment"># 获得程序结束时间</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># TODO：计算训练时间</span></span><br><span class="line">    results[<span class="string">'train_time'</span>] = end - start</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> 得到在验证集上的预测值</span></span><br><span class="line">    <span class="comment">#       然后得到对前300个训练数据的预测结果</span></span><br><span class="line">    start = time() <span class="comment"># 获得程序开始时间</span></span><br><span class="line">    predictions_val = learner.predict(X_val)</span><br><span class="line">    predictions_train = learner.predict(X_train[:<span class="number">300</span>])</span><br><span class="line">    end = time() <span class="comment"># 获得程序结束时间</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># TODO：计算预测用时</span></span><br><span class="line">    results[<span class="string">'pred_time'</span>] = end - start</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># TODO：计算在最前面的300个训练数据的准确率</span></span><br><span class="line">    results[<span class="string">'acc_train'</span>] = accuracy_score(y_test[:<span class="number">300</span>], predictions_train)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># TODO：计算在验证上的准确率</span></span><br><span class="line">    results[<span class="string">'acc_val'</span>] = accuracy_score(y_val, predictions_val)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># TODO：计算在最前面300个训练数据上的F-score</span></span><br><span class="line">    results[<span class="string">'f_train'</span>] = fbeta_score(y_train[:<span class="number">300</span>], predictions_train, beta=<span class="number">0.5</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># TODO：计算验证集上的F-score</span></span><br><span class="line">    results[<span class="string">'f_val'</span>] = fbeta_score(y_val, predictions_val, beta=<span class="number">0.5</span>)</span><br><span class="line">       </span><br><span class="line">    <span class="comment"># 成功</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"&#123;&#125; trained on &#123;&#125; samples."</span>.format(learner.__class__.__name__, sample_size))</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 返回结果</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure><h3 id="练习：初始模型的评估"><a href="#练习：初始模型的评估" class="headerlink" title="练习：初始模型的评估"></a>练习：初始模型的评估</h3><p>在下面的代码单元中，您将需要实现以下功能：             </p><ul><li>导入你在前面讨论的三个监督学习模型。             </li><li>初始化三个模型并存储在<code>&#39;clf_A&#39;</code>，<code>&#39;clf_B&#39;</code>和<code>&#39;clf_C&#39;</code>中。<ul><li>使用模型的默认参数值，在接下来的部分中你将需要对某一个模型的参数进行调整。             </li><li>设置<code>random_state</code>  (如果有这个参数)。       </li></ul></li><li>计算1%， 10%， 100%的训练数据分别对应多少个数据点，并将这些值存储在<code>&#39;samples_1&#39;</code>, <code>&#39;samples_10&#39;</code>, <code>&#39;samples_100&#39;</code>中</li></ul><p><strong>注意：</strong>取决于你选择的算法，下面实现的代码可能需要一些时间来运行！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO：从sklearn中导入三个监督学习模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="comment"># TODO：初始化三个模型</span></span><br><span class="line">clf_A = KNeighborsClassifier(n_neighbors=<span class="number">2</span>)</span><br><span class="line">clf_B = SVC(kernel=<span class="string">'linear'</span>)</span><br><span class="line">clf_C = RandomForestClassifier(random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：计算1%， 10%， 100%的训练数据分别对应多少点</span></span><br><span class="line">samples_1 = int(len(X_train) * <span class="number">0.01</span>)</span><br><span class="line">samples_10 = int(len(X_train) * <span class="number">0.10</span>)</span><br><span class="line">samples_100 = len(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 收集学习器的结果</span></span><br><span class="line">results = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> clf <span class="keyword">in</span> [clf_A, clf_B, clf_C]:</span><br><span class="line">    clf_name = clf.__class__.__name__</span><br><span class="line">    results[clf_name] = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, samples <span class="keyword">in</span> enumerate([samples_1, samples_10, samples_100]):</span><br><span class="line">        results[clf_name][i] = train_predict(clf, samples, X_train, y_train, X_val, y_val)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对选择的三个模型得到的评价结果进行可视化</span></span><br><span class="line">vs.evaluate(results, accuracy, fscore)</span><br></pre></td></tr></table></figure><pre><code>KNeighborsClassifier trained on 289 samples.KNeighborsClassifier trained on 2894 samples.KNeighborsClassifier trained on 28941 samples.SVC trained on 289 samples.SVC trained on 2894 samples.SVC trained on 28941 samples.RandomForestClassifier trained on 289 samples.RandomForestClassifier trained on 2894 samples.RandomForestClassifier trained on 28941 samples.</code></pre><p><img src="/2019/02/16/finding-donors/output_30_1.png" alt="png"></p><hr><h2 id="提高效果"><a href="#提高效果" class="headerlink" title="提高效果"></a>提高效果</h2><p>在这最后一节中，您将从三个有监督的学习模型中选择 <em>最好的</em> 模型来使用学生数据。你将在整个训练集（<code>X_train</code>和<code>y_train</code>）上使用网格搜索优化至少调节一个参数以获得一个比没有调节之前更好的 F-score。</p><h3 id="问题-3-选择最佳的模型"><a href="#问题-3-选择最佳的模型" class="headerlink" title="问题 3 - 选择最佳的模型"></a>问题 3 - 选择最佳的模型</h3><p><em>基于你前面做的评价，用一到两段话向 </em>CharityML<em> 解释这三个模型中哪一个对于判断被调查者的年收入大于 $50,000 是最合适的。</em><br><strong>提示：</strong>你的答案应该包括评价指标，预测/训练时间，以及该算法是否适合这里的数据。</p><p><strong>回答：<br>从F-score来看，RandomForest在所有训练集上和1%、10%的验证集表现最好，SVM在100%的验证集表现最好<br>从准确度上来看，RandomForest在所有训练集上和1%、10%的验证集表现最好，SVM在100%的验证集表现最好<br>从训练时间上来看，SVM明显多与其他两种算法<br>从预测时间上来看，KNeighbors最多，SVM次之，RandomForest最少<br>KNeighbors由于初始点选择的问题可能会导致分类效果不固定<br>综上所述RandomForest综合表现较好，时间短分类准，我认为最合适，有调优的空间。如果不考虑时间SVM也有可能调出更优化的结果。</strong></p><h3 id="问题-4-用通俗的话解释模型"><a href="#问题-4-用通俗的话解释模型" class="headerlink" title="问题 4 - 用通俗的话解释模型"></a>问题 4 - 用通俗的话解释模型</h3><p><em>用一到两段话，向 </em>CharityML<em> 用外行也听得懂的话来解释最终模型是如何工作的。你需要解释所选模型的主要特点。例如，这个模型是怎样被训练的，它又是如何做出预测的。避免使用高级的数学或技术术语，不要使用公式或特定的算法名词。</em></p><p><strong>回答：</strong> </p><p>训练</p><pre><code>根据所有数据依次找到能最大区分当前数据的一个特征，进行数据分割，然后对分割的数据接着重复上述步骤，直到所有的特征都判断完，这样一系列的判断对应的结果为数据的类别，这样的判断构成的就是决策树再多次执行上一步，每次执行的时候对样本进行随机有放回的抽样，构成多个不一样的决策树，这些决策树合并起来就是随机森林</code></pre><p>预测</p><pre><code>对于新来的样本，每个决策树做一个分类结果进行相等权重投票，然后以多数者投票的结果作为该样本的分类结果</code></pre><h3 id="练习：模型调优"><a href="#练习：模型调优" class="headerlink" title="练习：模型调优"></a>练习：模型调优</h3><p>调节选择的模型的参数。使用网格搜索（GridSearchCV）来至少调整模型的重要参数（至少调整一个），这个参数至少需尝试3个不同的值。你要使用整个训练集来完成这个过程。在接下来的代码单元中，你需要实现以下功能：</p><ul><li>导入<a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" target="_blank" rel="noopener"><code>sklearn.model_selection.GridSearchCV</code></a> 和 <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html" target="_blank" rel="noopener"><code>sklearn.metrics.make_scorer</code></a>.</li><li>初始化你选择的分类器，并将其存储在<code>clf</code>中。<ul><li>设置<code>random_state</code> (如果有这个参数)。</li></ul></li><li>创建一个对于这个模型你希望调整参数的字典。<ul><li>例如: parameters = {‘parameter’ : [list of values]}。</li><li><strong>注意：</strong> 如果你的学习器有 <code>max_features</code> 参数，请不要调节它！</li></ul></li><li>使用<code>make_scorer</code>来创建一个<code>fbeta_score</code>评分对象（设置$\beta = 0.5$）。</li><li>在分类器clf上用’scorer’作为评价函数运行网格搜索，并将结果存储在grid_obj中。</li><li>用训练集（X_train, y_train）训练grid search object,并将结果存储在<code>grid_fit</code>中。</li></ul><p><strong>注意：</strong> 取决于你选择的参数列表，下面实现的代码可能需要花一些时间运行！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO：导入'GridSearchCV', 'make_scorer'和其他一些需要的库</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> make_scorer</span><br><span class="line"><span class="comment"># TODO：初始化分类器</span></span><br><span class="line">clf = RandomForestClassifier(random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：创建你希望调节的参数列表</span></span><br><span class="line">parameters = &#123;<span class="string">'n_estimators'</span>:[<span class="number">10</span>,<span class="number">50</span>,<span class="number">100</span>,<span class="number">150</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：创建一个fbeta_score打分对象</span></span><br><span class="line">scorer = make_scorer(fbeta_score, beta=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：在分类器上使用网格搜索，使用'scorer'作为评价函数</span></span><br><span class="line">grid_obj = GridSearchCV(clf, parameters, scorer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：用训练数据拟合网格搜索对象并找到最佳参数</span></span><br><span class="line">grid_obj.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># 得到estimator</span></span><br><span class="line">best_clf = grid_obj.best_estimator_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用没有调优的模型做预测</span></span><br><span class="line">predictions = (clf.fit(X_train, y_train)).predict(X_val)</span><br><span class="line">best_predictions = best_clf.predict(X_val)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 汇报调优后的模型</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"best_clf\n------"</span>)</span><br><span class="line"><span class="keyword">print</span> (best_clf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 汇报调参前和调参后的分数</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"\nUnoptimized model\n------"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Accuracy score on validation data: &#123;:.4f&#125;"</span>.format(accuracy_score(y_val, predictions)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"F-score on validation data: &#123;:.4f&#125;"</span>.format(fbeta_score(y_val, predictions, beta = <span class="number">0.5</span>)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"\nOptimized Model\n------"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Final accuracy score on the validation data: &#123;:.4f&#125;"</span>.format(accuracy_score(y_val, best_predictions)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Final F-score on the validation data: &#123;:.4f&#125;"</span>.format(fbeta_score(y_val, best_predictions, beta = <span class="number">0.5</span>)))</span><br></pre></td></tr></table></figure><pre><code>best_clf------RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,            max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None,            min_impurity_split=1e-07, min_samples_leaf=1,            min_samples_split=2, min_weight_fraction_leaf=0.0,            n_estimators=100, n_jobs=1, oob_score=False, random_state=0,            verbose=0, warm_start=False)Unoptimized model------Accuracy score on validation data: 0.8389F-score on validation data: 0.6812Optimized Model------Final accuracy score on the validation data: 0.8456Final F-score on the validation data: 0.6940</code></pre><h3 id="问题-5-最终模型评估"><a href="#问题-5-最终模型评估" class="headerlink" title="问题 5 - 最终模型评估"></a>问题 5 - 最终模型评估</h3><p><em>你的最优模型在测试数据上的准确率和 F-score 是多少？这些分数比没有优化的模型好还是差？</em><br><strong>注意：</strong>请在下面的表格中填写你的结果，然后在答案框中提供讨论。</p><h4 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h4><div class="table-container"><table><thead><tr><th style="text-align:center">评价指标</th><th style="text-align:center">未优化的模型</th><th style="text-align:center">优化的模型</th></tr></thead><tbody><tr><td style="text-align:center">准确率</td><td style="text-align:center">0.8389</td><td style="text-align:center">0.8456</td></tr><tr><td style="text-align:center">F-score</td><td style="text-align:center">0.6812</td><td style="text-align:center">0.6940</td></tr></tbody></table></div><p><strong>回答：相比较没有优化的模型有少许提升</strong></p><hr><h2 id="特征的重要性"><a href="#特征的重要性" class="headerlink" title="特征的重要性"></a>特征的重要性</h2><p>在数据上（比如我们这里使用的人口普查的数据）使用监督学习算法的一个重要的任务是决定哪些特征能够提供最强的预测能力。专注于少量的有效特征和标签之间的关系，我们能够更加简单地理解这些现象，这在很多情况下都是十分有用的。在这个项目的情境下这表示我们希望选择一小部分特征，这些特征能够在预测被调查者是否年收入大于$50,000这个问题上有很强的预测能力。</p><p>选择一个有 <code>&#39;feature_importance_&#39;</code> 属性的scikit学习分类器（例如 AdaBoost，随机森林）。<code>&#39;feature_importance_&#39;</code> 属性是对特征的重要性排序的函数。在下一个代码单元中用这个分类器拟合训练集数据并使用这个属性来决定人口普查数据中最重要的5个特征。</p><h3 id="问题-6-观察特征相关性"><a href="#问题-6-观察特征相关性" class="headerlink" title="问题 6 - 观察特征相关性"></a>问题 6 - 观察特征相关性</h3><p>当<strong>探索数据</strong>的时候，它显示在这个人口普查数据集中每一条记录我们有十三个可用的特征。<br><em>在这十三个记录中，你认为哪五个特征对于预测是最重要的，选择每个特征的理由是什么？你会怎样对他们排序？</em></p><p><strong>回答：</strong></p><ul><li>特征1:age        年龄的增长事业和收入也会增长</li><li>特征2:hours-per-week   理论上工作时间越多收入越高</li><li>特征3:captial-gain   拥有其他资本收益代表经济条件更好</li><li>特征4:martial-status   收入大雨50K说明拥有更好的承担能力</li><li>特征5:education-num    教育程度越高 越有可能高收入</li></ul><h3 id="练习-提取特征重要性"><a href="#练习-提取特征重要性" class="headerlink" title="练习 - 提取特征重要性"></a>练习 - 提取特征重要性</h3><p>选择一个<code>scikit-learn</code>中有<code>feature_importance_</code>属性的监督学习分类器，这个属性是一个在做预测的时候根据所选择的算法来对特征重要性进行排序的功能。</p><p>在下面的代码单元中，你将要实现以下功能：</p><ul><li>如果这个模型和你前面使用的三个模型不一样的话从sklearn中导入一个监督学习模型。</li><li>在整个训练集上训练一个监督学习模型。</li><li>使用模型中的 <code>&#39;feature_importances_&#39;</code>提取特征的重要性。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO：导入一个有'feature_importances_'的监督学习模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：在训练集上训练一个监督学习模型</span></span><br><span class="line">model = best_clf</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO： 提取特征重要性</span></span><br><span class="line">importances = model.feature_importances_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图best_clf</span></span><br><span class="line">vs.feature_plot(importances, X_train, y_train)</span><br></pre></td></tr></table></figure><p><img src="/2019/02/16/finding-donors/output_45_0.png" alt="png"></p><h3 id="问题-7-提取特征重要性"><a href="#问题-7-提取特征重要性" class="headerlink" title="问题 7 - 提取特征重要性"></a>问题 7 - 提取特征重要性</h3><p>观察上面创建的展示五个用于预测被调查者年收入是否大于$50,000最相关的特征的可视化图像。</p><p><em>这五个特征的权重加起来是否超过了0.5?</em><br><br><em>这五个特征和你在<strong>问题 6</strong>中讨论的特征比较怎么样？</em><br><br><em>如果说你的答案和这里的相近，那么这个可视化怎样佐证了你的想法？</em><br><br><em>如果你的选择不相近，那么为什么你觉得这些特征更加相关？</em></p><p><strong>回答：0.24+0.12+0.10+0.07+0.06&gt;0.5  基本一致。权重越高说明重要性越高</strong></p><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>如果我们只是用可用特征的一个子集的话模型表现会怎么样？通过使用更少的特征来训练，在评价指标的角度来看我们的期望是训练和预测的时间会更少。从上面的可视化来看，我们可以看到前五个最重要的特征贡献了数据中<strong>所有</strong>特征中超过一半的重要性。这提示我们可以尝试去<strong>减小特征空间</strong>，简化模型需要学习的信息。下面代码单元将使用你前面发现的优化模型，并<strong>只使用五个最重要的特征</strong>在相同的训练集上训练模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入克隆模型的功能</span></span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> clone</span><br><span class="line"></span><br><span class="line"><span class="comment"># 减小特征空间</span></span><br><span class="line">X_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::<span class="number">-1</span>])[:<span class="number">5</span>]]]</span><br><span class="line">X_val_reduced = X_val[X_val.columns.values[(np.argsort(importances)[::<span class="number">-1</span>])[:<span class="number">5</span>]]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在前面的网格搜索的基础上训练一个“最好的”模型</span></span><br><span class="line">clf_on_reduced = (clone(best_clf)).fit(X_train_reduced, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 做一个新的预测</span></span><br><span class="line">reduced_predictions = clf_on_reduced.predict(X_val_reduced)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于每一个版本的数据汇报最终模型的分数</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Final Model trained on full data\n------"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Accuracy on validation data: &#123;:.4f&#125;"</span>.format(accuracy_score(y_val, best_predictions)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"F-score on validation data: &#123;:.4f&#125;"</span>.format(fbeta_score(y_val, best_predictions, beta = <span class="number">0.5</span>)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"\nFinal Model trained on reduced data\n------"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Accuracy on validation data: &#123;:.4f&#125;"</span>.format(accuracy_score(y_val, reduced_predictions)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"F-score on validation data: &#123;:.4f&#125;"</span>.format(fbeta_score(y_val, reduced_predictions, beta = <span class="number">0.5</span>)))</span><br></pre></td></tr></table></figure><pre><code>Final Model trained on full data------Accuracy on validation data: 0.8456F-score on validation data: 0.6940Final Model trained on reduced data------Accuracy on validation data: 0.8333F-score on validation data: 0.6678</code></pre><h3 id="问题-8-特征选择的影响"><a href="#问题-8-特征选择的影响" class="headerlink" title="问题 8 - 特征选择的影响"></a>问题 8 - 特征选择的影响</h3><p><em>最终模型在只是用五个特征的数据上和使用所有的特征数据上的 F-score 和准确率相比怎么样？</em><br><em>如果训练时间是一个要考虑的因素，你会考虑使用部分特征的数据作为你的训练集吗？</em></p><p><strong>回答：</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center">评价指标</th><th style="text-align:center">使用所有特征</th><th style="text-align:center">使用部分特征</th></tr></thead><tbody><tr><td style="text-align:center">准确率</td><td style="text-align:center">0.8456</td><td style="text-align:center">0.8333</td></tr><tr><td style="text-align:center">F-score</td><td style="text-align:center">0.6940</td><td style="text-align:center">0.6678</td></tr></tbody></table></div><p>使用部分特征和所有特征相比准确率和F1值出现了下降现象 如果考虑时间问题我会考虑使用部分特征 会缩短大量训练时间</p><h3 id="问题-9-在测试集上测试你的模型"><a href="#问题-9-在测试集上测试你的模型" class="headerlink" title="问题 9 - 在测试集上测试你的模型"></a>问题 9 - 在测试集上测试你的模型</h3><p>终于到了测试的时候，记住，测试集只能用一次。</p><p><em>使用你最有信心的模型，在测试集上测试，计算出准确率和 F-score。</em><br><em>简述你选择这个模型的原因，并分析测试结果</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#TODO test your model on testing data and report accuracy and F score</span></span><br><span class="line">res = best_clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> fbeta_score,accuracy_score</span><br><span class="line">print(<span class="string">'accuracy score:&#123;&#125;'</span>.format(accuracy_score(y_test, res)))</span><br><span class="line">print(<span class="string">'fbeta score:&#123;&#125;'</span>.format(fbeta_score(y_test, res, beta=<span class="number">0.5</span>)))</span><br></pre></td></tr></table></figure><pre><code>accuracy score:0.8383637368711996fbeta score:0.6780398221534892</code></pre><blockquote><p><strong>注意：</strong> 当你写完了所有的代码，并且回答了所有的问题。你就可以把你的 iPython Notebook 导出成 HTML 文件。你可以在菜单栏，这样导出<strong>File -&gt; Download as -&gt; HTML (.html)</strong>把这个 HTML 和这个 iPython notebook 一起做为你的作业提交。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;机器学习纳米学位&quot;&gt;&lt;a href=&quot;#机器学习纳米学位&quot; class=&quot;headerlink&quot; title=&quot;机器学习纳米学位&quot;&gt;&lt;/a&gt;机器学习纳米学位&lt;/h1&gt;&lt;h2 id=&quot;监督学习&quot;&gt;&lt;a href=&quot;#监督学习&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>dog_app by CNN</title>
    <link href="http://yoursite.com/2019/02/15/dog-app-for-CNN/"/>
    <id>http://yoursite.com/2019/02/15/dog-app-for-CNN/</id>
    <published>2019-02-15T13:34:56.000Z</published>
    <updated>2019-02-16T11:07:20.711Z</updated>
    
    <content type="html"><![CDATA[<h2 id="卷积神经网络（Convolutional-Neural-Network-CNN）"><a href="#卷积神经网络（Convolutional-Neural-Network-CNN）" class="headerlink" title="卷积神经网络（Convolutional Neural Network, CNN）"></a>卷积神经网络（Convolutional Neural Network, CNN）</h2><h2 id="项目：实现一个狗品种识别算法App"><a href="#项目：实现一个狗品种识别算法App" class="headerlink" title="项目：实现一个狗品种识别算法App"></a>项目：实现一个狗品种识别算法App</h2><p>在这个notebook文件中，有些模板代码已经提供给你，但你还需要实现更多的功能来完成这个项目。除非有明确要求，你无须修改任何已给出的代码。以<strong>‘(练习)’</strong>开始的标题表示接下来的代码部分中有你需要实现的功能。这些部分都配有详细的指导，需要实现的部分也会在注释中以’TODO’标出。请仔细阅读所有的提示。</p><p>除了实现代码外，你还<strong>需要</strong>回答一些与项目及代码相关的问题。每个需要回答的问题都会以 <strong>‘问题 X’</strong> 标记。请仔细阅读每个问题，并且在问题后的 <strong>‘回答’</strong> 部分写出完整的答案。我们将根据 你对问题的回答 和 撰写代码实现的功能 来对你提交的项目进行评分。</p><blockquote><p><strong>提示：</strong>Code 和 Markdown 区域可通过 <strong>Shift + Enter</strong> 快捷键运行。此外，Markdown可以通过双击进入编辑模式。</p></blockquote><p>项目中显示为<em>选做</em>的部分可以帮助你的项目脱颖而出，而不是仅仅达到通过的最低要求。如果你决定追求更高的挑战，请在此 notebook 中完成<em>选做</em>部分的代码。</p><hr><h3 id="让我们开始吧"><a href="#让我们开始吧" class="headerlink" title="让我们开始吧"></a>让我们开始吧</h3><p>在这个notebook中，你将迈出第一步，来开发可以作为移动端或 Web应用程序一部分的算法。在这个项目的最后，你的程序将能够把用户提供的任何一个图像作为输入。如果可以从图像中检测到一只狗，它会输出对狗品种的预测。如果图像中是一个人脸，它会预测一个与其最相似的狗的种类。下面这张图展示了完成项目后可能的输出结果。（……实际上我们希望每个学生的输出结果不相同！）</p><p><img src="/2019/02/15/dog-app-for-CNN/sample_dog_output.png" alt="Sample Dog Output"></p><p>在现实世界中，你需要拼凑一系列的模型来完成不同的任务；举个例子，用来预测狗种类的算法会与预测人类的算法不同。在做项目的过程中，你可能会遇到不少失败的预测，因为并不存在完美的算法和模型。你最终提交的不完美的解决方案也一定会给你带来一个有趣的学习经验！</p><h3 id="项目内容"><a href="#项目内容" class="headerlink" title="项目内容"></a>项目内容</h3><p>我们将这个notebook分为不同的步骤，你可以使用下面的链接来浏览此notebook。</p><ul><li><a href="#step0">Step 0</a>: 导入数据集</li><li><a href="#step1">Step 1</a>: 检测人脸</li><li><a href="#step2">Step 2</a>: 检测狗狗</li><li><a href="#step3">Step 3</a>: 从头创建一个CNN来分类狗品种</li><li><a href="#step4">Step 4</a>: 使用一个CNN来区分狗的品种(使用迁移学习)</li><li><a href="#step5">Step 5</a>: 建立一个CNN来分类狗的品种（使用迁移学习）</li><li><a href="#step6">Step 6</a>: 完成你的算法</li><li><a href="#step7">Step 7</a>: 测试你的算法</li></ul><p>在该项目中包含了如下的问题：</p><ul><li><a href="#question1">问题 1</a></li><li><a href="#question2">问题 2</a></li><li><a href="#question3">问题 3</a></li><li><a href="#question4">问题 4</a></li><li><a href="#question5">问题 5</a></li><li><a href="#question6">问题 6</a></li><li><a href="#question7">问题 7</a></li><li><a href="#question8">问题 8</a></li><li><a href="#question9">问题 9</a></li><li><a href="#question10">问题 10</a></li><li><a href="#question11">问题 11</a></li></ul><hr><p><a id="step0"></a></p><h2 id="步骤-0-导入数据集"><a href="#步骤-0-导入数据集" class="headerlink" title="步骤 0: 导入数据集"></a>步骤 0: 导入数据集</h2><h3 id="导入狗数据集"><a href="#导入狗数据集" class="headerlink" title="导入狗数据集"></a>导入狗数据集</h3><p>在下方的代码单元（cell）中，我们导入了一个狗图像的数据集。我们使用 scikit-learn 库中的 <code>load_files</code> 函数来获取一些变量：</p><ul><li><code>train_files</code>, <code>valid_files</code>, <code>test_files</code> - 包含图像的文件路径的numpy数组</li><li><code>train_targets</code>, <code>valid_targets</code>, <code>test_targets</code> - 包含独热编码分类标签的numpy数组</li><li><code>dog_names</code> - 由字符串构成的与标签相对应的狗的种类</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_files       </span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> glob <span class="keyword">import</span> glob</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义函数来加载train，test和validation数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">(path)</span>:</span></span><br><span class="line">    data = load_files(path)</span><br><span class="line">    dog_files = np.array(data[<span class="string">'filenames'</span>])</span><br><span class="line">    dog_targets = np_utils.to_categorical(np.array(data[<span class="string">'target'</span>]), <span class="number">133</span>)</span><br><span class="line">    <span class="keyword">return</span> dog_files, dog_targets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载train，test和validation数据集</span></span><br><span class="line">train_files, train_targets = load_dataset(<span class="string">'dogImages/dogImages/train'</span>)</span><br><span class="line">valid_files, valid_targets = load_dataset(<span class="string">'dogImages/dogImages/valid'</span>)</span><br><span class="line">test_files, test_targets = load_dataset(<span class="string">'dogImages/dogImages/test'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载狗品种列表</span></span><br><span class="line">dog_names = [item[<span class="number">20</span>:<span class="number">-1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> sorted(glob(<span class="string">"dogImages/dogImages/train/*/"</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印数据统计描述</span></span><br><span class="line">print(<span class="string">'There are %d total dog categories.'</span> % len(dog_names))</span><br><span class="line">print(<span class="string">'There are %s total dog images.\n'</span> % len(np.hstack([train_files, valid_files, test_files])))</span><br><span class="line">print(<span class="string">'There are %d training dog images.'</span> % len(train_files))</span><br><span class="line">print(<span class="string">'There are %d validation dog images.'</span> % len(valid_files))</span><br><span class="line">print(<span class="string">'There are %d test dog images.'</span>% len(test_files))</span><br></pre></td></tr></table></figure><pre><code>Using TensorFlow backend.There are 133 total dog categories.There are 8351 total dog images.There are 6680 training dog images.There are 835 validation dog images.There are 836 test dog images.</code></pre><h3 id="导入人脸数据集"><a href="#导入人脸数据集" class="headerlink" title="导入人脸数据集"></a>导入人脸数据集</h3><p>在下方的代码单元中，我们导入人脸图像数据集，文件所在路径存储在名为 <code>human_files</code> 的 numpy 数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">random.seed(<span class="number">8675309</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载打乱后的人脸数据集的文件名</span></span><br><span class="line">human_files = np.array(glob(<span class="string">"lfw/lfw/*/*"</span>))</span><br><span class="line">random.shuffle(human_files)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印数据集的数据量</span></span><br><span class="line">print(<span class="string">'There are %d total human images.'</span> % len(human_files))</span><br></pre></td></tr></table></figure><pre><code>There are 13233 total human images.</code></pre><hr><p><a id="step1"></a></p><h2 id="步骤1：检测人脸"><a href="#步骤1：检测人脸" class="headerlink" title="步骤1：检测人脸"></a>步骤1：检测人脸</h2><p>我们将使用 OpenCV 中的 <a href="http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html" target="_blank" rel="noopener">Haar feature-based cascade classifiers</a> 来检测图像中的人脸。OpenCV 提供了很多预训练的人脸检测模型，它们以XML文件保存在 <a href="https://github.com/opencv/opencv/tree/master/data/haarcascades" target="_blank" rel="noopener">github</a>。我们已经下载了其中一个检测模型，并且把它存储在 <code>haarcascades</code> 的目录中。</p><p>在如下代码单元中，我们将演示如何使用这个检测模型在样本图像中找到人脸。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2                </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt                        </span><br><span class="line">%matplotlib inline                               </span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取预训练的人脸检测模型</span></span><br><span class="line">face_cascade = cv2.CascadeClassifier(<span class="string">'haarcascades/haarcascade_frontalface_alt.xml'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载彩色（通道顺序为BGR）图像</span></span><br><span class="line">img = cv2.imread(human_files[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将BGR图像进行灰度处理</span></span><br><span class="line">gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在图像中找出脸</span></span><br><span class="line">faces = face_cascade.detectMultiScale(gray)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印图像中检测到的脸的个数</span></span><br><span class="line">print(<span class="string">'Number of faces detected:'</span>, len(faces))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取每一个所检测到的脸的识别框</span></span><br><span class="line"><span class="keyword">for</span> (x,y,w,h) <span class="keyword">in</span> faces:</span><br><span class="line">    <span class="comment"># 在人脸图像中绘制出识别框</span></span><br><span class="line">    cv2.rectangle(img,(x,y),(x+w,y+h),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 将BGR图像转变为RGB图像以打印</span></span><br><span class="line">cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示含有识别框的图像</span></span><br><span class="line">plt.imshow(cv_rgb)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>Number of faces detected: 1</code></pre><p><img src="/2019/02/15/dog-app-for-CNN/output_5_1.png" alt="png"></p><p>在使用任何一个检测模型之前，将图像转换为灰度图是常用过程。<code>detectMultiScale</code> 函数使用储存在 <code>face_cascade</code> 中的的数据，对输入的灰度图像进行分类。</p><p>在上方的代码中，<code>faces</code> 以 numpy 数组的形式，保存了识别到的面部信息。它其中每一行表示一个被检测到的脸，该数据包括如下四个信息：前两个元素  <code>x</code>、<code>y</code> 代表识别框左上角的 x 和 y 坐标（参照上图，注意 y 坐标的方向和我们默认的方向不同）；后两个元素代表识别框在 x 和 y 轴两个方向延伸的长度 <code>w</code> 和 <code>d</code>。 </p><h3 id="写一个人脸识别器"><a href="#写一个人脸识别器" class="headerlink" title="写一个人脸识别器"></a>写一个人脸识别器</h3><p>我们可以将这个程序封装为一个函数。该函数的输入为人脸图像的<strong>路径</strong>，当图像中包含人脸时，该函数返回 <code>True</code>，反之返回 <code>False</code>。该函数定义如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果img_path路径表示的图像检测到了脸，返回"True" </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">face_detector</span><span class="params">(img_path)</span>:</span></span><br><span class="line">    img = cv2.imread(img_path)</span><br><span class="line">    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><br><span class="line">    faces = face_cascade.detectMultiScale(gray)</span><br><span class="line">    <span class="keyword">return</span> len(faces) &gt; <span class="number">0</span></span><br></pre></td></tr></table></figure><h3 id="【练习】-评估人脸检测模型"><a href="#【练习】-评估人脸检测模型" class="headerlink" title="【练习】 评估人脸检测模型"></a><strong>【练习】</strong> 评估人脸检测模型</h3><hr><p><a id="question1"></a></p><h3 id="问题-1"><a href="#问题-1" class="headerlink" title="问题 1:"></a><strong>问题 1:</strong></h3><p>在下方的代码块中，使用 <code>face_detector</code> 函数，计算：</p><ul><li><code>human_files</code> 的前100张图像中，能够检测到<strong>人脸</strong>的图像占比多少？</li><li><code>dog_files</code> 的前100张图像中，能够检测到<strong>人脸</strong>的图像占比多少？</li></ul><p>理想情况下，人图像中检测到人脸的概率应当为100%，而狗图像中检测到人脸的概率应该为0%。你会发现我们的算法并非完美，但结果仍然是可以接受的。我们从每个数据集中提取前100个图像的文件路径，并将它们存储在<code>human_files_short</code>和<code>dog_files_short</code>中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">human_files_short = human_files[:<span class="number">100</span>]</span><br><span class="line">dog_files_short = train_files[:<span class="number">100</span>]</span><br><span class="line"><span class="comment">## 请不要修改上方代码</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detect</span><span class="params">(detector, files)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(list(map(detector, files)))</span><br><span class="line"><span class="comment">## <span class="doctag">TODO:</span> 基于human_files_short和dog_files_short</span></span><br><span class="line"><span class="comment">## 中的图像测试face_detector的表现</span></span><br><span class="line">print(<span class="string">'humna: &#123;:.2%&#125;'</span>.format(detect(face_detector,human_files_short)))</span><br><span class="line">print(<span class="string">'dog: &#123;:.2%&#125;'</span>.format(detect(face_detector, dog_files_short)))</span><br></pre></td></tr></table></figure><pre><code>humna: 99.00%dog: 12.00%</code></pre><hr><p><a id="question2"></a></p><h3 id="问题-2"><a href="#问题-2" class="headerlink" title="问题 2:"></a><strong>问题 2:</strong></h3><p>就算法而言，该算法成功与否的关键在于，用户能否提供含有清晰面部特征的人脸图像。<br>那么你认为，这样的要求在实际使用中对用户合理吗？如果你觉得不合理，你能否想到一个方法，即使图像中并没有清晰的面部特征，也能够检测到人脸？</p><p><strong>回答: 不合理 轮廓检测 压缩图像特征 通过正图像和负图像训练分类器 给予权重 确定位置  最后保留正图像</strong></p><hr><p><a id="Selection1"></a></p><h3 id="选做："><a href="#选做：" class="headerlink" title="选做："></a>选做：</h3><p>我们建议在你的算法中使用opencv的人脸检测模型去检测人类图像，不过你可以自由地探索其他的方法，尤其是尝试使用深度学习来解决它:)。请用下方的代码单元来设计和测试你的面部监测算法。如果你决定完成这个<em>选做</em>任务，你需要报告算法在每一个数据集上的表现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## (选做) <span class="doctag">TODO:</span> 报告另一个面部检测算法在LFW数据集上的表现</span></span><br><span class="line"><span class="comment">### 你可以随意使用所需的代码单元数</span></span><br><span class="line"><span class="keyword">import</span> cv2                </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt                        </span><br><span class="line">%matplotlib inline                               </span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取预训练的人脸检测模型</span></span><br><span class="line">face_cascade = cv2.CascadeClassifier(<span class="string">'haarcascades/haarcascade_frontalface_alt2.xml'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载彩色（通道顺序为BGR）图像</span></span><br><span class="line">img = cv2.imread(human_files[<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将BGR图像进行灰度处理</span></span><br><span class="line">gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在图像中找出脸</span></span><br><span class="line">faces = face_cascade.detectMultiScale(gray)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印图像中检测到的脸的个数</span></span><br><span class="line">print(<span class="string">'Number of faces detected:'</span>, len(faces))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取每一个所检测到的脸的识别框</span></span><br><span class="line"><span class="keyword">for</span> (x,y,w,h) <span class="keyword">in</span> faces:</span><br><span class="line">    <span class="comment"># 在人脸图像中绘制出识别框</span></span><br><span class="line">    cv2.rectangle(img,(x,y),(x+w,y+h),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 将BGR图像转变为RGB图像以打印</span></span><br><span class="line">cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示含有识别框的图像</span></span><br><span class="line">plt.imshow(cv_rgb)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>Number of faces detected: 1</code></pre><p><img src="/2019/02/15/dog-app-for-CNN/output_13_1.png" alt="png"></p><hr><p><a id="step2"></a></p><h2 id="步骤-2-检测狗狗"><a href="#步骤-2-检测狗狗" class="headerlink" title="步骤 2: 检测狗狗"></a>步骤 2: 检测狗狗</h2><p>在这个部分中，我们使用预训练的 <a href="http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006" target="_blank" rel="noopener">ResNet-50</a> 模型去检测图像中的狗。下方的第一行代码就是下载了 ResNet-50 模型的网络结构参数，以及基于 <a href="http://www.image-net.org/" target="_blank" rel="noopener">ImageNet</a> 数据集的预训练权重。</p><p>ImageNet 这目前一个非常流行的数据集，常被用来测试图像分类等计算机视觉任务相关的算法。它包含超过一千万个 URL，每一个都链接到 <a href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a" target="_blank" rel="noopener">1000 categories</a> 中所对应的一个物体的图像。任给输入一个图像，该 ResNet-50 模型会返回一个对图像中物体的预测结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.applications.resnet50 <span class="keyword">import</span> ResNet50</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义ResNet50模型</span></span><br><span class="line">ResNet50_model = ResNet50(weights=<span class="string">'imagenet'</span>)</span><br></pre></td></tr></table></figure><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><ul><li>在使用 TensorFlow 作为后端的时候，在 Keras 中，CNN 的输入是一个4维数组（也被称作4维张量），它的各维度尺寸为 <code>(nb_samples, rows, columns, channels)</code>。其中 <code>nb_samples</code> 表示图像（或者样本）的总数，<code>rows</code>, <code>columns</code>, 和 <code>channels</code> 分别表示图像的行数、列数和通道数。</li></ul><ul><li>下方的 <code>path_to_tensor</code> 函数实现如下将彩色图像的字符串型的文件路径作为输入，返回一个4维张量，作为 Keras CNN 输入。因为我们的输入图像是彩色图像，因此它们具有三个通道（ <code>channels</code> 为 <code>3</code>）。<ol><li>该函数首先读取一张图像，然后将其缩放为 224×224 的图像。</li><li>随后，该图像被调整为具有4个维度的张量。</li><li>对于任一输入图像，最后返回的张量的维度是：<code>(1, 224, 224, 3)</code>。</li></ol></li></ul><ul><li><code>paths_to_tensor</code> 函数将图像路径的字符串组成的 numpy 数组作为输入，并返回一个4维张量，各维度尺寸为 <code>(nb_samples, 224, 224, 3)</code>。 在这里，<code>nb_samples</code>是提供的图像路径的数据中的样本数量或图像数量。你也可以将 <code>nb_samples</code> 理解为数据集中3维张量的个数（每个3维张量表示一个不同的图像。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image                  </span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">path_to_tensor</span><span class="params">(img_path)</span>:</span></span><br><span class="line">    <span class="comment"># 用PIL加载RGB图像为PIL.Image.Image类型</span></span><br><span class="line">    img = image.load_img(img_path, target_size=(<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    <span class="comment"># 将PIL.Image.Image类型转化为格式为(224, 224, 3)的3维张量</span></span><br><span class="line">    x = image.img_to_array(img)</span><br><span class="line">    <span class="comment"># 将3维张量转化为格式为(1, 224, 224, 3)的4维张量并返回</span></span><br><span class="line">    <span class="keyword">return</span> np.expand_dims(x, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">paths_to_tensor</span><span class="params">(img_paths)</span>:</span></span><br><span class="line">    list_of_tensors = [path_to_tensor(img_path) <span class="keyword">for</span> img_path <span class="keyword">in</span> tqdm(img_paths)]</span><br><span class="line">    <span class="keyword">return</span> np.vstack(list_of_tensors)</span><br></pre></td></tr></table></figure><h3 id="基于-ResNet-50-架构进行预测"><a href="#基于-ResNet-50-架构进行预测" class="headerlink" title="基于 ResNet-50 架构进行预测"></a>基于 ResNet-50 架构进行预测</h3><p>对于通过上述步骤得到的四维张量，在把它们输入到 ResNet-50 网络、或 Keras 中其他类似的预训练模型之前，还需要进行一些额外的处理：</p><ol><li>首先，这些图像的通道顺序为 RGB，我们需要重排他们的通道顺序为 BGR。</li><li>其次，预训练模型的输入都进行了额外的归一化过程。因此我们在这里也要对这些张量进行归一化，即对所有图像所有像素都减去像素均值 <code>[103.939, 116.779, 123.68]</code>（以 RGB 模式表示，根据所有的 ImageNet 图像算出）。</li></ol><p>导入的 <code>preprocess_input</code> 函数实现了这些功能。如果你对此很感兴趣，可以在 <a href="https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py" target="_blank" rel="noopener">这里</a> 查看 <code>preprocess_input</code>的代码。</p><p>在实现了图像处理的部分之后，我们就可以使用模型来进行预测。这一步通过 <code>predict</code> 方法来实现，它返回一个向量，向量的第 i 个元素表示该图像属于第 i 个 ImageNet 类别的概率。这通过如下的 <code>ResNet50_predict_labels</code> 函数实现。</p><p>通过对预测出的向量取用 argmax 函数（找到有最大概率值的下标序号），我们可以得到一个整数，即模型预测到的物体的类别。进而根据这个 <a href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a" target="_blank" rel="noopener">清单</a>，我们能够知道这具体是哪个品种的狗狗。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.applications.resnet50 <span class="keyword">import</span> preprocess_input, decode_predictions</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet50_predict_labels</span><span class="params">(img_path)</span>:</span></span><br><span class="line">    <span class="comment"># 返回img_path路径的图像的预测向量</span></span><br><span class="line">    img = preprocess_input(path_to_tensor(img_path))</span><br><span class="line">    <span class="keyword">return</span> np.argmax(ResNet50_model.predict(img))</span><br></pre></td></tr></table></figure><h3 id="完成狗检测模型"><a href="#完成狗检测模型" class="headerlink" title="完成狗检测模型"></a>完成狗检测模型</h3><p>在研究该 <a href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a" target="_blank" rel="noopener">清单</a> 的时候，你会注意到，狗类别对应的序号为151-268。因此，在检查预训练模型判断图像是否包含狗的时候，我们只需要检查如上的 <code>ResNet50_predict_labels</code> 函数是否返回一个介于151和268之间（包含区间端点）的值。</p><p>我们通过这些想法来完成下方的 <code>dog_detector</code> 函数，如果从图像中检测到狗就返回 <code>True</code>，否则返回 <code>False</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dog_detector</span><span class="params">(img_path)</span>:</span></span><br><span class="line">    prediction = ResNet50_predict_labels(img_path)</span><br><span class="line">    <span class="keyword">return</span> ((prediction &lt;= <span class="number">268</span>) &amp; (prediction &gt;= <span class="number">151</span>))</span><br></pre></td></tr></table></figure><h3 id="【作业】评估狗狗检测模型"><a href="#【作业】评估狗狗检测模型" class="headerlink" title="【作业】评估狗狗检测模型"></a>【作业】评估狗狗检测模型</h3><hr><p><a id="question3"></a></p><h3 id="问题-3"><a href="#问题-3" class="headerlink" title="问题 3:"></a><strong>问题 3:</strong></h3><p>在下方的代码块中，使用 <code>dog_detector</code> 函数，计算：</p><ul><li><code>human_files_short</code>中图像检测到狗狗的百分比？</li><li><code>dog_files_short</code>中图像检测到狗狗的百分比？</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### <span class="doctag">TODO:</span> 测试dog_detector函数在human_files_short和dog_files_short的表现</span></span><br><span class="line">print(<span class="string">'humna: &#123;:.2%&#125;'</span>.format(detect(dog_detector,human_files_short)))</span><br><span class="line">print(<span class="string">'dog: &#123;:.2%&#125;'</span>.format(detect(dog_detector, dog_files_short)))</span><br></pre></td></tr></table></figure><pre><code>humna: 2.00%dog: 100.00%</code></pre><hr><p><a id="step3"></a></p><h2 id="步骤-3-从头开始创建一个CNN来分类狗品种"><a href="#步骤-3-从头开始创建一个CNN来分类狗品种" class="headerlink" title="步骤 3: 从头开始创建一个CNN来分类狗品种"></a>步骤 3: 从头开始创建一个CNN来分类狗品种</h2><p>现在我们已经实现了一个函数，能够在图像中识别人类及狗狗。但我们需要更进一步的方法，来对狗的类别进行识别。在这一步中，你需要实现一个卷积神经网络来对狗的品种进行分类。你需要<strong>从头实现</strong>你的卷积神经网络（在这一阶段，你还不能使用迁移学习），并且你需要达到超过1%的测试集准确率。在本项目的步骤五种，你还有机会使用迁移学习来实现一个准确率大大提高的模型。</p><p>在添加卷积层的时候，注意不要加上太多的（可训练的）层。更多的参数意味着更长的训练时间，也就是说你更可能需要一个 GPU 来加速训练过程。万幸的是，Keras 提供了能够轻松预测每次迭代（epoch）花费时间所需的函数。你可以据此推断你算法所需的训练时间。</p><p>值得注意的是，对狗的图像进行分类是一项极具挑战性的任务。因为即便是一个正常人，也很难区分布列塔尼犬和威尔士史宾格犬。</p><div class="table-container"><table><thead><tr><th>布列塔尼犬（Brittany）</th><th>威尔士史宾格犬（Welsh Springer Spaniel）</th></tr></thead><tbody><tr><td><img src="/2019/02/15/dog-app-for-CNN/Brittany_02625.jpg" width="100"></td><td><img src="/2019/02/15/dog-app-for-CNN/Welsh_springer_spaniel_08203.jpg" width="200"></td></tr></tbody></table></div><p>不难发现其他的狗品种会有很小的类间差别（比如金毛寻回犬和美国水猎犬）。</p><div class="table-container"><table><thead><tr><th>金毛寻回犬（Curly-Coated Retriever）</th><th>美国水猎犬（American Water Spaniel）</th></tr></thead><tbody><tr><td><img src="/2019/02/15/dog-app-for-CNN/Curly-coated_retriever_03896.jpg" width="200"></td><td><img src="/2019/02/15/dog-app-for-CNN/American_water_spaniel_00648.jpg" width="200"></td></tr></tbody></table></div><p>同样，拉布拉多犬（labradors）有黄色、棕色和黑色这三种。那么你设计的基于视觉的算法将不得不克服这种较高的类间差别，以达到能够将这些不同颜色的同类狗分到同一个品种中。</p><div class="table-container"><table><thead><tr><th>黄色拉布拉多犬（Yellow Labrador）</th><th>棕色拉布拉多犬（Chocolate Labrador）</th><th>黑色拉布拉多犬（Black Labrador）</th></tr></thead><tbody><tr><td><img src="/2019/02/15/dog-app-for-CNN/Labrador_retriever_06457.jpg" width="150"></td><td><img src="/2019/02/15/dog-app-for-CNN/Labrador_retriever_06455.jpg" width="240"></td><td><img src="/2019/02/15/dog-app-for-CNN/Labrador_retriever_06449.jpg" width="220"></td></tr></tbody></table></div><p>我们也提到了随机分类将得到一个非常低的结果：不考虑品种略有失衡的影响，随机猜测到正确品种的概率是1/133，相对应的准确率是低于1%的。</p><p>请记住，在深度学习领域，实践远远高于理论。大量尝试不同的框架吧，相信你的直觉！当然，玩得开心！</p><h3 id="数据预处理-1"><a href="#数据预处理-1" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>通过对每张图像的像素值除以255，我们对图像实现了归一化处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> ImageFile                            </span><br><span class="line">ImageFile.LOAD_TRUNCATED_IMAGES = <span class="keyword">True</span>                 </span><br><span class="line"></span><br><span class="line"><span class="comment"># Keras中的数据预处理过程</span></span><br><span class="line">train_tensors = paths_to_tensor(train_files).astype(<span class="string">'float32'</span>)/<span class="number">255</span></span><br><span class="line">valid_tensors = paths_to_tensor(valid_files).astype(<span class="string">'float32'</span>)/<span class="number">255</span></span><br><span class="line">test_tensors = paths_to_tensor(test_files).astype(<span class="string">'float32'</span>)/<span class="number">255</span></span><br></pre></td></tr></table></figure><pre><code>100%|██████████| 6680/6680 [00:56&lt;00:00, 117.40it/s]100%|██████████| 835/835 [00:32&lt;00:00, 25.94it/s]100%|██████████| 836/836 [00:15&lt;00:00, 54.77it/s]</code></pre><h3 id="【练习】模型架构"><a href="#【练习】模型架构" class="headerlink" title="【练习】模型架构"></a>【练习】模型架构</h3><p>创建一个卷积神经网络来对狗品种进行分类。在你代码块的最后，执行 <code>model.summary()</code> 来输出你模型的总结信息。</p><p>我们已经帮你导入了一些所需的 Python 库，如有需要你可以自行导入。如果你在过程中遇到了困难，如下是给你的一点小提示——该模型能够在5个 epoch 内取得超过1%的测试准确率，并且能在CPU上很快地训练。</p><p><img src="/2019/02/15/dog-app-for-CNN/sample_cnn.png" alt="Sample CNN"></p><hr><p><a id="question4"></a>  </p><h3 id="问题-4"><a href="#问题-4" class="headerlink" title="问题 4:"></a><strong>问题 4:</strong></h3><p>在下方的代码块中尝试使用 Keras 搭建卷积网络的架构，并回答相关的问题。</p><ol><li>你可以尝试自己搭建一个卷积网络的模型，那么你需要回答你搭建卷积网络的具体步骤（用了哪些层）以及为什么这样搭建。</li><li>你也可以根据上图提示的步骤搭建卷积网络，那么请说明为何如上的架构能够在该问题上取得很好的表现。</li></ol><p><strong>回答: 建立一个卷基层 建立一个池化层 再建立一个卷基层 再建立一个池化层 再建立一个卷基层 再建立一个池化层 添加一个全局池化层 再添加一个全连接层 可以对图像进行更细致的处理 但最终结果不会因为层数越深处理越好</strong> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Flatten, BatchNormalization, Activation</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dropout, Flatten, Dense</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment">### <span class="doctag">TODO:</span> 定义你的网络架构</span></span><br><span class="line">model.add(Conv2D(filters=<span class="number">3</span>, kernel_size=<span class="number">2</span>, padding=<span class="string">'same'</span>,activation=<span class="string">'relu'</span>,input_shape=(<span class="number">224</span>,<span class="number">224</span>,<span class="number">3</span>)))</span><br><span class="line">model.add(MaxPooling2D(pool_size=<span class="number">2</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.2</span>))</span><br><span class="line">model.add(Conv2D(filters=<span class="number">6</span>, kernel_size=<span class="number">2</span>, padding=<span class="string">'same'</span>,activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(MaxPooling2D(pool_size=<span class="number">2</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.2</span>))</span><br><span class="line">model.add(Conv2D(<span class="number">16</span>, (<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>))</span><br><span class="line">model.add(MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.2</span>))</span><br><span class="line">model.add(GlobalAveragePooling2D())</span><br><span class="line">model.add(Dense(<span class="number">133</span> ,activation=<span class="string">'softmax'</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d_1 (Conv2D)            (None, 224, 224, 3)       39        _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 112, 112, 3)       0         _________________________________________________________________dropout_1 (Dropout)          (None, 112, 112, 3)       0         _________________________________________________________________conv2d_2 (Conv2D)            (None, 112, 112, 6)       78        _________________________________________________________________max_pooling2d_3 (MaxPooling2 (None, 56, 56, 6)         0         _________________________________________________________________dropout_2 (Dropout)          (None, 56, 56, 6)         0         _________________________________________________________________conv2d_3 (Conv2D)            (None, 54, 54, 16)        880       _________________________________________________________________max_pooling2d_4 (MaxPooling2 (None, 27, 27, 16)        0         _________________________________________________________________batch_normalization_1 (Batch (None, 27, 27, 16)        64        _________________________________________________________________activation_50 (Activation)   (None, 27, 27, 16)        0         _________________________________________________________________dropout_3 (Dropout)          (None, 27, 27, 16)        0         _________________________________________________________________global_average_pooling2d_1 ( (None, 16)                0         _________________________________________________________________dense_1 (Dense)              (None, 133)               2261      =================================================================Total params: 3,322Trainable params: 3,290Non-trainable params: 32_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 编译模型</span></span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>, loss=<span class="string">'categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure><hr><h2 id="【练习】训练模型"><a href="#【练习】训练模型" class="headerlink" title="【练习】训练模型"></a>【练习】训练模型</h2><hr><p><a id="question5"></a>  </p><h3 id="问题-5"><a href="#问题-5" class="headerlink" title="问题 5:"></a><strong>问题 5:</strong></h3><p>在下方代码单元训练模型。使用模型检查点（model checkpointing）来储存具有最低验证集 loss 的模型。</p><p>可选题：你也可以对训练集进行 <a href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html" target="_blank" rel="noopener">数据增强</a>，来优化模型的表现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint  </span><br><span class="line"></span><br><span class="line"><span class="comment">### <span class="doctag">TODO:</span> 设置训练模型的epochs的数量</span></span><br><span class="line"></span><br><span class="line">epochs = <span class="number">35</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 不要修改下方代码</span></span><br><span class="line"></span><br><span class="line">checkpointer = ModelCheckpoint(filepath=<span class="string">'saved_models/weights.best.from_scratch.hdf5'</span>, </span><br><span class="line">                               verbose=<span class="number">1</span>, save_best_only=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">model.fit(train_tensors, train_targets, </span><br><span class="line">          validation_data=(valid_tensors, valid_targets),</span><br><span class="line">          epochs=epochs, batch_size=<span class="number">20</span>, callbacks=[checkpointer], verbose=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>Train on 6680 samples, validate on 835 samplesEpoch 1/356680/6680 [==============================] - 77s 12ms/step - loss: 4.8886 - acc: 0.0073 - val_loss: 4.8812 - val_acc: 0.0108Epoch 00001: val_loss improved from inf to 4.88124, saving model to saved_models/weights.best.from_scratch.hdf5Epoch 2/356680/6680 [==============================] - 67s 10ms/step - loss: 4.8673 - acc: 0.0099 - val_loss: 4.8750 - val_acc: 0.0108 - ETA: 1:06 - loss: 4.8847 - acc: 0.0000e+00Epoch 00002: val_loss improved from 4.88124 to 4.87504, saving model to saved_models/weights.best.from_scratch.hdf5Epoch 3/356680/6680 [==============================] - 67s 10ms/step - loss: 4.8608 - acc: 0.0121 - val_loss: 4.8750 - val_acc: 0.0132 - ETA: 51s - loss: 4.8555 - acc: 0.0149Epoch 00003: val_loss did not improveEpoch 4/356680/6680 [==============================] - 67s 10ms/step - loss: 4.8569 - acc: 0.0123 - val_loss: 4.8735 - val_acc: 0.0156Epoch 00004: val_loss improved from 4.87504 to 4.87351, saving model to saved_models/weights.best.from_scratch.hdf5Epoch 5/356680/6680 [==============================] - 68s 10ms/step - loss: 4.8508 - acc: 0.0144 - val_loss: 4.8756 - val_acc: 0.0120Epoch 00005: val_loss did not improveEpoch 6/356680/6680 [==============================] - 68s 10ms/step - loss: 4.8452 - acc: 0.0138 - val_loss: 4.8623 - val_acc: 0.0156Epoch 00006: val_loss improved from 4.87351 to 4.86231, saving model to saved_models/weights.best.from_scratch.hdf5Epoch 7/356680/6680 [==============================] - 68s 10ms/step - loss: 4.8388 - acc: 0.0163 - val_loss: 4.8579 - val_acc: 0.0144Epoch 00007: val_loss improved from 4.86231 to 4.85791, saving model to saved_models/weights.best.from_scratch.hdf5Epoch 8/356680/6680 [==============================] - 68s 10ms/step - loss: 4.8278 - acc: 0.0196 - val_loss: 4.8477 - val_acc: 0.0240 - ETA: 55s - loss: 4.8244 - acc: 0.0224 - ETA: 41s - loss: 4.8203 - acc: 0.0230Epoch 00008: val_loss improved from 4.85791 to 4.84765, saving model to saved_models/weights.best.from_scratch.hdf5Epoch 9/356680/6680 [==============================] - 68s 10ms/step - loss: 4.8167 - acc: 0.0220 - val_loss: 4.8453 - val_acc: 0.0204 - ETA: 31s - loss: 4.8156 - acc: 0.0231Epoch 00009: val_loss improved from 4.84765 to 4.84525, saving model to saved_models/weights.best.from_scratch.hdf5Epoch 10/356680/6680 [==============================] - 69s 10ms/step - loss: 4.8049 - acc: 0.0228 - val_loss: 4.8268 - val_acc: 0.0275 - ETA: 1:02 - loss: 4.7917 - acc: 0.0214 - ETA: 49s - loss: 4.8110 - acc: 0.0207 - ETA: 5s - loss: 4.8058 - acc: 0.0220Epoch 00010: val_loss improved from 4.84525 to 4.82678, saving model to saved_models/weights.best.from_scratch.hdf5Epoch 11/356680/6680 [==============================] - 69s 10ms/step - loss: 4.7932 - acc: 0.0216 - val_loss: 4.8224 - val_acc: 0.0287 - ETA: 2s - loss: 4.7937 - acc: 0.0218Epoch 00011: val_loss improved from 4.82678 to 4.82244, saving model to saved_models/weights.best.from_scratch.hdf5Epoch 12/356680/6680 [==============================] - 69s 10ms/step - loss: 4.7788 - acc: 0.0234 - val_loss: 4.8113 - val_acc: 0.0216Epoch 00012: val_loss improved from 4.82244 to 4.81131, saving model to saved_models/weights.best.from_scratch.hdf5Epoch 13/356680/6680 [==============================] - 69s 10ms/step - loss: 4.7625 - acc: 0.0238 - val_loss: 4.8150 - val_acc: 0.0263 - ETA: 37s - loss: 4.7487 - acc: 0.0236Epoch 00013: val_loss did not improveEpoch 14/356680/6680 [==============================] - 69s 10ms/step - loss: 4.7494 - acc: 0.0256 - val_loss: 4.8236 - val_acc: 0.0204 - ETA: 30s - loss: 4.7515 - acc: 0.0266Epoch 00014: val_loss did not improveEpoch 15/356680/6680 [==============================] - 69s 10ms/step - loss: 4.7343 - acc: 0.0244 - val_loss: 4.8024 - val_acc: 0.0180 - ETA: 36s - loss: 4.7482 - acc: 0.0243 - ETA: 32s - loss: 4.7423 - acc: 0.0234 - ETA: 15s - loss: 4.7333 - acc: 0.0235Epoch 00015: val_loss improved from 4.81131 to 4.80236, saving model to saved_models/weights.best.from_scratch.hdf5Epoch 16/356680/6680 [==============================] - 69s 10ms/step - loss: 4.7182 - acc: 0.0295 - val_loss: 4.7744 - val_acc: 0.0263 - ETA: 12s - loss: 4.7098 - acc: 0.0305Epoch 00016: val_loss improved from 4.80236 to 4.77445, saving model to saved_models/weights.best.from_scratch.hdf5Epoch 17/356680/6680 [==============================] - 68s 10ms/step - loss: 4.7020 - acc: 0.0299 - val_loss: 4.7623 - val_acc: 0.0311 - ETA: 1:00 - loss: 4.6920 - acc: 0.0222 - ETA: 35s - loss: 4.7126 - acc: 0.0307 - ETA: 30s - loss: 4.7073 - acc: 0.0305Epoch 00017: val_loss improved from 4.77445 to 4.76233, saving model to saved_models/weights.best.from_scratch.hdf5Epoch 18/356680/6680 [==============================] - 68s 10ms/step - loss: 4.6870 - acc: 0.0331 - val_loss: 4.7441 - val_acc: 0.0323 - ETA: 59s - loss: 4.6866 - acc: 0.0357 - ETA: 55s - loss: 4.6717 - acc: 0.0327 - ETA: 42s - loss: 4.6893 - acc: 0.0343 - ETA: 17s - loss: 4.6844 - acc: 0.0329Epoch 00018: val_loss improved from 4.76233 to 4.74409, saving model to saved_models/weights.best.from_scratch.hdf5Epoch 19/356680/6680 [==============================] - 69s 10ms/step - loss: 4.6697 - acc: 0.0332 - val_loss: 4.7471 - val_acc: 0.0299 - ETA: 57s - loss: 4.6509 - acc: 0.0463 - ETA: 5s - loss: 4.6693 - acc: 0.0331Epoch 00019: val_loss did not improveEpoch 20/356680/6680 [==============================] - 69s 10ms/step - loss: 4.6548 - acc: 0.0349 - val_loss: 4.7546 - val_acc: 0.0299 - ETA: 9s - loss: 4.6577 - acc: 0.0352  - ETA: 0s - loss: 4.6559 - acc: 0.0350Epoch 00020: val_loss did not improveEpoch 21/356680/6680 [==============================] - 69s 10ms/step - loss: 4.6410 - acc: 0.0365 - val_loss: 4.7458 - val_acc: 0.0323Epoch 00021: val_loss did not improveEpoch 22/356680/6680 [==============================] - 68s 10ms/step - loss: 4.6292 - acc: 0.0380 - val_loss: 4.7081 - val_acc: 0.0371 - ETA: 29s - loss: 4.6224 - acc: 0.0403Epoch 00022: val_loss improved from 4.74409 to 4.70811, saving model to saved_models/weights.best.from_scratch.hdf5Epoch 23/356680/6680 [==============================] - 68s 10ms/step - loss: 4.6140 - acc: 0.0362 - val_loss: 4.7357 - val_acc: 0.0180 - ETA: 32s - loss: 4.6099 - acc: 0.0361Epoch 00023: val_loss did not improveEpoch 24/356680/6680 [==============================] - 69s 10ms/step - loss: 4.6050 - acc: 0.0365 - val_loss: 4.6911 - val_acc: 0.0347Epoch 00024: val_loss improved from 4.70811 to 4.69114, saving model to saved_models/weights.best.from_scratch.hdf5Epoch 25/356680/6680 [==============================] - 68s 10ms/step - loss: 4.5925 - acc: 0.0379 - val_loss: 4.6984 - val_acc: 0.0287Epoch 00025: val_loss did not improveEpoch 26/356680/6680 [==============================] - 69s 10ms/step - loss: 4.5854 - acc: 0.0391 - val_loss: 4.6992 - val_acc: 0.0323 - ETA: 1:03 - loss: 4.6114 - acc: 0.0250Epoch 00026: val_loss did not improveEpoch 27/356680/6680 [==============================] - 68s 10ms/step - loss: 4.5708 - acc: 0.0397 - val_loss: 4.7011 - val_acc: 0.0311 - ETA: 55s - loss: 4.5531 - acc: 0.0390 - ETA: 37s - loss: 4.5666 - acc: 0.0388 - ETA: 25s - loss: 4.5774 - acc: 0.0397Epoch 00027: val_loss did not improveEpoch 28/356680/6680 [==============================] - 69s 10ms/step - loss: 4.5707 - acc: 0.0400 - val_loss: 4.6913 - val_acc: 0.0287 - ETA: 22s - loss: 4.5671 - acc: 0.0396 - ETA: 10s - loss: 4.5716 - acc: 0.0389 - ETA: 4s - loss: 4.5709 - acc: 0.0400Epoch 00028: val_loss did not improveEpoch 29/356680/6680 [==============================] - 69s 10ms/step - loss: 4.5552 - acc: 0.0428 - val_loss: 4.6915 - val_acc: 0.0311Epoch 00029: val_loss did not improveEpoch 30/356680/6680 [==============================] - 69s 10ms/step - loss: 4.5480 - acc: 0.0394 - val_loss: 4.6864 - val_acc: 0.0359 - ETA: 45s - loss: 4.5450 - acc: 0.0424Epoch 00030: val_loss improved from 4.69114 to 4.68639, saving model to saved_models/weights.best.from_scratch.hdf5Epoch 31/356680/6680 [==============================] - 69s 10ms/step - loss: 4.5408 - acc: 0.0431 - val_loss: 4.6938 - val_acc: 0.0311 - ETA: 30s - loss: 4.5420 - acc: 0.0447 - ETA: 1s - loss: 4.5420 - acc: 0.0434Epoch 00031: val_loss did not improveEpoch 32/356680/6680 [==============================] - 69s 10ms/step - loss: 4.5377 - acc: 0.0424 - val_loss: 4.6689 - val_acc: 0.0311Epoch 00032: val_loss improved from 4.68639 to 4.66890, saving model to saved_models/weights.best.from_scratch.hdf5Epoch 33/356680/6680 [==============================] - 69s 10ms/step - loss: 4.5289 - acc: 0.0434 - val_loss: 4.6723 - val_acc: 0.0323 - ETA: 46s - loss: 4.5195 - acc: 0.0368Epoch 00033: val_loss did not improveEpoch 34/356680/6680 [==============================] - 68s 10ms/step - loss: 4.5216 - acc: 0.0436 - val_loss: 4.7055 - val_acc: 0.0263 - ETA: 55s - loss: 4.5110 - acc: 0.0490 - ETA: 26s - loss: 4.5125 - acc: 0.0468Epoch 00034: val_loss did not improveEpoch 35/356680/6680 [==============================] - 69s 10ms/step - loss: 4.5189 - acc: 0.0443 - val_loss: 4.6770 - val_acc: 0.0287Epoch 00035: val_loss did not improve&lt;keras.callbacks.History at 0x7f9b4cfe3da0&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 加载具有最好验证loss的模型</span></span><br><span class="line"></span><br><span class="line">model.load_weights(<span class="string">'saved_models/weights.best.from_scratch.hdf5'</span>)</span><br></pre></td></tr></table></figure><h3 id="测试模型"><a href="#测试模型" class="headerlink" title="测试模型"></a>测试模型</h3><p>在狗图像的测试数据集上试用你的模型。确保测试准确率大于1%。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取测试数据集中每一个图像所预测的狗品种的index</span></span><br><span class="line">dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=<span class="number">0</span>))) <span class="keyword">for</span> tensor <span class="keyword">in</span> test_tensors]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 报告测试准确率</span></span><br><span class="line">test_accuracy = <span class="number">100</span>*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=<span class="number">1</span>))/len(dog_breed_predictions)</span><br><span class="line">print(<span class="string">'Test accuracy: %.4f%%'</span> % test_accuracy)</span><br></pre></td></tr></table></figure><pre><code>Test accuracy: 3.1100%</code></pre><hr><p><a id="step4"></a></p><h2 id="步骤-4-使用一个CNN来区分狗的品种"><a href="#步骤-4-使用一个CNN来区分狗的品种" class="headerlink" title="步骤 4: 使用一个CNN来区分狗的品种"></a>步骤 4: 使用一个CNN来区分狗的品种</h2><p>使用 迁移学习（Transfer Learning）的方法，能帮助我们在不损失准确率的情况下大大减少训练时间。在以下步骤中，你可以尝试使用迁移学习来训练你自己的CNN。</p><h3 id="得到从图像中提取的特征向量（Bottleneck-Features）"><a href="#得到从图像中提取的特征向量（Bottleneck-Features）" class="headerlink" title="得到从图像中提取的特征向量（Bottleneck Features）"></a>得到从图像中提取的特征向量（Bottleneck Features）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bottleneck_features = np.load(<span class="string">'bottleneck_features/DogVGG16Data.npz'</span>)</span><br><span class="line">train_VGG16 = bottleneck_features[<span class="string">'train'</span>]</span><br><span class="line">valid_VGG16 = bottleneck_features[<span class="string">'valid'</span>]</span><br><span class="line">test_VGG16 = bottleneck_features[<span class="string">'test'</span>]</span><br></pre></td></tr></table></figure><h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><p>该模型使用预训练的 VGG-16 模型作为固定的图像特征提取器，其中 VGG-16 最后一层卷积层的输出被直接输入到我们的模型。我们只需要添加一个全局平均池化层以及一个全连接层，其中全连接层使用 softmax 激活函数，对每一个狗的种类都包含一个节点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">VGG16_model = Sequential()</span><br><span class="line">VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[<span class="number">1</span>:]))</span><br><span class="line">VGG16_model.add(Dense(<span class="number">133</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">VGG16_model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================global_average_pooling2d_2 ( (None, 512)               0         _________________________________________________________________dense_2 (Dense)              (None, 133)               68229     =================================================================Total params: 68,229Trainable params: 68,229Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 编译模型</span></span><br><span class="line"></span><br><span class="line">VGG16_model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'rmsprop'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 训练模型</span></span><br><span class="line"></span><br><span class="line">checkpointer = ModelCheckpoint(filepath=<span class="string">'saved_models/weights.best.VGG16.hdf5'</span>, </span><br><span class="line">                               verbose=<span class="number">1</span>, save_best_only=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">VGG16_model.fit(train_VGG16, train_targets, </span><br><span class="line">          validation_data=(valid_VGG16, valid_targets),</span><br><span class="line">          epochs=<span class="number">20</span>, batch_size=<span class="number">20</span>, callbacks=[checkpointer], verbose=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>Train on 6680 samples, validate on 835 samplesEpoch 1/20Epoch 00001: val_loss improved from inf to 10.56545, saving model to saved_models/weights.best.VGG16.hdf5Epoch 2/20Epoch 00002: val_loss improved from 10.56545 to 9.88948, saving model to saved_models/weights.best.VGG16.hdf5Epoch 3/20Epoch 00003: val_loss improved from 9.88948 to 9.41489, saving model to saved_models/weights.best.VGG16.hdf5Epoch 4/20Epoch 00004: val_loss improved from 9.41489 to 9.28659, saving model to saved_models/weights.best.VGG16.hdf5Epoch 5/20Epoch 00005: val_loss improved from 9.28659 to 9.20906, saving model to saved_models/weights.best.VGG16.hdf5Epoch 6/20Epoch 00006: val_loss improved from 9.20906 to 9.11723, saving model to saved_models/weights.best.VGG16.hdf5Epoch 7/20Epoch 00007: val_loss improved from 9.11723 to 9.03948, saving model to saved_models/weights.best.VGG16.hdf5Epoch 8/20Epoch 00008: val_loss improved from 9.03948 to 8.85330, saving model to saved_models/weights.best.VGG16.hdf5Epoch 9/20Epoch 00009: val_loss did not improveEpoch 10/20Epoch 00010: val_loss improved from 8.85330 to 8.65410, saving model to saved_models/weights.best.VGG16.hdf5Epoch 11/20Epoch 00011: val_loss improved from 8.65410 to 8.40498, saving model to saved_models/weights.best.VGG16.hdf5Epoch 12/20Epoch 00012: val_loss improved from 8.40498 to 8.39156, saving model to saved_models/weights.best.VGG16.hdf5Epoch 13/20Epoch 00013: val_loss improved from 8.39156 to 8.36496, saving model to saved_models/weights.best.VGG16.hdf5Epoch 14/20Epoch 00014: val_loss improved from 8.36496 to 8.30643, saving model to saved_models/weights.best.VGG16.hdf5Epoch 15/20Epoch 00015: val_loss improved from 8.30643 to 8.16639, saving model to saved_models/weights.best.VGG16.hdf5Epoch 16/20Epoch 00016: val_loss improved from 8.16639 to 8.06120, saving model to saved_models/weights.best.VGG16.hdf5Epoch 17/20Epoch 00017: val_loss improved from 8.06120 to 7.94562, saving model to saved_models/weights.best.VGG16.hdf5Epoch 18/20Epoch 00018: val_loss improved from 7.94562 to 7.90346, saving model to saved_models/weights.best.VGG16.hdf5Epoch 19/20Epoch 00019: val_loss did not improveEpoch 20/20Epoch 00020: val_loss improved from 7.90346 to 7.84868, saving model to saved_models/weights.best.VGG16.hdf5&lt;keras.callbacks.History at 0x7f9b4c84af28&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 加载具有最好验证loss的模型</span></span><br><span class="line"></span><br><span class="line">VGG16_model.load_weights(<span class="string">'saved_models/weights.best.VGG16.hdf5'</span>)</span><br></pre></td></tr></table></figure><h3 id="测试模型-1"><a href="#测试模型-1" class="headerlink" title="测试模型"></a>测试模型</h3><p>现在，我们可以测试此CNN在狗图像测试数据集中识别品种的效果如何。我们在下方打印出测试准确率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取测试数据集中每一个图像所预测的狗品种的index</span></span><br><span class="line">VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=<span class="number">0</span>))) <span class="keyword">for</span> feature <span class="keyword">in</span> test_VGG16]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 报告测试准确率</span></span><br><span class="line">test_accuracy = <span class="number">100</span>*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=<span class="number">1</span>))/len(VGG16_predictions)</span><br><span class="line">print(<span class="string">'Test accuracy: %.4f%%'</span> % test_accuracy)</span><br></pre></td></tr></table></figure><pre><code>Test accuracy: 42.8230%</code></pre><h3 id="使用模型预测狗的品种"><a href="#使用模型预测狗的品种" class="headerlink" title="使用模型预测狗的品种"></a>使用模型预测狗的品种</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> extract_bottleneck_features <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">VGG16_predict_breed</span><span class="params">(img_path)</span>:</span></span><br><span class="line">    <span class="comment"># 提取bottleneck特征</span></span><br><span class="line">    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))</span><br><span class="line">    <span class="comment"># 获取预测向量</span></span><br><span class="line">    predicted_vector = VGG16_model.predict(bottleneck_feature)</span><br><span class="line">    <span class="comment"># 返回此模型预测的狗的品种</span></span><br><span class="line">    <span class="keyword">return</span> dog_names[np.argmax(predicted_vector)]</span><br></pre></td></tr></table></figure><hr><p><a id="step5"></a></p><h2 id="步骤-5-建立一个CNN来分类狗的品种（使用迁移学习）"><a href="#步骤-5-建立一个CNN来分类狗的品种（使用迁移学习）" class="headerlink" title="步骤 5: 建立一个CNN来分类狗的品种（使用迁移学习）"></a>步骤 5: 建立一个CNN来分类狗的品种（使用迁移学习）</h2><p>现在你将使用迁移学习来建立一个CNN，从而可以从图像中识别狗的品种。你的 CNN 在测试集上的准确率必须至少达到60%。</p><p>在步骤4中，我们使用了迁移学习来创建一个使用基于 VGG-16 提取的特征向量来搭建一个 CNN。在本部分内容中，你必须使用另一个预训练模型来搭建一个 CNN。为了让这个任务更易实现，我们已经预先对目前 keras 中可用的几种网络进行了预训练：</p><ul><li><a href="https://s3.cn-north-1.amazonaws.com.cn/static-documents/nd101/DLND+documents/DogVGG19Data.npz" target="_blank" rel="noopener">VGG-19</a> bottleneck features</li><li><a href="https://s3.cn-north-1.amazonaws.com.cn/static-documents/nd101/DLND+documents/DogResnet50Data.npz" target="_blank" rel="noopener">ResNet-50</a> bottleneck features</li><li><a href="https://s3.cn-north-1.amazonaws.com.cn/static-documents/nd101/DLND+documents/DogInceptionV3Data.npz" target="_blank" rel="noopener">Inception</a> bottleneck features</li><li><a href="https://s3.cn-north-1.amazonaws.com.cn/static-documents/nd101/DLND+documents/DogXceptionData.npz" target="_blank" rel="noopener">Xception</a> bottleneck features</li></ul><p>这些文件被命名为为：</p><pre><code>Dog{network}Data.npz</code></pre><p>其中 <code>{network}</code> 可以是 <code>VGG19</code>、<code>Resnet50</code>、<code>InceptionV3</code> 或 <code>Xception</code> 中的一个。选择上方网络架构中的一个，下载相对应的bottleneck特征，并将所下载的文件保存在目录 <code>bottleneck_features/</code> 中。</p><h3 id="【练习】获取模型的特征向量"><a href="#【练习】获取模型的特征向量" class="headerlink" title="【练习】获取模型的特征向量"></a>【练习】获取模型的特征向量</h3><p>在下方代码块中，通过运行下方代码提取训练、测试与验证集相对应的bottleneck特征。</p><pre><code>bottleneck_features = np.load(&#39;bottleneck_features/Dog{network}Data.npz&#39;)train_{network} = bottleneck_features[&#39;train&#39;]valid_{network} = bottleneck_features[&#39;valid&#39;]test_{network} = bottleneck_features[&#39;test&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### <span class="doctag">TODO:</span> 从另一个预训练的CNN获取bottleneck特征</span></span><br><span class="line">bottleneck_features = np.load(<span class="string">'bottleneck_features/DogResnet50Data.npz'</span>)</span><br><span class="line">train_Resnet = bottleneck_features[<span class="string">'train'</span>]</span><br><span class="line">valid_Resnet = bottleneck_features[<span class="string">'valid'</span>]</span><br><span class="line">test_Resnet = bottleneck_features[<span class="string">'test'</span>]</span><br></pre></td></tr></table></figure><h3 id="【练习】模型架构-1"><a href="#【练习】模型架构-1" class="headerlink" title="【练习】模型架构"></a>【练习】模型架构</h3><p>建立一个CNN来分类狗品种。在你的代码单元块的最后，通过运行如下代码输出网络的结构：</p><pre><code>    &lt;your model&#39;s name&gt;.summary()</code></pre><hr><p><a id="question6"></a>  </p><h3 id="问题-6"><a href="#问题-6" class="headerlink" title="问题 6:"></a><strong>问题 6:</strong></h3><p>在下方的代码块中尝试使用 Keras 搭建最终的网络架构，并回答你实现最终 CNN 架构的步骤与每一步的作用，并描述你在迁移学习过程中，使用该网络架构的原因。</p><p><strong>回答:Resnet50网络相较于其他net 速度更快 准确率更高 且实验证明 经过多种类训练过的网络比训练单一种类识别的网络效果更优 所以选择resent50网络来进行迁移学习</strong> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### <span class="doctag">TODO:</span> 定义你的框架</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(GlobalAveragePooling2D(input_shape=train_Resnet.shape[<span class="number">1</span>:]))</span><br><span class="line">model.add(Dense(<span class="number">133</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================global_average_pooling2d_3 ( (None, 2048)              0         _________________________________________________________________dense_3 (Dense)              (None, 133)               272517    =================================================================Total params: 272,517Trainable params: 272,517Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### <span class="doctag">TODO:</span> 编译模型</span></span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure><hr><h3 id="【练习】训练模型-1"><a href="#【练习】训练模型-1" class="headerlink" title="【练习】训练模型"></a>【练习】训练模型</h3><p><a id="question7"></a>  </p><h3 id="问题-7"><a href="#问题-7" class="headerlink" title="问题 7:"></a><strong>问题 7:</strong></h3><p>在下方代码单元中训练你的模型。使用模型检查点（model checkpointing）来储存具有最低验证集 loss 的模型。</p><p>当然，你也可以对训练集进行 <a href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html" target="_blank" rel="noopener">数据增强</a> 以优化模型的表现，不过这不是必须的步骤。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### <span class="doctag">TODO:</span> 训练模型</span></span><br><span class="line"></span><br><span class="line">checkpointer = ModelCheckpoint(filepath=<span class="string">'saved_models/weights.best.Resnet50.hdf5'</span>, </span><br><span class="line">                               verbose=<span class="number">10</span>, save_best_only=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">model.fit(train_Resnet, train_targets, </span><br><span class="line">          validation_data=(valid_Resnet, valid_targets),</span><br><span class="line">          epochs=<span class="number">20</span>, batch_size=<span class="number">20</span>, callbacks=[checkpointer], verbose=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>Train on 6680 samples, validate on 835 samplesEpoch 1/20Epoch 00001: val_loss improved from inf to 0.83976, saving model to saved_models/weights.best.Resnet50.hdf5Epoch 2/20Epoch 00002: val_loss improved from 0.83976 to 0.73381, saving model to saved_models/weights.best.Resnet50.hdf5Epoch 3/20Epoch 00003: val_loss improved from 0.73381 to 0.67062, saving model to saved_models/weights.best.Resnet50.hdf5Epoch 4/20Epoch 00004: val_loss improved from 0.67062 to 0.64336, saving model to saved_models/weights.best.Resnet50.hdf5Epoch 5/20Epoch 00005: val_loss did not improveEpoch 6/20Epoch 00006: val_loss improved from 0.64336 to 0.55801, saving model to saved_models/weights.best.Resnet50.hdf5Epoch 7/20Epoch 00007: val_loss did not improveEpoch 8/20Epoch 00008: val_loss did not improveEpoch 9/20Epoch 00009: val_loss did not improveEpoch 10/20Epoch 00010: val_loss did not improveEpoch 11/20Epoch 00011: val_loss did not improveEpoch 12/20Epoch 00012: val_loss did not improveEpoch 13/20Epoch 00013: val_loss did not improveEpoch 14/20Epoch 00014: val_loss did not improveEpoch 15/20Epoch 00015: val_loss did not improveEpoch 16/20Epoch 00016: val_loss did not improveEpoch 17/20Epoch 00017: val_loss did not improveEpoch 18/20Epoch 00018: val_loss did not improveEpoch 19/20Epoch 00019: val_loss did not improveEpoch 20/20Epoch 00020: val_loss did not improve&lt;keras.callbacks.History at 0x7f9b4c0d0c88&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### <span class="doctag">TODO:</span> 加载具有最佳验证loss的模型权重</span></span><br><span class="line">model.load_weights(<span class="string">'saved_models/weights.best.Resnet50.hdf5'</span>)</span><br></pre></td></tr></table></figure><hr><h3 id="【练习】测试模型"><a href="#【练习】测试模型" class="headerlink" title="【练习】测试模型"></a>【练习】测试模型</h3><p><a id="question8"></a>  </p><h3 id="问题-8"><a href="#问题-8" class="headerlink" title="问题 8:"></a><strong>问题 8:</strong></h3><p>在狗图像的测试数据集上试用你的模型。确保测试准确率大于60%。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### <span class="doctag">TODO:</span> 在测试集上计算分类准确率</span></span><br><span class="line">model_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=<span class="number">0</span>))) <span class="keyword">for</span> feature <span class="keyword">in</span> test_Resnet]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 报告测试准确率</span></span><br><span class="line">test_accuracy = <span class="number">100</span>*np.sum(np.array(model_predictions)==np.argmax(test_targets, axis=<span class="number">1</span>))/len(model_predictions)</span><br><span class="line">print(<span class="string">'Test accuracy: %.4f%%'</span> % test_accuracy)</span><br><span class="line"><span class="comment"># Fit the model</span></span><br><span class="line">history = model.fit(train_Resnet, train_targets, validation_split=<span class="number">0.33</span>, epochs=<span class="number">150</span>, batch_size=<span class="number">10</span>, verbose=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># list all data in history</span></span><br><span class="line">print(history.history.keys())</span><br><span class="line"><span class="comment"># summarize history for accuracy</span></span><br><span class="line">plt.plot(history.history[<span class="string">'acc'</span>])</span><br><span class="line">plt.plot(history.history[<span class="string">'val_acc'</span>])</span><br><span class="line">plt.title(<span class="string">'model accuracy'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'epoch'</span>)</span><br><span class="line">plt.legend([<span class="string">'train'</span>, <span class="string">'test'</span>], loc=<span class="string">'upper left'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># summarize history for loss</span></span><br><span class="line">plt.plot(history.history[<span class="string">'loss'</span>])</span><br><span class="line">plt.plot(history.history[<span class="string">'val_loss'</span>])</span><br><span class="line">plt.title(<span class="string">'model loss'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'epoch'</span>)</span><br><span class="line">plt.legend([<span class="string">'train'</span>, <span class="string">'test'</span>], loc=<span class="string">'upper left'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>Test accuracy: 82.7751%dict_keys([&#39;val_loss&#39;, &#39;val_acc&#39;, &#39;loss&#39;, &#39;acc&#39;])</code></pre><p><img src="/2019/02/15/dog-app-for-CNN/output_57_1.png" alt="png"></p><p><img src="/2019/02/15/dog-app-for-CNN/output_57_2.png" alt="png"></p><hr><h3 id="【练习】使用模型测试狗的品种"><a href="#【练习】使用模型测试狗的品种" class="headerlink" title="【练习】使用模型测试狗的品种"></a>【练习】使用模型测试狗的品种</h3><p>实现一个函数，它的输入为图像路径，功能为预测对应图像的类别，输出为你模型预测出的狗类别（<code>Affenpinscher</code>, <code>Afghan_hound</code> 等）。</p><p>与步骤5中的模拟函数类似，你的函数应当包含如下三个步骤：</p><ol><li>根据选定的模型载入图像特征（bottleneck features）</li><li>将图像特征输输入到你的模型中，并返回预测向量。注意，在该向量上使用 argmax 函数可以返回狗种类的序号。</li><li>使用在步骤0中定义的 <code>dog_names</code> 数组来返回对应的狗种类名称。</li></ol><p>提取图像特征过程中使用到的函数可以在 <code>extract_bottleneck_features.py</code> 中找到。同时，他们应已在之前的代码块中被导入。根据你选定的 CNN 网络，你可以使用 <code>extract_{network}</code> 函数来获得对应的图像特征，其中 <code>{network}</code> 代表 <code>VGG19</code>, <code>Resnet50</code>, <code>InceptionV3</code>, 或 <code>Xception</code> 中的一个。</p><hr><p><a id="question9"></a>  </p><h3 id="问题-9"><a href="#问题-9" class="headerlink" title="问题 9:"></a><strong>问题 9:</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### <span class="doctag">TODO:</span> 写一个函数，该函数将图像的路径作为输入</span></span><br><span class="line"><span class="comment">### 然后返回此模型所预测的狗的品种</span></span><br><span class="line"><span class="keyword">from</span> extract_bottleneck_features <span class="keyword">import</span> *</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Resnet50_predict_breed</span><span class="params">(img_path)</span>:</span></span><br><span class="line">    bottleneck_features = extract_Resnet50(path_to_tensor(img_path))</span><br><span class="line">    predicted_vector = model.predict(bottleneck_features)</span><br><span class="line">    <span class="keyword">return</span> dog_names[np.argmax(predicted_vector)]</span><br></pre></td></tr></table></figure><hr><p><a id="step6"></a></p><h2 id="步骤-6-完成你的算法"><a href="#步骤-6-完成你的算法" class="headerlink" title="步骤 6: 完成你的算法"></a>步骤 6: 完成你的算法</h2><p>实现一个算法，它的输入为图像的路径，它能够区分图像是否包含一个人、狗或两者都不包含，然后：</p><ul><li>如果从图像中检测到一只<strong>狗</strong>，返回被预测的品种。</li><li>如果从图像中检测到<strong>人</strong>，返回最相像的狗品种。</li><li>如果两者都不能在图像中检测到，输出错误提示。</li></ul><p>我们非常欢迎你来自己编写检测图像中人类与狗的函数，你可以随意地使用上方完成的 <code>face_detector</code> 和 <code>dog_detector</code> 函数。你<strong>需要</strong>在步骤5使用你的CNN来预测狗品种。</p><p>下面提供了算法的示例输出，但你可以自由地设计自己的模型！</p><p><img src="/2019/02/15/dog-app-for-CNN/sample_human_output.png" alt="Sample Human Output"></p><p><a id="question10"></a>  </p><h3 id="问题-10"><a href="#问题-10" class="headerlink" title="问题 10:"></a><strong>问题 10:</strong></h3><p>在下方代码块中完成你的代码。</p><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### <span class="doctag">TODO:</span> 设计你的算法</span></span><br><span class="line"><span class="comment">### 自由地使用所需的代码单元数吧</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Predict</span><span class="params">(path)</span>:</span></span><br><span class="line">    image = cv2.imread(path)</span><br><span class="line">    </span><br><span class="line">    cv = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br><span class="line">    plt.imshow(cv)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> face_detector(path) &gt; <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'you look like a ...'</span> + Resnet50_predict_breed(path))</span><br><span class="line">    <span class="keyword">elif</span> dog_detector(path) == <span class="keyword">True</span>:</span><br><span class="line">        print(<span class="string">'you are a &#123;&#125; dog'</span>.format(Resnet50_predict_breed(path)))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"sorry"</span>)</span><br></pre></td></tr></table></figure><hr><p><a id="step7"></a></p><h2 id="步骤-7-测试你的算法"><a href="#步骤-7-测试你的算法" class="headerlink" title="步骤 7: 测试你的算法"></a>步骤 7: 测试你的算法</h2><p>在这个部分中，你将尝试一下你的新算法！算法认为<strong>你</strong>看起来像什么类型的狗？如果你有一只狗，它可以准确地预测你的狗的品种吗？如果你有一只猫，它会将你的猫误判为一只狗吗？</p><p><a id="question11"></a>  </p><h3 id="问题-11"><a href="#问题-11" class="headerlink" title="问题 11:"></a><strong>问题 11:</strong></h3><p>在下方编写代码，用至少6张现实中的图片来测试你的算法。你可以使用任意照片，不过请至少使用两张人类图片（要征得当事人同意哦）和两张狗的图片。<br>同时请回答如下问题：</p><ol><li>输出结果比你预想的要好吗 :) ？或者更糟 :( ？：输出结果比我预想的要好</li><li>提出至少三点改进你的模型的想法。添加dropout防止过拟合 减少或增加隐藏层层数 压缩input时图像的像素</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## <span class="doctag">TODO:</span> 在你的电脑上，在步骤6中，至少在6张图片上运行你的算法。</span></span><br><span class="line"><span class="comment">## 自由地使用所需的代码单元数吧</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">jpg = os.listdir(<span class="string">'images'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> jpg[:<span class="number">6</span>]:</span><br><span class="line">    Predict(<span class="string">'images/'</span>+i)</span><br></pre></td></tr></table></figure><p><img src="/2019/02/15/dog-app-for-CNN/output_63_0.png" alt="png"></p><pre><code>you are a train/130.Welsh_springer_spaniel dog</code></pre><p><img src="/2019/02/15/dog-app-for-CNN/output_63_2.png" alt="png"></p><pre><code>you are a train/009.American_water_spaniel dog</code></pre><p><img src="/2019/02/15/dog-app-for-CNN/output_63_4.png" alt="png"></p><pre><code>you look like a ...train/037.Brittany</code></pre><p><img src="/2019/02/15/dog-app-for-CNN/output_63_6.png" alt="png"></p><pre><code>you are a train/055.Curly-coated_retriever dog</code></pre><p><img src="/2019/02/15/dog-app-for-CNN/output_63_8.png" alt="png"></p><pre><code>you are a train/096.Labrador_retriever dog</code></pre><p><img src="/2019/02/15/dog-app-for-CNN/output_63_10.png" alt="png"></p><pre><code>sorry</code></pre><p><strong>注意: 当你写完了所有的代码，并且回答了所有的问题。你就可以把你的 iPython Notebook 导出成 HTML 文件。你可以在菜单栏，这样导出File -&gt; Download as -&gt; HTML (.html)把这个 HTML 和这个 iPython notebook 一起做为你的作业提交。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;卷积神经网络（Convolutional-Neural-Network-CNN）&quot;&gt;&lt;a href=&quot;#卷积神经网络（Convolutional-Neural-Network-CNN）&quot; class=&quot;headerlink&quot; title=&quot;卷积神经网络（Convo
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PredictYourCuisine</title>
    <link href="http://yoursite.com/2019/02/15/PredictYourCuisine/"/>
    <id>http://yoursite.com/2019/02/15/PredictYourCuisine/</id>
    <published>2019-02-15T13:06:53.000Z</published>
    <updated>2019-02-16T11:18:20.791Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习工程师纳米学位"><a href="#机器学习工程师纳米学位" class="headerlink" title="机器学习工程师纳米学位"></a>机器学习工程师纳米学位</h1><h2 id="项目-0-预测你的下一道世界料理"><a href="#项目-0-预测你的下一道世界料理" class="headerlink" title="项目 0: 预测你的下一道世界料理"></a>项目 0: 预测你的下一道世界料理</h2><p>欢迎来到机器学习的预测烹饪菜系项目！在此文件中，有些示例代码已经提供给你，但你还需要实现更多的功能来让项目成功运行。除非有明确要求，你无须修改任何已给出的代码。以<strong>编程练习</strong>开始的标题表示接下来的内容中有需要你必须实现的功能。每一部分都会有详细的指导，需要实现的部分也会在注释中以<strong>TODO</strong>标出。请仔细阅读所有的提示！</p><ul><li><strong>实验任务</strong>：给定佐料名称，预测菜品所属的菜系。</li><li><strong>实验步骤</strong>：菜品数据载入；佐料名称预处理，并预览数据集结构；载入逻辑回归模型，并训练；结果测试并提交，查看实验分数。</li></ul><blockquote><p><strong>提示：</strong>Code 和 Markdown 区域可通过 <strong>Shift + Enter</strong> 快捷键运行。此外，Markdown可以通过双击进入编辑模式。</p></blockquote><hr><h2 id="第一步-下载并导入数据"><a href="#第一步-下载并导入数据" class="headerlink" title="第一步. 下载并导入数据"></a>第一步. 下载并导入数据</h2><p>在这个项目中，你将利用<a href="https://www.yummly.com/" target="_blank" rel="noopener">Yummly</a>所提供的数据集来训练和测试一个模型，并对模型的性能和预测能力进行测试。通过该数据训练后的好的模型可以被用来对菜系进行预测。</p><p>此项目的数据集来自<a href="https://www.kaggle.com/c/whats-cooking/data" target="_blank" rel="noopener">Kaggle What’s Cooking 竞赛</a>。共 39774/9944 个训练和测试数据点，涵盖了中国菜、越南菜、法国菜等的信息。数据集包含以下特征：</p><ul><li>‘id’：24717, 数据编号</li><li>‘cuisine’：”indian”, 菜系</li><li>‘ingredients’：[“tumeric”, “vegetable stock”, …] 此菜所包含的佐料</li></ul><p>首先你需要前往此 <a href="https://www.kaggle.com/c/whats-cooking/data" target="_blank" rel="noopener">菜系数据集</a> 下载(选择 <strong>Download All</strong> )。如果不能正常下载，请参考教室中的下载教程。然后运行下面区域的代码以载入数据集，以及一些此项目所需的 Python 库。如果成功返回数据集的大小，表示数据集已载入成功。</p><h3 id="1-1-配置环境"><a href="#1-1-配置环境" class="headerlink" title="1.1 配置环境"></a>1.1 配置环境</h3><p>首先按照本目录中<code>README.md</code>文件中的第一部分内容，配置实验开发环境和所需库函数。</p><h3 id="1-2-加载数据"><a href="#1-2-加载数据" class="headerlink" title="1.2 加载数据"></a>1.2 加载数据</h3><p>其次，在下载完实验数据集后，我们将其解压至当前目录中(即：<code>MLND-cn-trial\</code>目录下面)， 然后依次输入以下代码，加载本次实验的训练集和测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line"><span class="comment"># 导入依赖库</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">train_filename=<span class="string">'all/train.json'</span></span><br><span class="line">train_content = pd.read_json(codecs.open(train_filename, mode=<span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>))</span><br><span class="line"></span><br><span class="line">test_filename = <span class="string">'all/test.json'</span></span><br><span class="line">test_content = pd.read_json(codecs.open(test_filename, mode=<span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 打印加载的数据集数量</span></span><br><span class="line">print(<span class="string">"菜名数据集一共包含 &#123;&#125; 训练数据 和 &#123;&#125; 测试样例。\n"</span>.format(len(train_content), len(test_content)))</span><br><span class="line"><span class="keyword">if</span> len(train_content)==<span class="number">39774</span> <span class="keyword">and</span> len(test_content)==<span class="number">9944</span>:</span><br><span class="line">    print(<span class="string">"数据成功载入！"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"数据载入有问题，请检查文件路径！"</span>)</span><br></pre></td></tr></table></figure><pre><code>菜名数据集一共包含 39774 训练数据 和 9944 测试样例。数据成功载入！</code></pre><h3 id="1-3-数据预览"><a href="#1-3-数据预览" class="headerlink" title="1.3 数据预览"></a>1.3 数据预览</h3><p>为了查看我们的数据集的分布和菜品总共的种类，我们打印出部分数据样例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line">pd.set_option(<span class="string">'display.max_colwidth'</span>,<span class="number">120</span>)</span><br></pre></td></tr></table></figure><h3 id="编程练习"><a href="#编程练习" class="headerlink" title="编程练习"></a>编程练习</h3><p>你需要通过<code>head()</code>函数来预览训练集<code>train_content</code>数据。（输出前5条）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### TODO：打印train_content中前5个数据样例以预览数据</span></span><br><span class="line">train_content[<span class="string">'cuisine'</span>]</span><br></pre></td></tr></table></figure><pre><code>0               greek1         southern_us2            filipino3              indian4              indian5            jamaican6             spanish7             italian8             mexican9             italian10            italian11            chinese12            italian13            mexican14            italian15             indian16            british17            italian18               thai19         vietnamese20               thai21            mexican22        southern_us23            chinese24            italian25            chinese26       cajun_creole27            italian28            chinese29            mexican             ...     39744           greek39745         spanish39746          indian39747        moroccan39748         italian39749         mexican39750         mexican39751        moroccan39752     southern_us39753         italian39754      vietnamese39755          indian39756         mexican39757           greek39758           greek39759          korean39760     southern_us39761         chinese39762          indian39763         italian39764         mexican39765          indian39766           irish39767         italian39768         mexican39769           irish39770         italian39771           irish39772         chinese39773         mexicanName: cuisine, Length: 39774, dtype: object</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line"><span class="comment">## 查看总共菜品分类</span></span><br><span class="line">categories=np.unique(train_content[<span class="string">'cuisine'</span>])</span><br><span class="line">print(<span class="string">"一共包含 &#123;&#125; 种菜品，分别是:\n&#123;&#125;"</span>.format(len(categories),categories))</span><br></pre></td></tr></table></figure><pre><code>一共包含 20 种菜品，分别是:[&#39;brazilian&#39; &#39;british&#39; &#39;cajun_creole&#39; &#39;chinese&#39; &#39;filipino&#39; &#39;french&#39; &#39;greek&#39; &#39;indian&#39; &#39;irish&#39; &#39;italian&#39; &#39;jamaican&#39; &#39;japanese&#39; &#39;korean&#39; &#39;mexican&#39; &#39;moroccan&#39; &#39;russian&#39; &#39;southern_us&#39; &#39;spanish&#39; &#39;thai&#39; &#39;vietnamese&#39;]</code></pre><hr><h2 id="第二步-分析数据"><a href="#第二步-分析数据" class="headerlink" title="第二步. 分析数据"></a>第二步. 分析数据</h2><p>在项目的第二个部分，你会对菜肴数据进行初步的观察并给出你的分析。通过对数据的探索来熟悉数据可以让你更好地理解和解释你的结果。</p><p>由于这个项目的最终目标是建立一个预测世界菜系的模型，我们需要将数据集分为<strong>特征(Features)</strong>和<strong>目标变量(Target Variables)</strong>。</p><ul><li><strong>特征</strong>: <code>&#39;ingredients&#39;</code>，给我们提供了每个菜品所包含的佐料名称。</li><li><strong>目标变量</strong>：<code>&#39;cuisine&#39;</code>，是我们希望预测的菜系分类。</li></ul><p>他们分别被存在 <code>train_ingredients</code> 和 <code>train_targets</code> 两个变量名中。</p><h3 id="编程练习：数据提取"><a href="#编程练习：数据提取" class="headerlink" title="编程练习：数据提取"></a>编程练习：数据提取</h3><ul><li>将<code>train_content</code>中的<code>ingredients</code>赋值到<code>train_integredients</code></li><li>将<code>train_content</code>中的<code>cuisine</code>赋值到<code>train_targets</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### TODO：将特征与目标变量分别赋值</span></span><br><span class="line">train_ingredients = train_content[<span class="string">'ingredients'</span>]</span><br><span class="line">train_targets = train_content[<span class="string">'cuisine'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">### <span class="doctag">TODO:</span> 打印结果，检查是否正确赋值</span></span><br><span class="line">display(train_ingredients)</span><br><span class="line">display(train_targets)</span><br></pre></td></tr></table></figure><pre><code>0        [romaine lettuce, black olives, grape tomatoes, garlic, pepper, purple onion, seasoning, garbanzo beans, feta cheese...1        [plain flour, ground pepper, salt, tomatoes, ground black pepper, thyme, eggs, green tomatoes, yellow corn meal, mil...2        [eggs, pepper, salt, mayonaise, cooking oil, green chilies, grilled chicken breasts, garlic powder, yellow onion, so...3                                                                                            [water, vegetable oil, wheat, salt]4        [black pepper, shallots, cornflour, cayenne pepper, onions, garlic paste, milk, butter, salt, lemon juice, water, ch...5        [plain flour, sugar, butter, eggs, fresh ginger root, salt, ground cinnamon, milk, vanilla extract, ground ginger, p...6        [olive oil, salt, medium shrimp, pepper, garlic, chopped cilantro, jalapeno chilies, flat leaf parsley, skirt steak,...7        [sugar, pistachio nuts, white almond bark, flour, vanilla extract, olive oil, almond extract, eggs, baking powder, d...8        [olive oil, purple onion, fresh pineapple, pork, poblano peppers, corn tortillas, cheddar cheese, ground black peppe...9                                [chopped tomatoes, fresh basil, garlic, extra-virgin olive oil, kosher salt, flat leaf parsley]10       [pimentos, sweet pepper, dried oregano, olive oil, garlic, sharp cheddar cheese, pepper, swiss cheese, provolone che...11       [low sodium soy sauce, fresh ginger, dry mustard, green beans, white pepper, sesame oil, scallions, canola oil, suga...12       [Italian parsley leaves, walnuts, hot red pepper flakes, extra-virgin olive oil, fresh lemon juice, trout fillet, ga...13       [ground cinnamon, fresh cilantro, chili powder, ground coriander, kosher salt, ground black pepper, garlic, plum tom...14       [fresh parmesan cheese, butter, all-purpose flour, fat free less sodium chicken broth, chopped fresh chives, gruyere...15       [tumeric, vegetable stock, tomatoes, garam masala, naan, red lentils, red chili peppers, onions, spinach, sweet pota...16                                                                  [greek yogurt, lemon curd, confectioners sugar, raspberries]17                                                 [italian seasoning, broiler-fryer chicken, mayonaise, zesty italian dressing]18                                                                              [sugar, hot chili, asian fish sauce, lime juice]19       [soy sauce, vegetable oil, red bell pepper, chicken broth, yellow squash, garlic chili sauce, sliced green onions, b...20       [pork loin, roasted peanuts, chopped cilantro fresh, hoisin sauce, creamy peanut butter, chopped fresh mint, thai ba...21                                          [roma tomatoes, kosher salt, purple onion, jalapeno chilies, lime, chopped cilantro]22                                                [low-fat mayonnaise, pepper, salt, baking potatoes, eggs, spicy brown mustard]23       [sesame seeds, red pepper, yellow peppers, water, extra firm tofu, broccoli, soy sauce, orange bell pepper, arrowroo...24       [marinara sauce, flat leaf parsley, olive oil, linguine, capers, crushed red pepper flakes, olives, lemon zest, garlic]25       [sugar, lo mein noodles, salt, chicken broth, light soy sauce, flank steak, beansprouts, dried black mushrooms, pepp...26                     [herbs, lemon juice, fresh tomatoes, paprika, mango, stock, chile pepper, onions, red chili peppers, oil]27       [ground black pepper, butter, sliced mushrooms, sherry, salt, grated parmesan cheese, heavy cream, spaghetti, chicke...28       [green bell pepper, egg roll wrappers, sweet and sour sauce, corn starch, molasses, vegetable oil, oil, soy sauce, s...29                                                                     [flour tortillas, cheese, breakfast sausages, large eggs]                                                                  ...                                                           39744                [extra-virgin olive oil, oregano, potatoes, garlic cloves, pepper, salt, yellow mustard, fresh lemon juice]39745                                                      [quinoa, extra-virgin olive oil, fresh thyme leaves, scallion greens]39746    [clove, bay leaves, ginger, chopped cilantro, ground turmeric, white onion, cinnamon, cardamom pods, serrano chile, ...39747                                                   [water, sugar, grated lemon zest, butter, pitted date, blanched almonds]39748    [sea salt, pizza doughs, all-purpose flour, cornmeal, extra-virgin olive oil, shredded mozzarella cheese, kosher sal...39749    [kosher salt, minced onion, tortilla chips, sugar, tomato juice, cilantro leaves, avocado, lime juice, roma tomatoes...39750    [ground black pepper, chicken breasts, salsa, cheddar cheese, pepper jack, heavy cream, red enchilada sauce, unsalte...39751    [olive oil, cayenne pepper, chopped cilantro fresh, boneless chicken skinless thigh, fine sea salt, low salt chicken...39752                                                     [self rising flour, milk, white sugar, butter, peaches in light syrup]39753    [rosemary sprigs, lemon zest, garlic cloves, ground black pepper, vegetable broth, fresh basil leaves, minced garlic...39754    [jasmine rice, bay leaves, sticky rice, rotisserie chicken, chopped cilantro, large eggs, vegetable oil, yellow onio...39755    [mint leaves, cilantro leaves, ghee, tomatoes, cinnamon, oil, basmati rice, garlic paste, salt, coconut milk, clove,...39756        [vegetable oil, cinnamon sticks, water, all-purpose flour, piloncillo, salt, orange zest, baking powder, hot water]39757                                             [red bell pepper, garlic cloves, extra-virgin olive oil, feta cheese crumbles]39758    [milk, salt, ground cayenne pepper, ground lamb, ground cinnamon, ground black pepper, pomegranate, chopped fresh mi...39759    [red chili peppers, sea salt, onions, water, chilli bean sauce, caster sugar, garlic, white vinegar, chili oil, cucu...39760                                                   [butter, large eggs, cornmeal, baking powder, boiling water, milk, salt]39761    [honey, chicken breast halves, cilantro leaves, carrots, soy sauce, Sriracha, wonton wrappers, freshly ground pepper...39762    [curry powder, salt, chicken, water, vegetable oil, basmati rice, eggs, finely chopped onion, lemon juice, pepper, m...39763    [fettuccine pasta, low-fat cream cheese, garlic, nonfat evaporated milk, grated parmesan cheese, corn starch, nonfat...39764    [chili powder, worcestershire sauce, celery, red kidney beans, lean ground beef, stewed tomatoes, dried parsley, pep...39765                                                             [coconut, unsweetened coconut milk, mint leaves, plain yogurt]39766                    [rutabaga, ham, thick-cut bacon, potatoes, fresh parsley, salt, onions, pepper, carrots, pork sausages]39767    [low-fat sour cream, grated parmesan cheese, salt, dried oregano, low-fat cottage cheese, butter, onions, olive oil,...39768    [shredded cheddar cheese, crushed cheese crackers, cheddar cheese soup, cream of chicken soup, hot sauce, diced gree...39769    [light brown sugar, granulated sugar, butter, warm water, large eggs, all-purpose flour, whole wheat flour, cooking ...39770    [KRAFT Zesty Italian Dressing, purple onion, broccoli florets, rotini, pitted black olives, Kraft Grated Parmesan Ch...39771    [eggs, citrus fruit, raisins, sourdough starter, flour, hot tea, sugar, ground nutmeg, salt, ground cinnamon, milk, ...39772    [boneless chicken skinless thigh, minced garlic, steamed white rice, baking powder, corn starch, dark soy sauce, kos...39773    [green chile, jalapeno chilies, onions, ground black pepper, salt, chopped cilantro fresh, green bell pepper, garlic...Name: ingredients, Length: 39774, dtype: object0               greek1         southern_us2            filipino3              indian4              indian5            jamaican6             spanish7             italian8             mexican9             italian10            italian11            chinese12            italian13            mexican14            italian15             indian16            british17            italian18               thai19         vietnamese20               thai21            mexican22        southern_us23            chinese24            italian25            chinese26       cajun_creole27            italian28            chinese29            mexican             ...     39744           greek39745         spanish39746          indian39747        moroccan39748         italian39749         mexican39750         mexican39751        moroccan39752     southern_us39753         italian39754      vietnamese39755          indian39756         mexican39757           greek39758           greek39759          korean39760     southern_us39761         chinese39762          indian39763         italian39764         mexican39765          indian39766           irish39767         italian39768         mexican39769           irish39770         italian39771           irish39772         chinese39773         mexicanName: cuisine, Length: 39774, dtype: object</code></pre><h3 id="编程练习：基础统计运算"><a href="#编程练习：基础统计运算" class="headerlink" title="编程练习：基础统计运算"></a>编程练习：基础统计运算</h3><p>你的第一个编程练习是计算有关菜系佐料的统计数据。我们已为你导入了 <code>numpy</code>，你需要使用这个库来执行必要的计算。这些统计数据对于分析模型的预测结果非常重要的。<br>在下面的代码中，你要做的是：</p><ul><li>使用最频繁的佐料前10分别有哪些？</li><li>意大利菜中最常见的10个佐料有哪些？</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## <span class="doctag">TODO:</span> 统计佐料出现次数，并赋值到sum_ingredients字典中</span></span><br><span class="line">sum_ingredients = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> train_content[<span class="string">'ingredients'</span>]:</span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> i:</span><br><span class="line">        <span class="keyword">if</span> a <span class="keyword">not</span> <span class="keyword">in</span> sum_ingredients:</span><br><span class="line">            sum_ingredients[a] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> a <span class="keyword">in</span> sum_ingredients:</span><br><span class="line">            sum_ingredients[a] += <span class="number">1</span></span><br><span class="line">sum_ingredients</span><br></pre></td></tr></table></figure><pre><code>{&#39;sugar&#39;: 760, &#39;pistachio nuts&#39;: 7, &#39;white almond bark&#39;: 1, &#39;flour&#39;: 142, &#39;vanilla extract&#39;: 219, &#39;olive oil&#39;: 3111, &#39;almond extract&#39;: 56, &#39;eggs&#39;: 627, &#39;baking powder&#39;: 186, &#39;dried cranberries&#39;: 8, &#39;chopped tomatoes&#39;: 37, &#39;fresh basil&#39;: 787, &#39;garlic&#39;: 1471, &#39;extra-virgin olive oil&#39;: 1362, &#39;kosher salt&#39;: 656, &#39;flat leaf parsley&#39;: 588, &#39;pimentos&#39;: 16, &#39;sweet pepper&#39;: 7, &#39;dried oregano&#39;: 626, &#39;sharp cheddar cheese&#39;: 9, &#39;pepper&#39;: 965, &#39;swiss cheese&#39;: 7, &#39;provolone cheese&#39;: 138, &#39;canola oil&#39;: 41, &#39;mushrooms&#39;: 184, &#39;black olives&#39;: 67, &#39;sausages&#39;: 58, &#39;Italian parsley leaves&#39;: 74, &#39;walnuts&#39;: 38, &#39;hot red pepper flakes&#39;: 76, &#39;fresh lemon juice&#39;: 471, &#39;trout fillet&#39;: 3, &#39;garlic cloves&#39;: 1619, &#39;chipotle chile&#39;: 2, &#39;fine sea salt&#39;: 77, &#39;fresh parmesan cheese&#39;: 251, &#39;butter&#39;: 1030, &#39;all-purpose flour&#39;: 918, &#39;fat free less sodium chicken broth&#39;: 158, &#39;chopped fresh chives&#39;: 68, &#39;gruyere cheese&#39;: 18, &#39;ground black pepper&#39;: 1444, &#39;bacon slices&#39;: 48, &#39;gnocchi&#39;: 41, &#39;fat free milk&#39;: 42, &#39;cooking spray&#39;: 491, &#39;salt&#39;: 3454, &#39;italian seasoning&#39;: 347, &#39;broiler-fryer chicken&#39;: 1, &#39;mayonaise&#39;: 63, &#39;zesty italian dressing&#39;: 11, &#39;marinara sauce&#39;: 222, &#39;linguine&#39;: 193, &#39;capers&#39;: 306, &#39;crushed red pepper flakes&#39;: 179, &#39;olives&#39;: 29, &#39;lemon zest&#39;: 98, &#39;sliced mushrooms&#39;: 131, &#39;sherry&#39;: 13, &#39;grated parmesan cheese&#39;: 1580, &#39;heavy cream&#39;: 300, &#39;spaghetti&#39;: 296, &#39;chicken broth&#39;: 245, &#39;cooked chicken&#39;: 33, &#39;yellow corn meal&#39;: 64, &#39;boiling water&#39;: 63, &#39;sea salt&#39;: 202, &#39;onions&#39;: 1240, &#39;crushed garlic&#39;: 20, &#39;green onions&#39;: 144, &#39;white sugar&#39;: 231, &#39;dried basil&#39;: 425, &#39;diced tomatoes&#39;: 429, &#39;bread slices&#39;: 15, &#39;great northern beans&#39;: 21, &#39;shrimp&#39;: 59, &#39;sage leaves&#39;: 89, &#39;Oscar Mayer Deli Fresh Smoked Ham&#39;: 1, &#39;hoagie rolls&#39;: 8, &#39;salami&#39;: 41, &#39;giardiniera&#39;: 5, &#39;mozzarella cheese&#39;: 396, &#39;pepperoni&#39;: 48, &#39;bay leaves&#39;: 107, &#39;crushed red pepper&#39;: 418, &#39;mussels&#39;: 38, &#39;basil&#39;: 174, &#39;black pepper&#39;: 636, &#39;dry white wine&#39;: 658, &#39;tomatoes&#39;: 601, &#39;finely chopped onion&#39;: 145, &#39;lemon&#39;: 236, &#39;pesto&#39;: 113, &#39;salmon fillets&#39;: 11, &#39;white wine&#39;: 176, &#39;pizza crust&#39;: 36, &#39;plum tomatoes&#39;: 340, &#39;part-skim mozzarella cheese&#39;: 201, &#39;crushed tomatoes&#39;: 241, &#39;fresh rosemary&#39;: 292, &#39;boneless pork loin&#39;: 9, &#39;pappardelle&#39;: 11, &#39;red pepper&#39;: 49, &#39;Italian bread&#39;: 89, &#39;balsamic vinegar&#39;: 348, &#39;sausage casings&#39;: 78, &#39;honey&#39;: 126, &#39;shredded mozzarella cheese&#39;: 413, &#39;roasted red peppers&#39;: 114, &#39;penne pasta&#39;: 146, &#39;spinach&#39;: 119, &#39;asiago&#39;: 72, &#39;whole wheat pasta&#39;: 13, &#39;sweet onion&#39;: 69, &#39;grape tomatoes&#39;: 98, &#39;chestnuts&#39;: 9, &#39;granulated sugar&#39;: 82, &#39;whole milk ricotta cheese&#39;: 47, &#39;coffee ice cream&#39;: 3, &#39;large eggs&#39;: 625, &#39;mascarpone&#39;: 124, &#39;rum&#39;: 12, &#39;powdered sugar&#39;: 69, &#39;semisweet chocolate&#39;: 46, &#39;chestnut flour&#39;: 1, &#39;starchy potatoes&#39;: 2, &#39;grated nutmeg&#39;: 64, &#39;blood orange&#39;: 5, &#39;freshly ground pepper&#39;: 316, &#39;fennel bulb&#39;: 103, &#39;low salt chicken broth&#39;: 138, &#39;dijon mustard&#39;: 99, &#39;corn starch&#39;: 83, &#39;white wine vinegar&#39;: 73, &#39;tomato sauce&#39;: 357, &#39;shredded carrots&#39;: 11, &#39;english muffins, split and toasted&#39;: 2, &#39;chopped onion&#39;: 327, &#39;vegetable oil cooking spray&#39;: 73, &#39;chopped green bell pepper&#39;: 39, &#39;cheddar cheese&#39;: 18, &#39;lasagna noodles&#39;: 196, &#39;ranch dressing&#39;: 5, &#39;evaporated milk&#39;: 13, &#39;fresh parsley&#39;: 631, &#39;fresh oregano&#39;: 209, &#39;cold water&#39;: 58, &#39;chocolate morsels&#39;: 4, &#39;cream sweeten whip&#39;: 3, &#39;instant espresso granules&#39;: 4, &#39;whipping cream&#39;: 162, &#39;kahlúa&#39;: 12, &#39;chocolate covered coffee beans&#39;: 1, &#39;unflavored gelatin&#39;: 48, &#39;pound cake&#39;: 7, &#39;pinenuts&#39;: 252, &#39;zucchini&#39;: 326, &#39;baby carrots&#39;: 15, &#39;fresh basil leaves&#39;: 352, &#39;asparagus spears&#39;: 24, &#39;white onion&#39;: 48, &#39;carrots&#39;: 379, &#39;frozen peas&#39;: 67, &#39;arborio rice&#39;: 261, &#39;yellow crookneck squash&#39;: 3, &#39;fresh leav spinach&#39;: 25, &#39;cheese tortellini&#39;: 53, &#39;cherry tomatoes&#39;: 170, &#39;navy beans&#39;: 8, &#39;pecorino romano cheese&#39;: 146, &#39;fresh fava bean&#39;: 12, &#39;italian sausage&#39;: 129, &#39;large garlic cloves&#39;: 293, &#39;pasta sheets&#39;: 9, &#39;water&#39;: 1052, &#39;Turkish bay leaves&#39;: 3, &#39;dried chickpeas&#39;: 1, &#39;celery ribs&#39;: 130, &#39;semolina&#39;: 14, &#39;warm water&#39;: 182, &#39;vine ripened tomatoes&#39;: 12, &#39;bittersweet chocolate&#39;: 42, &#39;fat free yogurt&#39;: 4, &#39;skim milk&#39;: 16, &#39;angel food cake&#39;: 4, &#39;unsweetened cocoa powder&#39;: 84, &#39;instant espresso&#39;: 7, &#39;garlic salt&#39;: 61, &#39;tomato paste&#39;: 376, &#39;veal cutlets&#39;: 18, &#39;broccoli rabe&#39;: 47, &#39;whole milk&#39;: 157, &#39;parmigiano reggiano cheese&#39;: 306, &#39;dry bread crumbs&#39;: 115, &#39;fontina&#39;: 27, &#39;unsalted butter&#39;: 564, &#39;pasta sauce&#39;: 246, &#39;olive oil flavored cooking spray&#39;: 29, &#39;frozen chopped spinach&#39;: 131, &#39;large egg whites&#39;: 138, &#39;part-skim ricotta cheese&#39;: 111, &#39;manicotti shells&#39;: 18, &#39;fettucine&#39;: 136, &#39;parmesan cheese&#39;: 474, &#39;large shrimp&#39;: 77, &#39;chicken bouillon&#39;: 10, &#39;cream of tartar&#39;: 9, &#39;orange liqueur&#39;: 15, &#39;heavy whipping cream&#39;: 73, &#39;semi-sweet chocolate morsels&#39;: 21, &#39;cake flour&#39;: 24, &#39;confectioners sugar&#39;: 94, &#39;ground cinnamon&#39;: 85, &#39;ricotta cheese&#39;: 333, &#39;cream cheese&#39;: 105, &#39;dried porcini mushrooms&#39;: 81, &#39;chopped fresh thyme&#39;: 151, &#39;beef rib short&#39;: 5, &#39;dry red wine&#39;: 131, &#39;hot water&#39;: 85, &#39;fat free less sodium beef broth&#39;: 7, &#39;cremini mushrooms&#39;: 67, &#39;pitted kalamata olives&#39;: 94, &#39;cauliflower&#39;: 34, &#39;whole peeled tomatoes&#39;: 60, &#39;swiss chard&#39;: 63, &#39;banana squash&#39;: 1, &#39;vegetable broth&#39;: 81, &#39;bay leaf&#39;: 130, &#39;cannellini beans&#39;: 161, &#39;boneless skinless chicken breast halves&#39;: 182, &#39;light sour cream&#39;: 5, &#39;fava beans&#39;: 13, &#39;finely chopped fresh parsley&#39;: 33, &#39;fresh tarragon&#39;: 33, &#39;grated lemon zest&#39;: 142, &#39;chopped almonds&#39;: 12, &#39;broccoli&#39;: 64, &#39;potatoes&#39;: 59, &#39;self rising flour&#39;: 1, &#39;eggplant&#39;: 204, &#39;chicken&#39;: 84, &#39;pitas&#39;: 5, &#39;goat cheese&#39;: 92, &#39;reduced fat milk&#39;: 29, &#39;smoked trout&#39;: 1, &#39;fresh dill&#39;: 16, &#39;asparagus&#39;: 123, &#39;oil&#39;: 92, &#39;ricotta salata&#39;: 22, &#39;pecorino cheese&#39;: 52, &#39;grana padano&#39;: 7, &#39;pasta&#39;: 255, &#39;cracked black pepper&#39;: 117, &#39;green bell pepper&#39;: 163, &#39;vidalia onion&#39;: 19, &#39;smoked gouda&#39;: 14, &#39;baby spinach leaves&#39;: 39, &#39;shredded sharp cheddar cheese&#39;: 17, &#39;small red potato&#39;: 13, &#39;Bertolli® Classico Olive Oil&#39;: 16, &#39;bacon, crisp-cooked and crumbled&#39;: 2, &#39;bertolli vineyard premium collect marinara with burgundi wine sauc&#39;: 5, &#39;bread crumb fresh&#39;: 74, &#39;(    oz.) tomato sauce&#39;: 7, &#39;ground veal&#39;: 35, &#39;italian seasoning mix&#39;: 1, &#39;beef&#39;: 22, &#39;fat skimmed chicken broth&#39;: 28, &#39;solid pack pumpkin&#39;: 4, &#39;ground nutmeg&#39;: 127, &#39;white rice&#39;: 11, &#39;ground pepper&#39;: 77, &#39;fresh thyme leaves&#39;: 53, &#39;pistachios&#39;: 10, &#39;dried cherry&#39;: 6, &#39;prosciutto&#39;: 256, &#39;romano cheese&#39;: 89, &#39;parsley leaves&#39;: 23, &#39;shallots&#39;: 316, &#39;fresh raspberries&#39;: 14, &#39;dry vermouth&#39;: 18, &#39;canned low sodium chicken broth&#39;: 46, &#39;chicken livers&#39;: 14, &#39;raisins&#39;: 39, &#39;seasoned bread crumbs&#39;: 71, &#39;minced garlic&#39;: 334, &#39;half &amp; half&#39;: 89, &#39;ground beef&#39;: 237, &#39;jack cheese&#39;: 9, &#39;vegetables&#39;: 29, &#39;lemon juice&#39;: 216, &#39;ricotta&#39;: 95, &#39;yellow onion&#39;: 153, &#39;reduced sodium chicken broth&#39;: 36, &#39;chopped fresh mint&#39;: 54, &#39;purple onion&#39;: 350, &#39;low sodium chicken broth&#39;: 78, &#39;polenta&#39;: 132, &#39;bell pepper&#39;: 38, &#39;olive oil cooking spray&#39;: 13, &#39;bacon&#39;: 124, &#39;egg yolks&#39;: 82, &#39;duck breast halves&#39;: 1, &#39;fennel seeds&#39;: 114, &#39;roma tomatoes&#39;: 44, &#39;pesto sauce&#39;: 26, &#39;arugula&#39;: 125, &#39;summer squash&#39;: 9, &#39;red wine vinegar&#39;: 227, &#39;ciabatta&#39;: 21, &#39;juice&#39;: 72, &#39;fresh herbs&#39;: 14, &#39;grated lemon peel&#39;: 89, &#39;dough&#39;: 46, &#39;coarse sea salt&#39;: 17, &#39;rosemary leaves&#39;: 10, &#39;red pepper flakes&#39;: 206, &#39;brie cheese&#39;: 14, &#39;cheese ravioli&#39;: 18, &#39;Italian seasoned breadcrumbs&#39;: 23, &#39;cheese&#39;: 127, &#39;parsley&#39;: 95, &#39;fat-free cottage cheese&#39;: 11, &#39;oven-ready lasagna noodles&#39;: 43, &#39;margarine&#39;: 38, &#39;radicchio&#39;: 40, &#39;garbanzo beans&#39;: 18, &#39;orzo pasta&#39;: 16, &#39;rubbed sage&#39;: 17, &#39;dried rosemary&#39;: 96, &#39;canned beef broth&#39;: 4, &#39;kale leaves&#39;: 7, &#39;chicken noodle soup&#39;: 1, &#39;italian style rolls&#39;: 1, &#39;genoa salami&#39;: 22, &#39;oregano&#39;: 114, &#39;boiled ham&#39;: 1, &#39;capicola&#39;: 4, &#39;iceberg lettuce&#39;: 6, &#39;jalapeno chilies&#39;: 20, &#39;diced celery&#39;: 15, &#39;italian salad dressing mix&#39;: 7, &#39;chopped cilantro fresh&#39;: 25, &#39;cider vinegar&#39;: 14, &#39;red bell pepper&#39;: 354, &#39;sliced green onions&#39;: 40, &#39;barbecue sauce&#39;: 9, &#39;prepared pizza crust&#39;: 2, &#39;boneless skinless chicken breasts&#39;: 132, &#39;Sargento® Traditional Cut Shredded Mozzarella Cheese&#39;: 1, &#39;vegetable oil&#39;: 253, &#39;basil pesto sauce&#39;: 33, &#39;aioli&#39;: 3, &#39;cooked shrimp&#39;: 11, &#39;mozzarella balls&#39;: 4, &#39;sun-dried tomatoes&#39;: 118, &#39;milk&#39;: 277, &#39;condensed cream of mushroom soup&#39;: 16, &#39;fettuccine pasta&#39;: 24, &#39;frozen spinach&#39;: 24, &#39;lasagne&#39;: 6, &#39;passata&#39;: 3, &#39;fat free cream cheese&#39;: 8, &#39;french bread&#39;: 54, &#39;non-fat sour cream&#39;: 14, &#39;reduced fat swiss cheese&#39;: 3, &#39;fat-free mayonnaise&#39;: 8, &#39;roasted garlic&#39;: 7, &#39;seasoning&#39;: 12, &#39;kale&#39;: 58, &#39;sundried tomato paste&#39;: 4, &#39;dried thyme&#39;: 145, &#39;porcini&#39;: 7, &#39;fresh thyme&#39;: 62, &#39;sourdough loaf&#39;: 4, &#39;crust&#39;: 3, &#39;duck fat&#39;: 1, &#39;squabs&#39;: 1, &#39;confit&#39;: 2, &#39;aged gouda&#39;: 1, &#39;soppressata&#39;: 16, &#39;artichokes&#39;: 44, &#39;anchovy fillets&#39;: 124, &#39;lemon slices&#39;: 12, &#39;chicken cutlets&#39;: 23, &#39;saffron threads&#39;: 28, &#39;orzo&#39;: 51, &#39;veal chops&#39;: 4, &#39;oil cured olives&#39;: 3, &#39;pasta rotel&#39;: 2, &#39;pasta water&#39;: 10, &#39;cooking wine&#39;: 3, &#39;crusty bread&#39;: 10, &#39;fresh tomatoes&#39;: 32, &#39;uncooked rigatoni&#39;: 6, &#39;grating cheese&#39;: 6, &#39;vegetable stock&#39;: 29, &#39;freshly grated parmesan&#39;: 87, &#39;florets&#39;: 15, &#39;sliced almonds&#39;: 37, &#39;thyme sprigs&#39;: 45, &#39;egg whites&#39;: 95, &#39;greek yogurt&#39;: 6, &#39;penne&#39;: 99, &#39;fish fillets&#39;: 10, &#39;russet potatoes&#39;: 40, &#39;crumbled gorgonzola&#39;: 29, &#39;fontina cheese&#39;: 84, &#39;spinach leaves&#39;: 25, &#39;orange&#39;: 38, &#39;lamb shanks&#39;: 11, &#39;clove&#39;: 19, &#39;rosemary sprigs&#39;: 50, &#39;italian eggplant&#39;: 7, &#39;Sicilian olives&#39;: 7, &#39;mint sprigs&#39;: 28, &#39;peasant bread&#39;: 7, &#39;garlic powder&#39;: 252, &#39;Kraft Grated Parmesan Cheese&#39;: 10, &#39;peas&#39;: 42, &#39;Oscar Mayer Bacon&#39;: 1, &#39;Philadelphia Cream Cheese&#39;: 9, &#39;garnish&#39;: 3, &#39;dried navy beans&#39;: 5, &#39;celery&#39;: 155, &#39;diced onions&#39;: 42, &#39;wheat berries&#39;: 4, &#39;parsley sprigs&#39;: 38, &#39;thyme&#39;: 51, &#39;polenta prepar&#39;: 5, &#39;chicken breast halves&#39;: 35, &#39;dry sherry&#39;: 22, &#39;cocoa powder&#39;: 9, &#39;dried parsley&#39;: 90, &#39;chopped garlic&#39;: 69, &#39;baking soda&#39;: 68, &#39;coarse salt&#39;: 128, &#39;rigatoni&#39;: 60, &#39;nutmeg&#39;: 40, &#39;artichoke hearts&#39;: 99, &#39;pancetta&#39;: 153, &#39;parsnips&#39;: 7, &#39;whole wheat fettuccine&#39;: 4, &#39;ground sirloin&#39;: 20, &#39;red wine&#39;: 99, &#39;fresh marjoram&#39;: 29, &#39;castellane&#39;: 2, &#39;hot Italian sausages&#39;: 39, &#39;jumbo pasta shells&#39;: 37, &#39;fresh spinach&#39;: 87, &#39;chicken breasts&#39;: 93, &#39;gluten free blend&#39;: 1, &#39;paprika&#39;: 76, &#39;salt and ground black pepper&#39;: 66, &#39;bone in skinless chicken thigh&#39;: 1, &#39;figs&#39;: 10, &#39;egg substitute&#39;: 26, &#39;hot sauce&#39;: 20, &#39;ham&#39;: 42, &#39;balsamic vinaigrette&#39;: 6, &#39;pitted olives&#39;: 15, &#39;fresh chives&#39;: 24, &#39;white mushrooms&#39;: 20, &#39;haricots verts&#39;: 4, &#39;fresh peas&#39;: 16, &#39;bow-tie pasta&#39;: 86, &#39;asparagus tips&#39;: 3, &#39;green beans&#39;: 70, &#39;low-fat cottage cheese&#39;: 14, &#39;whole wheat lasagna noodles&#39;: 8, &#39;shredded parmesan cheese&#39;: 38, &#39;apricots&#39;: 5, &#39;grappa&#39;: 9, &#39;all purpose unbleached flour&#39;: 68, &#39;dry yeast&#39;: 71, &#39;worcestershire sauce&#39;: 54, &#39;amaretti&#39;: 6, &#39;frozen strawberries&#39;: 1, &#39;strawberries&#39;: 48, &#39;cooked ham&#39;: 9, &#39;Alfredo sauce&#39;: 43, &#39;chopped fresh sage&#39;: 91, &#39;cheese slices&#39;: 11, &#39;bread&#39;: 51, &#39;morel&#39;: 2, &#39;leeks&#39;: 106, &#39;1% low-fat cottage cheese&#39;: 10, &#39;feta cheese crumbles&#39;: 68, &#39;pork belly&#39;: 2, &#39;fresh sage&#39;: 29, &#39;pork loin&#39;: 3, &#39;sliced black olives&#39;: 49, &#39;medium shrimp&#39;: 70, &#39;red chili peppers&#39;: 22, &#39;parmigiano-reggiano cheese&#39;: 53, &#39;fresh mint&#39;: 53, &#39;baguette&#39;: 119, &#39;chicken legs&#39;: 7, &#39;baby spinach&#39;: 117, &#39;white beans&#39;: 38, &#39;ground pork&#39;: 72, &#39;romana&#39;: 1, &#39;fresh mushrooms&#39;: 117, &#39;crimini mushrooms&#39;: 26, &#39;navel oranges&#39;: 10, &#39;kalamata&#39;: 62, &#39;sea scallops&#39;: 31, &#39;ladyfingers&#39;: 40, &#39;reduced fat cream cheese&#39;: 3, &#39;whipped topping&#39;: 12, &#39;wine&#39;: 15, &#39;frozen broccoli&#39;: 5, &#39;nonfat ricotta cheese&#39;: 32, &#39;shells&#39;: 8, &#39;reduced-fat cheese&#39;: 1, &#39;cornflake cereal&#39;: 2, &#39;beef brisket&#39;: 2, &#39;dry pasta&#39;: 8, &#39;white bread&#39;: 35, &#39;pork&#39;: 10, &#39;sweet italian sausage&#39;: 72, &#39;prosecco&#39;: 10, &#39;bread dough&#39;: 16, &#39;baby lima beans&#39;: 3, &#39;whipped cream&#39;: 15, &#39;hot cocoa mix&#39;: 2, &#39;brewed coffee&#39;: 18, &#39;abbamele&#39;: 1, &#39;wild mushrooms&#39;: 30, &#39;chopped walnuts&#39;: 46, &#39;fregola&#39;: 4, &#39;savoy cabbage&#39;: 10, &#39;mushroom caps&#39;: 17, &#39;pinot grigio&#39;: 3, &#39;liquid egg substitute&#39;: 1, &#39;chocolate candy bars&#39;: 5, &#39;cooked rice&#39;: 5, &#39;bread crumbs&#39;: 103, &#39;banana peppers&#39;: 5, &#39;toasted walnuts&#39;: 7, &#39;dark rum&#39;: 22, &#39;dried fig&#39;: 11, &#39;chopped parsley&#39;: 99, &#39;ground white pepper&#39;: 33, &quot;soft goat&#39;s cheese&quot;: 10, &#39;truffle oil&#39;: 17, &#39;hazelnuts&#39;: 53, &#39;veal scallopini&#39;: 2, &#39;elbow macaroni&#39;: 19, &#39;bread flour&#39;: 77, &#39;bread yeast&#39;: 1, &#39;red potato&#39;: 26, &#39;vegan parmesan cheese&#39;: 4, &#39;ahi&#39;: 1, &#39;golden brown sugar&#39;: 14, &#39;anjou pears&#39;: 3, &#39;granny smith apples&#39;: 6, &#39;crystallized ginger&#39;: 10, &#39;ice water&#39;: 7, &#39;vegetable shortening&#39;: 5, &#39;chinese five-spice powder&#39;: 1, &#39;whole wheat flour&#39;: 21, &#39;large egg yolks&#39;: 142, &#39;lean ground beef&#39;: 135, &#39;sausage links&#39;: 32, &#39;pork chops&#39;: 8, &#39;rosemary&#39;: 57, &#39;nectarines&#39;: 5, &#39;sweet cherries&#39;: 12, &#39;orange zest&#39;: 28, &#39;lavender buds&#39;: 1, &#39;apricot halves&#39;: 4, &#39;diced tomatoes in juice&#39;: 7, &#39;scallions&#39;: 61, &#39;italian salad dressing&#39;: 41, &#39;short pasta&#39;: 7, &#39;lemon wedge&#39;: 58, &#39;porterhouse steaks&#39;: 4, &#39;1% low-fat milk&#39;: 68, &#39;golden raisins&#39;: 46, &#39;cinnamon sticks&#39;: 18, &#39;broccolini&#39;: 3, &#39;salted butter&#39;: 11, &#39;chicken stock&#39;: 142, &#39;coffee granules&#39;: 12, &#39;lemon rind&#39;: 34, &#39;baby portobello mushrooms&#39;: 10, &#39;broccoli florets&#39;: 64, &#39;orecchiette&#39;: 41, &#39;melted butter&#39;: 24, &#39;pizza shells&#39;: 3, &#39;frozen mixed thawed vegetables,&#39;: 2, &#39;ragu old world style pasta sauc&#39;: 11, &#39;loosely packed fresh basil leaves&#39;: 21, &#39;whole wheat spaghetti&#39;: 22, &#39;butternut squash&#39;: 62, &#39;wonton wrappers&#39;: 17, &#39;meat&#39;: 15, &#39;pear tomatoes&#39;: 6, &#39;gaeta olives&#39;: 5, &#39;yukon gold potatoes&#39;: 39, &#39;turbot&#39;: 2, &#39;ground lamb&#39;: 6, &#39;refrigerated pizza dough&#39;: 31, &#39;potato gnocchi&#39;: 20, &#39;cream&#39;: 27, &#39;angel hair&#39;: 60, &#39;lime juice&#39;: 6, &#39;gelato&#39;: 4, &#39;cherry preserves&#39;: 2, &#39;amaretto liqueur&#39;: 4, &#39;cherries&#39;: 14, &#39;instant espresso powder&#39;: 22, &#39;nuts&#39;: 3, &#39;brown sugar&#39;: 67, &#39;pie shell&#39;: 2, &#39;marsala wine&#39;: 105, &#39;basil leaves&#39;: 153, &#39;cake&#39;: 7, &#39;crabmeat&#39;: 11, &#39;chopped fresh herbs&#39;: 8, &#39;button mushrooms&#39;: 35, &#39;escarole&#39;: 47, &#39;chopped pecans&#39;: 28, &#39;chocolate bars&#39;: 2, &#39;coffee liqueur&#39;: 11, &#39;flat anchovy&#39;: 5, &#39;italian loaf&#39;: 4, &#39;salad dressing&#39;: 23, &#39;pitted black olives&#39;: 24, &#39;rotini&#39;: 46, &#39;frozen mixed vegetables&#39;: 4, &#39;canned tomatoes&#39;: 26, &#39;feta cheese&#39;: 36, &#39;anise seed&#39;: 13, &#39;top sirloin&#39;: 1, &#39;candy&#39;: 3, &#39;angel food cake mix&#39;: 2, &#39;buttercream frosting&#39;: 1, &#39;sour cream&#39;: 71, &#39;shredded cheddar cheese&#39;: 42, &#39;cottage cheese&#39;: 44, &#39;noodles&#39;: 38, &#39;lime&#39;: 5, &#39;watermelon&#39;: 5, &#39;risotto&#39;: 11, &#39;ice cubes&#39;: 8, &#39;peeled tomatoes&#39;: 32, &#39;cooked vermicelli&#39;: 5, &#39;buns&#39;: 1, &#39;apple cider vinegar&#39;: 7, &#39;ground red pepper&#39;: 34, &#39;yellow bell pepper&#39;: 85, &#39;active dry yeast&#39;: 128, &#39;baking potatoes&#39;: 34, &#39;grated romano cheese&#39;: 19, &#39;quinoa&#39;: 8, &#39;littleneck clams&#39;: 26, &#39;whole wheat bread&#39;: 8, &#39;cream cheese, soften&#39;: 40, &#39;salad seasoning mix&#39;: 4, &#39;cucumber&#39;: 25, &#39;pappardelle pasta&#39;: 6, &#39;fresh mozzarella&#39;: 95, &#39;tuna steaks&#39;: 16, &#39;shredded zucchini&#39;: 4, &#39;dried pasta&#39;: 13, &#39;berries&#39;: 5, &#39;clams&#39;: 50, &#39;ground round&#39;: 25, &#39;fettuccine, cook and drain&#39;: 4, &#39;tomato purée&#39;: 47, &#39;chopped celery&#39;: 103, &#39;ditalini pasta&#39;: 15, &#39;lobster&#39;: 11, &#39;almonds&#39;: 35, &#39;anise&#39;: 13, &#39;anise extract&#39;: 12, &#39;brandy&#39;: 25, &#39;boneless chicken breast&#39;: 19, &#39;buffalo sauce&#39;: 2, &#39;blue cheese dressing&#39;: 3, &#39;cannelloni shells&#39;: 2, &#39;butter cooking spray&#39;: 3, &#39;light alfredo sauce&#39;: 7, &#39;rice&#39;: 12, &#39;cream of chicken soup&#39;: 9, &#39;chees fresh mozzarella&#39;: 72, &#39;shrimp tails&#39;: 2, &#39;bay scallops&#39;: 12, &#39;lump crab meat&#39;: 17, &#39;fish stock&#39;: 11, &#39;capellini&#39;: 8, &#39;veal&#39;: 17, &#39;beef stock&#39;: 24, &#39;chard&#39;: 3, &#39;grated Gruyère cheese&#39;: 5, &#39;panko&#39;: 25, &#39;chickpeas&#39;: 37, &#39;white cornmeal&#39;: 4, &#39;red bell pepper, sliced&#39;: 2, &#39;herbs&#39;: 20, &#39;chicken thighs&#39;: 32, &#39;green bell pepper, slice&#39;: 13, &#39;vanilla&#39;: 28, &#39;liqueur&#39;: 10, &#39;aged balsamic vinegar&#39;: 8, &#39;limoncello&#39;: 6, &#39;golden beets&#39;: 4, &#39;pizza doughs&#39;: 103, &#39;black cod&#39;: 2, &#39;green cabbage&#39;: 12, &#39;cavolo nero&#39;: 4, &#39;winter squash&#39;: 1, &#39;thin pizza crust&#39;: 9, &#39;toasted pine nuts&#39;: 30, &#39;grated parmesan romano&#39;: 2, &#39;cumin seed&#39;: 4, &#39;cilantro leaves&#39;: 7, &#39;seasoning salt&#39;: 15, &#39;mixed greens&#39;: 16, &#39;turkey breast cutlets&#39;: 9, &#39;cod fillets&#39;: 8, &#39;barilla&#39;: 4, &#39;linguini&#39;: 9, &#39;perciatelli&#39;: 7, &#39;crumbled blue cheese&#39;: 16, &#39;black mission figs&#39;: 4, &#39;swordfish steaks&#39;: 11, &#39;anchovy paste&#39;: 47, &#39;chuck&#39;: 8, &#39;tomatoes with juice&#39;: 30, &#39;store bought low sodium chicken stock&#39;: 2, &#39;fresh lavender&#39;: 1, &#39;grated orange&#39;: 31, &#39;vanilla wafers&#39;: 3, &#39;amaretto&#39;: 19, &#39;toasted almonds&#39;: 6, &#39;light corn syrup&#39;: 8, &#39;focaccia&#39;: 10, &#39;oyster mushrooms&#39;: 6, &#39;shiitake mushroom caps&#39;: 16, &#39;onion powder&#39;: 39, &#39;sourdough&#39;: 7, &#39;orange bell pepper&#39;: 27, &#39;nonfat cottage cheese&#39;: 6, &#39;stewed tomatoes&#39;: 37, &#39;raspberries&#39;: 25, &#39;vanilla beans&#39;: 26, &#39;Frangelico&#39;: 9, &#39;vegetable oil spray&#39;: 22, &#39;table salt&#39;: 21, &#39;white peppercorns&#39;: 2, &#39;herb vinegar&#39;: 3, &#39;reduced fat sharp cheddar cheese&#39;: 8, &#39;deli ham&#39;: 4, &#39;ground turkey&#39;: 48, &#39;hot dogs&#39;: 2, &#39;italian style stewed tomatoes&#39;: 15, &#39;veal stock&#39;: 7, &#39;portabello mushroom&#39;: 41, &#39;rocket leaves&#39;: 35, &#39;country bread&#39;: 26, &#39;bottled balsamic vinaigrette&#39;: 1, &#39;scallops&#39;: 12, &#39;italian tomatoes&#39;: 16, &#39;peeled shrimp&#39;: 3, &#39;whole wheat pizza crust&#39;: 3, &#39;dried mixed herbs&#39;: 3, &#39;whole wheat pastry flour&#39;: 6, &#39;nonstick spray&#39;: 7, &#39;low-fat sour cream&#39;: 8, &#39;day old bread&#39;: 2, &#39;champagne vinegar&#39;: 9, &#39;pizza sauce&#39;: 71, &#39;red vermouth&#39;: 5, &#39;low sodium chicken stock&#39;: 6, &#39;peppercorns&#39;: 3, &#39;turkey stock&#39;: 3, &#39;cooked turkey&#39;: 6, &#39;green olives&#39;: 52, &#39;spaghettini&#39;: 23, &#39;minced onion&#39;: 41, &#39;beef broth&#39;: 62, &#39;dried mint flakes&#39;: 3, &#39;ravioli&#39;: 16, &#39;meat loaf mix&#39;: 1, &#39;rub&#39;: 2, &#39;clam juice&#39;: 26, &#39;bottled clam juice&#39;: 15, &#39;cayenne pepper&#39;: 50, &#39;pears&#39;: 19, &#39;gorgonzola&#39;: 27, &#39;duck&#39;: 2, &#39;pure vanilla extract&#39;: 22, &#39;panettone&#39;: 4, &#39;sambuca&#39;: 4, &#39;light brown sugar&#39;: 15, &#39;fat free frozen top whip&#39;: 5, &#39;frozen bread dough&#39;: 9, &#39;whole wheat breadcrumbs&#39;: 10, &#39;ground chuck&#39;: 16, &#39;bows&#39;: 3, &#39;sauce&#39;: 53, &#39;ground oregano&#39;: 4, &#39;lean ground turkey&#39;: 2, &#39;milk chocolate&#39;: 11, &#39;hazelnut butter&#39;: 4, &#39;fig jam&#39;: 2, &#39;crackers&#39;: 7, &#39;sun-dried tomatoes in oil&#39;: 28, &#39;dark chocolate&#39;: 7, &#39;vanilla ice cream&#39;: 17, &#39;chives&#39;: 31, &#39;radishes&#39;: 13, &#39;lettuce&#39;: 6, &#39;grilled chicken&#39;: 2, &#39;flatbread&#39;: 4, &#39;fresh parsley leaves&#39;: 56, &#39;lemon extract&#39;: 7, &#39;salad greens&#39;: 11, &#39;brown rice&#39;: 6, &#39;tortellini&#39;: 19, &#39;reduced fat alfredo sauce&#39;: 4, &#39;fresh asparagus&#39;: 24, &#39;round steaks&#39;: 5, &#39;chili powder&#39;: 26, &#39;ground cumin&#39;: 30, &#39;rolls&#39;: 21, &#39;prego traditional italian sauce&#39;: 4, &#39;brown hash potato&#39;: 5, &#39;nonfat milk&#39;: 7, &#39;frozen cheese ravioli&#39;: 6, &#39;mild Italian sausage&#39;: 17, &#39;sandwich rolls&#39;: 3, &#39;orange juice concentrate&#39;: 5, &#39;rotelle&#39;: 3, &#39;sweet italian sausag links, cut into&#39;: 2, &#39;idaho potatoes&#39;: 4, &#39;popcorn&#39;: 1, &#39;grated orange peel&#39;: 26, &#39;french fried onions&#39;: 1, &#39;turbinado&#39;: 7, &#39;cooked chicken breasts&#39;: 16, &#39;farro&#39;: 14, &#39;flour tortillas&#39;: 11, &#39;english cucumber&#39;: 7, &#39;minced peperoncini&#39;: 2, &#39;teleme&#39;: 2, &#39;brine cured green olives&#39;: 4, &#39;black forest ham&#39;: 1, &#39;frozen whole kernel corn&#39;: 5, &#39;sweet potatoes&#39;: 15, &#39;applewood smoked bacon&#39;: 7, &#39;panko breadcrumbs&#39;: 24, &#39;coarse kosher salt&#39;: 9, &#39;cornmeal&#39;: 72, &#39;dried sage&#39;: 28, &#39;dri leav thyme&#39;: 6, &#39;low sodium beef broth&#39;: 3, &#39;cabbage&#39;: 12, &#39;fresh shiitake mushrooms&#39;: 15, &#39;rabbit&#39;: 8, &#39;herbes de provence&#39;: 12, &#39;dressing&#39;: 7, &#39;chicken fingers&#39;: 3, &#39;reduced-fat sour cream&#39;: 13, &#39;dried fettuccine&#39;: 14, &#39;white pepper&#39;: 22, &#39;albacore tuna in water&#39;: 5, &#39;light mayonnaise&#39;: 7, &#39;turkey tenderloins&#39;: 4, &#39;almond flour&#39;: 8, &#39;raw almond&#39;: 3, &#39;fresh orange juice&#39;: 32, &#39;cranberries&#39;: 3, &#39;orange marmalade&#39;: 6, &#39;fresh lemon&#39;: 4, &#39;condensed chicken broth&#39;: 1, &#39;oil packed anchovy fillets&#39;: 3, &#39;boneless chicken skinless thigh&#39;: 20, &#39;sugar pea&#39;: 16, &#39;nonfat yogurt&#39;: 4, &#39;roast red peppers, drain&#39;: 24, &#39;french baguette&#39;: 25, &#39;ripe olives&#39;: 26, &#39;honey glazed ham&#39;: 2, &#39;chiles&#39;: 9, &#39;spring onions&#39;: 3, &#39;candied orange peel&#39;: 13, &#39;lard&#39;: 5, &#39;cinnamon&#39;: 23, &#39;semolina flour&#39;: 32, &#39;onion salt&#39;: 2, &#39;beef demi-glace&#39;: 1, &#39;veal shanks&#39;: 21, &#39;orange peel&#39;: 7, &#39;lemon peel&#39;: 11, &#39;plain yogurt&#39;: 7, &#39;Quinoa Flour&#39;: 1, &#39;spelt flour&#39;: 2, &#39;plums&#39;: 14, &#39;heirloom tomatoes&#39;: 21, &#39;fresh lime juice&#39;: 25, &#39;artichok heart marin&#39;: 33, &#39;bucatini&#39;: 16, &#39;processed cheese&#39;: 5, &#39;egg noodles, cooked and drained&#39;: 3, &#39;tapioca flour&#39;: 1, &#39;lasagna noodles, cooked and drained&#39;: 26, &#39;firm tofu&#39;: 9, &#39;sherry vinegar&#39;: 27, &#39;country style bread&#39;: 7, &#39;teardrop tomatoes&#39;: 2, &#39;ground sausage&#39;: 5, &#39;pita pockets&#39;: 1, &#39;orange juice&#39;: 19, &#39;chili pepper&#39;: 5, &#39;currant&#39;: 7, &#39;small capers, rins and drain&#39;: 2, &#39;filet&#39;: 4, &#39;lettuce leaves&#39;: 8, &#39;vodka&#39;: 27, &#39;stolichnaya&#39;: 1, &#39;romaine lettuce&#39;: 33, &#39;croutons&#39;: 18, &#39;pepperocini&#39;: 2, &#39;cherry peppers&#39;: 3, &#39;dandelion&#39;: 1, &#39;beans&#39;: 6, &#39;hot pepper sauce&#39;: 7, &#39;shredded Monterey Jack cheese&#39;: 13, &#39;cane sugar&#39;: 1, &#39;mixed nuts&#39;: 1, &#39;meatballs&#39;: 17, &#39;plain dry bread crumb&#39;: 11, &#39;spanish onion&#39;: 9, &#39;cuban peppers&#39;: 3, &#39;green tomatoes&#39;: 2, &#39;sesame seeds&#39;: 13, &#39;boneless beef chuck roast&#39;: 3, &#39;hard-boiled egg&#39;: 6, &#39;pork tenderloin&#39;: 24, &#39;bulk italian sausag&#39;: 21, &#39;beef bouillon granules&#39;: 3, &#39;prebaked pizza crusts&#39;: 13, &#39;buttermilk&#39;: 32, &#39;flaked coconut&#39;: 5, &#39;lower sodium chicken broth&#39;: 20, &#39;carnaroli rice&#39;: 13, &#39;fresh oregano leaves&#39;: 24, &#39;cavatappi&#39;: 9, &#39;cooking oil&#39;: 24, &#39;cayenne&#39;: 8, &#39;ground cloves&#39;: 30, &#39;corn syrup&#39;: 4, &#39;small pasta&#39;: 7, &#39;ground fennel&#39;: 7, &#39;low-fat buttermilk&#39;: 9, &#39;ice&#39;: 7, &#39;chicken bouillon granules&#39;: 6, &#39;poultry seasoning&#39;: 10, &#39;roast&#39;: 3, &#39;fusilli&#39;: 43, &#39;Italian herbs&#39;: 7, &#39;diced yellow onion&#39;: 1, &#39;manicotti pasta&#39;: 4, &#39;parsley flakes&#39;: 17, &#39;vinaigrette&#39;: 7, &#39;bread ciabatta&#39;: 16, &#39;kidney beans&#39;: 12, &#39;creole seasoning&#39;: 4, &#39;prepared pasta sauce&#39;: 5, &#39;small curd cottage cheese&#39;: 14, &#39;white sandwich bread&#39;: 9, &#39;mini chocolate chips&#39;: 5, &#39;lean beef&#39;: 2, &#39;breadstick&#39;: 13, &#39;pickled okra&#39;: 2, &#39;fronds&#39;: 15, &#39;thick-cut bacon&#39;: 11, &#39;boiling potatoes&#39;: 13, &#39;ditalini&#39;: 11, &#39;cranberry beans&#39;: 4, &#39;center cut bacon&#39;: 7, &#39;roasting chickens&#39;: 8, &#39;fleur de sel&#39;: 6, &#39;Margherita Pepperoni&#39;: 1, &#39;soft-shell clams&#39;: 1, &#39;liquid&#39;: 5, &#39;fresh chevre&#39;: 4, &#39;pork sausages&#39;: 14, &#39;dried minced onion&#39;: 5, &#39;msg&#39;: 2, &#39;pork stew meat&#39;: 1, &#39;beef stew meat&#39;: 3, &#39;ziti&#39;: 26, &#39;Balsamico Bianco&#39;: 2, &#39;ground mustard&#39;: 1, ...}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line"><span class="comment"># Finally, plot the 10 most used ingredients</span></span><br><span class="line">plt.style.use(<span class="string">u'ggplot'</span>)</span><br><span class="line">fig = pd.DataFrame(sum_ingredients, index=[<span class="number">0</span>]).transpose()[<span class="number">0</span>].sort_values(ascending=<span class="keyword">False</span>, inplace=<span class="keyword">False</span>)[:<span class="number">10</span>].plot(kind=<span class="string">'barh'</span>)</span><br><span class="line">fig.invert_yaxis()</span><br><span class="line">fig = fig.get_figure()</span><br><span class="line">fig.tight_layout()</span><br><span class="line">fig.show()</span><br></pre></td></tr></table></figure><pre><code>/opt/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:448: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.  % get_backend())</code></pre><p><img src="/2019/02/15/PredictYourCuisine/output_15_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## <span class="doctag">TODO:</span> 统计意大利菜系中佐料出现次数，并赋值到italian_ingredients字典中</span></span><br><span class="line">italian_ingredients = &#123;&#125;</span><br><span class="line">train_content_italian = train_content[train_content[<span class="string">'cuisine'</span>] == <span class="string">'italian'</span>]</span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> train_content_italian[<span class="string">'ingredients'</span>]:</span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> l:</span><br><span class="line">        <span class="keyword">if</span> a <span class="keyword">not</span> <span class="keyword">in</span> italian_ingredients:</span><br><span class="line">            italian_ingredients[a] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> a <span class="keyword">in</span> sum_italian:</span><br><span class="line">            italian_ingredients[a] += <span class="number">1</span></span><br><span class="line">italian_ingredients</span><br></pre></td></tr></table></figure><pre><code>{&#39;sugar&#39;: 760, &#39;pistachio nuts&#39;: 7, &#39;white almond bark&#39;: 1, &#39;flour&#39;: 142, &#39;vanilla extract&#39;: 219, &#39;olive oil&#39;: 3111, &#39;almond extract&#39;: 56, &#39;eggs&#39;: 627, &#39;baking powder&#39;: 186, &#39;dried cranberries&#39;: 8, &#39;chopped tomatoes&#39;: 37, &#39;fresh basil&#39;: 787, &#39;garlic&#39;: 1471, &#39;extra-virgin olive oil&#39;: 1362, &#39;kosher salt&#39;: 656, &#39;flat leaf parsley&#39;: 588, &#39;pimentos&#39;: 16, &#39;sweet pepper&#39;: 7, &#39;dried oregano&#39;: 626, &#39;sharp cheddar cheese&#39;: 9, &#39;pepper&#39;: 965, &#39;swiss cheese&#39;: 7, &#39;provolone cheese&#39;: 138, &#39;canola oil&#39;: 41, &#39;mushrooms&#39;: 184, &#39;black olives&#39;: 67, &#39;sausages&#39;: 58, &#39;Italian parsley leaves&#39;: 74, &#39;walnuts&#39;: 38, &#39;hot red pepper flakes&#39;: 76, &#39;fresh lemon juice&#39;: 471, &#39;trout fillet&#39;: 3, &#39;garlic cloves&#39;: 1619, &#39;chipotle chile&#39;: 2, &#39;fine sea salt&#39;: 77, &#39;fresh parmesan cheese&#39;: 251, &#39;butter&#39;: 1030, &#39;all-purpose flour&#39;: 918, &#39;fat free less sodium chicken broth&#39;: 158, &#39;chopped fresh chives&#39;: 68, &#39;gruyere cheese&#39;: 18, &#39;ground black pepper&#39;: 1444, &#39;bacon slices&#39;: 48, &#39;gnocchi&#39;: 41, &#39;fat free milk&#39;: 42, &#39;cooking spray&#39;: 491, &#39;salt&#39;: 3454, &#39;italian seasoning&#39;: 347, &#39;broiler-fryer chicken&#39;: 1, &#39;mayonaise&#39;: 63, &#39;zesty italian dressing&#39;: 11, &#39;marinara sauce&#39;: 222, &#39;linguine&#39;: 193, &#39;capers&#39;: 306, &#39;crushed red pepper flakes&#39;: 179, &#39;olives&#39;: 29, &#39;lemon zest&#39;: 98, &#39;sliced mushrooms&#39;: 131, &#39;sherry&#39;: 13, &#39;grated parmesan cheese&#39;: 1580, &#39;heavy cream&#39;: 300, &#39;spaghetti&#39;: 296, &#39;chicken broth&#39;: 245, &#39;cooked chicken&#39;: 33, &#39;yellow corn meal&#39;: 64, &#39;boiling water&#39;: 63, &#39;sea salt&#39;: 202, &#39;onions&#39;: 1240, &#39;crushed garlic&#39;: 20, &#39;green onions&#39;: 144, &#39;white sugar&#39;: 231, &#39;dried basil&#39;: 425, &#39;diced tomatoes&#39;: 429, &#39;bread slices&#39;: 15, &#39;great northern beans&#39;: 21, &#39;shrimp&#39;: 59, &#39;sage leaves&#39;: 89, &#39;Oscar Mayer Deli Fresh Smoked Ham&#39;: 1, &#39;hoagie rolls&#39;: 8, &#39;salami&#39;: 41, &#39;giardiniera&#39;: 5, &#39;mozzarella cheese&#39;: 396, &#39;pepperoni&#39;: 48, &#39;bay leaves&#39;: 107, &#39;crushed red pepper&#39;: 418, &#39;mussels&#39;: 38, &#39;basil&#39;: 174, &#39;black pepper&#39;: 636, &#39;dry white wine&#39;: 658, &#39;tomatoes&#39;: 601, &#39;finely chopped onion&#39;: 145, &#39;lemon&#39;: 236, &#39;pesto&#39;: 113, &#39;salmon fillets&#39;: 11, &#39;white wine&#39;: 176, &#39;pizza crust&#39;: 36, &#39;plum tomatoes&#39;: 340, &#39;part-skim mozzarella cheese&#39;: 201, &#39;crushed tomatoes&#39;: 241, &#39;fresh rosemary&#39;: 292, &#39;boneless pork loin&#39;: 9, &#39;pappardelle&#39;: 11, &#39;red pepper&#39;: 49, &#39;Italian bread&#39;: 89, &#39;balsamic vinegar&#39;: 348, &#39;sausage casings&#39;: 78, &#39;honey&#39;: 126, &#39;shredded mozzarella cheese&#39;: 413, &#39;roasted red peppers&#39;: 114, &#39;penne pasta&#39;: 146, &#39;spinach&#39;: 119, &#39;asiago&#39;: 72, &#39;whole wheat pasta&#39;: 13, &#39;sweet onion&#39;: 69, &#39;grape tomatoes&#39;: 98, &#39;chestnuts&#39;: 9, &#39;granulated sugar&#39;: 82, &#39;whole milk ricotta cheese&#39;: 47, &#39;coffee ice cream&#39;: 3, &#39;large eggs&#39;: 625, &#39;mascarpone&#39;: 124, &#39;rum&#39;: 12, &#39;powdered sugar&#39;: 69, &#39;semisweet chocolate&#39;: 46, &#39;chestnut flour&#39;: 1, &#39;starchy potatoes&#39;: 2, &#39;grated nutmeg&#39;: 64, &#39;blood orange&#39;: 5, &#39;freshly ground pepper&#39;: 316, &#39;fennel bulb&#39;: 103, &#39;low salt chicken broth&#39;: 138, &#39;dijon mustard&#39;: 99, &#39;corn starch&#39;: 83, &#39;white wine vinegar&#39;: 73, &#39;tomato sauce&#39;: 357, &#39;shredded carrots&#39;: 11, &#39;english muffins, split and toasted&#39;: 2, &#39;chopped onion&#39;: 327, &#39;vegetable oil cooking spray&#39;: 73, &#39;chopped green bell pepper&#39;: 39, &#39;cheddar cheese&#39;: 18, &#39;lasagna noodles&#39;: 196, &#39;ranch dressing&#39;: 5, &#39;evaporated milk&#39;: 13, &#39;fresh parsley&#39;: 631, &#39;fresh oregano&#39;: 209, &#39;cold water&#39;: 58, &#39;chocolate morsels&#39;: 4, &#39;cream sweeten whip&#39;: 3, &#39;instant espresso granules&#39;: 4, &#39;whipping cream&#39;: 162, &#39;kahlúa&#39;: 12, &#39;chocolate covered coffee beans&#39;: 1, &#39;unflavored gelatin&#39;: 48, &#39;pound cake&#39;: 7, &#39;pinenuts&#39;: 252, &#39;zucchini&#39;: 326, &#39;baby carrots&#39;: 15, &#39;fresh basil leaves&#39;: 352, &#39;asparagus spears&#39;: 24, &#39;white onion&#39;: 48, &#39;carrots&#39;: 379, &#39;frozen peas&#39;: 67, &#39;arborio rice&#39;: 261, &#39;yellow crookneck squash&#39;: 3, &#39;fresh leav spinach&#39;: 25, &#39;cheese tortellini&#39;: 53, &#39;cherry tomatoes&#39;: 170, &#39;navy beans&#39;: 8, &#39;pecorino romano cheese&#39;: 146, &#39;fresh fava bean&#39;: 12, &#39;italian sausage&#39;: 129, &#39;large garlic cloves&#39;: 293, &#39;pasta sheets&#39;: 9, &#39;water&#39;: 1052, &#39;Turkish bay leaves&#39;: 3, &#39;dried chickpeas&#39;: 1, &#39;celery ribs&#39;: 130, &#39;semolina&#39;: 14, &#39;warm water&#39;: 182, &#39;vine ripened tomatoes&#39;: 12, &#39;bittersweet chocolate&#39;: 42, &#39;fat free yogurt&#39;: 4, &#39;skim milk&#39;: 16, &#39;angel food cake&#39;: 4, &#39;unsweetened cocoa powder&#39;: 84, &#39;instant espresso&#39;: 7, &#39;garlic salt&#39;: 61, &#39;tomato paste&#39;: 376, &#39;veal cutlets&#39;: 18, &#39;broccoli rabe&#39;: 47, &#39;whole milk&#39;: 157, &#39;parmigiano reggiano cheese&#39;: 306, &#39;dry bread crumbs&#39;: 115, &#39;fontina&#39;: 27, &#39;unsalted butter&#39;: 564, &#39;pasta sauce&#39;: 246, &#39;olive oil flavored cooking spray&#39;: 29, &#39;frozen chopped spinach&#39;: 131, &#39;large egg whites&#39;: 138, &#39;part-skim ricotta cheese&#39;: 111, &#39;manicotti shells&#39;: 18, &#39;fettucine&#39;: 136, &#39;parmesan cheese&#39;: 474, &#39;large shrimp&#39;: 77, &#39;chicken bouillon&#39;: 10, &#39;cream of tartar&#39;: 9, &#39;orange liqueur&#39;: 15, &#39;heavy whipping cream&#39;: 73, &#39;semi-sweet chocolate morsels&#39;: 21, &#39;cake flour&#39;: 24, &#39;confectioners sugar&#39;: 94, &#39;ground cinnamon&#39;: 85, &#39;ricotta cheese&#39;: 333, &#39;cream cheese&#39;: 105, &#39;dried porcini mushrooms&#39;: 81, &#39;chopped fresh thyme&#39;: 151, &#39;beef rib short&#39;: 5, &#39;dry red wine&#39;: 131, &#39;hot water&#39;: 85, &#39;fat free less sodium beef broth&#39;: 7, &#39;cremini mushrooms&#39;: 67, &#39;pitted kalamata olives&#39;: 94, &#39;cauliflower&#39;: 34, &#39;whole peeled tomatoes&#39;: 60, &#39;swiss chard&#39;: 63, &#39;banana squash&#39;: 1, &#39;vegetable broth&#39;: 81, &#39;bay leaf&#39;: 130, &#39;cannellini beans&#39;: 161, &#39;boneless skinless chicken breast halves&#39;: 182, &#39;light sour cream&#39;: 5, &#39;fava beans&#39;: 13, &#39;finely chopped fresh parsley&#39;: 33, &#39;fresh tarragon&#39;: 33, &#39;grated lemon zest&#39;: 142, &#39;chopped almonds&#39;: 12, &#39;broccoli&#39;: 64, &#39;potatoes&#39;: 59, &#39;self rising flour&#39;: 1, &#39;eggplant&#39;: 204, &#39;chicken&#39;: 84, &#39;pitas&#39;: 5, &#39;goat cheese&#39;: 92, &#39;reduced fat milk&#39;: 29, &#39;smoked trout&#39;: 1, &#39;fresh dill&#39;: 16, &#39;asparagus&#39;: 123, &#39;oil&#39;: 92, &#39;ricotta salata&#39;: 22, &#39;pecorino cheese&#39;: 52, &#39;grana padano&#39;: 7, &#39;pasta&#39;: 255, &#39;cracked black pepper&#39;: 117, &#39;green bell pepper&#39;: 163, &#39;vidalia onion&#39;: 19, &#39;smoked gouda&#39;: 14, &#39;baby spinach leaves&#39;: 39, &#39;shredded sharp cheddar cheese&#39;: 17, &#39;small red potato&#39;: 13, &#39;Bertolli® Classico Olive Oil&#39;: 16, &#39;bacon, crisp-cooked and crumbled&#39;: 2, &#39;bertolli vineyard premium collect marinara with burgundi wine sauc&#39;: 5, &#39;bread crumb fresh&#39;: 74, &#39;(    oz.) tomato sauce&#39;: 7, &#39;ground veal&#39;: 35, &#39;italian seasoning mix&#39;: 1, &#39;beef&#39;: 22, &#39;fat skimmed chicken broth&#39;: 28, &#39;solid pack pumpkin&#39;: 4, &#39;ground nutmeg&#39;: 127, &#39;white rice&#39;: 11, &#39;ground pepper&#39;: 77, &#39;fresh thyme leaves&#39;: 53, &#39;pistachios&#39;: 10, &#39;dried cherry&#39;: 6, &#39;prosciutto&#39;: 256, &#39;romano cheese&#39;: 89, &#39;parsley leaves&#39;: 23, &#39;shallots&#39;: 316, &#39;fresh raspberries&#39;: 14, &#39;dry vermouth&#39;: 18, &#39;canned low sodium chicken broth&#39;: 46, &#39;chicken livers&#39;: 14, &#39;raisins&#39;: 39, &#39;seasoned bread crumbs&#39;: 71, &#39;minced garlic&#39;: 334, &#39;half &amp; half&#39;: 89, &#39;ground beef&#39;: 237, &#39;jack cheese&#39;: 9, &#39;vegetables&#39;: 29, &#39;lemon juice&#39;: 216, &#39;ricotta&#39;: 95, &#39;yellow onion&#39;: 153, &#39;reduced sodium chicken broth&#39;: 36, &#39;chopped fresh mint&#39;: 54, &#39;purple onion&#39;: 350, &#39;low sodium chicken broth&#39;: 78, &#39;polenta&#39;: 132, &#39;bell pepper&#39;: 38, &#39;olive oil cooking spray&#39;: 13, &#39;bacon&#39;: 124, &#39;egg yolks&#39;: 82, &#39;duck breast halves&#39;: 1, &#39;fennel seeds&#39;: 114, &#39;roma tomatoes&#39;: 44, &#39;pesto sauce&#39;: 26, &#39;arugula&#39;: 125, &#39;summer squash&#39;: 9, &#39;red wine vinegar&#39;: 227, &#39;ciabatta&#39;: 21, &#39;juice&#39;: 72, &#39;fresh herbs&#39;: 14, &#39;grated lemon peel&#39;: 89, &#39;dough&#39;: 46, &#39;coarse sea salt&#39;: 17, &#39;rosemary leaves&#39;: 10, &#39;red pepper flakes&#39;: 206, &#39;brie cheese&#39;: 14, &#39;cheese ravioli&#39;: 18, &#39;Italian seasoned breadcrumbs&#39;: 23, &#39;cheese&#39;: 127, &#39;parsley&#39;: 95, &#39;fat-free cottage cheese&#39;: 11, &#39;oven-ready lasagna noodles&#39;: 43, &#39;margarine&#39;: 38, &#39;radicchio&#39;: 40, &#39;garbanzo beans&#39;: 18, &#39;orzo pasta&#39;: 16, &#39;rubbed sage&#39;: 17, &#39;dried rosemary&#39;: 96, &#39;canned beef broth&#39;: 4, &#39;kale leaves&#39;: 7, &#39;chicken noodle soup&#39;: 1, &#39;italian style rolls&#39;: 1, &#39;genoa salami&#39;: 22, &#39;oregano&#39;: 114, &#39;boiled ham&#39;: 1, &#39;capicola&#39;: 4, &#39;iceberg lettuce&#39;: 6, &#39;jalapeno chilies&#39;: 20, &#39;diced celery&#39;: 15, &#39;italian salad dressing mix&#39;: 7, &#39;chopped cilantro fresh&#39;: 25, &#39;cider vinegar&#39;: 14, &#39;red bell pepper&#39;: 354, &#39;sliced green onions&#39;: 40, &#39;barbecue sauce&#39;: 9, &#39;prepared pizza crust&#39;: 2, &#39;boneless skinless chicken breasts&#39;: 132, &#39;Sargento® Traditional Cut Shredded Mozzarella Cheese&#39;: 1, &#39;vegetable oil&#39;: 253, &#39;basil pesto sauce&#39;: 33, &#39;aioli&#39;: 3, &#39;cooked shrimp&#39;: 11, &#39;mozzarella balls&#39;: 4, &#39;sun-dried tomatoes&#39;: 118, &#39;milk&#39;: 277, &#39;condensed cream of mushroom soup&#39;: 16, &#39;fettuccine pasta&#39;: 24, &#39;frozen spinach&#39;: 24, &#39;lasagne&#39;: 6, &#39;passata&#39;: 3, &#39;fat free cream cheese&#39;: 8, &#39;french bread&#39;: 54, &#39;non-fat sour cream&#39;: 14, &#39;reduced fat swiss cheese&#39;: 3, &#39;fat-free mayonnaise&#39;: 8, &#39;roasted garlic&#39;: 7, &#39;seasoning&#39;: 12, &#39;kale&#39;: 58, &#39;sundried tomato paste&#39;: 4, &#39;dried thyme&#39;: 145, &#39;porcini&#39;: 7, &#39;fresh thyme&#39;: 62, &#39;sourdough loaf&#39;: 4, &#39;crust&#39;: 3, &#39;duck fat&#39;: 1, &#39;squabs&#39;: 1, &#39;confit&#39;: 2, &#39;aged gouda&#39;: 1, &#39;soppressata&#39;: 16, &#39;artichokes&#39;: 44, &#39;anchovy fillets&#39;: 124, &#39;lemon slices&#39;: 12, &#39;chicken cutlets&#39;: 23, &#39;saffron threads&#39;: 28, &#39;orzo&#39;: 51, &#39;veal chops&#39;: 4, &#39;oil cured olives&#39;: 3, &#39;pasta rotel&#39;: 2, &#39;pasta water&#39;: 10, &#39;cooking wine&#39;: 3, &#39;crusty bread&#39;: 10, &#39;fresh tomatoes&#39;: 32, &#39;uncooked rigatoni&#39;: 6, &#39;grating cheese&#39;: 6, &#39;vegetable stock&#39;: 29, &#39;freshly grated parmesan&#39;: 87, &#39;florets&#39;: 15, &#39;sliced almonds&#39;: 37, &#39;thyme sprigs&#39;: 45, &#39;egg whites&#39;: 95, &#39;greek yogurt&#39;: 6, &#39;penne&#39;: 99, &#39;fish fillets&#39;: 10, &#39;russet potatoes&#39;: 40, &#39;crumbled gorgonzola&#39;: 29, &#39;fontina cheese&#39;: 84, &#39;spinach leaves&#39;: 25, &#39;orange&#39;: 38, &#39;lamb shanks&#39;: 11, &#39;clove&#39;: 19, &#39;rosemary sprigs&#39;: 50, &#39;italian eggplant&#39;: 7, &#39;Sicilian olives&#39;: 7, &#39;mint sprigs&#39;: 28, &#39;peasant bread&#39;: 7, &#39;garlic powder&#39;: 252, &#39;Kraft Grated Parmesan Cheese&#39;: 10, &#39;peas&#39;: 42, &#39;Oscar Mayer Bacon&#39;: 1, &#39;Philadelphia Cream Cheese&#39;: 9, &#39;garnish&#39;: 3, &#39;dried navy beans&#39;: 5, &#39;celery&#39;: 155, &#39;diced onions&#39;: 42, &#39;wheat berries&#39;: 4, &#39;parsley sprigs&#39;: 38, &#39;thyme&#39;: 51, &#39;polenta prepar&#39;: 5, &#39;chicken breast halves&#39;: 35, &#39;dry sherry&#39;: 22, &#39;cocoa powder&#39;: 9, &#39;dried parsley&#39;: 90, &#39;chopped garlic&#39;: 69, &#39;baking soda&#39;: 68, &#39;coarse salt&#39;: 128, &#39;rigatoni&#39;: 60, &#39;nutmeg&#39;: 40, &#39;artichoke hearts&#39;: 99, &#39;pancetta&#39;: 153, &#39;parsnips&#39;: 7, &#39;whole wheat fettuccine&#39;: 4, &#39;ground sirloin&#39;: 20, &#39;red wine&#39;: 99, &#39;fresh marjoram&#39;: 29, &#39;castellane&#39;: 2, &#39;hot Italian sausages&#39;: 39, &#39;jumbo pasta shells&#39;: 37, &#39;fresh spinach&#39;: 87, &#39;chicken breasts&#39;: 93, &#39;gluten free blend&#39;: 1, &#39;paprika&#39;: 76, &#39;salt and ground black pepper&#39;: 66, &#39;bone in skinless chicken thigh&#39;: 1, &#39;figs&#39;: 10, &#39;egg substitute&#39;: 26, &#39;hot sauce&#39;: 20, &#39;ham&#39;: 42, &#39;balsamic vinaigrette&#39;: 6, &#39;pitted olives&#39;: 15, &#39;fresh chives&#39;: 24, &#39;white mushrooms&#39;: 20, &#39;haricots verts&#39;: 4, &#39;fresh peas&#39;: 16, &#39;bow-tie pasta&#39;: 86, &#39;asparagus tips&#39;: 3, &#39;green beans&#39;: 70, &#39;low-fat cottage cheese&#39;: 14, &#39;whole wheat lasagna noodles&#39;: 8, &#39;shredded parmesan cheese&#39;: 38, &#39;apricots&#39;: 5, &#39;grappa&#39;: 9, &#39;all purpose unbleached flour&#39;: 68, &#39;dry yeast&#39;: 71, &#39;worcestershire sauce&#39;: 54, &#39;amaretti&#39;: 6, &#39;frozen strawberries&#39;: 1, &#39;strawberries&#39;: 48, &#39;cooked ham&#39;: 9, &#39;Alfredo sauce&#39;: 43, &#39;chopped fresh sage&#39;: 91, &#39;cheese slices&#39;: 11, &#39;bread&#39;: 51, &#39;morel&#39;: 2, &#39;leeks&#39;: 106, &#39;1% low-fat cottage cheese&#39;: 10, &#39;feta cheese crumbles&#39;: 68, &#39;pork belly&#39;: 2, &#39;fresh sage&#39;: 29, &#39;pork loin&#39;: 3, &#39;sliced black olives&#39;: 49, &#39;medium shrimp&#39;: 70, &#39;red chili peppers&#39;: 22, &#39;parmigiano-reggiano cheese&#39;: 53, &#39;fresh mint&#39;: 53, &#39;baguette&#39;: 119, &#39;chicken legs&#39;: 7, &#39;baby spinach&#39;: 117, &#39;white beans&#39;: 38, &#39;ground pork&#39;: 72, &#39;romana&#39;: 1, &#39;fresh mushrooms&#39;: 117, &#39;crimini mushrooms&#39;: 26, &#39;navel oranges&#39;: 10, &#39;kalamata&#39;: 62, &#39;sea scallops&#39;: 31, &#39;ladyfingers&#39;: 40, &#39;reduced fat cream cheese&#39;: 3, &#39;whipped topping&#39;: 12, &#39;wine&#39;: 15, &#39;frozen broccoli&#39;: 5, &#39;nonfat ricotta cheese&#39;: 32, &#39;shells&#39;: 8, &#39;reduced-fat cheese&#39;: 1, &#39;cornflake cereal&#39;: 2, &#39;beef brisket&#39;: 2, &#39;dry pasta&#39;: 8, &#39;white bread&#39;: 35, &#39;pork&#39;: 10, &#39;sweet italian sausage&#39;: 72, &#39;prosecco&#39;: 10, &#39;bread dough&#39;: 16, &#39;baby lima beans&#39;: 3, &#39;whipped cream&#39;: 15, &#39;hot cocoa mix&#39;: 2, &#39;brewed coffee&#39;: 18, &#39;abbamele&#39;: 1, &#39;wild mushrooms&#39;: 30, &#39;chopped walnuts&#39;: 46, &#39;fregola&#39;: 4, &#39;savoy cabbage&#39;: 10, &#39;mushroom caps&#39;: 17, &#39;pinot grigio&#39;: 3, &#39;liquid egg substitute&#39;: 1, &#39;chocolate candy bars&#39;: 5, &#39;cooked rice&#39;: 5, &#39;bread crumbs&#39;: 103, &#39;banana peppers&#39;: 5, &#39;toasted walnuts&#39;: 7, &#39;dark rum&#39;: 22, &#39;dried fig&#39;: 11, &#39;chopped parsley&#39;: 99, &#39;ground white pepper&#39;: 33, &quot;soft goat&#39;s cheese&quot;: 10, &#39;truffle oil&#39;: 17, &#39;hazelnuts&#39;: 53, &#39;veal scallopini&#39;: 2, &#39;elbow macaroni&#39;: 19, &#39;bread flour&#39;: 77, &#39;bread yeast&#39;: 1, &#39;red potato&#39;: 26, &#39;vegan parmesan cheese&#39;: 4, &#39;ahi&#39;: 1, &#39;golden brown sugar&#39;: 14, &#39;anjou pears&#39;: 3, &#39;granny smith apples&#39;: 6, &#39;crystallized ginger&#39;: 10, &#39;ice water&#39;: 7, &#39;vegetable shortening&#39;: 5, &#39;chinese five-spice powder&#39;: 1, &#39;whole wheat flour&#39;: 21, &#39;large egg yolks&#39;: 142, &#39;lean ground beef&#39;: 135, &#39;sausage links&#39;: 32, &#39;pork chops&#39;: 8, &#39;rosemary&#39;: 57, &#39;nectarines&#39;: 5, &#39;sweet cherries&#39;: 12, &#39;orange zest&#39;: 28, &#39;lavender buds&#39;: 1, &#39;apricot halves&#39;: 4, &#39;diced tomatoes in juice&#39;: 7, &#39;scallions&#39;: 61, &#39;italian salad dressing&#39;: 41, &#39;short pasta&#39;: 7, &#39;lemon wedge&#39;: 58, &#39;porterhouse steaks&#39;: 4, &#39;1% low-fat milk&#39;: 68, &#39;golden raisins&#39;: 46, &#39;cinnamon sticks&#39;: 18, &#39;broccolini&#39;: 3, &#39;salted butter&#39;: 11, &#39;chicken stock&#39;: 142, &#39;coffee granules&#39;: 12, &#39;lemon rind&#39;: 34, &#39;baby portobello mushrooms&#39;: 10, &#39;broccoli florets&#39;: 64, &#39;orecchiette&#39;: 41, &#39;melted butter&#39;: 24, &#39;pizza shells&#39;: 3, &#39;frozen mixed thawed vegetables,&#39;: 2, &#39;ragu old world style pasta sauc&#39;: 11, &#39;loosely packed fresh basil leaves&#39;: 21, &#39;whole wheat spaghetti&#39;: 22, &#39;butternut squash&#39;: 62, &#39;wonton wrappers&#39;: 17, &#39;meat&#39;: 15, &#39;pear tomatoes&#39;: 6, &#39;gaeta olives&#39;: 5, &#39;yukon gold potatoes&#39;: 39, &#39;turbot&#39;: 2, &#39;ground lamb&#39;: 6, &#39;refrigerated pizza dough&#39;: 31, &#39;potato gnocchi&#39;: 20, &#39;cream&#39;: 27, &#39;angel hair&#39;: 60, &#39;lime juice&#39;: 6, &#39;gelato&#39;: 4, &#39;cherry preserves&#39;: 2, &#39;amaretto liqueur&#39;: 4, &#39;cherries&#39;: 14, &#39;instant espresso powder&#39;: 22, &#39;nuts&#39;: 3, &#39;brown sugar&#39;: 67, &#39;pie shell&#39;: 2, &#39;marsala wine&#39;: 105, &#39;basil leaves&#39;: 153, &#39;cake&#39;: 7, &#39;crabmeat&#39;: 11, &#39;chopped fresh herbs&#39;: 8, &#39;button mushrooms&#39;: 35, &#39;escarole&#39;: 47, &#39;chopped pecans&#39;: 28, &#39;chocolate bars&#39;: 2, &#39;coffee liqueur&#39;: 11, &#39;flat anchovy&#39;: 5, &#39;italian loaf&#39;: 4, &#39;salad dressing&#39;: 23, &#39;pitted black olives&#39;: 24, &#39;rotini&#39;: 46, &#39;frozen mixed vegetables&#39;: 4, &#39;canned tomatoes&#39;: 26, &#39;feta cheese&#39;: 36, &#39;anise seed&#39;: 13, &#39;top sirloin&#39;: 1, &#39;candy&#39;: 3, &#39;angel food cake mix&#39;: 2, &#39;buttercream frosting&#39;: 1, &#39;sour cream&#39;: 71, &#39;shredded cheddar cheese&#39;: 42, &#39;cottage cheese&#39;: 44, &#39;noodles&#39;: 38, &#39;lime&#39;: 5, &#39;watermelon&#39;: 5, &#39;risotto&#39;: 11, &#39;ice cubes&#39;: 8, &#39;peeled tomatoes&#39;: 32, &#39;cooked vermicelli&#39;: 5, &#39;buns&#39;: 1, &#39;apple cider vinegar&#39;: 7, &#39;ground red pepper&#39;: 34, &#39;yellow bell pepper&#39;: 85, &#39;active dry yeast&#39;: 128, &#39;baking potatoes&#39;: 34, &#39;grated romano cheese&#39;: 19, &#39;quinoa&#39;: 8, &#39;littleneck clams&#39;: 26, &#39;whole wheat bread&#39;: 8, &#39;cream cheese, soften&#39;: 40, &#39;salad seasoning mix&#39;: 4, &#39;cucumber&#39;: 25, &#39;pappardelle pasta&#39;: 6, &#39;fresh mozzarella&#39;: 95, &#39;tuna steaks&#39;: 16, &#39;shredded zucchini&#39;: 4, &#39;dried pasta&#39;: 13, &#39;berries&#39;: 5, &#39;clams&#39;: 50, &#39;ground round&#39;: 25, &#39;fettuccine, cook and drain&#39;: 4, &#39;tomato purée&#39;: 47, &#39;chopped celery&#39;: 103, &#39;ditalini pasta&#39;: 15, &#39;lobster&#39;: 11, &#39;almonds&#39;: 35, &#39;anise&#39;: 13, &#39;anise extract&#39;: 12, &#39;brandy&#39;: 25, &#39;boneless chicken breast&#39;: 19, &#39;buffalo sauce&#39;: 2, &#39;blue cheese dressing&#39;: 3, &#39;cannelloni shells&#39;: 2, &#39;butter cooking spray&#39;: 3, &#39;light alfredo sauce&#39;: 7, &#39;rice&#39;: 12, &#39;cream of chicken soup&#39;: 9, &#39;chees fresh mozzarella&#39;: 72, &#39;shrimp tails&#39;: 2, &#39;bay scallops&#39;: 12, &#39;lump crab meat&#39;: 17, &#39;fish stock&#39;: 11, &#39;capellini&#39;: 8, &#39;veal&#39;: 17, &#39;beef stock&#39;: 24, &#39;chard&#39;: 3, &#39;grated Gruyère cheese&#39;: 5, &#39;panko&#39;: 25, &#39;chickpeas&#39;: 37, &#39;white cornmeal&#39;: 4, &#39;red bell pepper, sliced&#39;: 2, &#39;herbs&#39;: 20, &#39;chicken thighs&#39;: 32, &#39;green bell pepper, slice&#39;: 13, &#39;vanilla&#39;: 28, &#39;liqueur&#39;: 10, &#39;aged balsamic vinegar&#39;: 8, &#39;limoncello&#39;: 6, &#39;golden beets&#39;: 4, &#39;pizza doughs&#39;: 103, &#39;black cod&#39;: 2, &#39;green cabbage&#39;: 12, &#39;cavolo nero&#39;: 4, &#39;winter squash&#39;: 1, &#39;thin pizza crust&#39;: 9, &#39;toasted pine nuts&#39;: 30, &#39;grated parmesan romano&#39;: 2, &#39;cumin seed&#39;: 4, &#39;cilantro leaves&#39;: 7, &#39;seasoning salt&#39;: 15, &#39;mixed greens&#39;: 16, &#39;turkey breast cutlets&#39;: 9, &#39;cod fillets&#39;: 8, &#39;barilla&#39;: 4, &#39;linguini&#39;: 9, &#39;perciatelli&#39;: 7, &#39;crumbled blue cheese&#39;: 16, &#39;black mission figs&#39;: 4, &#39;swordfish steaks&#39;: 11, &#39;anchovy paste&#39;: 47, &#39;chuck&#39;: 8, &#39;tomatoes with juice&#39;: 30, &#39;store bought low sodium chicken stock&#39;: 2, &#39;fresh lavender&#39;: 1, &#39;grated orange&#39;: 31, &#39;vanilla wafers&#39;: 3, &#39;amaretto&#39;: 19, &#39;toasted almonds&#39;: 6, &#39;light corn syrup&#39;: 8, &#39;focaccia&#39;: 10, &#39;oyster mushrooms&#39;: 6, &#39;shiitake mushroom caps&#39;: 16, &#39;onion powder&#39;: 39, &#39;sourdough&#39;: 7, &#39;orange bell pepper&#39;: 27, &#39;nonfat cottage cheese&#39;: 6, &#39;stewed tomatoes&#39;: 37, &#39;raspberries&#39;: 25, &#39;vanilla beans&#39;: 26, &#39;Frangelico&#39;: 9, &#39;vegetable oil spray&#39;: 22, &#39;table salt&#39;: 21, &#39;white peppercorns&#39;: 2, &#39;herb vinegar&#39;: 3, &#39;reduced fat sharp cheddar cheese&#39;: 8, &#39;deli ham&#39;: 4, &#39;ground turkey&#39;: 48, &#39;hot dogs&#39;: 2, &#39;italian style stewed tomatoes&#39;: 15, &#39;veal stock&#39;: 7, &#39;portabello mushroom&#39;: 41, &#39;rocket leaves&#39;: 35, &#39;country bread&#39;: 26, &#39;bottled balsamic vinaigrette&#39;: 1, &#39;scallops&#39;: 12, &#39;italian tomatoes&#39;: 16, &#39;peeled shrimp&#39;: 3, &#39;whole wheat pizza crust&#39;: 3, &#39;dried mixed herbs&#39;: 3, &#39;whole wheat pastry flour&#39;: 6, &#39;nonstick spray&#39;: 7, &#39;low-fat sour cream&#39;: 8, &#39;day old bread&#39;: 2, &#39;champagne vinegar&#39;: 9, &#39;pizza sauce&#39;: 71, &#39;red vermouth&#39;: 5, &#39;low sodium chicken stock&#39;: 6, &#39;peppercorns&#39;: 3, &#39;turkey stock&#39;: 3, &#39;cooked turkey&#39;: 6, &#39;green olives&#39;: 52, &#39;spaghettini&#39;: 23, &#39;minced onion&#39;: 41, &#39;beef broth&#39;: 62, &#39;dried mint flakes&#39;: 3, &#39;ravioli&#39;: 16, &#39;meat loaf mix&#39;: 1, &#39;rub&#39;: 2, &#39;clam juice&#39;: 26, &#39;bottled clam juice&#39;: 15, &#39;cayenne pepper&#39;: 50, &#39;pears&#39;: 19, &#39;gorgonzola&#39;: 27, &#39;duck&#39;: 2, &#39;pure vanilla extract&#39;: 22, &#39;panettone&#39;: 4, &#39;sambuca&#39;: 4, &#39;light brown sugar&#39;: 15, &#39;fat free frozen top whip&#39;: 5, &#39;frozen bread dough&#39;: 9, &#39;whole wheat breadcrumbs&#39;: 10, &#39;ground chuck&#39;: 16, &#39;bows&#39;: 3, &#39;sauce&#39;: 53, &#39;ground oregano&#39;: 4, &#39;lean ground turkey&#39;: 2, &#39;milk chocolate&#39;: 11, &#39;hazelnut butter&#39;: 4, &#39;fig jam&#39;: 2, &#39;crackers&#39;: 7, &#39;sun-dried tomatoes in oil&#39;: 28, &#39;dark chocolate&#39;: 7, &#39;vanilla ice cream&#39;: 17, &#39;chives&#39;: 31, &#39;radishes&#39;: 13, &#39;lettuce&#39;: 6, &#39;grilled chicken&#39;: 2, &#39;flatbread&#39;: 4, &#39;fresh parsley leaves&#39;: 56, &#39;lemon extract&#39;: 7, &#39;salad greens&#39;: 11, &#39;brown rice&#39;: 6, &#39;tortellini&#39;: 19, &#39;reduced fat alfredo sauce&#39;: 4, &#39;fresh asparagus&#39;: 24, &#39;round steaks&#39;: 5, &#39;chili powder&#39;: 26, &#39;ground cumin&#39;: 30, &#39;rolls&#39;: 21, &#39;prego traditional italian sauce&#39;: 4, &#39;brown hash potato&#39;: 5, &#39;nonfat milk&#39;: 7, &#39;frozen cheese ravioli&#39;: 6, &#39;mild Italian sausage&#39;: 17, &#39;sandwich rolls&#39;: 3, &#39;orange juice concentrate&#39;: 5, &#39;rotelle&#39;: 3, &#39;sweet italian sausag links, cut into&#39;: 2, &#39;idaho potatoes&#39;: 4, &#39;popcorn&#39;: 1, &#39;grated orange peel&#39;: 26, &#39;french fried onions&#39;: 1, &#39;turbinado&#39;: 7, &#39;cooked chicken breasts&#39;: 16, &#39;farro&#39;: 14, &#39;flour tortillas&#39;: 11, &#39;english cucumber&#39;: 7, &#39;minced peperoncini&#39;: 2, &#39;teleme&#39;: 2, &#39;brine cured green olives&#39;: 4, &#39;black forest ham&#39;: 1, &#39;frozen whole kernel corn&#39;: 5, &#39;sweet potatoes&#39;: 15, &#39;applewood smoked bacon&#39;: 7, &#39;panko breadcrumbs&#39;: 24, &#39;coarse kosher salt&#39;: 9, &#39;cornmeal&#39;: 72, &#39;dried sage&#39;: 28, &#39;dri leav thyme&#39;: 6, &#39;low sodium beef broth&#39;: 3, &#39;cabbage&#39;: 12, &#39;fresh shiitake mushrooms&#39;: 15, &#39;rabbit&#39;: 8, &#39;herbes de provence&#39;: 12, &#39;dressing&#39;: 7, &#39;chicken fingers&#39;: 3, &#39;reduced-fat sour cream&#39;: 13, &#39;dried fettuccine&#39;: 14, &#39;white pepper&#39;: 22, &#39;albacore tuna in water&#39;: 5, &#39;light mayonnaise&#39;: 7, &#39;turkey tenderloins&#39;: 4, &#39;almond flour&#39;: 8, &#39;raw almond&#39;: 3, &#39;fresh orange juice&#39;: 32, &#39;cranberries&#39;: 3, &#39;orange marmalade&#39;: 6, &#39;fresh lemon&#39;: 4, &#39;condensed chicken broth&#39;: 1, &#39;oil packed anchovy fillets&#39;: 3, &#39;boneless chicken skinless thigh&#39;: 20, &#39;sugar pea&#39;: 16, &#39;nonfat yogurt&#39;: 4, &#39;roast red peppers, drain&#39;: 24, &#39;french baguette&#39;: 25, &#39;ripe olives&#39;: 26, &#39;honey glazed ham&#39;: 2, &#39;chiles&#39;: 9, &#39;spring onions&#39;: 3, &#39;candied orange peel&#39;: 13, &#39;lard&#39;: 5, &#39;cinnamon&#39;: 23, &#39;semolina flour&#39;: 32, &#39;onion salt&#39;: 2, &#39;beef demi-glace&#39;: 1, &#39;veal shanks&#39;: 21, &#39;orange peel&#39;: 7, &#39;lemon peel&#39;: 11, &#39;plain yogurt&#39;: 7, &#39;Quinoa Flour&#39;: 1, &#39;spelt flour&#39;: 2, &#39;plums&#39;: 14, &#39;heirloom tomatoes&#39;: 21, &#39;fresh lime juice&#39;: 25, &#39;artichok heart marin&#39;: 33, &#39;bucatini&#39;: 16, &#39;processed cheese&#39;: 5, &#39;egg noodles, cooked and drained&#39;: 3, &#39;tapioca flour&#39;: 1, &#39;lasagna noodles, cooked and drained&#39;: 26, &#39;firm tofu&#39;: 9, &#39;sherry vinegar&#39;: 27, &#39;country style bread&#39;: 7, &#39;teardrop tomatoes&#39;: 2, &#39;ground sausage&#39;: 5, &#39;pita pockets&#39;: 1, &#39;orange juice&#39;: 19, &#39;chili pepper&#39;: 5, &#39;currant&#39;: 7, &#39;small capers, rins and drain&#39;: 2, &#39;filet&#39;: 4, &#39;lettuce leaves&#39;: 8, &#39;vodka&#39;: 27, &#39;stolichnaya&#39;: 1, &#39;romaine lettuce&#39;: 33, &#39;croutons&#39;: 18, &#39;pepperocini&#39;: 2, &#39;cherry peppers&#39;: 3, &#39;dandelion&#39;: 1, &#39;beans&#39;: 6, &#39;hot pepper sauce&#39;: 7, &#39;shredded Monterey Jack cheese&#39;: 13, &#39;cane sugar&#39;: 1, &#39;mixed nuts&#39;: 1, &#39;meatballs&#39;: 17, &#39;plain dry bread crumb&#39;: 11, &#39;spanish onion&#39;: 9, &#39;cuban peppers&#39;: 3, &#39;green tomatoes&#39;: 2, &#39;sesame seeds&#39;: 13, &#39;boneless beef chuck roast&#39;: 3, &#39;hard-boiled egg&#39;: 6, &#39;pork tenderloin&#39;: 24, &#39;bulk italian sausag&#39;: 21, &#39;beef bouillon granules&#39;: 3, &#39;prebaked pizza crusts&#39;: 13, &#39;buttermilk&#39;: 32, &#39;flaked coconut&#39;: 5, &#39;lower sodium chicken broth&#39;: 20, &#39;carnaroli rice&#39;: 13, &#39;fresh oregano leaves&#39;: 24, &#39;cavatappi&#39;: 9, &#39;cooking oil&#39;: 24, &#39;cayenne&#39;: 8, &#39;ground cloves&#39;: 30, &#39;corn syrup&#39;: 4, &#39;small pasta&#39;: 7, &#39;ground fennel&#39;: 7, &#39;low-fat buttermilk&#39;: 9, &#39;ice&#39;: 7, &#39;chicken bouillon granules&#39;: 6, &#39;poultry seasoning&#39;: 10, &#39;roast&#39;: 3, &#39;fusilli&#39;: 43, &#39;Italian herbs&#39;: 7, &#39;diced yellow onion&#39;: 1, &#39;manicotti pasta&#39;: 4, &#39;parsley flakes&#39;: 17, &#39;vinaigrette&#39;: 7, &#39;bread ciabatta&#39;: 16, &#39;kidney beans&#39;: 12, &#39;creole seasoning&#39;: 4, &#39;prepared pasta sauce&#39;: 5, &#39;small curd cottage cheese&#39;: 14, &#39;white sandwich bread&#39;: 9, &#39;mini chocolate chips&#39;: 5, &#39;lean beef&#39;: 2, &#39;breadstick&#39;: 13, &#39;pickled okra&#39;: 2, &#39;fronds&#39;: 15, &#39;thick-cut bacon&#39;: 11, &#39;boiling potatoes&#39;: 13, &#39;ditalini&#39;: 11, &#39;cranberry beans&#39;: 4, &#39;center cut bacon&#39;: 7, &#39;roasting chickens&#39;: 8, &#39;fleur de sel&#39;: 6, &#39;Margherita Pepperoni&#39;: 1, &#39;soft-shell clams&#39;: 1, &#39;liquid&#39;: 5, &#39;fresh chevre&#39;: 4, &#39;pork sausages&#39;: 14, &#39;dried minced onion&#39;: 5, &#39;msg&#39;: 2, &#39;pork stew meat&#39;: 1, &#39;beef stew meat&#39;: 3, &#39;ziti&#39;: 26, &#39;Balsamico Bianco&#39;: 2, &#39;ground mustard&#39;: 1, ...}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line"><span class="comment"># Finally, plot the 10 most used ingredients</span></span><br><span class="line">fig = pd.DataFrame(italian_ingredients, index=[<span class="number">0</span>]).transpose()[<span class="number">0</span>].sort_values(ascending=<span class="keyword">False</span>, inplace=<span class="keyword">False</span>)[:<span class="number">10</span>].plot(kind=<span class="string">'barh'</span>)</span><br><span class="line">fig.invert_yaxis()</span><br><span class="line">fig = fig.get_figure()</span><br><span class="line">fig.tight_layout()</span><br><span class="line">fig.show()</span><br></pre></td></tr></table></figure><pre><code>/opt/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:448: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.  % get_backend())</code></pre><p><img src="/2019/02/15/PredictYourCuisine/output_17_1.png" alt="png"></p><p>若想要对数据分析做更深入的了解，可以参考<a href="https://cn.udacity.com/dand" target="_blank" rel="noopener">数据分析师入门课程</a>或者<a href="https://www.udacity.com/legal/ai-programming" target="_blank" rel="noopener">基于Python语言的人工智能Nano课程</a>.</p><hr><h2 id="第三步-建立模型"><a href="#第三步-建立模型" class="headerlink" title="第三步. 建立模型"></a>第三步. 建立模型</h2><p>在项目的第三步中，你需要了解必要的工具和技巧来让你的模型进行预测。用这些工具和技巧对每一个模型的表现做精确的衡量可以极大地增强你预测的信心。</p><h3 id="3-1-单词清洗"><a href="#3-1-单词清洗" class="headerlink" title="3.1 单词清洗"></a>3.1 单词清洗</h3><p>由于菜品包含的佐料众多，同一种佐料也可能有单复数、时态等变化，为了去除这类差异，我们考虑将<strong>ingredients</strong> 进行过滤</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_clean</span><span class="params">(ingredients)</span>:</span></span><br><span class="line">    <span class="comment">#去除单词的标点符号，只保留 a..z A...Z的单词字符</span></span><br><span class="line">    ingredients= np.array(ingredients).tolist()</span><br><span class="line">    print(<span class="string">"菜品佐料：\n&#123;&#125;"</span>.format(ingredients[<span class="number">9</span>]))</span><br><span class="line">    ingredients=[[re.sub(<span class="string">'[^A-Za-z]'</span>, <span class="string">' '</span>, word) <span class="keyword">for</span> word <span class="keyword">in</span> component]<span class="keyword">for</span> component <span class="keyword">in</span> ingredients]</span><br><span class="line">    print(<span class="string">"去除标点符号之后的结果：\n&#123;&#125;"</span>.format(ingredients[<span class="number">9</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 去除单词的单复数，时态，只保留单词的词干</span></span><br><span class="line">    lemma=WordNetLemmatizer()</span><br><span class="line">    ingredients=[<span class="string">" "</span>.join([ <span class="string">" "</span>.join([lemma.lemmatize(w) <span class="keyword">for</span> w <span class="keyword">in</span> words.split(<span class="string">" "</span>)]) <span class="keyword">for</span> words <span class="keyword">in</span> component])  <span class="keyword">for</span> component <span class="keyword">in</span> ingredients]</span><br><span class="line">    print(<span class="string">"去除时态和单复数之后的结果：\n&#123;&#125;"</span>.format(ingredients[<span class="number">9</span>]))</span><br><span class="line">    <span class="keyword">return</span> ingredients</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\n处理训练集..."</span>)</span><br><span class="line">train_ingredients = text_clean(train_content[<span class="string">'ingredients'</span>])</span><br><span class="line">print(<span class="string">"\n处理测试集..."</span>)</span><br><span class="line">test_ingredients = text_clean(test_content[<span class="string">'ingredients'</span>])</span><br></pre></td></tr></table></figure><pre><code>[nltk_data] Downloading package wordnet to[nltk_data]     /Users/jindongwang/nltk_data...[nltk_data]   Unzipping corpora/wordnet.zip.处理训练集...菜品佐料：[&#39;chopped tomatoes&#39;, &#39;fresh basil&#39;, &#39;garlic&#39;, &#39;extra-virgin olive oil&#39;, &#39;kosher salt&#39;, &#39;flat leaf parsley&#39;]去除标点符号之后的结果：[&#39;chopped tomatoes&#39;, &#39;fresh basil&#39;, &#39;garlic&#39;, &#39;extra virgin olive oil&#39;, &#39;kosher salt&#39;, &#39;flat leaf parsley&#39;]去除时态和单复数之后的结果：chopped tomato fresh basil garlic extra virgin olive oil kosher salt flat leaf parsley处理测试集...菜品佐料：[&#39;eggs&#39;, &#39;cherries&#39;, &#39;dates&#39;, &#39;dark muscovado sugar&#39;, &#39;ground cinnamon&#39;, &#39;mixed spice&#39;, &#39;cake&#39;, &#39;vanilla extract&#39;, &#39;self raising flour&#39;, &#39;sultana&#39;, &#39;rum&#39;, &#39;raisins&#39;, &#39;prunes&#39;, &#39;glace cherries&#39;, &#39;butter&#39;, &#39;port&#39;]去除标点符号之后的结果：[&#39;eggs&#39;, &#39;cherries&#39;, &#39;dates&#39;, &#39;dark muscovado sugar&#39;, &#39;ground cinnamon&#39;, &#39;mixed spice&#39;, &#39;cake&#39;, &#39;vanilla extract&#39;, &#39;self raising flour&#39;, &#39;sultana&#39;, &#39;rum&#39;, &#39;raisins&#39;, &#39;prunes&#39;, &#39;glace cherries&#39;, &#39;butter&#39;, &#39;port&#39;]去除时态和单复数之后的结果：egg cherry date dark muscovado sugar ground cinnamon mixed spice cake vanilla extract self raising flour sultana rum raisin prune glace cherry butter port</code></pre><h3 id="3-2-特征提取"><a href="#3-2-特征提取" class="headerlink" title="3.2 特征提取"></a>3.2 特征提取</h3><p>在该步骤中，我们将菜品的佐料转换成数值特征向量。考虑到绝大多数菜中都包含<code>salt, water, sugar, butter</code>等，采用one-hot的方法提取的向量将不能很好的对菜系作出区分。我们将考虑按照佐料出现的次数对佐料做一定的加权，即：佐料出现次数越多，佐料的区分性就越低。我们采用的特征为TF-IDF，相关介绍内容可以参考：<a href="http://www.ruanyifeng.com/blog/2013/03/tf-idf.html" target="_blank" rel="noopener">TF-IDF与余弦相似性的应用（一）：自动提取关键词</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="comment"># 将佐料转换成特征向量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理 训练集</span></span><br><span class="line">vectorizer = TfidfVectorizer(stop_words=<span class="string">'english'</span>, ngram_range=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                analyzer=<span class="string">'word'</span>, max_df=<span class="number">.57</span>, binary=<span class="keyword">False</span>,</span><br><span class="line">                token_pattern=<span class="string">r"\w+"</span>,sublinear_tf=<span class="keyword">False</span>)</span><br><span class="line">train_tfidf = vectorizer.fit_transform(train_ingredients).todense()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 处理 测试集</span></span><br><span class="line">test_tfidf = vectorizer.transform(test_ingredients)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line">train_targets=np.array(train_content[<span class="string">'cuisine'</span>]).tolist()</span><br><span class="line">train_targets[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><pre><code>[&#39;greek&#39;, &#39;southern_us&#39;, &#39;filipino&#39;, &#39;indian&#39;, &#39;indian&#39;, &#39;jamaican&#39;, &#39;spanish&#39;, &#39;italian&#39;, &#39;mexican&#39;, &#39;italian&#39;]</code></pre><h3 id="编程练习-1"><a href="#编程练习-1" class="headerlink" title="编程练习"></a>编程练习</h3><p>这里我们为了防止前面步骤中累积的错误，导致以下步骤无法正常运行。我们在此检查处理完的实验数据是否正确，请打印<code>train_tfidf</code>和<code>train_targets</code>中前五个数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 你需要预览训练集train_tfidf,train_targets中前5条数据，试试Python的切片语法</span></span><br><span class="line">display(train_tfidf[:<span class="number">5</span>])</span><br><span class="line">display(train_targets[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>matrix([[0., 0., 0., ..., 0., 0., 0.],        [0., 0., 0., ..., 0., 0., 0.],        [0., 0., 0., ..., 0., 0., 0.],        [0., 0., 0., ..., 0., 0., 0.],        [0., 0., 0., ..., 0., 0., 0.]])[&#39;greek&#39;, &#39;southern_us&#39;, &#39;filipino&#39;, &#39;indian&#39;, &#39;indian&#39;]</code></pre><h3 id="3-3-验证集划分"><a href="#3-3-验证集划分" class="headerlink" title="3.3 验证集划分"></a>3.3 验证集划分</h3><p>为了在实验中大致估计模型的精确度我们将从原本的<code>train_ingredients</code> 划分出 <code>20%</code> 的数据用作<code>valid_ingredients</code>。</p><h3 id="编程练习：数据分割与重排"><a href="#编程练习：数据分割与重排" class="headerlink" title="编程练习：数据分割与重排"></a>编程练习：数据分割与重排</h3><p>调用<code>train_test_split</code>函数将训练集划分为新的训练集和验证集，便于之后的模型精度观测。</p><ul><li>从<code>sklearn.model_selection</code>中导入<code>train_test_split</code></li><li>将<code>train_tfidf</code>和<code>train_targets</code>作为<code>train_test_split</code>的输入变量</li><li>设置<code>test_size</code>为0.2，划分出20%的验证集，80%的数据留作新的训练集。</li><li>设置<code>random_state</code>随机种子，以确保每一次运行都可以得到相同划分的结果。（随机种子固定，生成的随机序列就是确定的）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### TODO：划分出验证集</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train , X_valid , y_train, y_valid = train_test_split(train_tfidf, train_targets, test_size=<span class="number">0.2</span>, random_state=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="3-2-建立模型"><a href="#3-2-建立模型" class="headerlink" title="3.2 建立模型"></a>3.2 建立模型</h3><p>调用 <code>sklearn</code> 中的逻辑回归模型（Logistic Regression）。</p><h3 id="编程练习：训练模型"><a href="#编程练习：训练模型" class="headerlink" title="编程练习：训练模型"></a>编程练习：训练模型</h3><ul><li>从<code>sklearn.linear_model</code>导入<code>LogisticRegression</code></li><li>从<code>sklearn.model_selection</code>导入<code>GridSearchCV</code>, 参数自动搜索，只要把参数输进去，就能给出最优的结果和参数，这个方法适合小数据集。</li><li>定义<code>parameters</code>变量：为<code>C</code>参数创造一个字典，它的值是从1至10的数组;</li><li>定义<code>classifier</code>变量: 使用导入的<code>LogisticRegression</code>创建一个分类函数;</li><li>定义<code>grid</code>变量: 使用导入的<code>GridSearchCV</code>创建一个网格搜索对象；将变量’classifier’, ‘parameters’作为参数传至这个对象构造函数中；</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line"><span class="comment">## <span class="doctag">TODO:</span> 建立逻辑回归模型</span></span><br><span class="line">parameters = &#123;<span class="string">'C'</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]&#125;</span><br><span class="line"></span><br><span class="line">classifier = LogisticRegression()</span><br><span class="line"></span><br><span class="line">grid = GridSearchCV(classifier, parameters)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line">grid = grid.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><p>模型训练结束之后，我们计算模型在验证集<code>X_valid</code>上预测结果，并计算模型的预测精度（与<code>y_valid</code>逐个比较）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score <span class="comment">## 计算模型的准确率</span></span><br><span class="line"></span><br><span class="line">valid_predict = grid.predict(X_valid)</span><br><span class="line">valid_score=accuracy_score(y_valid,valid_predict)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"验证集上的得分为：&#123;&#125;"</span>.format(valid_score))</span><br></pre></td></tr></table></figure><pre><code>验证集上的得分为：0.7912005028284098</code></pre><hr><h2 id="第四步-模型预测（可选）"><a href="#第四步-模型预测（可选）" class="headerlink" title="第四步. 模型预测（可选）"></a>第四步. 模型预测（可选）</h2><h3 id="4-1-预测测试集"><a href="#4-1-预测测试集" class="headerlink" title="4.1 预测测试集"></a>4.1 预测测试集</h3><h3 id="编程练习-2"><a href="#编程练习-2" class="headerlink" title="编程练习"></a>编程练习</h3><ul><li>将模型<code>grid</code>对测试集<code>test_tfidf</code>做预测，然后查看预测结果。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### TODO：预测测试结果</span></span><br><span class="line">predictions = grid.predict(test_tfidf)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line">print(<span class="string">"预测的测试集个数为：&#123;&#125;"</span>.format(len(predictions)))</span><br><span class="line">test_content[<span class="string">'cuisine'</span>]=predictions</span><br><span class="line">test_content.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>预测的测试集个数为：9944</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>ingredients</th>      <th>cuisine</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>18009</td>      <td>[baking powder, eggs, all-purpose flour, raisins, milk, white sugar]</td>      <td>british</td>    </tr>    <tr>      <th>1</th>      <td>28583</td>      <td>[sugar, egg yolks, corn starch, cream of tartar, bananas, vanilla wafers, milk, vanilla extract, toasted pecans, egg...</td>      <td>southern_us</td>    </tr>    <tr>      <th>2</th>      <td>41580</td>      <td>[sausage links, fennel bulb, fronds, olive oil, cuban peppers, onions]</td>      <td>italian</td>    </tr>    <tr>      <th>3</th>      <td>29752</td>      <td>[meat cuts, file powder, smoked sausage, okra, shrimp, andouille sausage, water, paprika, hot sauce, garlic cloves, ...</td>      <td>cajun_creole</td>    </tr>    <tr>      <th>4</th>      <td>35687</td>      <td>[ground black pepper, salt, sausage casings, leeks, parmigiano reggiano cheese, cornmeal, water, extra-virgin olive ...</td>      <td>italian</td>    </tr>    <tr>      <th>5</th>      <td>38527</td>      <td>[baking powder, all-purpose flour, peach slices, corn starch, heavy cream, lemon juice, unsalted butter, salt, white...</td>      <td>southern_us</td>    </tr>    <tr>      <th>6</th>      <td>19666</td>      <td>[grape juice, orange, white zinfandel]</td>      <td>french</td>    </tr>    <tr>      <th>7</th>      <td>41217</td>      <td>[ground ginger, white pepper, green onions, orange juice, sugar, Sriracha, vegetable oil, orange zest, chicken broth...</td>      <td>chinese</td>    </tr>    <tr>      <th>8</th>      <td>28753</td>      <td>[diced onions, taco seasoning mix, all-purpose flour, chopped cilantro fresh, ground cumin, ground cinnamon, vegetab...</td>      <td>mexican</td>    </tr>    <tr>      <th>9</th>      <td>22659</td>      <td>[eggs, cherries, dates, dark muscovado sugar, ground cinnamon, mixed spice, cake, vanilla extract, self raising flou...</td>      <td>british</td>    </tr>  </tbody></table></div><h3 id="4-2-提交结果"><a href="#4-2-提交结果" class="headerlink" title="4.2 提交结果"></a>4.2 提交结果</h3><p>为了更好的测试模型的效果，同时比较与其他人的差距，我们将模型的测试集上的结果提交至 <a href="https://www.kaggle.com/c/whats-cooking/submit" target="_blank" rel="noopener">kaggle What’s Cooking?</a> （需要提前注册kaggle账号）。</p><p><strong>注意</strong>：在提交作业时，请将提交排名得分截图，附在压缩包中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 加载结果格式</span></span><br><span class="line">submit_frame = pd.read_csv(<span class="string">"sample_submission.csv"</span>)</span><br><span class="line"><span class="comment">## 保存结果</span></span><br><span class="line">result = pd.merge(submit_frame, test_content, on=<span class="string">"id"</span>, how=<span class="string">'left'</span>)</span><br><span class="line">result = result.rename(index=str, columns=&#123;<span class="string">"cuisine_y"</span>: <span class="string">"cuisine"</span>&#125;)</span><br><span class="line">test_result_name = <span class="string">"tfidf_cuisine_test.csv"</span></span><br><span class="line">result[[<span class="string">'id'</span>,<span class="string">'cuisine'</span>]].to_csv(test_result_name,index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p>将生成的 <strong>tfidf_cuisine_test.csv</strong> 提交至 <a href="https://www.kaggle.com/c/whats-cooking/submit" target="_blank" rel="noopener">https://www.kaggle.com/c/whats-cooking/submit</a> 然后选择 <strong>Upload Submission File</strong>, 点击 <strong>Make submission</strong>即可。稍作等待，就可以看到右上角的评分结果（得分大致为：<code>0.78580</code> 左右）。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;机器学习工程师纳米学位&quot;&gt;&lt;a href=&quot;#机器学习工程师纳米学位&quot; class=&quot;headerlink&quot; title=&quot;机器学习工程师纳米学位&quot;&gt;&lt;/a&gt;机器学习工程师纳米学位&lt;/h1&gt;&lt;h2 id=&quot;项目-0-预测你的下一道世界料理&quot;&gt;&lt;a href=&quot;#项
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Scorecard</title>
    <link href="http://yoursite.com/2019/02/15/Scorecard/"/>
    <id>http://yoursite.com/2019/02/15/Scorecard/</id>
    <published>2019-02-15T10:40:26.000Z</published>
    <updated>2019-02-15T10:42:13.539Z</updated>
    
    <content type="html"><![CDATA[<h4 id="信用评分卡模型是一种比较成熟的预测方法，广泛应用于信用风险评估以及金融风险控制等领域，其基本原理是：将模型变量以WOE编码方式离散化之后运用-logistic-回归模型进行二分类变量的拟合及预测。"><a href="#信用评分卡模型是一种比较成熟的预测方法，广泛应用于信用风险评估以及金融风险控制等领域，其基本原理是：将模型变量以WOE编码方式离散化之后运用-logistic-回归模型进行二分类变量的拟合及预测。" class="headerlink" title="信用评分卡模型是一种比较成熟的预测方法，广泛应用于信用风险评估以及金融风险控制等领域，其基本原理是：将模型变量以WOE编码方式离散化之后运用 logistic 回归模型进行二分类变量的拟合及预测。"></a>信用评分卡模型是一种比较成熟的预测方法，广泛应用于信用风险评估以及金融风险控制等领域，其基本原理是：将模型变量以WOE编码方式离散化之后运用 logistic 回归模型进行二分类变量的拟合及预测。</h4><p>信用评分卡一般可以分为申请评分卡、行为评分卡、催收评分卡等。本文主要讲述申请评分卡模型的建模分析过程。<br>主要分以下几个步骤：</p><ol><li>目标定义</li><li>数据获取</li><li>数据预处理</li><li>模型开发</li><li>模型评估</li><li>评分系统建立</li></ol><h3 id="1-目标定义"><a href="#1-目标定义" class="headerlink" title="1. 目标定义"></a>1. 目标定义</h3><p>数据来源kaggle project: ‘give-me-some-credit-dataset’，<br>找出关键的特征变量，建立信用评分模型</p><h3 id="2-数据获取"><a href="#2-数据获取" class="headerlink" title="2. 数据获取"></a>2. 数据获取</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入必要的库包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#读取数据</span></span><br><span class="line">data=pd.read_csv(<span class="string">'cs-training.csv'</span>)</span><br><span class="line">data=data.drop(axis=<span class="number">1</span>, columns=[data.columns[<span class="number">0</span>]])</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>SeriousDlqin2yrs</th>      <th>RevolvingUtilizationOfUnsecuredLines</th>      <th>age</th>      <th>NumberOfTime30-59DaysPastDueNotWorse</th>      <th>DebtRatio</th>      <th>MonthlyIncome</th>      <th>NumberOfOpenCreditLinesAndLoans</th>      <th>NumberOfTimes90DaysLate</th>      <th>NumberRealEstateLoansOrLines</th>      <th>NumberOfTime60-89DaysPastDueNotWorse</th>      <th>NumberOfDependents</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>0.766127</td>      <td>45</td>      <td>2</td>      <td>0.802982</td>      <td>9120.0</td>      <td>13</td>      <td>0</td>      <td>6</td>      <td>0</td>      <td>2.0</td>    </tr>    <tr>      <th>1</th>      <td>0</td>      <td>0.957151</td>      <td>40</td>      <td>0</td>      <td>0.121876</td>      <td>2600.0</td>      <td>4</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>1.0</td>    </tr>    <tr>      <th>2</th>      <td>0</td>      <td>0.658180</td>      <td>38</td>      <td>1</td>      <td>0.085113</td>      <td>3042.0</td>      <td>2</td>      <td>1</td>      <td>0</td>      <td>0</td>      <td>0.0</td>    </tr>    <tr>      <th>3</th>      <td>0</td>      <td>0.233810</td>      <td>30</td>      <td>0</td>      <td>0.036050</td>      <td>3300.0</td>      <td>5</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0.0</td>    </tr>    <tr>      <th>4</th>      <td>0</td>      <td>0.907239</td>      <td>49</td>      <td>1</td>      <td>0.024926</td>      <td>63588.0</td>      <td>7</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0.0</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.info()</span><br></pre></td></tr></table></figure><pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;RangeIndex: 150000 entries, 0 to 149999Data columns (total 11 columns):SeriousDlqin2yrs                        150000 non-null int64RevolvingUtilizationOfUnsecuredLines    150000 non-null float64age                                     150000 non-null int64NumberOfTime30-59DaysPastDueNotWorse    150000 non-null int64DebtRatio                               150000 non-null float64MonthlyIncome                           120269 non-null float64NumberOfOpenCreditLinesAndLoans         150000 non-null int64NumberOfTimes90DaysLate                 150000 non-null int64NumberRealEstateLoansOrLines            150000 non-null int64NumberOfTime60-89DaysPastDueNotWorse    150000 non-null int64NumberOfDependents                      146076 non-null float64dtypes: float64(4), int64(7)memory usage: 12.6 MB</code></pre><h3 id="3-数据预处理"><a href="#3-数据预处理" class="headerlink" title="3. 数据预处理"></a>3. 数据预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#缺失值处理 </span></span><br><span class="line"><span class="comment">#随机森林法填补MonthlyIncome</span></span><br><span class="line">MI_df=data.iloc[:,<span class="number">0</span>:<span class="number">10</span>]</span><br><span class="line">MI_known=MI_df.loc[MI_df[<span class="string">'MonthlyIncome'</span>].notnull()]</span><br><span class="line">MI_unknown=MI_df.loc[MI_df[<span class="string">'MonthlyIncome'</span>].isnull()]</span><br><span class="line">X_known=MI_known.drop(<span class="string">'MonthlyIncome'</span>,axis=<span class="number">1</span>)</span><br><span class="line">y_known=MI_known[<span class="string">'MonthlyIncome'</span>]</span><br><span class="line">X_unknown=MI_unknown.drop(<span class="string">'MonthlyIncome'</span>,axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">rfr=RandomForestRegressor(random_state=<span class="number">0</span>, n_estimators=<span class="number">100</span>, max_depth=<span class="number">3</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line">rfr.fit(X_known, y_known)</span><br><span class="line"></span><br><span class="line">data.loc[MI_df[<span class="string">'MonthlyIncome'</span>].isnull(), <span class="string">'MonthlyIncome'</span>]=rfr.predict(X_unknown).round(<span class="number">0</span>)</span><br><span class="line">print(<span class="string">'Done'</span>)</span><br></pre></td></tr></table></figure><pre><code>Done</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#NumberOfDependents 缺失值较少，可以直接删除</span></span><br><span class="line">data.dropna(inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment">#去除重复值</span></span><br><span class="line">data.drop_duplicates(inplace=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#异常值处理</span></span><br><span class="line">sns.boxplot(y=<span class="string">'age'</span>, data=data)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x10e034cc0&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/20190215183554743.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data=data[((data[<span class="string">'age'</span>]&gt;<span class="number">0</span>) &amp; (data[<span class="string">'age'</span>]&lt;<span class="number">100</span>))]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#逾期次数</span></span><br><span class="line">sns.boxplot(data=data[[<span class="string">'NumberOfTime30-59DaysPastDueNotWorse'</span>,<span class="string">'NumberOfTimes90DaysLate'</span>,<span class="string">'NumberOfTime60-89DaysPastDueNotWorse'</span>]], palette=<span class="string">'Set2'</span>)</span><br><span class="line">plt.xticks(rotation=<span class="number">20</span>)</span><br></pre></td></tr></table></figure><pre><code>(array([0, 1, 2]), &lt;a list of 3 Text xticklabel objects&gt;)</code></pre><p><img src="https://img-blog.csdnimg.cn/2019021518370423.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data=data[data[<span class="string">'NumberOfTime30-59DaysPastDueNotWorse'</span>]&lt;<span class="number">80</span>]</span><br><span class="line"><span class="comment">#再次检查异常点</span></span><br><span class="line">sns.boxplot(data=data[[<span class="string">'NumberOfTime30-59DaysPastDueNotWorse'</span>,<span class="string">'NumberOfTimes90DaysLate'</span>,<span class="string">'NumberOfTime60-89DaysPastDueNotWorse'</span>]], palette=<span class="string">'Set2'</span>)</span><br><span class="line">plt.xticks(rotation=<span class="number">20</span>)</span><br></pre></td></tr></table></figure><pre><code>(array([0, 1, 2]), &lt;a list of 3 Text xticklabel objects&gt;)</code></pre><p><img src="https://img-blog.csdnimg.cn/20190215183720775.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#月收入和年龄变量的分布</span></span><br><span class="line">sns.boxplot(x=<span class="string">'MonthlyIncome'</span>, data=data)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x10f596b70&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/2019021518373271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'%.2f%% of customers monthly income under 40000.'</span> %(data.loc[data[<span class="string">'MonthlyIncome'</span>]&lt;=<span class="number">40000</span>].shape[<span class="number">0</span>]*<span class="number">100</span>/data.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="comment">#月收入绝大部分集中在40000以下，可画出对应的月收入的分布</span></span><br><span class="line">sns.distplot(data.loc[data[<span class="string">'MonthlyIncome'</span>]&lt;=<span class="number">40000</span>,<span class="string">'MonthlyIncome'</span>], bins=<span class="number">80</span>, label=<span class="string">'MonthlyIncome dist'</span>, kde=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><pre><code>99.69% of customers monthly income under 40000.&lt;matplotlib.axes._subplots.AxesSubplot at 0x107f14048&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/20190215183745111.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.distplot(data[<span class="string">'age'</span>], bins=<span class="number">30</span>, label=<span class="string">'age dist'</span>, kde=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x10e075978&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/20190215183754535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><h4 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#训练集与测试集划分</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">y = data[<span class="string">'SeriousDlqin2yrs'</span>]</span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:]</span><br><span class="line">X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义最优分箱函数</span></span><br><span class="line"><span class="keyword">import</span> scipy.stats <span class="keyword">as</span> stats</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mono_bin</span><span class="params">(Y, X, n = <span class="number">20</span>)</span>:</span></span><br><span class="line">    r = <span class="number">0</span></span><br><span class="line">    total_bad=Y.sum()</span><br><span class="line">    total_good=Y.count()-total_bad</span><br><span class="line">    <span class="comment">#print(total_bad, total_good)</span></span><br><span class="line">    <span class="keyword">while</span> np.abs(r) &lt; <span class="number">1</span>:</span><br><span class="line">        d1 = pd.DataFrame(&#123;<span class="string">"X"</span>: X, <span class="string">"Y"</span>: Y, <span class="string">"Bucket"</span>: pd.qcut(X, n)&#125;)</span><br><span class="line">        d2 = d1.groupby(<span class="string">'Bucket'</span>, as_index = <span class="keyword">True</span>)</span><br><span class="line">        r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)</span><br><span class="line">        n = n - <span class="number">1</span></span><br><span class="line">    d3 = pd.DataFrame(d2.X.min(), columns = [<span class="string">'min'</span>])</span><br><span class="line">    d3[<span class="string">'min'</span>]=d2.min().X</span><br><span class="line">    d3[<span class="string">'max'</span>] = d2.max().X</span><br><span class="line">    d3[<span class="string">'bad'</span>] = d2.sum().Y</span><br><span class="line">    d3[<span class="string">'good'</span>] = d2.count().Y-d3[<span class="string">'bad'</span>]</span><br><span class="line">    d3[<span class="string">'bad_rate'</span>] = d2.mean().Y</span><br><span class="line">    d3[<span class="string">'woe'</span>]=np.log(((d2.count().Y-d2.sum().Y)/total_good)/(d2.sum().Y/total_bad))</span><br><span class="line">    iv = ((d3[<span class="string">'good'</span>]/total_good - d3[<span class="string">'bad'</span>]/total_bad)*d3[<span class="string">'woe'</span>]).sum()</span><br><span class="line">    d4 = (d3.sort_index(by = <span class="string">'min'</span>)).reset_index(drop=<span class="keyword">True</span>)</span><br><span class="line">    woe = list(d4[<span class="string">'woe'</span>].round(<span class="number">3</span>))</span><br><span class="line">    cut=[]</span><br><span class="line">    cut.append(float(<span class="string">'-inf'</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">        qua = X.quantile(i / (n+<span class="number">1</span>))</span><br><span class="line">        cut.append(round(qua, <span class="number">4</span>))</span><br><span class="line">    cut.append(float(<span class="string">'inf'</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> d4, iv, cut, woe</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#举例，将‘age’最优分箱</span></span><br><span class="line">mono_bin(y, data[<span class="string">'age'</span>], n=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>(   min  max   bad   good  bad_rate       woe 0   21   33  1816  14471  0.111500 -0.561845 1   34   40  1664  16073  0.093815 -0.369439 2   41   45  1360  14683  0.084772 -0.258150 3   46   49  1209  13619  0.081535 -0.215683 4   50   54  1298  16516  0.072864 -0.093851 5   55   59   913  15757  0.054769  0.210948 6   60   64   690  15923  0.041534  0.501473 7   65   71   414  14194  0.028341  0.897353 8   72   99   341  14403  0.023128  1.105954, 0.2414021266070617, [-inf, 33.0, 40.0, 45.0, 49.0, 54.0, 59.0, 64.0, 71.0, inf], [-0.562, -0.369, -0.258, -0.216, -0.094, 0.211, 0.501, 0.897, 1.106])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#每个变量的个数，从而确定连续变量与分类变量</span></span><br><span class="line">var_lst=data.columns[<span class="number">1</span>:]</span><br><span class="line">var_num=&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> var_lst:</span><br><span class="line">    var_num[var]=len(data[var].unique())</span><br><span class="line">var_num</span><br></pre></td></tr></table></figure><pre><code>{&#39;RevolvingUtilizationOfUnsecuredLines&#39;: 122950, &#39;age&#39;: 79, &#39;NumberOfTime30-59DaysPastDueNotWorse&#39;: 14, &#39;DebtRatio&#39;: 114065, &#39;MonthlyIncome&#39;: 13592, &#39;NumberOfOpenCreditLinesAndLoans&#39;: 58, &#39;NumberOfTimes90DaysLate&#39;: 17, &#39;NumberRealEstateLoansOrLines&#39;: 28, &#39;NumberOfTime60-89DaysPastDueNotWorse&#39;: 11, &#39;NumberOfDependents&#39;: 13}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将四个连续变量最优分箱</span></span><br><span class="line">x1_df, x1_iv, x1_cut, x1_woe = mono_bin( y_train, X_train[<span class="string">'RevolvingUtilizationOfUnsecuredLines'</span>], n=<span class="number">10</span>)</span><br><span class="line">x2_df, x2_iv, x2_cut, x2_woe = mono_bin( y_train, X_train[<span class="string">'age'</span>], n=<span class="number">10</span>)</span><br><span class="line">x4_df, x4_iv, x4_cut, x4_woe = mono_bin( y_train, X_train[<span class="string">'DebtRatio'</span>], n=<span class="number">10</span>)</span><br><span class="line">x5_df, x5_iv, x5_cut, x5_woe = mono_bin( y_train, X_train[<span class="string">'MonthlyIncome'</span>], n=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#不能最优分箱的变量则进行手动分箱，WOE计算函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">woe_value</span><span class="params">(Y, X, cut)</span>:</span></span><br><span class="line">    total_bad=Y.sum()</span><br><span class="line">    total_good=Y.count()-total_bad</span><br><span class="line">    d1 = pd.DataFrame(&#123;<span class="string">"X"</span>: X, <span class="string">"Y"</span>: Y, <span class="string">"Bucket"</span>: cut&#125;)</span><br><span class="line">    d2 = d1.groupby(<span class="string">'Bucket'</span>, as_index = <span class="keyword">True</span>)</span><br><span class="line">    d3 = pd.DataFrame(d2.X.min(), columns = [<span class="string">'min'</span>])</span><br><span class="line">    d3[<span class="string">'min'</span>]=d2.min().X</span><br><span class="line">    d3[<span class="string">'max'</span>] = d2.max().X</span><br><span class="line">    d3[<span class="string">'bad'</span>] = d2.sum().Y</span><br><span class="line">    d3[<span class="string">'good'</span>] = d2.count().Y-d3[<span class="string">'bad'</span>]</span><br><span class="line">    d3[<span class="string">'bad_rate'</span>] = d2.mean().Y</span><br><span class="line">    d3[<span class="string">'woe'</span>]=np.log(((d2.count().Y-d2.sum().Y)/total_good)/(d2.sum().Y/total_bad))</span><br><span class="line">    iv = ((d3[<span class="string">'good'</span>]/total_good - d3[<span class="string">'bad'</span>]/total_bad)*d3[<span class="string">'woe'</span>]).sum()</span><br><span class="line">    d4 = (d3.sort_index(by = <span class="string">'min'</span>)).reset_index(drop=<span class="keyword">True</span>)</span><br><span class="line">    woe = list(d4[<span class="string">'woe'</span>].round(<span class="number">3</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> d4, iv, woe</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">x3_cut = [-np.Inf, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, np.Inf]</span><br><span class="line">x6_cut = [-np.Inf, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, np.Inf]</span><br><span class="line">x7_cut = [-np.Inf, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, np.Inf]</span><br><span class="line">x8_cut = [-np.Inf, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, np.Inf]</span><br><span class="line">x9_cut = [-np.Inf, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, np.Inf]</span><br><span class="line">x10_cut = [-np.Inf, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, np.Inf]</span><br><span class="line"></span><br><span class="line">x3_bin = pd.cut(X_train[<span class="string">'NumberOfTime30-59DaysPastDueNotWorse'</span>], bins= x3_cut)</span><br><span class="line">x6_bin = pd.cut(X_train[<span class="string">'NumberOfOpenCreditLinesAndLoans'</span>], bins= x6_cut)</span><br><span class="line">x7_bin = pd.cut(X_train[<span class="string">'NumberOfTimes90DaysLate'</span>], bins=x7_cut)</span><br><span class="line">x8_bin = pd.cut(X_train[<span class="string">'NumberRealEstateLoansOrLines'</span>], bins=x8_cut)</span><br><span class="line">x9_bin = pd.cut(X_train[<span class="string">'NumberOfTime60-89DaysPastDueNotWorse'</span>], bins=x9_cut)</span><br><span class="line">x10_bin = pd.cut(X_train[<span class="string">'NumberOfDependents'</span>], bins=x10_cut)</span><br><span class="line"></span><br><span class="line">x3_df, x3_iv, x3_woe = woe_value(y_train, X_train[<span class="string">'NumberOfTime30-59DaysPastDueNotWorse'</span>], x3_bin)</span><br><span class="line">x6_df, x6_iv, x6_woe = woe_value(y_train, X_train[<span class="string">'NumberOfOpenCreditLinesAndLoans'</span>], x6_bin)</span><br><span class="line">x7_df, x7_iv, x7_woe = woe_value(y_train, X_train[<span class="string">'NumberOfTimes90DaysLate'</span>], x7_bin)</span><br><span class="line">x8_df, x8_iv, x8_woe = woe_value(y_train, X_train[<span class="string">'NumberRealEstateLoansOrLines'</span>], x8_bin)</span><br><span class="line">x9_df, x9_iv, x9_woe = woe_value(y_train, X_train[<span class="string">'NumberOfTime60-89DaysPastDueNotWorse'</span>], x9_bin)</span><br><span class="line">x10_df, x10_iv, x10_woe = woe_value(y_train, X_train[<span class="string">'NumberOfDependents'</span>], x10_bin)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#相关性分析</span></span><br><span class="line">corr = data.corr()</span><br><span class="line">xticks = [<span class="string">'x'</span>+str(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">12</span>)]</span><br><span class="line">yticks = list(data.columns)</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">sns.heatmap(corr, cmap=<span class="string">'GnBu'</span>, annot=<span class="keyword">True</span>, ax= ax1, annot_kws=&#123;<span class="string">'size'</span>:<span class="number">6</span>, <span class="string">'color'</span>:<span class="string">'red'</span>&#125;)</span><br><span class="line">ax1.set_xticklabels(xticks, rotation=<span class="number">0</span>, fontsize=<span class="number">10</span>)</span><br><span class="line">ax1.set_yticklabels(yticks, rotation=<span class="number">0</span>, fontsize=<span class="number">10</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190215183807960.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#从相关系数热力图可以看出，自变量之间的线性相关性比较弱</span></span><br><span class="line"><span class="comment">#画出每个变量的IV值</span></span><br><span class="line">iv = [eval(<span class="string">'x'</span>+str(i)+<span class="string">'_iv'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>)]</span><br><span class="line">index=[<span class="string">'x'</span>+str(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>)]</span><br><span class="line">sns.barplot(x=index, y=iv)</span><br><span class="line">plt.ylabel(<span class="string">'IV'</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0,0.5,&#39;IV&#39;)</code></pre><p><img src="https://img-blog.csdnimg.cn/20190215183820781.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#选x1, x2, x3, x7, x9， IV&gt;0.2高预测性</span></span><br><span class="line"><span class="comment">#WOE编码</span></span><br><span class="line">x1= <span class="string">'RevolvingUtilizationOfUnsecuredLines'</span></span><br><span class="line">x2= <span class="string">'age'</span></span><br><span class="line">x3= <span class="string">'NumberOfTime30-59DaysPastDueNotWorse'</span></span><br><span class="line">x7= <span class="string">'NumberOfTimes90DaysLate'</span></span><br><span class="line">x9= <span class="string">'NumberOfTime60-89DaysPastDueNotWorse'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义WOE编码函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">woe_trans</span><span class="params">(data, var, woe, cut)</span>:</span></span><br><span class="line">    woe_name = var+<span class="string">'_woe'</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(woe)):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            data.loc[(data[var]&lt;=cut[i+<span class="number">1</span>]), woe_name] = woe[i]</span><br><span class="line">        <span class="keyword">elif</span> ((i&gt;<span class="number">0</span>) <span class="keyword">and</span> (i&lt;=(len(woe)<span class="number">-2</span>))):</span><br><span class="line">            data.loc[((data[var]&lt;=cut[i+<span class="number">1</span>]) &amp; (data[var]&gt;cut[i])), woe_name] = woe[i]</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            data.loc[(data[var]&gt;cut[i]), woe_name] = woe[i]</span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">9</span>]:</span><br><span class="line">    X_train = woe_trans(X_train, eval(<span class="string">'x'</span>+str(i)), eval(<span class="string">'x'</span>+str(i)+<span class="string">'_woe'</span>), eval(<span class="string">'x'</span>+str(i)+<span class="string">'_cut'</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#选取WOE编码之后的列作为训练数据集</span></span><br><span class="line">X_train = X_train.iloc[:, <span class="number">-5</span>:]</span><br></pre></td></tr></table></figure><h3 id="4-模型开发"><a href="#4-模型开发" class="headerlink" title="4. 模型开发"></a>4. 模型开发</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#建立逻辑回归模型</span></span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line">X1=sm.add_constant(X_train)</span><br><span class="line">logit=sm.Logit(y_train, X1)</span><br><span class="line">result=logit.fit()</span><br><span class="line">print(result.summary())</span><br></pre></td></tr></table></figure><pre><code>Optimization terminated successfully.         Current function value: 0.185840         Iterations 8                           Logit Regression Results                           ==============================================================================Dep. Variable:       SeriousDlqin2yrs   No. Observations:               101740Model:                          Logit   Df Residuals:                   101734Method:                           MLE   Df Model:                            5Date:                Thu, 20 Sep 2018   Pseudo R-squ.:                  0.2382Time:                        21:57:47   Log-Likelihood:                -18907.converged:                       True   LL-Null:                       -24820.                                        LLR p-value:                     0.000============================================================================================================                                               coef    std err          z      P&gt;|z|      [0.025      0.975]------------------------------------------------------------------------------------------------------------const                                       -2.6195      0.015   -171.961      0.000      -2.649      -2.590RevolvingUtilizationOfUnsecuredLines_woe    -0.6441      0.016    -41.387      0.000      -0.675      -0.614age_woe                                     -0.4992      0.033    -15.294      0.000      -0.563      -0.435NumberOfTime30-59DaysPastDueNotWorse_woe    -0.5455      0.016    -34.470      0.000      -0.577      -0.515NumberOfTimes90DaysLate_woe                 -0.5683      0.014    -41.833      0.000      -0.595      -0.542NumberOfTime60-89DaysPastDueNotWorse_woe    -0.4019      0.017    -23.032      0.000      -0.436      -0.368============================================================================================================</code></pre><h3 id="5-模型评估"><a href="#5-模型评估" class="headerlink" title="5. 模型评估"></a>5. 模型评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对测试集进行WOE编码</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">9</span>]:</span><br><span class="line">    X_test = woe_trans(X_test, eval(<span class="string">'x'</span>+str(i)), eval(<span class="string">'x'</span>+str(i)+<span class="string">'_woe'</span>), eval(<span class="string">'x'</span>+str(i)+<span class="string">'_cut'</span>))</span><br><span class="line">X_test = X_test.iloc[:, <span class="number">-5</span>:]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制ROC曲线，计算AUC</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">X2=sm.add_constant(X_test)</span><br><span class="line">y_pred = result.predict(X2)</span><br><span class="line"></span><br><span class="line">fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)</span><br><span class="line">auc = metrics.auc(fpr, tpr)</span><br><span class="line">plt.plot(fpr, tpr, <span class="string">'b'</span>, label=<span class="string">'AUC=%.2f'</span> %auc)</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>,<span class="number">1</span>], [<span class="number">0</span>,<span class="number">1</span>], <span class="string">'r--'</span>)</span><br><span class="line">plt.xlim([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.ylim([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.xlabel(<span class="string">'FPR'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'TPR'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190215183838146.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><h3 id="6-评分系统建立"><a href="#6-评分系统建立" class="headerlink" title="6. 评分系统建立"></a>6. 评分系统建立</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#分数计算函数</span></span><br><span class="line">PDO=<span class="number">20</span></span><br><span class="line">base=<span class="number">600</span></span><br><span class="line"><span class="comment">#all_woe是某一个体所有相关变量woe编码值构成的序列</span></span><br><span class="line"><span class="comment">#total_score = base- PDO*(all_woe.dot(coef))/np.log(2)</span></span><br><span class="line">factor= -PDO/np.log(<span class="number">2</span>)</span><br><span class="line">coef = result.params</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_score</span><span class="params">(coef, woe, factor)</span>:</span></span><br><span class="line">    scores=[round(coef*woe[i]*factor, <span class="number">0</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(woe))]</span><br><span class="line">    <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算各因子每个区间对应的分数</span></span><br><span class="line">x1_scores=get_score(coef[<span class="number">1</span>], x1_woe, factor)</span><br><span class="line">print(x1_scores)</span><br><span class="line">x2_scores=get_score(coef[<span class="number">2</span>], x2_woe, factor)</span><br><span class="line">x3_scores=get_score(coef[<span class="number">3</span>], x3_woe, factor)</span><br><span class="line">x7_scores=get_score(coef[<span class="number">4</span>], x7_woe, factor)</span><br><span class="line">x9_scores=get_score(coef[<span class="number">5</span>], x9_woe, factor)</span><br></pre></td></tr></table></figure><pre><code>[24.0, 23.0, 5.0, -20.0]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.head()</span><br><span class="line">datacopy=data</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data=datacopy</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">9</span>]:</span><br><span class="line">    data = woe_trans(data, eval(<span class="string">'x'</span>+str(i)), eval(<span class="string">'x'</span>+str(i)+<span class="string">'_woe'</span>), eval(<span class="string">'x'</span>+str(i)+<span class="string">'_cut'</span>))</span><br><span class="line">data = data.iloc[:, <span class="number">-5</span>:]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对整个data进行打分计算</span></span><br><span class="line">data_c=sm.add_constant(data)                                         </span><br><span class="line">data[<span class="string">'score'</span>]=<span class="number">500</span> + round(data_c.dot(coef)*factor, <span class="number">0</span>)  </span><br><span class="line">data.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>RevolvingUtilizationOfUnsecuredLines_woe</th>      <th>age_woe</th>      <th>NumberOfTime30-59DaysPastDueNotWorse_woe</th>      <th>NumberOfTimes90DaysLate_woe</th>      <th>NumberOfTime60-89DaysPastDueNotWorse_woe</th>      <th>score</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>-1.097</td>      <td>-0.264</td>      <td>-1.720</td>      <td>0.373</td>      <td>0.267</td>      <td>534.0</td>    </tr>    <tr>      <th>1</th>      <td>-1.097</td>      <td>-0.371</td>      <td>0.515</td>      <td>0.373</td>      <td>0.267</td>      <td>567.0</td>    </tr>    <tr>      <th>2</th>      <td>-1.097</td>      <td>-0.371</td>      <td>-0.878</td>      <td>-1.969</td>      <td>0.267</td>      <td>507.0</td>    </tr>    <tr>      <th>3</th>      <td>0.284</td>      <td>-0.564</td>      <td>0.515</td>      <td>0.373</td>      <td>0.267</td>      <td>590.0</td>    </tr>    <tr>      <th>4</th>      <td>-1.097</td>      <td>-0.184</td>      <td>-0.878</td>      <td>0.373</td>      <td>0.267</td>      <td>548.0</td>    </tr>    <tr>      <th>5</th>      <td>0.284</td>      <td>1.094</td>      <td>0.515</td>      <td>0.373</td>      <td>0.267</td>      <td>614.0</td>    </tr>    <tr>      <th>6</th>      <td>0.284</td>      <td>0.216</td>      <td>0.515</td>      <td>0.373</td>      <td>0.267</td>      <td>601.0</td>    </tr>    <tr>      <th>7</th>      <td>-1.097</td>      <td>-0.371</td>      <td>0.515</td>      <td>0.373</td>      <td>0.267</td>      <td>567.0</td>    </tr>    <tr>      <th>9</th>      <td>0.284</td>      <td>0.216</td>      <td>0.515</td>      <td>0.373</td>      <td>0.267</td>      <td>601.0</td>    </tr>    <tr>      <th>10</th>      <td>-1.097</td>      <td>-0.564</td>      <td>0.515</td>      <td>0.373</td>      <td>0.267</td>      <td>564.0</td>    </tr>  </tbody></table></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;信用评分卡模型是一种比较成熟的预测方法，广泛应用于信用风险评估以及金融风险控制等领域，其基本原理是：将模型变量以WOE编码方式离散化之后运用-logistic-回归模型进行二分类变量的拟合及预测。&quot;&gt;&lt;a href=&quot;#信用评分卡模型是一种比较成熟的预测方法，广泛
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2019/02/15/hello-world/"/>
    <id>http://yoursite.com/2019/02/15/hello-world/</id>
    <published>2019-02-15T08:09:15.599Z</published>
    <updated>2019-02-15T08:09:15.599Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>K-Means</title>
    <link href="http://yoursite.com/2019/02/15/k-Means/"/>
    <id>http://yoursite.com/2019/02/15/k-Means/</id>
    <published>2019-02-15T05:30:45.000Z</published>
    <updated>2019-02-15T08:09:15.599Z</updated>
    
    <content type="html"><![CDATA[<h1 id="电影评分的-k-均值聚类"><a href="#电影评分的-k-均值聚类" class="headerlink" title="电影评分的 k 均值聚类"></a>电影评分的 k 均值聚类</h1><p>假设你是 Netflix 的一名数据分析师，你想要根据用户对不同电影的评分研究用户在电影品位上的相似和不同之处。了解这些评分对用户电影推荐系统有帮助吗？我们来研究下这方面的数据。</p><p>我们将使用的数据来自精彩的 <a href="https://movielens.org/" target="_blank" rel="noopener">MovieLens</a> <a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">用户评分数据集</a>。我们稍后将在 notebook 中查看每个电影评分，先看看不同类型之间的评分比较情况。</p><h2 id="数据集概述"><a href="#数据集概述" class="headerlink" title="数据集概述"></a>数据集概述</h2><p>该数据集有两个文件。我们将这两个文件导入 pandas dataframe 中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> csr_matrix</span><br><span class="line"><span class="keyword">import</span> helper</span><br><span class="line"></span><br><span class="line"><span class="comment"># Import the Movies dataset</span></span><br><span class="line">movies = pd.read_csv(<span class="string">'ml-latest-small/movies.csv'</span>)</span><br><span class="line">movies.head()</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>movieId</th>      <th>title</th>      <th>genres</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>Toy Story (1995)</td>      <td>Adventure|Animation|Children|Comedy|Fantasy</td>    </tr>    <tr>      <th>1</th>      <td>2</td>      <td>Jumanji (1995)</td>      <td>Adventure|Children|Fantasy</td>    </tr>    <tr>      <th>2</th>      <td>3</td>      <td>Grumpier Old Men (1995)</td>      <td>Comedy|Romance</td>    </tr>    <tr>      <th>3</th>      <td>4</td>      <td>Waiting to Exhale (1995)</td>      <td>Comedy|Drama|Romance</td>    </tr>    <tr>      <th>4</th>      <td>5</td>      <td>Father of the Bride Part II (1995)</td>      <td>Comedy</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the ratings dataset</span></span><br><span class="line">ratings = pd.read_csv(<span class="string">'ml-latest-small/ratings.csv'</span>)</span><br><span class="line">ratings.head()</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>userId</th>      <th>movieId</th>      <th>rating</th>      <th>timestamp</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>31</td>      <td>2.5</td>      <td>1260759144</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>1029</td>      <td>3.0</td>      <td>1260759179</td>    </tr>    <tr>      <th>2</th>      <td>1</td>      <td>1061</td>      <td>3.0</td>      <td>1260759182</td>    </tr>    <tr>      <th>3</th>      <td>1</td>      <td>1129</td>      <td>2.0</td>      <td>1260759185</td>    </tr>    <tr>      <th>4</th>      <td>1</td>      <td>1172</td>      <td>4.0</td>      <td>1260759205</td>    </tr>  </tbody></table></div><p>现在我们已经知道数据集的结构，每个表格中有多少条记录。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'The dataset contains: '</span>, len(ratings), <span class="string">' ratings of '</span>, len(movies), <span class="string">' movies.'</span>)</span><br></pre></td></tr></table></figure><pre><code>The dataset contains:  100004  ratings of  9125  movies.</code></pre><h2 id="爱情片与科幻片"><a href="#爱情片与科幻片" class="headerlink" title="爱情片与科幻片"></a>爱情片与科幻片</h2><p>我们先查看一小部分用户，并看看他们喜欢什么类型的电影。我们将大部分数据预处理过程都隐藏在了辅助函数中，并重点研究聚类概念。在完成此 notebook 后，建议你快速浏览下 helper.py，了解这些辅助函数是如何实现的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Calculate the average rating of romance and scifi movies</span></span><br><span class="line"></span><br><span class="line">genre_ratings = helper.get_genre_ratings(ratings, movies, [<span class="string">'Romance'</span>, <span class="string">'Sci-Fi'</span>], [<span class="string">'avg_romance_rating'</span>, <span class="string">'avg_scifi_rating'</span>])</span><br><span class="line">genre_ratings.head()</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>avg_romance_rating</th>      <th>avg_scifi_rating</th>    </tr>    <tr>      <th>userId</th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>1</th>      <td>3.50</td>      <td>2.40</td>    </tr>    <tr>      <th>2</th>      <td>3.59</td>      <td>3.80</td>    </tr>    <tr>      <th>3</th>      <td>3.65</td>      <td>3.14</td>    </tr>    <tr>      <th>4</th>      <td>4.50</td>      <td>4.26</td>    </tr>    <tr>      <th>5</th>      <td>4.08</td>      <td>4.00</td>    </tr>  </tbody></table></div><p>函数 <code>get_genre_ratings</code> 计算了每位用户对所有爱情片和科幻片的平均评分。我们对数据集稍微进行偏倚，删除同时喜欢科幻片和爱情片的用户，使聚类能够将他们定义为更喜欢其中一种类型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">biased_dataset = helper.bias_genre_rating_dataset(genre_ratings, <span class="number">3.2</span>, <span class="number">2.5</span>)</span><br><span class="line"></span><br><span class="line">print( <span class="string">"Number of records: "</span>, len(biased_dataset))</span><br><span class="line">biased_dataset.head()</span><br></pre></td></tr></table></figure><pre><code>Number of records:  183</code></pre><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>userId</th>      <th>avg_romance_rating</th>      <th>avg_scifi_rating</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>3.50</td>      <td>2.40</td>    </tr>    <tr>      <th>1</th>      <td>3</td>      <td>3.65</td>      <td>3.14</td>    </tr>    <tr>      <th>2</th>      <td>6</td>      <td>2.90</td>      <td>2.75</td>    </tr>    <tr>      <th>3</th>      <td>7</td>      <td>2.93</td>      <td>3.36</td>    </tr>    <tr>      <th>4</th>      <td>12</td>      <td>2.89</td>      <td>2.62</td>    </tr>  </tbody></table></div><p>可以看出我们有 183 位用户，对于每位用户，我们都得出了他们对看过的爱情片和科幻片的平均评分。</p><p>我们来绘制该数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">helper.draw_scatterplot(biased_dataset[<span class="string">'avg_scifi_rating'</span>],<span class="string">'Avg scifi rating'</span>, biased_dataset[<span class="string">'avg_romance_rating'</span>], <span class="string">'Avg romance rating'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20181109164524473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>我们可以在此样本中看到明显的偏差（我们故意创建的）。如果使用 k 均值将样本分成两组，效果如何？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's turn our dataset into a list</span></span><br><span class="line">X = biased_dataset[[<span class="string">'avg_scifi_rating'</span>,<span class="string">'avg_romance_rating'</span>]].values</span><br></pre></td></tr></table></figure><ul><li>导入 <a href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" target="_blank" rel="noopener">KMeans</a></li><li>通过 n_clusters = 2 准备 KMeans</li><li>将数据集 <strong>X</strong> 传递给 KMeans 的 fit_predict 方法，并将聚类标签放入 <em>predictions</em></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Import KMeans</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Create an instance of KMeans to find two clusters</span></span><br><span class="line">kmeans_1 = KMeans(n_clusters = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> use fit_predict to cluster the dataset</span></span><br><span class="line">predictions = kmeans_1.fit_predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot</span></span><br><span class="line">helper.draw_clusters(biased_dataset, predictions)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20181109164541299.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>可以看出分组的依据主要是每个人对爱情片的评分高低。如果爱情片的平均评分超过 3 星，则属于第一组，否则属于另一组。</p><p>如果分成三组，会发生什么？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Create an instance of KMeans to find three clusters</span></span><br><span class="line">kmeans_2 = KMeans(n_clusters = <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> use fit_predict to cluster the dataset</span></span><br><span class="line">predictions_2 = kmeans_2.fit_predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot</span></span><br><span class="line">helper.draw_clusters(biased_dataset, predictions_2)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20181109164554198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>现在平均科幻片评分开始起作用了，分组情况如下所示：</p><ul><li>喜欢爱情片但是不喜欢科幻片的用户</li><li>喜欢科幻片但是不喜欢爱情片的用户</li><li>即喜欢科幻片又喜欢爱情片的用户</li></ul><p>再添加一组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Create an instance of KMeans to find four clusters</span></span><br><span class="line">kmeans_3 = KMeans(n_clusters = <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> use fit_predict to cluster the dataset</span></span><br><span class="line">predictions_3 = kmeans_3.fit_predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot</span></span><br><span class="line">helper.draw_clusters(biased_dataset, predictions_3)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20181109164605843.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>可以看出将数据集分成的聚类越多，每个聚类中用户的兴趣就相互之间越相似。</p><h2 id="选择-K"><a href="#选择-K" class="headerlink" title="选择 K"></a>选择 K</h2><p>我们可以将数据点拆分为任何数量的聚类。对于此数据集来说，正确的聚类数量是多少？</p><p>可以通过<a href="https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set" target="_blank" rel="noopener">多种</a>方式选择聚类 k。我们将研究一种简单的方式，叫做“肘部方法”。肘部方法会绘制 k 的上升值与使用该 k 值计算的总误差分布情况。</p><p>如何计算总误差？<br>一种方法是计算平方误差。假设我们要计算 k=2 时的误差。有两个聚类，每个聚类有一个“图心”点。对于数据集中的每个点，我们将其坐标减去所属聚类的图心。然后将差值结果取平方（以便消除负值），并对结果求和。这样就可以获得每个点的误差值。如果将这些误差值求和，就会获得 k=2 时所有点的总误差。</p><p>现在的一个任务是对每个 k（介于 1 到数据集中的元素数量之间）执行相同的操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Choose the range of k values to test.</span></span><br><span class="line"><span class="comment"># We added a stride of 5 to improve performance. We don't need to calculate the error for every k value</span></span><br><span class="line">possible_k_values = range(<span class="number">2</span>, len(X)+<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate error values for all k values we're interested in</span></span><br><span class="line">errors_per_k = [helper.clustering_errors(k, X) <span class="keyword">for</span> k <span class="keyword">in</span> possible_k_values]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Optional: Look at the values of K vs the silhouette score of running K-means with that value of k</span></span><br><span class="line">list(zip(possible_k_values, errors_per_k))</span><br></pre></td></tr></table></figure><pre><code>[(2, 0.35588178764728251), (7, 0.37324118163771741), (12, 0.35650856326047475), (17, 0.3741137698024623), (22, 0.37718217339438476), (27, 0.36071909992215945), (32, 0.37104279808464452), (37, 0.3649882241766923), (42, 0.36895091450195883), (47, 0.37696003940733186), (52, 0.38716548900081571), (57, 0.35079775582937778), (62, 0.34916584233387205), (67, 0.34839937724907), (72, 0.34907390154971468), (77, 0.34837739216196456), (82, 0.3309353056966266), (87, 0.34005916910201761), (92, 0.32494553685658306), (97, 0.32418331059507227), (102, 0.31329160485165003), (107, 0.29407239955320186), (112, 0.27366896911138017), (117, 0.28906341363336779), (122, 0.27342563040040624), (127, 0.25219179857975438), (132, 0.25320773897416415), (137, 0.2412264569953621), (142, 0.21855949198498667), (147, 0.19924498428850082), (152, 0.18722856283659275), (157, 0.16447514022082693), (162, 0.14697529680439808), (167, 0.12609539969216882), (172, 0.096865005870864829), (177, 0.064230120163174503), (182, 0.054644808743169397)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the each value of K vs. the silhouette score at that value</span></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">16</span>, <span class="number">6</span>))</span><br><span class="line">ax.set_xlabel(<span class="string">'K - number of clusters'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Silhouette Score (higher is better)'</span>)</span><br><span class="line">ax.plot(possible_k_values, errors_per_k)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ticks and grid</span></span><br><span class="line">xticks = np.arange(min(possible_k_values), max(possible_k_values)+<span class="number">1</span>, <span class="number">5.0</span>)</span><br><span class="line">ax.set_xticks(xticks, minor=<span class="keyword">False</span>)</span><br><span class="line">ax.set_xticks(xticks, minor=<span class="keyword">True</span>)</span><br><span class="line">ax.xaxis.grid(<span class="keyword">True</span>, which=<span class="string">'both'</span>)</span><br><span class="line">yticks = np.arange(round(min(errors_per_k), <span class="number">2</span>), max(errors_per_k), <span class="number">.05</span>)</span><br><span class="line">ax.set_yticks(yticks, minor=<span class="keyword">False</span>)</span><br><span class="line">ax.set_yticks(yticks, minor=<span class="keyword">True</span>)</span><br><span class="line">ax.yaxis.grid(<span class="keyword">True</span>, which=<span class="string">'both'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20181109164621119.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>看了该图后发现，合适的 k 值包括 7、22、27、32 等（每次运行时稍微不同）。聚类  (k) 数量超过该范围将开始导致糟糕的聚类情况（根据轮廓分数）</p><p>我会选择 k=7，因为更容易可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Create an instance of KMeans to find seven clusters</span></span><br><span class="line">kmeans_4 = KMeans(n_clusters=<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> use fit_predict to cluster the dataset</span></span><br><span class="line">predictions_4 = kmeans_4.fit_predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">helper.draw_clusters(biased_dataset, predictions_4, cmap=<span class="string">'Accent'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20181109164634463.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>注意：当你尝试绘制更大的 k 值（超过 10）时，需要确保你的绘制库没有对聚类重复使用相同的颜色。对于此图，我们需要使用 <a href="https://matplotlib.org/examples/color/colormaps_reference.html" target="_blank" rel="noopener">matplotlib colormap</a> ‘Accent’，因为其他色图要么颜色之间的对比度不强烈，要么在超过 8 个或 10 个聚类后会重复利用某些颜色。</p><h2 id="再加入动作片类型"><a href="#再加入动作片类型" class="headerlink" title="再加入动作片类型"></a>再加入动作片类型</h2><p>到目前为止，我们只查看了用户如何对爱情片和科幻片进行评分。我们再添加另一种类型，看看加入动作片类型后效果如何。</p><p>现在数据集如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">biased_dataset_3_genres = helper.get_genre_ratings(ratings, movies, </span><br><span class="line">                                                     [<span class="string">'Romance'</span>, <span class="string">'Sci-Fi'</span>, <span class="string">'Action'</span>], </span><br><span class="line">                                                     [<span class="string">'avg_romance_rating'</span>, <span class="string">'avg_scifi_rating'</span>, <span class="string">'avg_action_rating'</span>])</span><br><span class="line">biased_dataset_3_genres = helper.bias_genre_rating_dataset(biased_dataset_3_genres, <span class="number">3.2</span>, <span class="number">2.5</span>).dropna()</span><br><span class="line"></span><br><span class="line">print( <span class="string">"Number of records: "</span>, len(biased_dataset_3_genres))</span><br><span class="line">biased_dataset_3_genres.head()</span><br></pre></td></tr></table></figure><pre><code>Number of records:  183</code></pre><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>userId</th>      <th>avg_romance_rating</th>      <th>avg_scifi_rating</th>      <th>avg_action_rating</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>3.50</td>      <td>2.40</td>      <td>2.80</td>    </tr>    <tr>      <th>1</th>      <td>3</td>      <td>3.65</td>      <td>3.14</td>      <td>3.47</td>    </tr>    <tr>      <th>2</th>      <td>6</td>      <td>2.90</td>      <td>2.75</td>      <td>3.27</td>    </tr>    <tr>      <th>3</th>      <td>7</td>      <td>2.93</td>      <td>3.36</td>      <td>3.29</td>    </tr>    <tr>      <th>4</th>      <td>12</td>      <td>2.89</td>      <td>2.62</td>      <td>3.21</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_with_action = biased_dataset_3_genres[[<span class="string">'avg_scifi_rating'</span>,</span><br><span class="line">                                         <span class="string">'avg_romance_rating'</span>, </span><br><span class="line">                                         <span class="string">'avg_action_rating'</span>]].values</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Create an instance of KMeans to find seven clusters</span></span><br><span class="line">kmeans_5 = KMeans(n_clusters=<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> use fit_predict to cluster the dataset</span></span><br><span class="line">predictions_5 = kmeans_5.fit_predict(X_with_action)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">helper.draw_clusters_3d(biased_dataset_3_genres, predictions_5)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20181109164652235.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>我们依然分别用 x 轴和 y 轴表示科幻片和爱情片。并用点的大小大致表示动作片评分情况（更大的点表示平均评分超过 3 颗星，更小的点表示不超过 3 颗星 ）。</p><p>可以看出添加类型后，用户的聚类分布发生了变化。为 k 均值提供的数据越多，每组中用户之间的兴趣越相似。但是如果继续这么绘制，我们将无法可视化二维或三维之外的情形。在下个部分，我们将使用另一种图表，看看多达 50 个维度的聚类情况。</p><h2 id="电影级别的聚类"><a href="#电影级别的聚类" class="headerlink" title="电影级别的聚类"></a>电影级别的聚类</h2><p>现在我们已经知道 k 均值会如何根据用户的类型品位对用户进行聚类，我们再进一步分析，看看用户对单个影片的评分情况。为此，我们将数据集构建成 userId 与用户对每部电影的评分形式。例如，我们来看看以下数据集子集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Merge the two tables then pivot so we have Users X Movies dataframe</span></span><br><span class="line">ratings_title = pd.merge(ratings, movies[[<span class="string">'movieId'</span>, <span class="string">'title'</span>]], on=<span class="string">'movieId'</span> )</span><br><span class="line">user_movie_ratings = pd.pivot_table(ratings_title, index=<span class="string">'userId'</span>, columns= <span class="string">'title'</span>, values=<span class="string">'rating'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'dataset dimensions: '</span>, user_movie_ratings.shape, <span class="string">'\n\nSubset example:'</span>)</span><br><span class="line">user_movie_ratings.iloc[:<span class="number">6</span>, :<span class="number">10</span>]</span><br></pre></td></tr></table></figure><pre><code>dataset dimensions:  (671, 9064) Subset example:</code></pre><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th>title</th>      <th>"Great Performances" Cats (1998)</th>      <th>$9.99 (2008)</th>      <th>'Hellboy': The Seeds of Creation (2004)</th>      <th>'Neath the Arizona Skies (1934)</th>      <th>'Round Midnight (1986)</th>      <th>'Salem's Lot (2004)</th>      <th>'Til There Was You (1997)</th>      <th>'burbs, The (1989)</th>      <th>'night Mother (1986)</th>      <th>(500) Days of Summer (2009)</th>    </tr>    <tr>      <th>userId</th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>1</th>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>2</th>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>3</th>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>4</th>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>5</th>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>6</th>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>4.0</td>      <td>NaN</td>      <td>NaN</td>    </tr>  </tbody></table></div><p>NaN 值的优势表明了第一个问题。大多数用户没有看过大部分电影，并且没有为这些电影评分。这种数据集称为“稀疏”数据集，因为只有少数单元格有值。</p><p>为了解决这一问题，我们按照获得评分次数最多的电影和对电影评分次数最多的用户排序。这样可以形成更“密集”的区域，使我们能够查看数据集的顶部数据。</p><p>如果我们要选择获得评分次数最多的电影和对电影评分次数最多的用户，则如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">n_movies = <span class="number">30</span></span><br><span class="line">n_users = <span class="number">18</span></span><br><span class="line">most_rated_movies_users_selection = helper.sort_by_rating_density(user_movie_ratings, n_movies, n_users)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'dataset dimensions: '</span>, most_rated_movies_users_selection.shape)</span><br><span class="line">most_rated_movies_users_selection.head()</span><br></pre></td></tr></table></figure><pre><code>dataset dimensions:  (18, 30)</code></pre><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th>title</th>      <th>Forrest Gump (1994)</th>      <th>Pulp Fiction (1994)</th>      <th>Shawshank Redemption, The (1994)</th>      <th>Silence of the Lambs, The (1991)</th>      <th>Star Wars: Episode IV - A New Hope (1977)</th>      <th>Jurassic Park (1993)</th>      <th>Matrix, The (1999)</th>      <th>Toy Story (1995)</th>      <th>Schindler's List (1993)</th>      <th>Terminator 2: Judgment Day (1991)</th>      <th>...</th>      <th>Dances with Wolves (1990)</th>      <th>Fight Club (1999)</th>      <th>Usual Suspects, The (1995)</th>      <th>Seven (a.k.a. Se7en) (1995)</th>      <th>Lion King, The (1994)</th>      <th>Godfather, The (1972)</th>      <th>Lord of the Rings: The Fellowship of the Ring, The (2001)</th>      <th>Apollo 13 (1995)</th>      <th>True Lies (1994)</th>      <th>Twelve Monkeys (a.k.a. 12 Monkeys) (1995)</th>    </tr>  </thead>  <tbody>    <tr>      <th>29</th>      <td>5.0</td>      <td>5.0</td>      <td>5.0</td>      <td>4.0</td>      <td>4.0</td>      <td>4.0</td>      <td>3.0</td>      <td>4.0</td>      <td>5.0</td>      <td>4.0</td>      <td>...</td>      <td>5.0</td>      <td>4.0</td>      <td>5.0</td>      <td>4.0</td>      <td>3.0</td>      <td>5.0</td>      <td>3.0</td>      <td>5.0</td>      <td>4.0</td>      <td>2.0</td>    </tr>    <tr>      <th>508</th>      <td>4.0</td>      <td>5.0</td>      <td>4.0</td>      <td>4.0</td>      <td>5.0</td>      <td>3.0</td>      <td>4.5</td>      <td>3.0</td>      <td>5.0</td>      <td>2.0</td>      <td>...</td>      <td>5.0</td>      <td>4.0</td>      <td>5.0</td>      <td>4.0</td>      <td>3.5</td>      <td>5.0</td>      <td>4.5</td>      <td>3.0</td>      <td>2.0</td>      <td>4.0</td>    </tr>    <tr>      <th>14</th>      <td>1.0</td>      <td>5.0</td>      <td>2.0</td>      <td>5.0</td>      <td>5.0</td>      <td>3.0</td>      <td>5.0</td>      <td>2.0</td>      <td>4.0</td>      <td>4.0</td>      <td>...</td>      <td>3.0</td>      <td>5.0</td>      <td>5.0</td>      <td>5.0</td>      <td>4.0</td>      <td>5.0</td>      <td>5.0</td>      <td>3.0</td>      <td>4.0</td>      <td>4.0</td>    </tr>    <tr>      <th>72</th>      <td>5.0</td>      <td>5.0</td>      <td>5.0</td>      <td>4.5</td>      <td>4.5</td>      <td>4.0</td>      <td>4.5</td>      <td>5.0</td>      <td>5.0</td>      <td>3.0</td>      <td>...</td>      <td>4.5</td>      <td>5.0</td>      <td>5.0</td>      <td>5.0</td>      <td>5.0</td>      <td>5.0</td>      <td>5.0</td>      <td>3.5</td>      <td>3.0</td>      <td>5.0</td>    </tr>    <tr>      <th>653</th>      <td>4.0</td>      <td>5.0</td>      <td>5.0</td>      <td>4.5</td>      <td>5.0</td>      <td>4.5</td>      <td>5.0</td>      <td>5.0</td>      <td>5.0</td>      <td>5.0</td>      <td>...</td>      <td>4.5</td>      <td>5.0</td>      <td>5.0</td>      <td>4.5</td>      <td>5.0</td>      <td>4.5</td>      <td>5.0</td>      <td>5.0</td>      <td>4.0</td>      <td>5.0</td>    </tr>  </tbody></table><p>5 rows × 30 columns</p></div><p>这样更好分析。我们还需要指定一个可视化这些评分的良好方式，以便在查看更庞大的子集时能够直观地识别这些评分（稍后变成聚类）。</p><p>我们使用颜色代替评分数字：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helper.draw_movies_heatmap(most_rated_movies_users_selection)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2018110916471948.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>每列表示一部电影。每行表示一位用户。单元格的颜色根据图表右侧的刻度表示用户对该电影的评分情况。</p><p>注意到某些单元格是白色吗？表示相应用户没有对该电影进行评分。在现实中进行聚类时就会遇到这种问题。与一开始经过整理的示例不同，现实中的数据集经常比较稀疏，数据集中的部分单元格没有值。这样的话，直接根据电影评分对用户进行聚类不太方便，因为 k 均值通常不喜欢缺失值。</p><p>为了提高性能，我们将仅使用 1000 部电影的评分（数据集中一共有 9000 部以上）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">user_movie_ratings =  pd.pivot_table(ratings_title, index=<span class="string">'userId'</span>, columns= <span class="string">'title'</span>, values=<span class="string">'rating'</span>)</span><br><span class="line">most_rated_movies_1k = helper.get_most_rated_movies(user_movie_ratings, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure><p>为了使 sklearn 对像这样缺少值的数据集运行 k 均值聚类，我们首先需要将其转型为<a href="https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.sparse.csr_matrix.html" target="_blank" rel="noopener">稀疏 csr 矩阵</a>类型（如 SciPi 库中所定义）。</p><p>要从 pandas dataframe 转换为稀疏矩阵，我们需要先转换为 SparseDataFrame，然后使用 pandas 的 <code>to_coo()</code> 方法进行转换。</p><p>注意：只有较新版本的 pandas 具有<code>to_coo()</code>。如果你在下个单元格中遇到问题，确保你的 pandas 是最新版本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sparse_ratings = csr_matrix(pd.SparseDataFrame(most_rated_movies_1k).to_coo())</span><br></pre></td></tr></table></figure><h2 id="我们来聚类吧！"><a href="#我们来聚类吧！" class="headerlink" title="我们来聚类吧！"></a>我们来聚类吧！</h2><p>对于 k 均值，我们需要指定 k，即聚类数量。我们随意地尝试 k=20（选择 k 的更佳方式如上述肘部方法所示。但是，该方法需要一定的运行时间。):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 20 clusters</span></span><br><span class="line">predictions = KMeans(n_clusters=<span class="number">20</span>, algorithm=<span class="string">'full'</span>).fit_predict(sparse_ratings)</span><br></pre></td></tr></table></figure><p>为了可视化其中一些聚类，我们需要将每个聚类绘制成热图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">max_users = <span class="number">70</span></span><br><span class="line">max_movies = <span class="number">50</span></span><br><span class="line"></span><br><span class="line">clustered = pd.concat([most_rated_movies_1k.reset_index(), pd.DataFrame(&#123;<span class="string">'group'</span>:predictions&#125;)], axis=<span class="number">1</span>)</span><br><span class="line">helper.draw_movie_clusters(clustered, max_users, max_movies)</span><br></pre></td></tr></table></figure><pre><code>cluster # 7# of users in cluster: 276. # of users in plot: 70</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164732568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 16# of users in cluster: 64. # of users in plot: 64</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164744447.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 0# of users in cluster: 26. # of users in plot: 26</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164755257.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 2# of users in cluster: 72. # of users in plot: 70</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164807895.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 6# of users in cluster: 17. # of users in plot: 17</code></pre><p><img src="https://img-blog.csdnimg.cn/2018110916482043.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 3# of users in cluster: 37. # of users in plot: 37</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164831735.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 11# of users in cluster: 12. # of users in plot: 12</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164842611.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 18# of users in cluster: 35. # of users in plot: 35</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164853952.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 9# of users in cluster: 55. # of users in plot: 55</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164905955.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 8# of users in cluster: 27. # of users in plot: 27</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164919664.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 15# of users in cluster: 15. # of users in plot: 15</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164944204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>需要注意以下几个事项：</p><ul><li>聚类中的评分越相似，你在该聚类中就越能发现颜色相似的<strong>垂直</strong>线。</li><li>在聚类中发现了非常有趣的规律：<ul><li>某些聚类比其他聚类更稀疏，其中的用户可能比其他聚类中的用户看的电影更少，评分的电影也更少。</li><li>某些聚类主要是黄色，汇聚了非常喜欢特定类型电影的用户。其他聚类主要是绿色或海蓝色，表示这些用户都认为某些电影可以评 2-3 颗星。</li><li>注意每个聚类中的电影有何变化。图表对数据进行了过滤，仅显示评分最多的电影，然后按照平均评分排序。</li><li>能找到《指环王》在每个聚类中位于哪个位置吗？《星球大战》呢？</li></ul></li><li>很容易发现具有相似颜色的<strong>水平</strong>线，表示评分变化不大的用户。这可能是 Netflix 从基于星级的评分切换到喜欢/不喜欢评分的原因之一。四颗星评分对不同的人来说，含义不同。</li><li>我们在可视化聚类时，采取了一些措施（过滤/排序/切片）。因为这种数据集比较“稀疏”，大多数单元格没有值（因为大部分用户没有看过大部分电影）。</li></ul><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>我们选择一个聚类和一位特定的用户，看看该聚类可以使我们执行哪些实用的操作。</p><p>首先选择一个聚类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Pick a cluster ID from the clusters above</span></span><br><span class="line">cluster_number = <span class="number">11</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Let's filter to only see the region of the dataset with the most number of values </span></span><br><span class="line">n_users = <span class="number">75</span></span><br><span class="line">n_movies = <span class="number">300</span></span><br><span class="line">cluster = clustered[clustered.group == cluster_number].drop([<span class="string">'index'</span>, <span class="string">'group'</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">cluster = helper.sort_by_rating_density(cluster, n_movies, n_users)</span><br><span class="line">helper.draw_movies_heatmap(cluster, axis_labels=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20181109164957936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>聚类中的实际评分如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cluster.fillna(<span class="string">''</span>).head()</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Amadeus (1984)</th>      <th>Annie Hall (1977)</th>      <th>One Flew Over the Cuckoo's Nest (1975)</th>      <th>Fargo (1996)</th>      <th>Cool Hand Luke (1967)</th>      <th>Chinatown (1974)</th>      <th>North by Northwest (1959)</th>      <th>Citizen Kane (1941)</th>      <th>Wizard of Oz, The (1939)</th>      <th>Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)</th>      <th>...</th>      <th>Sense and Sensibility (1995)</th>      <th>Top Gun (1986)</th>      <th>Flashdance (1983)</th>      <th>Jerry Maguire (1996)</th>      <th>Superman (1978)</th>      <th>Abyss, The (1989)</th>      <th>Devil in a Blue Dress (1995)</th>      <th>Beetlejuice (1988)</th>      <th>Dial M for Murder (1954)</th>      <th>Broken Arrow (1996)</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>5.0</td>      <td>4.0</td>      <td>4.0</td>      <td>5</td>      <td>4</td>      <td>4</td>      <td>4</td>      <td>5</td>      <td></td>      <td>4</td>      <td>...</td>      <td></td>      <td>3</td>      <td></td>      <td>3</td>      <td></td>      <td>3</td>      <td></td>      <td></td>      <td>4</td>      <td>3</td>    </tr>    <tr>      <th>1</th>      <td>4.0</td>      <td>4.0</td>      <td>4.0</td>      <td>4</td>      <td>5</td>      <td>5</td>      <td>3</td>      <td>5</td>      <td>4</td>      <td>3</td>      <td>...</td>      <td></td>      <td>2</td>      <td></td>      <td>3</td>      <td>2</td>      <td></td>      <td>4</td>      <td>2</td>      <td></td>      <td>3</td>    </tr>    <tr>      <th>2</th>      <td>5.0</td>      <td>4.0</td>      <td>5.0</td>      <td>5</td>      <td>5</td>      <td>5</td>      <td>5</td>      <td>5</td>      <td>5</td>      <td>5</td>      <td>...</td>      <td></td>      <td></td>      <td>3</td>      <td>4</td>      <td></td>      <td></td>      <td>5</td>      <td></td>      <td></td>      <td>4</td>    </tr>    <tr>      <th>8</th>      <td>2.0</td>      <td>5.0</td>      <td>2.0</td>      <td>5</td>      <td>3</td>      <td>5</td>      <td>3</td>      <td>4</td>      <td>5</td>      <td>3</td>      <td>...</td>      <td>4.5</td>      <td></td>      <td>2</td>      <td>4</td>      <td>3</td>      <td></td>      <td>3</td>      <td>3</td>      <td></td>      <td></td>    </tr>    <tr>      <th>10</th>      <td>3.0</td>      <td>4.0</td>      <td>3.0</td>      <td>4</td>      <td>5</td>      <td>4</td>      <td>4</td>      <td></td>      <td>4</td>      <td>5</td>      <td>...</td>      <td>5</td>      <td></td>      <td></td>      <td></td>      <td>4</td>      <td>3</td>      <td></td>      <td>2</td>      <td></td>      <td></td>    </tr>  </tbody></table><p>5 rows × 300 columns</p></div><p>从表格中选择一个空白单元格。因为用户没有对该电影评分，所以是空白状态。能够预测她是否喜欢该电影吗？因为该用户属于似乎具有相似品位的用户聚类，我们可以计算该电影在此聚类中的平均评分，结果可以作为她是否喜欢该电影的合理预测依据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Fill in the name of the column/movie. e.g. 'Forrest Gump (1994)'</span></span><br><span class="line"><span class="comment"># Pick a movie from the table above since we're looking at a subset</span></span><br><span class="line">movie_name = <span class="string">'Forrest Gump (1994)'</span></span><br><span class="line"></span><br><span class="line">cluster[movie_name].mean()</span><br></pre></td></tr></table></figure><pre><code>3.6666666666666665</code></pre><p>这就是我们关于她会如何对该电影进行评分的预测。</p><h2 id="推荐"><a href="#推荐" class="headerlink" title="推荐"></a>推荐</h2><p>我们回顾下上一步的操作。我们使用 k 均值根据用户的评分对用户进行聚类。这样就形成了具有相似评分的用户聚类，因此通常具有相似的电影品位。基于这一点，当某个用户对某部电影没有评分时，我们对该聚类中所有其他用户的评分取平均值，该平均值就是我们猜测该用户对该电影的喜欢程度。</p><p>根据这一逻辑，如果我们计算该聚类中每部电影的平均分数，就可以判断该“品位聚类”对数据集中每部电影的喜欢程度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The average rating of 20 movies as rated by the users in the cluster</span></span><br><span class="line">cluster.mean().head(<span class="number">20</span>)</span><br></pre></td></tr></table></figure><pre><code>Amadeus (1984)                                                                    3.833333Annie Hall (1977)                                                                 4.291667One Flew Over the Cuckoo&#39;s Nest (1975)                                            4.208333Fargo (1996)                                                                      4.454545Cool Hand Luke (1967)                                                             4.636364Chinatown (1974)                                                                  4.454545North by Northwest (1959)                                                         4.409091Citizen Kane (1941)                                                               4.681818Wizard of Oz, The (1939)                                                          4.500000Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)    4.272727Butch Cassidy and the Sundance Kid (1969)                                         4.045455Star Wars: Episode V - The Empire Strikes Back (1980)                             4.090909Groundhog Day (1993)                                                              3.727273Gone with the Wind (1939)                                                         4.272727It&#39;s a Wonderful Life (1946)                                                      4.2727272001: A Space Odyssey (1968)                                                      4.272727Shawshank Redemption, The (1994)                                                  4.363636Philadelphia Story, The (1940)                                                    4.409091Bonnie and Clyde (1967)                                                           4.150000To Kill a Mockingbird (1962)                                                      4.400000dtype: float64</code></pre><p>这对我们来说变得非常实用，因为现在我们可以使用它作为推荐引擎，使用户能够发现他们可能喜欢的电影。</p><p>当用户登录我们的应用时，现在我们可以向他们显示符合他们的兴趣品位的电影。推荐方式是选择聚类中该用户尚未评分的最高评分的电影。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Pick a user ID from the dataset</span></span><br><span class="line"><span class="comment"># Look at the table above outputted by the command "cluster.fillna('').head()" </span></span><br><span class="line"><span class="comment"># and pick one of the user ids (the first column in the table)</span></span><br><span class="line">user_id = <span class="number">11</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get all this user's ratings</span></span><br><span class="line">user_2_ratings  = cluster.loc[user_id, :]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Which movies did they not rate? (We don't want to recommend movies they've already rated)</span></span><br><span class="line">user_2_unrated_movies =  user_2_ratings[user_2_ratings.isnull()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># What are the ratings of these movies the user did not rate?</span></span><br><span class="line">avg_ratings = pd.concat([user_2_unrated_movies, cluster.mean()], axis=<span class="number">1</span>, join=<span class="string">'inner'</span>).loc[:,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Let's sort by rating so the highest rated movies are presented first</span></span><br><span class="line">avg_ratings.sort_values(ascending=<span class="keyword">False</span>)[:<span class="number">20</span>]</span><br></pre></td></tr></table></figure><pre><code>Remains of the Day, The (1993)    4.666667Saving Private Ryan (1998)        4.642857African Queen, The (1951)         4.625000Lone Star (1996)                  4.600000Godfather: Part II, The (1974)    4.500000Singin&#39; in the Rain (1952)        4.500000My Cousin Vinny (1992)            4.500000Raising Arizona (1987)            4.500000Fargo (1996)                      4.454545Rain Man (1988)                   4.400000Full Metal Jacket (1987)          4.400000Sense and Sensibility (1995)      4.375000Fried Green Tomatoes (1991)       4.333333Room with a View, A (1986)        4.300000It&#39;s a Wonderful Life (1946)      4.272727Dial M for Murder (1954)          4.250000Laura (1944)                      4.250000American Graffiti (1973)          4.250000Much Ado About Nothing (1993)     4.250000Ordinary People (1980)            4.250000Name: 0, dtype: float64</code></pre><p>这些是向用户推荐的前 20 部电影！</p><h3 id="练习："><a href="#练习：" class="headerlink" title="练习："></a>练习：</h3><ul><li>如果聚类中有一部电影只有一个评分，评分是 5 颗星。该电影在该聚类中的平均评分是多少？这会对我们的简单推荐引擎有何影响？你会如何调整推荐系统，以解决这一问题？</li></ul><h2 id="关于协同过滤的更多信息"><a href="#关于协同过滤的更多信息" class="headerlink" title="关于协同过滤的更多信息"></a>关于协同过滤的更多信息</h2><ul><li>这是一个简单的推荐引擎，展示了“协同过滤”的最基本概念。有很多可以改进该引擎的启发法和方法。为了推动在这一领域的发展，Netflix 设立了 <a href="https://en.wikipedia.org/wiki/Netflix_Prize" target="_blank" rel="noopener">Netflix 奖项</a> ，他们会向对 Netflix 的推荐算法做出最大改进的算法奖励 1,000,000 美元。</li><li>在 2009 年，“BellKor’s Pragmatic Chaos”团队获得了这一奖项。<a href="http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf" target="_blank" rel="noopener">这篇论文</a>介绍了他们采用的方式，其中包含大量方法。</li><li><a href="https://thenextweb.com/media/2012/04/13/remember-netflixs-1m-algorithm-contest-well-heres-why-it-didnt-use-the-winning-entry/" target="_blank" rel="noopener">Netflix 最终并没有使用这个荣获 1,000,000 美元奖励的算法</a>，因为他们采用了流式传输的方式，并产生了比电影评分要庞大得多的数据集——用户搜索了哪些内容？用户在此会话中试看了哪些其他电影？他们是否先看了一部电影，然后切换到了其他电影？这些新的数据点可以提供比评分本身更多的线索。</li></ul><h2 id="深入研究"><a href="#深入研究" class="headerlink" title="深入研究"></a>深入研究</h2><ul><li>该 notebook 显示了用户级推荐系统。我们实际上可以使用几乎一样的代码进行商品级推荐。例如亚马逊的“购买（评价或喜欢）此商品的客户也购买了（评价了或喜欢）以下商品：” 。我们可以在应用的每个电影页面显示这种推荐。为此，我们只需将数据集转置为“电影 X 用户”形状，然后根据评分之间的联系对电影（而不是用户）进行聚类。</li><li>我们从数据集 Movie Lens 中抽取了最小的子集，只包含 100,000 个评分。如果你想深入了解电影评分数据，可以查看他们的<a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">完整数据集</a>，其中包含 2400 万个评分。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;电影评分的-k-均值聚类&quot;&gt;&lt;a href=&quot;#电影评分的-k-均值聚类&quot; class=&quot;headerlink&quot; title=&quot;电影评分的 k 均值聚类&quot;&gt;&lt;/a&gt;电影评分的 k 均值聚类&lt;/h1&gt;&lt;p&gt;假设你是 Netflix 的一名数据分析师，你想要根据用户对
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>CNN网络图像识别</title>
    <link href="http://yoursite.com/2019/02/15/My-First-Post/"/>
    <id>http://yoursite.com/2019/02/15/My-First-Post/</id>
    <published>2019-02-15T05:05:16.000Z</published>
    <updated>2019-02-15T10:19:04.357Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p><strong>本文使用keras(2.1.4)——其他版本有坑. 网络框架搭建CNN网络，对cifar10数据集进行图像识别，cifar10是一种自带label的图像数据集，数据集种类十分丰富可以很好的检验网络性能，话不多说直接进入正题 </strong></p><h2 id="第一步获取数据集"><a href="#第一步获取数据集" class="headerlink" title="第一步获取数据集"></a>第一步获取数据集</h2><p>通过keras可以直接下载cifar10数据集(数据集比较大可能需要一些时间)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="comment">#使用cifar10数据集</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> cifar10</span><br><span class="line">(x_train, y_train), (x_test, y_test) = cifar10.load_data()</span><br></pre></td></tr></table></figure></p><h2 id="展示前24张图片"><a href="#展示前24张图片" class="headerlink" title="展示前24张图片"></a>展示前24张图片</h2><p>观察数据集的部分样本别问为什么，要有一个程序员的严谨！！严谨！！严谨！！(重要的事说3遍)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">36</span>):</span><br><span class="line">    ax = fig.add_subplot(<span class="number">3</span>, <span class="number">12</span>, i + <span class="number">1</span>, xticks=[], yticks=[])</span><br><span class="line">    ax.imshow(np.squeeze(x_train[i]))</span><br></pre></td></tr></table></figure></p><h2 id="所有数据集除以255重构图像"><a href="#所有数据集除以255重构图像" class="headerlink" title="所有数据集除以255重构图像"></a>所有数据集除以255重构图像</h2><p>因为图像单个像素中最大值为255，将其除以255是将每一个像素缩放到0-1之间，类似于标准化<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_train = x_train.astype(<span class="string">'float32'</span>)/<span class="number">255</span></span><br><span class="line">x_test = x_test.astype(<span class="string">'float32'</span>)/<span class="number">255</span></span><br></pre></td></tr></table></figure></p><h2 id="将数据分解为测试集、训练集、验证集"><a href="#将数据分解为测试集、训练集、验证集" class="headerlink" title="将数据分解为测试集、训练集、验证集"></a>将数据分解为测试集、训练集、验证集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将标签转化为one-hot</span></span><br><span class="line">num_classes = len(np.unique(y_train))</span><br><span class="line">y_train = keras.utils.to_categorical(y_train, num_classes)</span><br><span class="line">y_test = keras.utils.to_categorical(y_test, num_classes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据分解为训练集和测试集</span></span><br><span class="line">(x_train, x_valid) = x_train[<span class="number">5000</span>:], x_train[:<span class="number">5000</span>]</span><br><span class="line">(y_train, y_valid) = y_train[<span class="number">5000</span>:], y_train[:<span class="number">5000</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出训练集形状</span></span><br><span class="line">print(<span class="string">'x_train shape:'</span>, x_train.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出每一个集合的长度</span></span><br><span class="line">print(x_train.shape[<span class="number">0</span>], <span class="string">'train samples'</span>)</span><br><span class="line">print(x_test.shape[<span class="number">0</span>], <span class="string">'test samples'</span>)</span><br><span class="line">print(x_valid.shape[<span class="number">0</span>], <span class="string">'validation samples'</span>)</span><br></pre></td></tr></table></figure><h2 id="开始构建卷积神经网络"><a href="#开始构建卷积神经网络" class="headerlink" title="开始构建卷积神经网络"></a>开始构建卷积神经网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv2D, MaxPooling2D, Flatten, Dense, Dropout</span><br><span class="line"><span class="comment">#初始化网络类型，选择顺序网络</span></span><br><span class="line">model = Sequential()</span><br><span class="line"><span class="comment">#添加卷积层，使用same填充，relu激活</span></span><br><span class="line">model.add(Conv2D(filters=<span class="number">16</span>, kernel_size=<span class="number">2</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, </span><br><span class="line">                        input_shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)))</span><br><span class="line"><span class="comment">#添加池化层</span></span><br><span class="line">model.add(MaxPooling2D(pool_size=<span class="number">2</span>))</span><br><span class="line">model.add(Conv2D(filters=<span class="number">32</span>, kernel_size=<span class="number">2</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(MaxPooling2D(pool_size=<span class="number">2</span>))</span><br><span class="line">model.add(Conv2D(filters=<span class="number">64</span>, kernel_size=<span class="number">2</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(MaxPooling2D(pool_size=<span class="number">2</span>))</span><br><span class="line"><span class="comment">#舍弃部分神经元，避免过拟合</span></span><br><span class="line">model.add(Dropout(<span class="number">0.3</span>))</span><br><span class="line"><span class="comment">#数据扁平化</span></span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">500</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.4</span>))</span><br><span class="line">model.add(Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"><span class="comment">#模型确认</span></span><br><span class="line">model.summary()</span><br><span class="line"><span class="comment">#模型启动，定义损失函数，优化器，评分标准</span></span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'rmsprop'</span>, </span><br><span class="line">                  metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure><h2 id="模型训练开始！"><a href="#模型训练开始！" class="headerlink" title="模型训练开始！"></a>模型训练开始！</h2><p>心疼一波没有GPU的小伙伴。。。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint   </span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">checkpointer = ModelCheckpoint(filepath=<span class="string">'model.weights.best.hdf5'</span>, verbose=<span class="number">1</span>, </span><br><span class="line">                               save_best_only=<span class="keyword">True</span>)</span><br><span class="line">hist = model.fit(x_train, y_train, batch_size=<span class="number">32</span>, epochs=<span class="number">100</span>,</span><br><span class="line">          validation_data=(x_valid, y_valid), callbacks=[checkpointer], </span><br><span class="line">          verbose=<span class="number">2</span>, shuffle=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></p><h2 id="测试集预测"><a href="#测试集预测" class="headerlink" title="测试集预测"></a>测试集预测</h2><p>终于到了激动人心的时刻，想不想知道自己搭建的模型的性能? 等着吧！<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取训练集预测</span></span><br><span class="line">y_hat = model.predict(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义文本标签--来源:(source: https://www.cs.toronto.edu/~kriz/cifar.html)</span></span><br><span class="line">cifar10_labels = [<span class="string">'airplane'</span>, <span class="string">'automobile'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>, <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>]</span><br></pre></td></tr></table></figure></p><h2 id="结果展示！！！！！"><a href="#结果展示！！！！！" class="headerlink" title="结果展示！！！！！"></a>结果展示！！！！！</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 展示样本训练结果</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">20</span>, <span class="number">8</span>))</span><br><span class="line"><span class="keyword">for</span> i, idx <span class="keyword">in</span> enumerate(np.random.choice(x_test.shape[<span class="number">0</span>], size=<span class="number">32</span>, replace=<span class="keyword">False</span>)):</span><br><span class="line">    ax = fig.add_subplot(<span class="number">4</span>, <span class="number">8</span>, i + <span class="number">1</span>, xticks=[], yticks=[])</span><br><span class="line">    ax.imshow(np.squeeze(x_test[idx]))</span><br><span class="line">    pred_idx = np.argmax(y_hat[idx])</span><br><span class="line">    true_idx = np.argmax(y_test[idx])</span><br><span class="line">    ax.set_title(<span class="string">"&#123;&#125; (&#123;&#125;)"</span>.format(cifar10_labels[pred_idx], cifar10_labels[true_idx]),</span><br><span class="line">                 color=(<span class="string">"green"</span> <span class="keyword">if</span> pred_idx == true_idx <span class="keyword">else</span> <span class="string">"red"</span>))</span><br></pre></td></tr></table></figure><h2 id="感言"><a href="#感言" class="headerlink" title="感言:"></a>感言:</h2><p>说实话图像识别的发展是一个很漫长的过程，通过结果可以发现有时候我们确实有点为难机器了，不信你们自己看看那训练结果。。 有些图片你自己都不知道是什么东西。。 还有一点 感谢各位的支持 ！拜拜👋！</p><p>还没完。 没有GPU的小伙伴可以去亚马逊申请免费的GPU服务器后 嘿嘿😁<br>最后像提供数据集的前辈们致敬！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;本文使用keras(2.1.4)——其他版本有坑. 网络框架搭建CNN网络，对cifar10数据集进行图像识别，cifar1
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
</feed>
