<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>单身程序员的小窝</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-02-15T13:21:24.819Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Jindong</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PredictYourCuisine</title>
    <link href="http://yoursite.com/2019/02/15/PredictYourCuisine/"/>
    <id>http://yoursite.com/2019/02/15/PredictYourCuisine/</id>
    <published>2019-02-15T13:06:53.000Z</published>
    <updated>2019-02-15T13:21:24.819Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习工程师纳米学位（试学班）"><a href="#机器学习工程师纳米学位（试学班）" class="headerlink" title="机器学习工程师纳米学位（试学班）"></a>机器学习工程师纳米学位（试学班）</h1><h2 id="项目-0-预测你的下一道世界料理"><a href="#项目-0-预测你的下一道世界料理" class="headerlink" title="项目 0: 预测你的下一道世界料理"></a>项目 0: 预测你的下一道世界料理</h2><p>欢迎来到机器学习的预测烹饪菜系项目！在此文件中，有些示例代码已经提供给你，但你还需要实现更多的功能来让项目成功运行。除非有明确要求，你无须修改任何已给出的代码。以<strong>编程练习</strong>开始的标题表示接下来的内容中有需要你必须实现的功能。每一部分都会有详细的指导，需要实现的部分也会在注释中以<strong>TODO</strong>标出。请仔细阅读所有的提示！</p><ul><li><strong>实验任务</strong>：给定佐料名称，预测菜品所属的菜系。</li><li><strong>实验步骤</strong>：菜品数据载入；佐料名称预处理，并预览数据集结构；载入逻辑回归模型，并训练；结果测试并提交，查看实验分数。</li></ul><blockquote><p><strong>提示：</strong>Code 和 Markdown 区域可通过 <strong>Shift + Enter</strong> 快捷键运行。此外，Markdown可以通过双击进入编辑模式。</p></blockquote><hr><h2 id="第一步-下载并导入数据"><a href="#第一步-下载并导入数据" class="headerlink" title="第一步. 下载并导入数据"></a>第一步. 下载并导入数据</h2><p>在这个项目中，你将利用<a href="https://www.yummly.com/" target="_blank" rel="noopener">Yummly</a>所提供的数据集来训练和测试一个模型，并对模型的性能和预测能力进行测试。通过该数据训练后的好的模型可以被用来对菜系进行预测。</p><p>此项目的数据集来自<a href="https://www.kaggle.com/c/whats-cooking/data" target="_blank" rel="noopener">Kaggle What’s Cooking 竞赛</a>。共 39774/9944 个训练和测试数据点，涵盖了中国菜、越南菜、法国菜等的信息。数据集包含以下特征：</p><ul><li>‘id’：24717, 数据编号</li><li>‘cuisine’：”indian”, 菜系</li><li>‘ingredients’：[“tumeric”, “vegetable stock”, …] 此菜所包含的佐料</li></ul><p>首先你需要前往此 <a href="https://www.kaggle.com/c/whats-cooking/data" target="_blank" rel="noopener">菜系数据集</a> 下载(选择 <strong>Download All</strong> )。如果不能正常下载，请参考教室中的下载教程。然后运行下面区域的代码以载入数据集，以及一些此项目所需的 Python 库。如果成功返回数据集的大小，表示数据集已载入成功。</p><h3 id="1-1-配置环境"><a href="#1-1-配置环境" class="headerlink" title="1.1 配置环境"></a>1.1 配置环境</h3><p>首先按照本目录中<code>README.md</code>文件中的第一部分内容，配置实验开发环境和所需库函数。</p><h3 id="1-2-加载数据"><a href="#1-2-加载数据" class="headerlink" title="1.2 加载数据"></a>1.2 加载数据</h3><p>其次，在下载完实验数据集后，我们将其解压至当前目录中(即：<code>MLND-cn-trial\</code>目录下面)， 然后依次输入以下代码，加载本次实验的训练集和测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line"><span class="comment"># 导入依赖库</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">train_filename=<span class="string">'all/train.json'</span></span><br><span class="line">train_content = pd.read_json(codecs.open(train_filename, mode=<span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>))</span><br><span class="line"></span><br><span class="line">test_filename = <span class="string">'all/test.json'</span></span><br><span class="line">test_content = pd.read_json(codecs.open(test_filename, mode=<span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 打印加载的数据集数量</span></span><br><span class="line">print(<span class="string">"菜名数据集一共包含 &#123;&#125; 训练数据 和 &#123;&#125; 测试样例。\n"</span>.format(len(train_content), len(test_content)))</span><br><span class="line"><span class="keyword">if</span> len(train_content)==<span class="number">39774</span> <span class="keyword">and</span> len(test_content)==<span class="number">9944</span>:</span><br><span class="line">    print(<span class="string">"数据成功载入！"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"数据载入有问题，请检查文件路径！"</span>)</span><br></pre></td></tr></table></figure><pre><code>菜名数据集一共包含 39774 训练数据 和 9944 测试样例。数据成功载入！</code></pre><h3 id="1-3-数据预览"><a href="#1-3-数据预览" class="headerlink" title="1.3 数据预览"></a>1.3 数据预览</h3><p>为了查看我们的数据集的分布和菜品总共的种类，我们打印出部分数据样例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line">pd.set_option(<span class="string">'display.max_colwidth'</span>,<span class="number">120</span>)</span><br></pre></td></tr></table></figure><h3 id="编程练习"><a href="#编程练习" class="headerlink" title="编程练习"></a>编程练习</h3><p>你需要通过<code>head()</code>函数来预览训练集<code>train_content</code>数据。（输出前5条）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### TODO：打印train_content中前5个数据样例以预览数据</span></span><br><span class="line">train_content[<span class="string">'cuisine'</span>]</span><br></pre></td></tr></table></figure><pre><code>0               greek1         southern_us2            filipino3              indian4              indian5            jamaican6             spanish7             italian8             mexican9             italian10            italian11            chinese12            italian13            mexican14            italian15             indian16            british17            italian18               thai19         vietnamese20               thai21            mexican22        southern_us23            chinese24            italian25            chinese26       cajun_creole27            italian28            chinese29            mexican             ...     39744           greek39745         spanish39746          indian39747        moroccan39748         italian39749         mexican39750         mexican39751        moroccan39752     southern_us39753         italian39754      vietnamese39755          indian39756         mexican39757           greek39758           greek39759          korean39760     southern_us39761         chinese39762          indian39763         italian39764         mexican39765          indian39766           irish39767         italian39768         mexican39769           irish39770         italian39771           irish39772         chinese39773         mexicanName: cuisine, Length: 39774, dtype: object</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line"><span class="comment">## 查看总共菜品分类</span></span><br><span class="line">categories=np.unique(train_content[<span class="string">'cuisine'</span>])</span><br><span class="line">print(<span class="string">"一共包含 &#123;&#125; 种菜品，分别是:\n&#123;&#125;"</span>.format(len(categories),categories))</span><br></pre></td></tr></table></figure><pre><code>一共包含 20 种菜品，分别是:[&apos;brazilian&apos; &apos;british&apos; &apos;cajun_creole&apos; &apos;chinese&apos; &apos;filipino&apos; &apos;french&apos; &apos;greek&apos; &apos;indian&apos; &apos;irish&apos; &apos;italian&apos; &apos;jamaican&apos; &apos;japanese&apos; &apos;korean&apos; &apos;mexican&apos; &apos;moroccan&apos; &apos;russian&apos; &apos;southern_us&apos; &apos;spanish&apos; &apos;thai&apos; &apos;vietnamese&apos;]</code></pre><hr><h2 id="第二步-分析数据"><a href="#第二步-分析数据" class="headerlink" title="第二步. 分析数据"></a>第二步. 分析数据</h2><p>在项目的第二个部分，你会对菜肴数据进行初步的观察并给出你的分析。通过对数据的探索来熟悉数据可以让你更好地理解和解释你的结果。</p><p>由于这个项目的最终目标是建立一个预测世界菜系的模型，我们需要将数据集分为<strong>特征(Features)</strong>和<strong>目标变量(Target Variables)</strong>。</p><ul><li><strong>特征</strong>: <code>&#39;ingredients&#39;</code>，给我们提供了每个菜品所包含的佐料名称。</li><li><strong>目标变量</strong>：<code>&#39;cuisine&#39;</code>，是我们希望预测的菜系分类。</li></ul><p>他们分别被存在 <code>train_ingredients</code> 和 <code>train_targets</code> 两个变量名中。</p><h3 id="编程练习：数据提取"><a href="#编程练习：数据提取" class="headerlink" title="编程练习：数据提取"></a>编程练习：数据提取</h3><ul><li>将<code>train_content</code>中的<code>ingredients</code>赋值到<code>train_integredients</code></li><li>将<code>train_content</code>中的<code>cuisine</code>赋值到<code>train_targets</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### TODO：将特征与目标变量分别赋值</span></span><br><span class="line">train_ingredients = train_content[<span class="string">'ingredients'</span>]</span><br><span class="line">train_targets = train_content[<span class="string">'cuisine'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">### <span class="doctag">TODO:</span> 打印结果，检查是否正确赋值</span></span><br><span class="line">display(train_ingredients)</span><br><span class="line">display(train_targets)</span><br></pre></td></tr></table></figure><pre><code>0        [romaine lettuce, black olives, grape tomatoes, garlic, pepper, purple onion, seasoning, garbanzo beans, feta cheese...1        [plain flour, ground pepper, salt, tomatoes, ground black pepper, thyme, eggs, green tomatoes, yellow corn meal, mil...2        [eggs, pepper, salt, mayonaise, cooking oil, green chilies, grilled chicken breasts, garlic powder, yellow onion, so...3                                                                                            [water, vegetable oil, wheat, salt]4        [black pepper, shallots, cornflour, cayenne pepper, onions, garlic paste, milk, butter, salt, lemon juice, water, ch...5        [plain flour, sugar, butter, eggs, fresh ginger root, salt, ground cinnamon, milk, vanilla extract, ground ginger, p...6        [olive oil, salt, medium shrimp, pepper, garlic, chopped cilantro, jalapeno chilies, flat leaf parsley, skirt steak,...7        [sugar, pistachio nuts, white almond bark, flour, vanilla extract, olive oil, almond extract, eggs, baking powder, d...8        [olive oil, purple onion, fresh pineapple, pork, poblano peppers, corn tortillas, cheddar cheese, ground black peppe...9                                [chopped tomatoes, fresh basil, garlic, extra-virgin olive oil, kosher salt, flat leaf parsley]10       [pimentos, sweet pepper, dried oregano, olive oil, garlic, sharp cheddar cheese, pepper, swiss cheese, provolone che...11       [low sodium soy sauce, fresh ginger, dry mustard, green beans, white pepper, sesame oil, scallions, canola oil, suga...12       [Italian parsley leaves, walnuts, hot red pepper flakes, extra-virgin olive oil, fresh lemon juice, trout fillet, ga...13       [ground cinnamon, fresh cilantro, chili powder, ground coriander, kosher salt, ground black pepper, garlic, plum tom...14       [fresh parmesan cheese, butter, all-purpose flour, fat free less sodium chicken broth, chopped fresh chives, gruyere...15       [tumeric, vegetable stock, tomatoes, garam masala, naan, red lentils, red chili peppers, onions, spinach, sweet pota...16                                                                  [greek yogurt, lemon curd, confectioners sugar, raspberries]17                                                 [italian seasoning, broiler-fryer chicken, mayonaise, zesty italian dressing]18                                                                              [sugar, hot chili, asian fish sauce, lime juice]19       [soy sauce, vegetable oil, red bell pepper, chicken broth, yellow squash, garlic chili sauce, sliced green onions, b...20       [pork loin, roasted peanuts, chopped cilantro fresh, hoisin sauce, creamy peanut butter, chopped fresh mint, thai ba...21                                          [roma tomatoes, kosher salt, purple onion, jalapeno chilies, lime, chopped cilantro]22                                                [low-fat mayonnaise, pepper, salt, baking potatoes, eggs, spicy brown mustard]23       [sesame seeds, red pepper, yellow peppers, water, extra firm tofu, broccoli, soy sauce, orange bell pepper, arrowroo...24       [marinara sauce, flat leaf parsley, olive oil, linguine, capers, crushed red pepper flakes, olives, lemon zest, garlic]25       [sugar, lo mein noodles, salt, chicken broth, light soy sauce, flank steak, beansprouts, dried black mushrooms, pepp...26                     [herbs, lemon juice, fresh tomatoes, paprika, mango, stock, chile pepper, onions, red chili peppers, oil]27       [ground black pepper, butter, sliced mushrooms, sherry, salt, grated parmesan cheese, heavy cream, spaghetti, chicke...28       [green bell pepper, egg roll wrappers, sweet and sour sauce, corn starch, molasses, vegetable oil, oil, soy sauce, s...29                                                                     [flour tortillas, cheese, breakfast sausages, large eggs]                                                                  ...                                                           39744                [extra-virgin olive oil, oregano, potatoes, garlic cloves, pepper, salt, yellow mustard, fresh lemon juice]39745                                                      [quinoa, extra-virgin olive oil, fresh thyme leaves, scallion greens]39746    [clove, bay leaves, ginger, chopped cilantro, ground turmeric, white onion, cinnamon, cardamom pods, serrano chile, ...39747                                                   [water, sugar, grated lemon zest, butter, pitted date, blanched almonds]39748    [sea salt, pizza doughs, all-purpose flour, cornmeal, extra-virgin olive oil, shredded mozzarella cheese, kosher sal...39749    [kosher salt, minced onion, tortilla chips, sugar, tomato juice, cilantro leaves, avocado, lime juice, roma tomatoes...39750    [ground black pepper, chicken breasts, salsa, cheddar cheese, pepper jack, heavy cream, red enchilada sauce, unsalte...39751    [olive oil, cayenne pepper, chopped cilantro fresh, boneless chicken skinless thigh, fine sea salt, low salt chicken...39752                                                     [self rising flour, milk, white sugar, butter, peaches in light syrup]39753    [rosemary sprigs, lemon zest, garlic cloves, ground black pepper, vegetable broth, fresh basil leaves, minced garlic...39754    [jasmine rice, bay leaves, sticky rice, rotisserie chicken, chopped cilantro, large eggs, vegetable oil, yellow onio...39755    [mint leaves, cilantro leaves, ghee, tomatoes, cinnamon, oil, basmati rice, garlic paste, salt, coconut milk, clove,...39756        [vegetable oil, cinnamon sticks, water, all-purpose flour, piloncillo, salt, orange zest, baking powder, hot water]39757                                             [red bell pepper, garlic cloves, extra-virgin olive oil, feta cheese crumbles]39758    [milk, salt, ground cayenne pepper, ground lamb, ground cinnamon, ground black pepper, pomegranate, chopped fresh mi...39759    [red chili peppers, sea salt, onions, water, chilli bean sauce, caster sugar, garlic, white vinegar, chili oil, cucu...39760                                                   [butter, large eggs, cornmeal, baking powder, boiling water, milk, salt]39761    [honey, chicken breast halves, cilantro leaves, carrots, soy sauce, Sriracha, wonton wrappers, freshly ground pepper...39762    [curry powder, salt, chicken, water, vegetable oil, basmati rice, eggs, finely chopped onion, lemon juice, pepper, m...39763    [fettuccine pasta, low-fat cream cheese, garlic, nonfat evaporated milk, grated parmesan cheese, corn starch, nonfat...39764    [chili powder, worcestershire sauce, celery, red kidney beans, lean ground beef, stewed tomatoes, dried parsley, pep...39765                                                             [coconut, unsweetened coconut milk, mint leaves, plain yogurt]39766                    [rutabaga, ham, thick-cut bacon, potatoes, fresh parsley, salt, onions, pepper, carrots, pork sausages]39767    [low-fat sour cream, grated parmesan cheese, salt, dried oregano, low-fat cottage cheese, butter, onions, olive oil,...39768    [shredded cheddar cheese, crushed cheese crackers, cheddar cheese soup, cream of chicken soup, hot sauce, diced gree...39769    [light brown sugar, granulated sugar, butter, warm water, large eggs, all-purpose flour, whole wheat flour, cooking ...39770    [KRAFT Zesty Italian Dressing, purple onion, broccoli florets, rotini, pitted black olives, Kraft Grated Parmesan Ch...39771    [eggs, citrus fruit, raisins, sourdough starter, flour, hot tea, sugar, ground nutmeg, salt, ground cinnamon, milk, ...39772    [boneless chicken skinless thigh, minced garlic, steamed white rice, baking powder, corn starch, dark soy sauce, kos...39773    [green chile, jalapeno chilies, onions, ground black pepper, salt, chopped cilantro fresh, green bell pepper, garlic...Name: ingredients, Length: 39774, dtype: object0               greek1         southern_us2            filipino3              indian4              indian5            jamaican6             spanish7             italian8             mexican9             italian10            italian11            chinese12            italian13            mexican14            italian15             indian16            british17            italian18               thai19         vietnamese20               thai21            mexican22        southern_us23            chinese24            italian25            chinese26       cajun_creole27            italian28            chinese29            mexican             ...     39744           greek39745         spanish39746          indian39747        moroccan39748         italian39749         mexican39750         mexican39751        moroccan39752     southern_us39753         italian39754      vietnamese39755          indian39756         mexican39757           greek39758           greek39759          korean39760     southern_us39761         chinese39762          indian39763         italian39764         mexican39765          indian39766           irish39767         italian39768         mexican39769           irish39770         italian39771           irish39772         chinese39773         mexicanName: cuisine, Length: 39774, dtype: object</code></pre><h3 id="编程练习：基础统计运算"><a href="#编程练习：基础统计运算" class="headerlink" title="编程练习：基础统计运算"></a>编程练习：基础统计运算</h3><p>你的第一个编程练习是计算有关菜系佐料的统计数据。我们已为你导入了 <code>numpy</code>，你需要使用这个库来执行必要的计算。这些统计数据对于分析模型的预测结果非常重要的。在下面的代码中，你要做的是：</p><ul><li>使用最频繁的佐料前10分别有哪些？</li><li>意大利菜中最常见的10个佐料有哪些？</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## <span class="doctag">TODO:</span> 统计佐料出现次数，并赋值到sum_ingredients字典中</span></span><br><span class="line">sum_ingredients = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> train_content[<span class="string">'ingredients'</span>]:</span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> i:</span><br><span class="line">        <span class="keyword">if</span> a <span class="keyword">not</span> <span class="keyword">in</span> sum_ingredients:</span><br><span class="line">            sum_ingredients[a] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> a <span class="keyword">in</span> sum_ingredients:</span><br><span class="line">            sum_ingredients[a] += <span class="number">1</span></span><br><span class="line">sum_ingredients</span><br></pre></td></tr></table></figure><pre><code>{&apos;sugar&apos;: 760, &apos;pistachio nuts&apos;: 7, &apos;white almond bark&apos;: 1, &apos;flour&apos;: 142, &apos;vanilla extract&apos;: 219, &apos;olive oil&apos;: 3111, &apos;almond extract&apos;: 56, &apos;eggs&apos;: 627, &apos;baking powder&apos;: 186, &apos;dried cranberries&apos;: 8, &apos;chopped tomatoes&apos;: 37, &apos;fresh basil&apos;: 787, &apos;garlic&apos;: 1471, &apos;extra-virgin olive oil&apos;: 1362, &apos;kosher salt&apos;: 656, &apos;flat leaf parsley&apos;: 588, &apos;pimentos&apos;: 16, &apos;sweet pepper&apos;: 7, &apos;dried oregano&apos;: 626, &apos;sharp cheddar cheese&apos;: 9, &apos;pepper&apos;: 965, &apos;swiss cheese&apos;: 7, &apos;provolone cheese&apos;: 138, &apos;canola oil&apos;: 41, &apos;mushrooms&apos;: 184, &apos;black olives&apos;: 67, &apos;sausages&apos;: 58, &apos;Italian parsley leaves&apos;: 74, &apos;walnuts&apos;: 38, &apos;hot red pepper flakes&apos;: 76, &apos;fresh lemon juice&apos;: 471, &apos;trout fillet&apos;: 3, &apos;garlic cloves&apos;: 1619, &apos;chipotle chile&apos;: 2, &apos;fine sea salt&apos;: 77, &apos;fresh parmesan cheese&apos;: 251, &apos;butter&apos;: 1030, &apos;all-purpose flour&apos;: 918, &apos;fat free less sodium chicken broth&apos;: 158, &apos;chopped fresh chives&apos;: 68, &apos;gruyere cheese&apos;: 18, &apos;ground black pepper&apos;: 1444, &apos;bacon slices&apos;: 48, &apos;gnocchi&apos;: 41, &apos;fat free milk&apos;: 42, &apos;cooking spray&apos;: 491, &apos;salt&apos;: 3454, &apos;italian seasoning&apos;: 347, &apos;broiler-fryer chicken&apos;: 1, &apos;mayonaise&apos;: 63, &apos;zesty italian dressing&apos;: 11, &apos;marinara sauce&apos;: 222, &apos;linguine&apos;: 193, &apos;capers&apos;: 306, &apos;crushed red pepper flakes&apos;: 179, &apos;olives&apos;: 29, &apos;lemon zest&apos;: 98, &apos;sliced mushrooms&apos;: 131, &apos;sherry&apos;: 13, &apos;grated parmesan cheese&apos;: 1580, &apos;heavy cream&apos;: 300, &apos;spaghetti&apos;: 296, &apos;chicken broth&apos;: 245, &apos;cooked chicken&apos;: 33, &apos;yellow corn meal&apos;: 64, &apos;boiling water&apos;: 63, &apos;sea salt&apos;: 202, &apos;onions&apos;: 1240, &apos;crushed garlic&apos;: 20, &apos;green onions&apos;: 144, &apos;white sugar&apos;: 231, &apos;dried basil&apos;: 425, &apos;diced tomatoes&apos;: 429, &apos;bread slices&apos;: 15, &apos;great northern beans&apos;: 21, &apos;shrimp&apos;: 59, &apos;sage leaves&apos;: 89, &apos;Oscar Mayer Deli Fresh Smoked Ham&apos;: 1, &apos;hoagie rolls&apos;: 8, &apos;salami&apos;: 41, &apos;giardiniera&apos;: 5, &apos;mozzarella cheese&apos;: 396, &apos;pepperoni&apos;: 48, &apos;bay leaves&apos;: 107, &apos;crushed red pepper&apos;: 418, &apos;mussels&apos;: 38, &apos;basil&apos;: 174, &apos;black pepper&apos;: 636, &apos;dry white wine&apos;: 658, &apos;tomatoes&apos;: 601, &apos;finely chopped onion&apos;: 145, &apos;lemon&apos;: 236, &apos;pesto&apos;: 113, &apos;salmon fillets&apos;: 11, &apos;white wine&apos;: 176, &apos;pizza crust&apos;: 36, &apos;plum tomatoes&apos;: 340, &apos;part-skim mozzarella cheese&apos;: 201, &apos;crushed tomatoes&apos;: 241, &apos;fresh rosemary&apos;: 292, &apos;boneless pork loin&apos;: 9, &apos;pappardelle&apos;: 11, &apos;red pepper&apos;: 49, &apos;Italian bread&apos;: 89, &apos;balsamic vinegar&apos;: 348, &apos;sausage casings&apos;: 78, &apos;honey&apos;: 126, &apos;shredded mozzarella cheese&apos;: 413, &apos;roasted red peppers&apos;: 114, &apos;penne pasta&apos;: 146, &apos;spinach&apos;: 119, &apos;asiago&apos;: 72, &apos;whole wheat pasta&apos;: 13, &apos;sweet onion&apos;: 69, &apos;grape tomatoes&apos;: 98, &apos;chestnuts&apos;: 9, &apos;granulated sugar&apos;: 82, &apos;whole milk ricotta cheese&apos;: 47, &apos;coffee ice cream&apos;: 3, &apos;large eggs&apos;: 625, &apos;mascarpone&apos;: 124, &apos;rum&apos;: 12, &apos;powdered sugar&apos;: 69, &apos;semisweet chocolate&apos;: 46, &apos;chestnut flour&apos;: 1, &apos;starchy potatoes&apos;: 2, &apos;grated nutmeg&apos;: 64, &apos;blood orange&apos;: 5, &apos;freshly ground pepper&apos;: 316, &apos;fennel bulb&apos;: 103, &apos;low salt chicken broth&apos;: 138, &apos;dijon mustard&apos;: 99, &apos;corn starch&apos;: 83, &apos;white wine vinegar&apos;: 73, &apos;tomato sauce&apos;: 357, &apos;shredded carrots&apos;: 11, &apos;english muffins, split and toasted&apos;: 2, &apos;chopped onion&apos;: 327, &apos;vegetable oil cooking spray&apos;: 73, &apos;chopped green bell pepper&apos;: 39, &apos;cheddar cheese&apos;: 18, &apos;lasagna noodles&apos;: 196, &apos;ranch dressing&apos;: 5, &apos;evaporated milk&apos;: 13, &apos;fresh parsley&apos;: 631, &apos;fresh oregano&apos;: 209, &apos;cold water&apos;: 58, &apos;chocolate morsels&apos;: 4, &apos;cream sweeten whip&apos;: 3, &apos;instant espresso granules&apos;: 4, &apos;whipping cream&apos;: 162, &apos;kahlúa&apos;: 12, &apos;chocolate covered coffee beans&apos;: 1, &apos;unflavored gelatin&apos;: 48, &apos;pound cake&apos;: 7, &apos;pinenuts&apos;: 252, &apos;zucchini&apos;: 326, &apos;baby carrots&apos;: 15, &apos;fresh basil leaves&apos;: 352, &apos;asparagus spears&apos;: 24, &apos;white onion&apos;: 48, &apos;carrots&apos;: 379, &apos;frozen peas&apos;: 67, &apos;arborio rice&apos;: 261, &apos;yellow crookneck squash&apos;: 3, &apos;fresh leav spinach&apos;: 25, &apos;cheese tortellini&apos;: 53, &apos;cherry tomatoes&apos;: 170, &apos;navy beans&apos;: 8, &apos;pecorino romano cheese&apos;: 146, &apos;fresh fava bean&apos;: 12, &apos;italian sausage&apos;: 129, &apos;large garlic cloves&apos;: 293, &apos;pasta sheets&apos;: 9, &apos;water&apos;: 1052, &apos;Turkish bay leaves&apos;: 3, &apos;dried chickpeas&apos;: 1, &apos;celery ribs&apos;: 130, &apos;semolina&apos;: 14, &apos;warm water&apos;: 182, &apos;vine ripened tomatoes&apos;: 12, &apos;bittersweet chocolate&apos;: 42, &apos;fat free yogurt&apos;: 4, &apos;skim milk&apos;: 16, &apos;angel food cake&apos;: 4, &apos;unsweetened cocoa powder&apos;: 84, &apos;instant espresso&apos;: 7, &apos;garlic salt&apos;: 61, &apos;tomato paste&apos;: 376, &apos;veal cutlets&apos;: 18, &apos;broccoli rabe&apos;: 47, &apos;whole milk&apos;: 157, &apos;parmigiano reggiano cheese&apos;: 306, &apos;dry bread crumbs&apos;: 115, &apos;fontina&apos;: 27, &apos;unsalted butter&apos;: 564, &apos;pasta sauce&apos;: 246, &apos;olive oil flavored cooking spray&apos;: 29, &apos;frozen chopped spinach&apos;: 131, &apos;large egg whites&apos;: 138, &apos;part-skim ricotta cheese&apos;: 111, &apos;manicotti shells&apos;: 18, &apos;fettucine&apos;: 136, &apos;parmesan cheese&apos;: 474, &apos;large shrimp&apos;: 77, &apos;chicken bouillon&apos;: 10, &apos;cream of tartar&apos;: 9, &apos;orange liqueur&apos;: 15, &apos;heavy whipping cream&apos;: 73, &apos;semi-sweet chocolate morsels&apos;: 21, &apos;cake flour&apos;: 24, &apos;confectioners sugar&apos;: 94, &apos;ground cinnamon&apos;: 85, &apos;ricotta cheese&apos;: 333, &apos;cream cheese&apos;: 105, &apos;dried porcini mushrooms&apos;: 81, &apos;chopped fresh thyme&apos;: 151, &apos;beef rib short&apos;: 5, &apos;dry red wine&apos;: 131, &apos;hot water&apos;: 85, &apos;fat free less sodium beef broth&apos;: 7, &apos;cremini mushrooms&apos;: 67, &apos;pitted kalamata olives&apos;: 94, &apos;cauliflower&apos;: 34, &apos;whole peeled tomatoes&apos;: 60, &apos;swiss chard&apos;: 63, &apos;banana squash&apos;: 1, &apos;vegetable broth&apos;: 81, &apos;bay leaf&apos;: 130, &apos;cannellini beans&apos;: 161, &apos;boneless skinless chicken breast halves&apos;: 182, &apos;light sour cream&apos;: 5, &apos;fava beans&apos;: 13, &apos;finely chopped fresh parsley&apos;: 33, &apos;fresh tarragon&apos;: 33, &apos;grated lemon zest&apos;: 142, &apos;chopped almonds&apos;: 12, &apos;broccoli&apos;: 64, &apos;potatoes&apos;: 59, &apos;self rising flour&apos;: 1, &apos;eggplant&apos;: 204, &apos;chicken&apos;: 84, &apos;pitas&apos;: 5, &apos;goat cheese&apos;: 92, &apos;reduced fat milk&apos;: 29, &apos;smoked trout&apos;: 1, &apos;fresh dill&apos;: 16, &apos;asparagus&apos;: 123, &apos;oil&apos;: 92, &apos;ricotta salata&apos;: 22, &apos;pecorino cheese&apos;: 52, &apos;grana padano&apos;: 7, &apos;pasta&apos;: 255, &apos;cracked black pepper&apos;: 117, &apos;green bell pepper&apos;: 163, &apos;vidalia onion&apos;: 19, &apos;smoked gouda&apos;: 14, &apos;baby spinach leaves&apos;: 39, &apos;shredded sharp cheddar cheese&apos;: 17, &apos;small red potato&apos;: 13, &apos;Bertolli® Classico Olive Oil&apos;: 16, &apos;bacon, crisp-cooked and crumbled&apos;: 2, &apos;bertolli vineyard premium collect marinara with burgundi wine sauc&apos;: 5, &apos;bread crumb fresh&apos;: 74, &apos;(    oz.) tomato sauce&apos;: 7, &apos;ground veal&apos;: 35, &apos;italian seasoning mix&apos;: 1, &apos;beef&apos;: 22, &apos;fat skimmed chicken broth&apos;: 28, &apos;solid pack pumpkin&apos;: 4, &apos;ground nutmeg&apos;: 127, &apos;white rice&apos;: 11, &apos;ground pepper&apos;: 77, &apos;fresh thyme leaves&apos;: 53, &apos;pistachios&apos;: 10, &apos;dried cherry&apos;: 6, &apos;prosciutto&apos;: 256, &apos;romano cheese&apos;: 89, &apos;parsley leaves&apos;: 23, &apos;shallots&apos;: 316, &apos;fresh raspberries&apos;: 14, &apos;dry vermouth&apos;: 18, &apos;canned low sodium chicken broth&apos;: 46, &apos;chicken livers&apos;: 14, &apos;raisins&apos;: 39, &apos;seasoned bread crumbs&apos;: 71, &apos;minced garlic&apos;: 334, &apos;half &amp; half&apos;: 89, &apos;ground beef&apos;: 237, &apos;jack cheese&apos;: 9, &apos;vegetables&apos;: 29, &apos;lemon juice&apos;: 216, &apos;ricotta&apos;: 95, &apos;yellow onion&apos;: 153, &apos;reduced sodium chicken broth&apos;: 36, &apos;chopped fresh mint&apos;: 54, &apos;purple onion&apos;: 350, &apos;low sodium chicken broth&apos;: 78, &apos;polenta&apos;: 132, &apos;bell pepper&apos;: 38, &apos;olive oil cooking spray&apos;: 13, &apos;bacon&apos;: 124, &apos;egg yolks&apos;: 82, &apos;duck breast halves&apos;: 1, &apos;fennel seeds&apos;: 114, &apos;roma tomatoes&apos;: 44, &apos;pesto sauce&apos;: 26, &apos;arugula&apos;: 125, &apos;summer squash&apos;: 9, &apos;red wine vinegar&apos;: 227, &apos;ciabatta&apos;: 21, &apos;juice&apos;: 72, &apos;fresh herbs&apos;: 14, &apos;grated lemon peel&apos;: 89, &apos;dough&apos;: 46, &apos;coarse sea salt&apos;: 17, &apos;rosemary leaves&apos;: 10, &apos;red pepper flakes&apos;: 206, &apos;brie cheese&apos;: 14, &apos;cheese ravioli&apos;: 18, &apos;Italian seasoned breadcrumbs&apos;: 23, &apos;cheese&apos;: 127, &apos;parsley&apos;: 95, &apos;fat-free cottage cheese&apos;: 11, &apos;oven-ready lasagna noodles&apos;: 43, &apos;margarine&apos;: 38, &apos;radicchio&apos;: 40, &apos;garbanzo beans&apos;: 18, &apos;orzo pasta&apos;: 16, &apos;rubbed sage&apos;: 17, &apos;dried rosemary&apos;: 96, &apos;canned beef broth&apos;: 4, &apos;kale leaves&apos;: 7, &apos;chicken noodle soup&apos;: 1, &apos;italian style rolls&apos;: 1, &apos;genoa salami&apos;: 22, &apos;oregano&apos;: 114, &apos;boiled ham&apos;: 1, &apos;capicola&apos;: 4, &apos;iceberg lettuce&apos;: 6, &apos;jalapeno chilies&apos;: 20, &apos;diced celery&apos;: 15, &apos;italian salad dressing mix&apos;: 7, &apos;chopped cilantro fresh&apos;: 25, &apos;cider vinegar&apos;: 14, &apos;red bell pepper&apos;: 354, &apos;sliced green onions&apos;: 40, &apos;barbecue sauce&apos;: 9, &apos;prepared pizza crust&apos;: 2, &apos;boneless skinless chicken breasts&apos;: 132, &apos;Sargento® Traditional Cut Shredded Mozzarella Cheese&apos;: 1, &apos;vegetable oil&apos;: 253, &apos;basil pesto sauce&apos;: 33, &apos;aioli&apos;: 3, &apos;cooked shrimp&apos;: 11, &apos;mozzarella balls&apos;: 4, &apos;sun-dried tomatoes&apos;: 118, &apos;milk&apos;: 277, &apos;condensed cream of mushroom soup&apos;: 16, &apos;fettuccine pasta&apos;: 24, &apos;frozen spinach&apos;: 24, &apos;lasagne&apos;: 6, &apos;passata&apos;: 3, &apos;fat free cream cheese&apos;: 8, &apos;french bread&apos;: 54, &apos;non-fat sour cream&apos;: 14, &apos;reduced fat swiss cheese&apos;: 3, &apos;fat-free mayonnaise&apos;: 8, &apos;roasted garlic&apos;: 7, &apos;seasoning&apos;: 12, &apos;kale&apos;: 58, &apos;sundried tomato paste&apos;: 4, &apos;dried thyme&apos;: 145, &apos;porcini&apos;: 7, &apos;fresh thyme&apos;: 62, &apos;sourdough loaf&apos;: 4, &apos;crust&apos;: 3, &apos;duck fat&apos;: 1, &apos;squabs&apos;: 1, &apos;confit&apos;: 2, &apos;aged gouda&apos;: 1, &apos;soppressata&apos;: 16, &apos;artichokes&apos;: 44, &apos;anchovy fillets&apos;: 124, &apos;lemon slices&apos;: 12, &apos;chicken cutlets&apos;: 23, &apos;saffron threads&apos;: 28, &apos;orzo&apos;: 51, &apos;veal chops&apos;: 4, &apos;oil cured olives&apos;: 3, &apos;pasta rotel&apos;: 2, &apos;pasta water&apos;: 10, &apos;cooking wine&apos;: 3, &apos;crusty bread&apos;: 10, &apos;fresh tomatoes&apos;: 32, &apos;uncooked rigatoni&apos;: 6, &apos;grating cheese&apos;: 6, &apos;vegetable stock&apos;: 29, &apos;freshly grated parmesan&apos;: 87, &apos;florets&apos;: 15, &apos;sliced almonds&apos;: 37, &apos;thyme sprigs&apos;: 45, &apos;egg whites&apos;: 95, &apos;greek yogurt&apos;: 6, &apos;penne&apos;: 99, &apos;fish fillets&apos;: 10, &apos;russet potatoes&apos;: 40, &apos;crumbled gorgonzola&apos;: 29, &apos;fontina cheese&apos;: 84, &apos;spinach leaves&apos;: 25, &apos;orange&apos;: 38, &apos;lamb shanks&apos;: 11, &apos;clove&apos;: 19, &apos;rosemary sprigs&apos;: 50, &apos;italian eggplant&apos;: 7, &apos;Sicilian olives&apos;: 7, &apos;mint sprigs&apos;: 28, &apos;peasant bread&apos;: 7, &apos;garlic powder&apos;: 252, &apos;Kraft Grated Parmesan Cheese&apos;: 10, &apos;peas&apos;: 42, &apos;Oscar Mayer Bacon&apos;: 1, &apos;Philadelphia Cream Cheese&apos;: 9, &apos;garnish&apos;: 3, &apos;dried navy beans&apos;: 5, &apos;celery&apos;: 155, &apos;diced onions&apos;: 42, &apos;wheat berries&apos;: 4, &apos;parsley sprigs&apos;: 38, &apos;thyme&apos;: 51, &apos;polenta prepar&apos;: 5, &apos;chicken breast halves&apos;: 35, &apos;dry sherry&apos;: 22, &apos;cocoa powder&apos;: 9, &apos;dried parsley&apos;: 90, &apos;chopped garlic&apos;: 69, &apos;baking soda&apos;: 68, &apos;coarse salt&apos;: 128, &apos;rigatoni&apos;: 60, &apos;nutmeg&apos;: 40, &apos;artichoke hearts&apos;: 99, &apos;pancetta&apos;: 153, &apos;parsnips&apos;: 7, &apos;whole wheat fettuccine&apos;: 4, &apos;ground sirloin&apos;: 20, &apos;red wine&apos;: 99, &apos;fresh marjoram&apos;: 29, &apos;castellane&apos;: 2, &apos;hot Italian sausages&apos;: 39, &apos;jumbo pasta shells&apos;: 37, &apos;fresh spinach&apos;: 87, &apos;chicken breasts&apos;: 93, &apos;gluten free blend&apos;: 1, &apos;paprika&apos;: 76, &apos;salt and ground black pepper&apos;: 66, &apos;bone in skinless chicken thigh&apos;: 1, &apos;figs&apos;: 10, &apos;egg substitute&apos;: 26, &apos;hot sauce&apos;: 20, &apos;ham&apos;: 42, &apos;balsamic vinaigrette&apos;: 6, &apos;pitted olives&apos;: 15, &apos;fresh chives&apos;: 24, &apos;white mushrooms&apos;: 20, &apos;haricots verts&apos;: 4, &apos;fresh peas&apos;: 16, &apos;bow-tie pasta&apos;: 86, &apos;asparagus tips&apos;: 3, &apos;green beans&apos;: 70, &apos;low-fat cottage cheese&apos;: 14, &apos;whole wheat lasagna noodles&apos;: 8, &apos;shredded parmesan cheese&apos;: 38, &apos;apricots&apos;: 5, &apos;grappa&apos;: 9, &apos;all purpose unbleached flour&apos;: 68, &apos;dry yeast&apos;: 71, &apos;worcestershire sauce&apos;: 54, &apos;amaretti&apos;: 6, &apos;frozen strawberries&apos;: 1, &apos;strawberries&apos;: 48, &apos;cooked ham&apos;: 9, &apos;Alfredo sauce&apos;: 43, &apos;chopped fresh sage&apos;: 91, &apos;cheese slices&apos;: 11, &apos;bread&apos;: 51, &apos;morel&apos;: 2, &apos;leeks&apos;: 106, &apos;1% low-fat cottage cheese&apos;: 10, &apos;feta cheese crumbles&apos;: 68, &apos;pork belly&apos;: 2, &apos;fresh sage&apos;: 29, &apos;pork loin&apos;: 3, &apos;sliced black olives&apos;: 49, &apos;medium shrimp&apos;: 70, &apos;red chili peppers&apos;: 22, &apos;parmigiano-reggiano cheese&apos;: 53, &apos;fresh mint&apos;: 53, &apos;baguette&apos;: 119, &apos;chicken legs&apos;: 7, &apos;baby spinach&apos;: 117, &apos;white beans&apos;: 38, &apos;ground pork&apos;: 72, &apos;romana&apos;: 1, &apos;fresh mushrooms&apos;: 117, &apos;crimini mushrooms&apos;: 26, &apos;navel oranges&apos;: 10, &apos;kalamata&apos;: 62, &apos;sea scallops&apos;: 31, &apos;ladyfingers&apos;: 40, &apos;reduced fat cream cheese&apos;: 3, &apos;whipped topping&apos;: 12, &apos;wine&apos;: 15, &apos;frozen broccoli&apos;: 5, &apos;nonfat ricotta cheese&apos;: 32, &apos;shells&apos;: 8, &apos;reduced-fat cheese&apos;: 1, &apos;cornflake cereal&apos;: 2, &apos;beef brisket&apos;: 2, &apos;dry pasta&apos;: 8, &apos;white bread&apos;: 35, &apos;pork&apos;: 10, &apos;sweet italian sausage&apos;: 72, &apos;prosecco&apos;: 10, &apos;bread dough&apos;: 16, &apos;baby lima beans&apos;: 3, &apos;whipped cream&apos;: 15, &apos;hot cocoa mix&apos;: 2, &apos;brewed coffee&apos;: 18, &apos;abbamele&apos;: 1, &apos;wild mushrooms&apos;: 30, &apos;chopped walnuts&apos;: 46, &apos;fregola&apos;: 4, &apos;savoy cabbage&apos;: 10, &apos;mushroom caps&apos;: 17, &apos;pinot grigio&apos;: 3, &apos;liquid egg substitute&apos;: 1, &apos;chocolate candy bars&apos;: 5, &apos;cooked rice&apos;: 5, &apos;bread crumbs&apos;: 103, &apos;banana peppers&apos;: 5, &apos;toasted walnuts&apos;: 7, &apos;dark rum&apos;: 22, &apos;dried fig&apos;: 11, &apos;chopped parsley&apos;: 99, &apos;ground white pepper&apos;: 33, &quot;soft goat&apos;s cheese&quot;: 10, &apos;truffle oil&apos;: 17, &apos;hazelnuts&apos;: 53, &apos;veal scallopini&apos;: 2, &apos;elbow macaroni&apos;: 19, &apos;bread flour&apos;: 77, &apos;bread yeast&apos;: 1, &apos;red potato&apos;: 26, &apos;vegan parmesan cheese&apos;: 4, &apos;ahi&apos;: 1, &apos;golden brown sugar&apos;: 14, &apos;anjou pears&apos;: 3, &apos;granny smith apples&apos;: 6, &apos;crystallized ginger&apos;: 10, &apos;ice water&apos;: 7, &apos;vegetable shortening&apos;: 5, &apos;chinese five-spice powder&apos;: 1, &apos;whole wheat flour&apos;: 21, &apos;large egg yolks&apos;: 142, &apos;lean ground beef&apos;: 135, &apos;sausage links&apos;: 32, &apos;pork chops&apos;: 8, &apos;rosemary&apos;: 57, &apos;nectarines&apos;: 5, &apos;sweet cherries&apos;: 12, &apos;orange zest&apos;: 28, &apos;lavender buds&apos;: 1, &apos;apricot halves&apos;: 4, &apos;diced tomatoes in juice&apos;: 7, &apos;scallions&apos;: 61, &apos;italian salad dressing&apos;: 41, &apos;short pasta&apos;: 7, &apos;lemon wedge&apos;: 58, &apos;porterhouse steaks&apos;: 4, &apos;1% low-fat milk&apos;: 68, &apos;golden raisins&apos;: 46, &apos;cinnamon sticks&apos;: 18, &apos;broccolini&apos;: 3, &apos;salted butter&apos;: 11, &apos;chicken stock&apos;: 142, &apos;coffee granules&apos;: 12, &apos;lemon rind&apos;: 34, &apos;baby portobello mushrooms&apos;: 10, &apos;broccoli florets&apos;: 64, &apos;orecchiette&apos;: 41, &apos;melted butter&apos;: 24, &apos;pizza shells&apos;: 3, &apos;frozen mixed thawed vegetables,&apos;: 2, &apos;ragu old world style pasta sauc&apos;: 11, &apos;loosely packed fresh basil leaves&apos;: 21, &apos;whole wheat spaghetti&apos;: 22, &apos;butternut squash&apos;: 62, &apos;wonton wrappers&apos;: 17, &apos;meat&apos;: 15, &apos;pear tomatoes&apos;: 6, &apos;gaeta olives&apos;: 5, &apos;yukon gold potatoes&apos;: 39, &apos;turbot&apos;: 2, &apos;ground lamb&apos;: 6, &apos;refrigerated pizza dough&apos;: 31, &apos;potato gnocchi&apos;: 20, &apos;cream&apos;: 27, &apos;angel hair&apos;: 60, &apos;lime juice&apos;: 6, &apos;gelato&apos;: 4, &apos;cherry preserves&apos;: 2, &apos;amaretto liqueur&apos;: 4, &apos;cherries&apos;: 14, &apos;instant espresso powder&apos;: 22, &apos;nuts&apos;: 3, &apos;brown sugar&apos;: 67, &apos;pie shell&apos;: 2, &apos;marsala wine&apos;: 105, &apos;basil leaves&apos;: 153, &apos;cake&apos;: 7, &apos;crabmeat&apos;: 11, &apos;chopped fresh herbs&apos;: 8, &apos;button mushrooms&apos;: 35, &apos;escarole&apos;: 47, &apos;chopped pecans&apos;: 28, &apos;chocolate bars&apos;: 2, &apos;coffee liqueur&apos;: 11, &apos;flat anchovy&apos;: 5, &apos;italian loaf&apos;: 4, &apos;salad dressing&apos;: 23, &apos;pitted black olives&apos;: 24, &apos;rotini&apos;: 46, &apos;frozen mixed vegetables&apos;: 4, &apos;canned tomatoes&apos;: 26, &apos;feta cheese&apos;: 36, &apos;anise seed&apos;: 13, &apos;top sirloin&apos;: 1, &apos;candy&apos;: 3, &apos;angel food cake mix&apos;: 2, &apos;buttercream frosting&apos;: 1, &apos;sour cream&apos;: 71, &apos;shredded cheddar cheese&apos;: 42, &apos;cottage cheese&apos;: 44, &apos;noodles&apos;: 38, &apos;lime&apos;: 5, &apos;watermelon&apos;: 5, &apos;risotto&apos;: 11, &apos;ice cubes&apos;: 8, &apos;peeled tomatoes&apos;: 32, &apos;cooked vermicelli&apos;: 5, &apos;buns&apos;: 1, &apos;apple cider vinegar&apos;: 7, &apos;ground red pepper&apos;: 34, &apos;yellow bell pepper&apos;: 85, &apos;active dry yeast&apos;: 128, &apos;baking potatoes&apos;: 34, &apos;grated romano cheese&apos;: 19, &apos;quinoa&apos;: 8, &apos;littleneck clams&apos;: 26, &apos;whole wheat bread&apos;: 8, &apos;cream cheese, soften&apos;: 40, &apos;salad seasoning mix&apos;: 4, &apos;cucumber&apos;: 25, &apos;pappardelle pasta&apos;: 6, &apos;fresh mozzarella&apos;: 95, &apos;tuna steaks&apos;: 16, &apos;shredded zucchini&apos;: 4, &apos;dried pasta&apos;: 13, &apos;berries&apos;: 5, &apos;clams&apos;: 50, &apos;ground round&apos;: 25, &apos;fettuccine, cook and drain&apos;: 4, &apos;tomato purée&apos;: 47, &apos;chopped celery&apos;: 103, &apos;ditalini pasta&apos;: 15, &apos;lobster&apos;: 11, &apos;almonds&apos;: 35, &apos;anise&apos;: 13, &apos;anise extract&apos;: 12, &apos;brandy&apos;: 25, &apos;boneless chicken breast&apos;: 19, &apos;buffalo sauce&apos;: 2, &apos;blue cheese dressing&apos;: 3, &apos;cannelloni shells&apos;: 2, &apos;butter cooking spray&apos;: 3, &apos;light alfredo sauce&apos;: 7, &apos;rice&apos;: 12, &apos;cream of chicken soup&apos;: 9, &apos;chees fresh mozzarella&apos;: 72, &apos;shrimp tails&apos;: 2, &apos;bay scallops&apos;: 12, &apos;lump crab meat&apos;: 17, &apos;fish stock&apos;: 11, &apos;capellini&apos;: 8, &apos;veal&apos;: 17, &apos;beef stock&apos;: 24, &apos;chard&apos;: 3, &apos;grated Gruyère cheese&apos;: 5, &apos;panko&apos;: 25, &apos;chickpeas&apos;: 37, &apos;white cornmeal&apos;: 4, &apos;red bell pepper, sliced&apos;: 2, &apos;herbs&apos;: 20, &apos;chicken thighs&apos;: 32, &apos;green bell pepper, slice&apos;: 13, &apos;vanilla&apos;: 28, &apos;liqueur&apos;: 10, &apos;aged balsamic vinegar&apos;: 8, &apos;limoncello&apos;: 6, &apos;golden beets&apos;: 4, &apos;pizza doughs&apos;: 103, &apos;black cod&apos;: 2, &apos;green cabbage&apos;: 12, &apos;cavolo nero&apos;: 4, &apos;winter squash&apos;: 1, &apos;thin pizza crust&apos;: 9, &apos;toasted pine nuts&apos;: 30, &apos;grated parmesan romano&apos;: 2, &apos;cumin seed&apos;: 4, &apos;cilantro leaves&apos;: 7, &apos;seasoning salt&apos;: 15, &apos;mixed greens&apos;: 16, &apos;turkey breast cutlets&apos;: 9, &apos;cod fillets&apos;: 8, &apos;barilla&apos;: 4, &apos;linguini&apos;: 9, &apos;perciatelli&apos;: 7, &apos;crumbled blue cheese&apos;: 16, &apos;black mission figs&apos;: 4, &apos;swordfish steaks&apos;: 11, &apos;anchovy paste&apos;: 47, &apos;chuck&apos;: 8, &apos;tomatoes with juice&apos;: 30, &apos;store bought low sodium chicken stock&apos;: 2, &apos;fresh lavender&apos;: 1, &apos;grated orange&apos;: 31, &apos;vanilla wafers&apos;: 3, &apos;amaretto&apos;: 19, &apos;toasted almonds&apos;: 6, &apos;light corn syrup&apos;: 8, &apos;focaccia&apos;: 10, &apos;oyster mushrooms&apos;: 6, &apos;shiitake mushroom caps&apos;: 16, &apos;onion powder&apos;: 39, &apos;sourdough&apos;: 7, &apos;orange bell pepper&apos;: 27, &apos;nonfat cottage cheese&apos;: 6, &apos;stewed tomatoes&apos;: 37, &apos;raspberries&apos;: 25, &apos;vanilla beans&apos;: 26, &apos;Frangelico&apos;: 9, &apos;vegetable oil spray&apos;: 22, &apos;table salt&apos;: 21, &apos;white peppercorns&apos;: 2, &apos;herb vinegar&apos;: 3, &apos;reduced fat sharp cheddar cheese&apos;: 8, &apos;deli ham&apos;: 4, &apos;ground turkey&apos;: 48, &apos;hot dogs&apos;: 2, &apos;italian style stewed tomatoes&apos;: 15, &apos;veal stock&apos;: 7, &apos;portabello mushroom&apos;: 41, &apos;rocket leaves&apos;: 35, &apos;country bread&apos;: 26, &apos;bottled balsamic vinaigrette&apos;: 1, &apos;scallops&apos;: 12, &apos;italian tomatoes&apos;: 16, &apos;peeled shrimp&apos;: 3, &apos;whole wheat pizza crust&apos;: 3, &apos;dried mixed herbs&apos;: 3, &apos;whole wheat pastry flour&apos;: 6, &apos;nonstick spray&apos;: 7, &apos;low-fat sour cream&apos;: 8, &apos;day old bread&apos;: 2, &apos;champagne vinegar&apos;: 9, &apos;pizza sauce&apos;: 71, &apos;red vermouth&apos;: 5, &apos;low sodium chicken stock&apos;: 6, &apos;peppercorns&apos;: 3, &apos;turkey stock&apos;: 3, &apos;cooked turkey&apos;: 6, &apos;green olives&apos;: 52, &apos;spaghettini&apos;: 23, &apos;minced onion&apos;: 41, &apos;beef broth&apos;: 62, &apos;dried mint flakes&apos;: 3, &apos;ravioli&apos;: 16, &apos;meat loaf mix&apos;: 1, &apos;rub&apos;: 2, &apos;clam juice&apos;: 26, &apos;bottled clam juice&apos;: 15, &apos;cayenne pepper&apos;: 50, &apos;pears&apos;: 19, &apos;gorgonzola&apos;: 27, &apos;duck&apos;: 2, &apos;pure vanilla extract&apos;: 22, &apos;panettone&apos;: 4, &apos;sambuca&apos;: 4, &apos;light brown sugar&apos;: 15, &apos;fat free frozen top whip&apos;: 5, &apos;frozen bread dough&apos;: 9, &apos;whole wheat breadcrumbs&apos;: 10, &apos;ground chuck&apos;: 16, &apos;bows&apos;: 3, &apos;sauce&apos;: 53, &apos;ground oregano&apos;: 4, &apos;lean ground turkey&apos;: 2, &apos;milk chocolate&apos;: 11, &apos;hazelnut butter&apos;: 4, &apos;fig jam&apos;: 2, &apos;crackers&apos;: 7, &apos;sun-dried tomatoes in oil&apos;: 28, &apos;dark chocolate&apos;: 7, &apos;vanilla ice cream&apos;: 17, &apos;chives&apos;: 31, &apos;radishes&apos;: 13, &apos;lettuce&apos;: 6, &apos;grilled chicken&apos;: 2, &apos;flatbread&apos;: 4, &apos;fresh parsley leaves&apos;: 56, &apos;lemon extract&apos;: 7, &apos;salad greens&apos;: 11, &apos;brown rice&apos;: 6, &apos;tortellini&apos;: 19, &apos;reduced fat alfredo sauce&apos;: 4, &apos;fresh asparagus&apos;: 24, &apos;round steaks&apos;: 5, &apos;chili powder&apos;: 26, &apos;ground cumin&apos;: 30, &apos;rolls&apos;: 21, &apos;prego traditional italian sauce&apos;: 4, &apos;brown hash potato&apos;: 5, &apos;nonfat milk&apos;: 7, &apos;frozen cheese ravioli&apos;: 6, &apos;mild Italian sausage&apos;: 17, &apos;sandwich rolls&apos;: 3, &apos;orange juice concentrate&apos;: 5, &apos;rotelle&apos;: 3, &apos;sweet italian sausag links, cut into&apos;: 2, &apos;idaho potatoes&apos;: 4, &apos;popcorn&apos;: 1, &apos;grated orange peel&apos;: 26, &apos;french fried onions&apos;: 1, &apos;turbinado&apos;: 7, &apos;cooked chicken breasts&apos;: 16, &apos;farro&apos;: 14, &apos;flour tortillas&apos;: 11, &apos;english cucumber&apos;: 7, &apos;minced peperoncini&apos;: 2, &apos;teleme&apos;: 2, &apos;brine cured green olives&apos;: 4, &apos;black forest ham&apos;: 1, &apos;frozen whole kernel corn&apos;: 5, &apos;sweet potatoes&apos;: 15, &apos;applewood smoked bacon&apos;: 7, &apos;panko breadcrumbs&apos;: 24, &apos;coarse kosher salt&apos;: 9, &apos;cornmeal&apos;: 72, &apos;dried sage&apos;: 28, &apos;dri leav thyme&apos;: 6, &apos;low sodium beef broth&apos;: 3, &apos;cabbage&apos;: 12, &apos;fresh shiitake mushrooms&apos;: 15, &apos;rabbit&apos;: 8, &apos;herbes de provence&apos;: 12, &apos;dressing&apos;: 7, &apos;chicken fingers&apos;: 3, &apos;reduced-fat sour cream&apos;: 13, &apos;dried fettuccine&apos;: 14, &apos;white pepper&apos;: 22, &apos;albacore tuna in water&apos;: 5, &apos;light mayonnaise&apos;: 7, &apos;turkey tenderloins&apos;: 4, &apos;almond flour&apos;: 8, &apos;raw almond&apos;: 3, &apos;fresh orange juice&apos;: 32, &apos;cranberries&apos;: 3, &apos;orange marmalade&apos;: 6, &apos;fresh lemon&apos;: 4, &apos;condensed chicken broth&apos;: 1, &apos;oil packed anchovy fillets&apos;: 3, &apos;boneless chicken skinless thigh&apos;: 20, &apos;sugar pea&apos;: 16, &apos;nonfat yogurt&apos;: 4, &apos;roast red peppers, drain&apos;: 24, &apos;french baguette&apos;: 25, &apos;ripe olives&apos;: 26, &apos;honey glazed ham&apos;: 2, &apos;chiles&apos;: 9, &apos;spring onions&apos;: 3, &apos;candied orange peel&apos;: 13, &apos;lard&apos;: 5, &apos;cinnamon&apos;: 23, &apos;semolina flour&apos;: 32, &apos;onion salt&apos;: 2, &apos;beef demi-glace&apos;: 1, &apos;veal shanks&apos;: 21, &apos;orange peel&apos;: 7, &apos;lemon peel&apos;: 11, &apos;plain yogurt&apos;: 7, &apos;Quinoa Flour&apos;: 1, &apos;spelt flour&apos;: 2, &apos;plums&apos;: 14, &apos;heirloom tomatoes&apos;: 21, &apos;fresh lime juice&apos;: 25, &apos;artichok heart marin&apos;: 33, &apos;bucatini&apos;: 16, &apos;processed cheese&apos;: 5, &apos;egg noodles, cooked and drained&apos;: 3, &apos;tapioca flour&apos;: 1, &apos;lasagna noodles, cooked and drained&apos;: 26, &apos;firm tofu&apos;: 9, &apos;sherry vinegar&apos;: 27, &apos;country style bread&apos;: 7, &apos;teardrop tomatoes&apos;: 2, &apos;ground sausage&apos;: 5, &apos;pita pockets&apos;: 1, &apos;orange juice&apos;: 19, &apos;chili pepper&apos;: 5, &apos;currant&apos;: 7, &apos;small capers, rins and drain&apos;: 2, &apos;filet&apos;: 4, &apos;lettuce leaves&apos;: 8, &apos;vodka&apos;: 27, &apos;stolichnaya&apos;: 1, &apos;romaine lettuce&apos;: 33, &apos;croutons&apos;: 18, &apos;pepperocini&apos;: 2, &apos;cherry peppers&apos;: 3, &apos;dandelion&apos;: 1, &apos;beans&apos;: 6, &apos;hot pepper sauce&apos;: 7, &apos;shredded Monterey Jack cheese&apos;: 13, &apos;cane sugar&apos;: 1, &apos;mixed nuts&apos;: 1, &apos;meatballs&apos;: 17, &apos;plain dry bread crumb&apos;: 11, &apos;spanish onion&apos;: 9, &apos;cuban peppers&apos;: 3, &apos;green tomatoes&apos;: 2, &apos;sesame seeds&apos;: 13, &apos;boneless beef chuck roast&apos;: 3, &apos;hard-boiled egg&apos;: 6, &apos;pork tenderloin&apos;: 24, &apos;bulk italian sausag&apos;: 21, &apos;beef bouillon granules&apos;: 3, &apos;prebaked pizza crusts&apos;: 13, &apos;buttermilk&apos;: 32, &apos;flaked coconut&apos;: 5, &apos;lower sodium chicken broth&apos;: 20, &apos;carnaroli rice&apos;: 13, &apos;fresh oregano leaves&apos;: 24, &apos;cavatappi&apos;: 9, &apos;cooking oil&apos;: 24, &apos;cayenne&apos;: 8, &apos;ground cloves&apos;: 30, &apos;corn syrup&apos;: 4, &apos;small pasta&apos;: 7, &apos;ground fennel&apos;: 7, &apos;low-fat buttermilk&apos;: 9, &apos;ice&apos;: 7, &apos;chicken bouillon granules&apos;: 6, &apos;poultry seasoning&apos;: 10, &apos;roast&apos;: 3, &apos;fusilli&apos;: 43, &apos;Italian herbs&apos;: 7, &apos;diced yellow onion&apos;: 1, &apos;manicotti pasta&apos;: 4, &apos;parsley flakes&apos;: 17, &apos;vinaigrette&apos;: 7, &apos;bread ciabatta&apos;: 16, &apos;kidney beans&apos;: 12, &apos;creole seasoning&apos;: 4, &apos;prepared pasta sauce&apos;: 5, &apos;small curd cottage cheese&apos;: 14, &apos;white sandwich bread&apos;: 9, &apos;mini chocolate chips&apos;: 5, &apos;lean beef&apos;: 2, &apos;breadstick&apos;: 13, &apos;pickled okra&apos;: 2, &apos;fronds&apos;: 15, &apos;thick-cut bacon&apos;: 11, &apos;boiling potatoes&apos;: 13, &apos;ditalini&apos;: 11, &apos;cranberry beans&apos;: 4, &apos;center cut bacon&apos;: 7, &apos;roasting chickens&apos;: 8, &apos;fleur de sel&apos;: 6, &apos;Margherita Pepperoni&apos;: 1, &apos;soft-shell clams&apos;: 1, &apos;liquid&apos;: 5, &apos;fresh chevre&apos;: 4, &apos;pork sausages&apos;: 14, &apos;dried minced onion&apos;: 5, &apos;msg&apos;: 2, &apos;pork stew meat&apos;: 1, &apos;beef stew meat&apos;: 3, &apos;ziti&apos;: 26, &apos;Balsamico Bianco&apos;: 2, &apos;ground mustard&apos;: 1, ...}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line"><span class="comment"># Finally, plot the 10 most used ingredients</span></span><br><span class="line">plt.style.use(<span class="string">u'ggplot'</span>)</span><br><span class="line">fig = pd.DataFrame(sum_ingredients, index=[<span class="number">0</span>]).transpose()[<span class="number">0</span>].sort_values(ascending=<span class="keyword">False</span>, inplace=<span class="keyword">False</span>)[:<span class="number">10</span>].plot(kind=<span class="string">'barh'</span>)</span><br><span class="line">fig.invert_yaxis()</span><br><span class="line">fig = fig.get_figure()</span><br><span class="line">fig.tight_layout()</span><br><span class="line">fig.show()</span><br></pre></td></tr></table></figure><pre><code>/opt/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:448: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.  % get_backend())</code></pre><p><img src="/2019/02/15/PredictYourCuisine/output_15_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## <span class="doctag">TODO:</span> 统计意大利菜系中佐料出现次数，并赋值到italian_ingredients字典中</span></span><br><span class="line">italian_ingredients = &#123;&#125;</span><br><span class="line">train_content_italian = train_content[train_content[<span class="string">'cuisine'</span>] == <span class="string">'italian'</span>]</span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> train_content_italian[<span class="string">'ingredients'</span>]:</span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> l:</span><br><span class="line">        <span class="keyword">if</span> a <span class="keyword">not</span> <span class="keyword">in</span> italian_ingredients:</span><br><span class="line">            italian_ingredients[a] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> a <span class="keyword">in</span> sum_italian:</span><br><span class="line">            italian_ingredients[a] += <span class="number">1</span></span><br><span class="line">italian_ingredients</span><br></pre></td></tr></table></figure><pre><code>{&apos;sugar&apos;: 760, &apos;pistachio nuts&apos;: 7, &apos;white almond bark&apos;: 1, &apos;flour&apos;: 142, &apos;vanilla extract&apos;: 219, &apos;olive oil&apos;: 3111, &apos;almond extract&apos;: 56, &apos;eggs&apos;: 627, &apos;baking powder&apos;: 186, &apos;dried cranberries&apos;: 8, &apos;chopped tomatoes&apos;: 37, &apos;fresh basil&apos;: 787, &apos;garlic&apos;: 1471, &apos;extra-virgin olive oil&apos;: 1362, &apos;kosher salt&apos;: 656, &apos;flat leaf parsley&apos;: 588, &apos;pimentos&apos;: 16, &apos;sweet pepper&apos;: 7, &apos;dried oregano&apos;: 626, &apos;sharp cheddar cheese&apos;: 9, &apos;pepper&apos;: 965, &apos;swiss cheese&apos;: 7, &apos;provolone cheese&apos;: 138, &apos;canola oil&apos;: 41, &apos;mushrooms&apos;: 184, &apos;black olives&apos;: 67, &apos;sausages&apos;: 58, &apos;Italian parsley leaves&apos;: 74, &apos;walnuts&apos;: 38, &apos;hot red pepper flakes&apos;: 76, &apos;fresh lemon juice&apos;: 471, &apos;trout fillet&apos;: 3, &apos;garlic cloves&apos;: 1619, &apos;chipotle chile&apos;: 2, &apos;fine sea salt&apos;: 77, &apos;fresh parmesan cheese&apos;: 251, &apos;butter&apos;: 1030, &apos;all-purpose flour&apos;: 918, &apos;fat free less sodium chicken broth&apos;: 158, &apos;chopped fresh chives&apos;: 68, &apos;gruyere cheese&apos;: 18, &apos;ground black pepper&apos;: 1444, &apos;bacon slices&apos;: 48, &apos;gnocchi&apos;: 41, &apos;fat free milk&apos;: 42, &apos;cooking spray&apos;: 491, &apos;salt&apos;: 3454, &apos;italian seasoning&apos;: 347, &apos;broiler-fryer chicken&apos;: 1, &apos;mayonaise&apos;: 63, &apos;zesty italian dressing&apos;: 11, &apos;marinara sauce&apos;: 222, &apos;linguine&apos;: 193, &apos;capers&apos;: 306, &apos;crushed red pepper flakes&apos;: 179, &apos;olives&apos;: 29, &apos;lemon zest&apos;: 98, &apos;sliced mushrooms&apos;: 131, &apos;sherry&apos;: 13, &apos;grated parmesan cheese&apos;: 1580, &apos;heavy cream&apos;: 300, &apos;spaghetti&apos;: 296, &apos;chicken broth&apos;: 245, &apos;cooked chicken&apos;: 33, &apos;yellow corn meal&apos;: 64, &apos;boiling water&apos;: 63, &apos;sea salt&apos;: 202, &apos;onions&apos;: 1240, &apos;crushed garlic&apos;: 20, &apos;green onions&apos;: 144, &apos;white sugar&apos;: 231, &apos;dried basil&apos;: 425, &apos;diced tomatoes&apos;: 429, &apos;bread slices&apos;: 15, &apos;great northern beans&apos;: 21, &apos;shrimp&apos;: 59, &apos;sage leaves&apos;: 89, &apos;Oscar Mayer Deli Fresh Smoked Ham&apos;: 1, &apos;hoagie rolls&apos;: 8, &apos;salami&apos;: 41, &apos;giardiniera&apos;: 5, &apos;mozzarella cheese&apos;: 396, &apos;pepperoni&apos;: 48, &apos;bay leaves&apos;: 107, &apos;crushed red pepper&apos;: 418, &apos;mussels&apos;: 38, &apos;basil&apos;: 174, &apos;black pepper&apos;: 636, &apos;dry white wine&apos;: 658, &apos;tomatoes&apos;: 601, &apos;finely chopped onion&apos;: 145, &apos;lemon&apos;: 236, &apos;pesto&apos;: 113, &apos;salmon fillets&apos;: 11, &apos;white wine&apos;: 176, &apos;pizza crust&apos;: 36, &apos;plum tomatoes&apos;: 340, &apos;part-skim mozzarella cheese&apos;: 201, &apos;crushed tomatoes&apos;: 241, &apos;fresh rosemary&apos;: 292, &apos;boneless pork loin&apos;: 9, &apos;pappardelle&apos;: 11, &apos;red pepper&apos;: 49, &apos;Italian bread&apos;: 89, &apos;balsamic vinegar&apos;: 348, &apos;sausage casings&apos;: 78, &apos;honey&apos;: 126, &apos;shredded mozzarella cheese&apos;: 413, &apos;roasted red peppers&apos;: 114, &apos;penne pasta&apos;: 146, &apos;spinach&apos;: 119, &apos;asiago&apos;: 72, &apos;whole wheat pasta&apos;: 13, &apos;sweet onion&apos;: 69, &apos;grape tomatoes&apos;: 98, &apos;chestnuts&apos;: 9, &apos;granulated sugar&apos;: 82, &apos;whole milk ricotta cheese&apos;: 47, &apos;coffee ice cream&apos;: 3, &apos;large eggs&apos;: 625, &apos;mascarpone&apos;: 124, &apos;rum&apos;: 12, &apos;powdered sugar&apos;: 69, &apos;semisweet chocolate&apos;: 46, &apos;chestnut flour&apos;: 1, &apos;starchy potatoes&apos;: 2, &apos;grated nutmeg&apos;: 64, &apos;blood orange&apos;: 5, &apos;freshly ground pepper&apos;: 316, &apos;fennel bulb&apos;: 103, &apos;low salt chicken broth&apos;: 138, &apos;dijon mustard&apos;: 99, &apos;corn starch&apos;: 83, &apos;white wine vinegar&apos;: 73, &apos;tomato sauce&apos;: 357, &apos;shredded carrots&apos;: 11, &apos;english muffins, split and toasted&apos;: 2, &apos;chopped onion&apos;: 327, &apos;vegetable oil cooking spray&apos;: 73, &apos;chopped green bell pepper&apos;: 39, &apos;cheddar cheese&apos;: 18, &apos;lasagna noodles&apos;: 196, &apos;ranch dressing&apos;: 5, &apos;evaporated milk&apos;: 13, &apos;fresh parsley&apos;: 631, &apos;fresh oregano&apos;: 209, &apos;cold water&apos;: 58, &apos;chocolate morsels&apos;: 4, &apos;cream sweeten whip&apos;: 3, &apos;instant espresso granules&apos;: 4, &apos;whipping cream&apos;: 162, &apos;kahlúa&apos;: 12, &apos;chocolate covered coffee beans&apos;: 1, &apos;unflavored gelatin&apos;: 48, &apos;pound cake&apos;: 7, &apos;pinenuts&apos;: 252, &apos;zucchini&apos;: 326, &apos;baby carrots&apos;: 15, &apos;fresh basil leaves&apos;: 352, &apos;asparagus spears&apos;: 24, &apos;white onion&apos;: 48, &apos;carrots&apos;: 379, &apos;frozen peas&apos;: 67, &apos;arborio rice&apos;: 261, &apos;yellow crookneck squash&apos;: 3, &apos;fresh leav spinach&apos;: 25, &apos;cheese tortellini&apos;: 53, &apos;cherry tomatoes&apos;: 170, &apos;navy beans&apos;: 8, &apos;pecorino romano cheese&apos;: 146, &apos;fresh fava bean&apos;: 12, &apos;italian sausage&apos;: 129, &apos;large garlic cloves&apos;: 293, &apos;pasta sheets&apos;: 9, &apos;water&apos;: 1052, &apos;Turkish bay leaves&apos;: 3, &apos;dried chickpeas&apos;: 1, &apos;celery ribs&apos;: 130, &apos;semolina&apos;: 14, &apos;warm water&apos;: 182, &apos;vine ripened tomatoes&apos;: 12, &apos;bittersweet chocolate&apos;: 42, &apos;fat free yogurt&apos;: 4, &apos;skim milk&apos;: 16, &apos;angel food cake&apos;: 4, &apos;unsweetened cocoa powder&apos;: 84, &apos;instant espresso&apos;: 7, &apos;garlic salt&apos;: 61, &apos;tomato paste&apos;: 376, &apos;veal cutlets&apos;: 18, &apos;broccoli rabe&apos;: 47, &apos;whole milk&apos;: 157, &apos;parmigiano reggiano cheese&apos;: 306, &apos;dry bread crumbs&apos;: 115, &apos;fontina&apos;: 27, &apos;unsalted butter&apos;: 564, &apos;pasta sauce&apos;: 246, &apos;olive oil flavored cooking spray&apos;: 29, &apos;frozen chopped spinach&apos;: 131, &apos;large egg whites&apos;: 138, &apos;part-skim ricotta cheese&apos;: 111, &apos;manicotti shells&apos;: 18, &apos;fettucine&apos;: 136, &apos;parmesan cheese&apos;: 474, &apos;large shrimp&apos;: 77, &apos;chicken bouillon&apos;: 10, &apos;cream of tartar&apos;: 9, &apos;orange liqueur&apos;: 15, &apos;heavy whipping cream&apos;: 73, &apos;semi-sweet chocolate morsels&apos;: 21, &apos;cake flour&apos;: 24, &apos;confectioners sugar&apos;: 94, &apos;ground cinnamon&apos;: 85, &apos;ricotta cheese&apos;: 333, &apos;cream cheese&apos;: 105, &apos;dried porcini mushrooms&apos;: 81, &apos;chopped fresh thyme&apos;: 151, &apos;beef rib short&apos;: 5, &apos;dry red wine&apos;: 131, &apos;hot water&apos;: 85, &apos;fat free less sodium beef broth&apos;: 7, &apos;cremini mushrooms&apos;: 67, &apos;pitted kalamata olives&apos;: 94, &apos;cauliflower&apos;: 34, &apos;whole peeled tomatoes&apos;: 60, &apos;swiss chard&apos;: 63, &apos;banana squash&apos;: 1, &apos;vegetable broth&apos;: 81, &apos;bay leaf&apos;: 130, &apos;cannellini beans&apos;: 161, &apos;boneless skinless chicken breast halves&apos;: 182, &apos;light sour cream&apos;: 5, &apos;fava beans&apos;: 13, &apos;finely chopped fresh parsley&apos;: 33, &apos;fresh tarragon&apos;: 33, &apos;grated lemon zest&apos;: 142, &apos;chopped almonds&apos;: 12, &apos;broccoli&apos;: 64, &apos;potatoes&apos;: 59, &apos;self rising flour&apos;: 1, &apos;eggplant&apos;: 204, &apos;chicken&apos;: 84, &apos;pitas&apos;: 5, &apos;goat cheese&apos;: 92, &apos;reduced fat milk&apos;: 29, &apos;smoked trout&apos;: 1, &apos;fresh dill&apos;: 16, &apos;asparagus&apos;: 123, &apos;oil&apos;: 92, &apos;ricotta salata&apos;: 22, &apos;pecorino cheese&apos;: 52, &apos;grana padano&apos;: 7, &apos;pasta&apos;: 255, &apos;cracked black pepper&apos;: 117, &apos;green bell pepper&apos;: 163, &apos;vidalia onion&apos;: 19, &apos;smoked gouda&apos;: 14, &apos;baby spinach leaves&apos;: 39, &apos;shredded sharp cheddar cheese&apos;: 17, &apos;small red potato&apos;: 13, &apos;Bertolli® Classico Olive Oil&apos;: 16, &apos;bacon, crisp-cooked and crumbled&apos;: 2, &apos;bertolli vineyard premium collect marinara with burgundi wine sauc&apos;: 5, &apos;bread crumb fresh&apos;: 74, &apos;(    oz.) tomato sauce&apos;: 7, &apos;ground veal&apos;: 35, &apos;italian seasoning mix&apos;: 1, &apos;beef&apos;: 22, &apos;fat skimmed chicken broth&apos;: 28, &apos;solid pack pumpkin&apos;: 4, &apos;ground nutmeg&apos;: 127, &apos;white rice&apos;: 11, &apos;ground pepper&apos;: 77, &apos;fresh thyme leaves&apos;: 53, &apos;pistachios&apos;: 10, &apos;dried cherry&apos;: 6, &apos;prosciutto&apos;: 256, &apos;romano cheese&apos;: 89, &apos;parsley leaves&apos;: 23, &apos;shallots&apos;: 316, &apos;fresh raspberries&apos;: 14, &apos;dry vermouth&apos;: 18, &apos;canned low sodium chicken broth&apos;: 46, &apos;chicken livers&apos;: 14, &apos;raisins&apos;: 39, &apos;seasoned bread crumbs&apos;: 71, &apos;minced garlic&apos;: 334, &apos;half &amp; half&apos;: 89, &apos;ground beef&apos;: 237, &apos;jack cheese&apos;: 9, &apos;vegetables&apos;: 29, &apos;lemon juice&apos;: 216, &apos;ricotta&apos;: 95, &apos;yellow onion&apos;: 153, &apos;reduced sodium chicken broth&apos;: 36, &apos;chopped fresh mint&apos;: 54, &apos;purple onion&apos;: 350, &apos;low sodium chicken broth&apos;: 78, &apos;polenta&apos;: 132, &apos;bell pepper&apos;: 38, &apos;olive oil cooking spray&apos;: 13, &apos;bacon&apos;: 124, &apos;egg yolks&apos;: 82, &apos;duck breast halves&apos;: 1, &apos;fennel seeds&apos;: 114, &apos;roma tomatoes&apos;: 44, &apos;pesto sauce&apos;: 26, &apos;arugula&apos;: 125, &apos;summer squash&apos;: 9, &apos;red wine vinegar&apos;: 227, &apos;ciabatta&apos;: 21, &apos;juice&apos;: 72, &apos;fresh herbs&apos;: 14, &apos;grated lemon peel&apos;: 89, &apos;dough&apos;: 46, &apos;coarse sea salt&apos;: 17, &apos;rosemary leaves&apos;: 10, &apos;red pepper flakes&apos;: 206, &apos;brie cheese&apos;: 14, &apos;cheese ravioli&apos;: 18, &apos;Italian seasoned breadcrumbs&apos;: 23, &apos;cheese&apos;: 127, &apos;parsley&apos;: 95, &apos;fat-free cottage cheese&apos;: 11, &apos;oven-ready lasagna noodles&apos;: 43, &apos;margarine&apos;: 38, &apos;radicchio&apos;: 40, &apos;garbanzo beans&apos;: 18, &apos;orzo pasta&apos;: 16, &apos;rubbed sage&apos;: 17, &apos;dried rosemary&apos;: 96, &apos;canned beef broth&apos;: 4, &apos;kale leaves&apos;: 7, &apos;chicken noodle soup&apos;: 1, &apos;italian style rolls&apos;: 1, &apos;genoa salami&apos;: 22, &apos;oregano&apos;: 114, &apos;boiled ham&apos;: 1, &apos;capicola&apos;: 4, &apos;iceberg lettuce&apos;: 6, &apos;jalapeno chilies&apos;: 20, &apos;diced celery&apos;: 15, &apos;italian salad dressing mix&apos;: 7, &apos;chopped cilantro fresh&apos;: 25, &apos;cider vinegar&apos;: 14, &apos;red bell pepper&apos;: 354, &apos;sliced green onions&apos;: 40, &apos;barbecue sauce&apos;: 9, &apos;prepared pizza crust&apos;: 2, &apos;boneless skinless chicken breasts&apos;: 132, &apos;Sargento® Traditional Cut Shredded Mozzarella Cheese&apos;: 1, &apos;vegetable oil&apos;: 253, &apos;basil pesto sauce&apos;: 33, &apos;aioli&apos;: 3, &apos;cooked shrimp&apos;: 11, &apos;mozzarella balls&apos;: 4, &apos;sun-dried tomatoes&apos;: 118, &apos;milk&apos;: 277, &apos;condensed cream of mushroom soup&apos;: 16, &apos;fettuccine pasta&apos;: 24, &apos;frozen spinach&apos;: 24, &apos;lasagne&apos;: 6, &apos;passata&apos;: 3, &apos;fat free cream cheese&apos;: 8, &apos;french bread&apos;: 54, &apos;non-fat sour cream&apos;: 14, &apos;reduced fat swiss cheese&apos;: 3, &apos;fat-free mayonnaise&apos;: 8, &apos;roasted garlic&apos;: 7, &apos;seasoning&apos;: 12, &apos;kale&apos;: 58, &apos;sundried tomato paste&apos;: 4, &apos;dried thyme&apos;: 145, &apos;porcini&apos;: 7, &apos;fresh thyme&apos;: 62, &apos;sourdough loaf&apos;: 4, &apos;crust&apos;: 3, &apos;duck fat&apos;: 1, &apos;squabs&apos;: 1, &apos;confit&apos;: 2, &apos;aged gouda&apos;: 1, &apos;soppressata&apos;: 16, &apos;artichokes&apos;: 44, &apos;anchovy fillets&apos;: 124, &apos;lemon slices&apos;: 12, &apos;chicken cutlets&apos;: 23, &apos;saffron threads&apos;: 28, &apos;orzo&apos;: 51, &apos;veal chops&apos;: 4, &apos;oil cured olives&apos;: 3, &apos;pasta rotel&apos;: 2, &apos;pasta water&apos;: 10, &apos;cooking wine&apos;: 3, &apos;crusty bread&apos;: 10, &apos;fresh tomatoes&apos;: 32, &apos;uncooked rigatoni&apos;: 6, &apos;grating cheese&apos;: 6, &apos;vegetable stock&apos;: 29, &apos;freshly grated parmesan&apos;: 87, &apos;florets&apos;: 15, &apos;sliced almonds&apos;: 37, &apos;thyme sprigs&apos;: 45, &apos;egg whites&apos;: 95, &apos;greek yogurt&apos;: 6, &apos;penne&apos;: 99, &apos;fish fillets&apos;: 10, &apos;russet potatoes&apos;: 40, &apos;crumbled gorgonzola&apos;: 29, &apos;fontina cheese&apos;: 84, &apos;spinach leaves&apos;: 25, &apos;orange&apos;: 38, &apos;lamb shanks&apos;: 11, &apos;clove&apos;: 19, &apos;rosemary sprigs&apos;: 50, &apos;italian eggplant&apos;: 7, &apos;Sicilian olives&apos;: 7, &apos;mint sprigs&apos;: 28, &apos;peasant bread&apos;: 7, &apos;garlic powder&apos;: 252, &apos;Kraft Grated Parmesan Cheese&apos;: 10, &apos;peas&apos;: 42, &apos;Oscar Mayer Bacon&apos;: 1, &apos;Philadelphia Cream Cheese&apos;: 9, &apos;garnish&apos;: 3, &apos;dried navy beans&apos;: 5, &apos;celery&apos;: 155, &apos;diced onions&apos;: 42, &apos;wheat berries&apos;: 4, &apos;parsley sprigs&apos;: 38, &apos;thyme&apos;: 51, &apos;polenta prepar&apos;: 5, &apos;chicken breast halves&apos;: 35, &apos;dry sherry&apos;: 22, &apos;cocoa powder&apos;: 9, &apos;dried parsley&apos;: 90, &apos;chopped garlic&apos;: 69, &apos;baking soda&apos;: 68, &apos;coarse salt&apos;: 128, &apos;rigatoni&apos;: 60, &apos;nutmeg&apos;: 40, &apos;artichoke hearts&apos;: 99, &apos;pancetta&apos;: 153, &apos;parsnips&apos;: 7, &apos;whole wheat fettuccine&apos;: 4, &apos;ground sirloin&apos;: 20, &apos;red wine&apos;: 99, &apos;fresh marjoram&apos;: 29, &apos;castellane&apos;: 2, &apos;hot Italian sausages&apos;: 39, &apos;jumbo pasta shells&apos;: 37, &apos;fresh spinach&apos;: 87, &apos;chicken breasts&apos;: 93, &apos;gluten free blend&apos;: 1, &apos;paprika&apos;: 76, &apos;salt and ground black pepper&apos;: 66, &apos;bone in skinless chicken thigh&apos;: 1, &apos;figs&apos;: 10, &apos;egg substitute&apos;: 26, &apos;hot sauce&apos;: 20, &apos;ham&apos;: 42, &apos;balsamic vinaigrette&apos;: 6, &apos;pitted olives&apos;: 15, &apos;fresh chives&apos;: 24, &apos;white mushrooms&apos;: 20, &apos;haricots verts&apos;: 4, &apos;fresh peas&apos;: 16, &apos;bow-tie pasta&apos;: 86, &apos;asparagus tips&apos;: 3, &apos;green beans&apos;: 70, &apos;low-fat cottage cheese&apos;: 14, &apos;whole wheat lasagna noodles&apos;: 8, &apos;shredded parmesan cheese&apos;: 38, &apos;apricots&apos;: 5, &apos;grappa&apos;: 9, &apos;all purpose unbleached flour&apos;: 68, &apos;dry yeast&apos;: 71, &apos;worcestershire sauce&apos;: 54, &apos;amaretti&apos;: 6, &apos;frozen strawberries&apos;: 1, &apos;strawberries&apos;: 48, &apos;cooked ham&apos;: 9, &apos;Alfredo sauce&apos;: 43, &apos;chopped fresh sage&apos;: 91, &apos;cheese slices&apos;: 11, &apos;bread&apos;: 51, &apos;morel&apos;: 2, &apos;leeks&apos;: 106, &apos;1% low-fat cottage cheese&apos;: 10, &apos;feta cheese crumbles&apos;: 68, &apos;pork belly&apos;: 2, &apos;fresh sage&apos;: 29, &apos;pork loin&apos;: 3, &apos;sliced black olives&apos;: 49, &apos;medium shrimp&apos;: 70, &apos;red chili peppers&apos;: 22, &apos;parmigiano-reggiano cheese&apos;: 53, &apos;fresh mint&apos;: 53, &apos;baguette&apos;: 119, &apos;chicken legs&apos;: 7, &apos;baby spinach&apos;: 117, &apos;white beans&apos;: 38, &apos;ground pork&apos;: 72, &apos;romana&apos;: 1, &apos;fresh mushrooms&apos;: 117, &apos;crimini mushrooms&apos;: 26, &apos;navel oranges&apos;: 10, &apos;kalamata&apos;: 62, &apos;sea scallops&apos;: 31, &apos;ladyfingers&apos;: 40, &apos;reduced fat cream cheese&apos;: 3, &apos;whipped topping&apos;: 12, &apos;wine&apos;: 15, &apos;frozen broccoli&apos;: 5, &apos;nonfat ricotta cheese&apos;: 32, &apos;shells&apos;: 8, &apos;reduced-fat cheese&apos;: 1, &apos;cornflake cereal&apos;: 2, &apos;beef brisket&apos;: 2, &apos;dry pasta&apos;: 8, &apos;white bread&apos;: 35, &apos;pork&apos;: 10, &apos;sweet italian sausage&apos;: 72, &apos;prosecco&apos;: 10, &apos;bread dough&apos;: 16, &apos;baby lima beans&apos;: 3, &apos;whipped cream&apos;: 15, &apos;hot cocoa mix&apos;: 2, &apos;brewed coffee&apos;: 18, &apos;abbamele&apos;: 1, &apos;wild mushrooms&apos;: 30, &apos;chopped walnuts&apos;: 46, &apos;fregola&apos;: 4, &apos;savoy cabbage&apos;: 10, &apos;mushroom caps&apos;: 17, &apos;pinot grigio&apos;: 3, &apos;liquid egg substitute&apos;: 1, &apos;chocolate candy bars&apos;: 5, &apos;cooked rice&apos;: 5, &apos;bread crumbs&apos;: 103, &apos;banana peppers&apos;: 5, &apos;toasted walnuts&apos;: 7, &apos;dark rum&apos;: 22, &apos;dried fig&apos;: 11, &apos;chopped parsley&apos;: 99, &apos;ground white pepper&apos;: 33, &quot;soft goat&apos;s cheese&quot;: 10, &apos;truffle oil&apos;: 17, &apos;hazelnuts&apos;: 53, &apos;veal scallopini&apos;: 2, &apos;elbow macaroni&apos;: 19, &apos;bread flour&apos;: 77, &apos;bread yeast&apos;: 1, &apos;red potato&apos;: 26, &apos;vegan parmesan cheese&apos;: 4, &apos;ahi&apos;: 1, &apos;golden brown sugar&apos;: 14, &apos;anjou pears&apos;: 3, &apos;granny smith apples&apos;: 6, &apos;crystallized ginger&apos;: 10, &apos;ice water&apos;: 7, &apos;vegetable shortening&apos;: 5, &apos;chinese five-spice powder&apos;: 1, &apos;whole wheat flour&apos;: 21, &apos;large egg yolks&apos;: 142, &apos;lean ground beef&apos;: 135, &apos;sausage links&apos;: 32, &apos;pork chops&apos;: 8, &apos;rosemary&apos;: 57, &apos;nectarines&apos;: 5, &apos;sweet cherries&apos;: 12, &apos;orange zest&apos;: 28, &apos;lavender buds&apos;: 1, &apos;apricot halves&apos;: 4, &apos;diced tomatoes in juice&apos;: 7, &apos;scallions&apos;: 61, &apos;italian salad dressing&apos;: 41, &apos;short pasta&apos;: 7, &apos;lemon wedge&apos;: 58, &apos;porterhouse steaks&apos;: 4, &apos;1% low-fat milk&apos;: 68, &apos;golden raisins&apos;: 46, &apos;cinnamon sticks&apos;: 18, &apos;broccolini&apos;: 3, &apos;salted butter&apos;: 11, &apos;chicken stock&apos;: 142, &apos;coffee granules&apos;: 12, &apos;lemon rind&apos;: 34, &apos;baby portobello mushrooms&apos;: 10, &apos;broccoli florets&apos;: 64, &apos;orecchiette&apos;: 41, &apos;melted butter&apos;: 24, &apos;pizza shells&apos;: 3, &apos;frozen mixed thawed vegetables,&apos;: 2, &apos;ragu old world style pasta sauc&apos;: 11, &apos;loosely packed fresh basil leaves&apos;: 21, &apos;whole wheat spaghetti&apos;: 22, &apos;butternut squash&apos;: 62, &apos;wonton wrappers&apos;: 17, &apos;meat&apos;: 15, &apos;pear tomatoes&apos;: 6, &apos;gaeta olives&apos;: 5, &apos;yukon gold potatoes&apos;: 39, &apos;turbot&apos;: 2, &apos;ground lamb&apos;: 6, &apos;refrigerated pizza dough&apos;: 31, &apos;potato gnocchi&apos;: 20, &apos;cream&apos;: 27, &apos;angel hair&apos;: 60, &apos;lime juice&apos;: 6, &apos;gelato&apos;: 4, &apos;cherry preserves&apos;: 2, &apos;amaretto liqueur&apos;: 4, &apos;cherries&apos;: 14, &apos;instant espresso powder&apos;: 22, &apos;nuts&apos;: 3, &apos;brown sugar&apos;: 67, &apos;pie shell&apos;: 2, &apos;marsala wine&apos;: 105, &apos;basil leaves&apos;: 153, &apos;cake&apos;: 7, &apos;crabmeat&apos;: 11, &apos;chopped fresh herbs&apos;: 8, &apos;button mushrooms&apos;: 35, &apos;escarole&apos;: 47, &apos;chopped pecans&apos;: 28, &apos;chocolate bars&apos;: 2, &apos;coffee liqueur&apos;: 11, &apos;flat anchovy&apos;: 5, &apos;italian loaf&apos;: 4, &apos;salad dressing&apos;: 23, &apos;pitted black olives&apos;: 24, &apos;rotini&apos;: 46, &apos;frozen mixed vegetables&apos;: 4, &apos;canned tomatoes&apos;: 26, &apos;feta cheese&apos;: 36, &apos;anise seed&apos;: 13, &apos;top sirloin&apos;: 1, &apos;candy&apos;: 3, &apos;angel food cake mix&apos;: 2, &apos;buttercream frosting&apos;: 1, &apos;sour cream&apos;: 71, &apos;shredded cheddar cheese&apos;: 42, &apos;cottage cheese&apos;: 44, &apos;noodles&apos;: 38, &apos;lime&apos;: 5, &apos;watermelon&apos;: 5, &apos;risotto&apos;: 11, &apos;ice cubes&apos;: 8, &apos;peeled tomatoes&apos;: 32, &apos;cooked vermicelli&apos;: 5, &apos;buns&apos;: 1, &apos;apple cider vinegar&apos;: 7, &apos;ground red pepper&apos;: 34, &apos;yellow bell pepper&apos;: 85, &apos;active dry yeast&apos;: 128, &apos;baking potatoes&apos;: 34, &apos;grated romano cheese&apos;: 19, &apos;quinoa&apos;: 8, &apos;littleneck clams&apos;: 26, &apos;whole wheat bread&apos;: 8, &apos;cream cheese, soften&apos;: 40, &apos;salad seasoning mix&apos;: 4, &apos;cucumber&apos;: 25, &apos;pappardelle pasta&apos;: 6, &apos;fresh mozzarella&apos;: 95, &apos;tuna steaks&apos;: 16, &apos;shredded zucchini&apos;: 4, &apos;dried pasta&apos;: 13, &apos;berries&apos;: 5, &apos;clams&apos;: 50, &apos;ground round&apos;: 25, &apos;fettuccine, cook and drain&apos;: 4, &apos;tomato purée&apos;: 47, &apos;chopped celery&apos;: 103, &apos;ditalini pasta&apos;: 15, &apos;lobster&apos;: 11, &apos;almonds&apos;: 35, &apos;anise&apos;: 13, &apos;anise extract&apos;: 12, &apos;brandy&apos;: 25, &apos;boneless chicken breast&apos;: 19, &apos;buffalo sauce&apos;: 2, &apos;blue cheese dressing&apos;: 3, &apos;cannelloni shells&apos;: 2, &apos;butter cooking spray&apos;: 3, &apos;light alfredo sauce&apos;: 7, &apos;rice&apos;: 12, &apos;cream of chicken soup&apos;: 9, &apos;chees fresh mozzarella&apos;: 72, &apos;shrimp tails&apos;: 2, &apos;bay scallops&apos;: 12, &apos;lump crab meat&apos;: 17, &apos;fish stock&apos;: 11, &apos;capellini&apos;: 8, &apos;veal&apos;: 17, &apos;beef stock&apos;: 24, &apos;chard&apos;: 3, &apos;grated Gruyère cheese&apos;: 5, &apos;panko&apos;: 25, &apos;chickpeas&apos;: 37, &apos;white cornmeal&apos;: 4, &apos;red bell pepper, sliced&apos;: 2, &apos;herbs&apos;: 20, &apos;chicken thighs&apos;: 32, &apos;green bell pepper, slice&apos;: 13, &apos;vanilla&apos;: 28, &apos;liqueur&apos;: 10, &apos;aged balsamic vinegar&apos;: 8, &apos;limoncello&apos;: 6, &apos;golden beets&apos;: 4, &apos;pizza doughs&apos;: 103, &apos;black cod&apos;: 2, &apos;green cabbage&apos;: 12, &apos;cavolo nero&apos;: 4, &apos;winter squash&apos;: 1, &apos;thin pizza crust&apos;: 9, &apos;toasted pine nuts&apos;: 30, &apos;grated parmesan romano&apos;: 2, &apos;cumin seed&apos;: 4, &apos;cilantro leaves&apos;: 7, &apos;seasoning salt&apos;: 15, &apos;mixed greens&apos;: 16, &apos;turkey breast cutlets&apos;: 9, &apos;cod fillets&apos;: 8, &apos;barilla&apos;: 4, &apos;linguini&apos;: 9, &apos;perciatelli&apos;: 7, &apos;crumbled blue cheese&apos;: 16, &apos;black mission figs&apos;: 4, &apos;swordfish steaks&apos;: 11, &apos;anchovy paste&apos;: 47, &apos;chuck&apos;: 8, &apos;tomatoes with juice&apos;: 30, &apos;store bought low sodium chicken stock&apos;: 2, &apos;fresh lavender&apos;: 1, &apos;grated orange&apos;: 31, &apos;vanilla wafers&apos;: 3, &apos;amaretto&apos;: 19, &apos;toasted almonds&apos;: 6, &apos;light corn syrup&apos;: 8, &apos;focaccia&apos;: 10, &apos;oyster mushrooms&apos;: 6, &apos;shiitake mushroom caps&apos;: 16, &apos;onion powder&apos;: 39, &apos;sourdough&apos;: 7, &apos;orange bell pepper&apos;: 27, &apos;nonfat cottage cheese&apos;: 6, &apos;stewed tomatoes&apos;: 37, &apos;raspberries&apos;: 25, &apos;vanilla beans&apos;: 26, &apos;Frangelico&apos;: 9, &apos;vegetable oil spray&apos;: 22, &apos;table salt&apos;: 21, &apos;white peppercorns&apos;: 2, &apos;herb vinegar&apos;: 3, &apos;reduced fat sharp cheddar cheese&apos;: 8, &apos;deli ham&apos;: 4, &apos;ground turkey&apos;: 48, &apos;hot dogs&apos;: 2, &apos;italian style stewed tomatoes&apos;: 15, &apos;veal stock&apos;: 7, &apos;portabello mushroom&apos;: 41, &apos;rocket leaves&apos;: 35, &apos;country bread&apos;: 26, &apos;bottled balsamic vinaigrette&apos;: 1, &apos;scallops&apos;: 12, &apos;italian tomatoes&apos;: 16, &apos;peeled shrimp&apos;: 3, &apos;whole wheat pizza crust&apos;: 3, &apos;dried mixed herbs&apos;: 3, &apos;whole wheat pastry flour&apos;: 6, &apos;nonstick spray&apos;: 7, &apos;low-fat sour cream&apos;: 8, &apos;day old bread&apos;: 2, &apos;champagne vinegar&apos;: 9, &apos;pizza sauce&apos;: 71, &apos;red vermouth&apos;: 5, &apos;low sodium chicken stock&apos;: 6, &apos;peppercorns&apos;: 3, &apos;turkey stock&apos;: 3, &apos;cooked turkey&apos;: 6, &apos;green olives&apos;: 52, &apos;spaghettini&apos;: 23, &apos;minced onion&apos;: 41, &apos;beef broth&apos;: 62, &apos;dried mint flakes&apos;: 3, &apos;ravioli&apos;: 16, &apos;meat loaf mix&apos;: 1, &apos;rub&apos;: 2, &apos;clam juice&apos;: 26, &apos;bottled clam juice&apos;: 15, &apos;cayenne pepper&apos;: 50, &apos;pears&apos;: 19, &apos;gorgonzola&apos;: 27, &apos;duck&apos;: 2, &apos;pure vanilla extract&apos;: 22, &apos;panettone&apos;: 4, &apos;sambuca&apos;: 4, &apos;light brown sugar&apos;: 15, &apos;fat free frozen top whip&apos;: 5, &apos;frozen bread dough&apos;: 9, &apos;whole wheat breadcrumbs&apos;: 10, &apos;ground chuck&apos;: 16, &apos;bows&apos;: 3, &apos;sauce&apos;: 53, &apos;ground oregano&apos;: 4, &apos;lean ground turkey&apos;: 2, &apos;milk chocolate&apos;: 11, &apos;hazelnut butter&apos;: 4, &apos;fig jam&apos;: 2, &apos;crackers&apos;: 7, &apos;sun-dried tomatoes in oil&apos;: 28, &apos;dark chocolate&apos;: 7, &apos;vanilla ice cream&apos;: 17, &apos;chives&apos;: 31, &apos;radishes&apos;: 13, &apos;lettuce&apos;: 6, &apos;grilled chicken&apos;: 2, &apos;flatbread&apos;: 4, &apos;fresh parsley leaves&apos;: 56, &apos;lemon extract&apos;: 7, &apos;salad greens&apos;: 11, &apos;brown rice&apos;: 6, &apos;tortellini&apos;: 19, &apos;reduced fat alfredo sauce&apos;: 4, &apos;fresh asparagus&apos;: 24, &apos;round steaks&apos;: 5, &apos;chili powder&apos;: 26, &apos;ground cumin&apos;: 30, &apos;rolls&apos;: 21, &apos;prego traditional italian sauce&apos;: 4, &apos;brown hash potato&apos;: 5, &apos;nonfat milk&apos;: 7, &apos;frozen cheese ravioli&apos;: 6, &apos;mild Italian sausage&apos;: 17, &apos;sandwich rolls&apos;: 3, &apos;orange juice concentrate&apos;: 5, &apos;rotelle&apos;: 3, &apos;sweet italian sausag links, cut into&apos;: 2, &apos;idaho potatoes&apos;: 4, &apos;popcorn&apos;: 1, &apos;grated orange peel&apos;: 26, &apos;french fried onions&apos;: 1, &apos;turbinado&apos;: 7, &apos;cooked chicken breasts&apos;: 16, &apos;farro&apos;: 14, &apos;flour tortillas&apos;: 11, &apos;english cucumber&apos;: 7, &apos;minced peperoncini&apos;: 2, &apos;teleme&apos;: 2, &apos;brine cured green olives&apos;: 4, &apos;black forest ham&apos;: 1, &apos;frozen whole kernel corn&apos;: 5, &apos;sweet potatoes&apos;: 15, &apos;applewood smoked bacon&apos;: 7, &apos;panko breadcrumbs&apos;: 24, &apos;coarse kosher salt&apos;: 9, &apos;cornmeal&apos;: 72, &apos;dried sage&apos;: 28, &apos;dri leav thyme&apos;: 6, &apos;low sodium beef broth&apos;: 3, &apos;cabbage&apos;: 12, &apos;fresh shiitake mushrooms&apos;: 15, &apos;rabbit&apos;: 8, &apos;herbes de provence&apos;: 12, &apos;dressing&apos;: 7, &apos;chicken fingers&apos;: 3, &apos;reduced-fat sour cream&apos;: 13, &apos;dried fettuccine&apos;: 14, &apos;white pepper&apos;: 22, &apos;albacore tuna in water&apos;: 5, &apos;light mayonnaise&apos;: 7, &apos;turkey tenderloins&apos;: 4, &apos;almond flour&apos;: 8, &apos;raw almond&apos;: 3, &apos;fresh orange juice&apos;: 32, &apos;cranberries&apos;: 3, &apos;orange marmalade&apos;: 6, &apos;fresh lemon&apos;: 4, &apos;condensed chicken broth&apos;: 1, &apos;oil packed anchovy fillets&apos;: 3, &apos;boneless chicken skinless thigh&apos;: 20, &apos;sugar pea&apos;: 16, &apos;nonfat yogurt&apos;: 4, &apos;roast red peppers, drain&apos;: 24, &apos;french baguette&apos;: 25, &apos;ripe olives&apos;: 26, &apos;honey glazed ham&apos;: 2, &apos;chiles&apos;: 9, &apos;spring onions&apos;: 3, &apos;candied orange peel&apos;: 13, &apos;lard&apos;: 5, &apos;cinnamon&apos;: 23, &apos;semolina flour&apos;: 32, &apos;onion salt&apos;: 2, &apos;beef demi-glace&apos;: 1, &apos;veal shanks&apos;: 21, &apos;orange peel&apos;: 7, &apos;lemon peel&apos;: 11, &apos;plain yogurt&apos;: 7, &apos;Quinoa Flour&apos;: 1, &apos;spelt flour&apos;: 2, &apos;plums&apos;: 14, &apos;heirloom tomatoes&apos;: 21, &apos;fresh lime juice&apos;: 25, &apos;artichok heart marin&apos;: 33, &apos;bucatini&apos;: 16, &apos;processed cheese&apos;: 5, &apos;egg noodles, cooked and drained&apos;: 3, &apos;tapioca flour&apos;: 1, &apos;lasagna noodles, cooked and drained&apos;: 26, &apos;firm tofu&apos;: 9, &apos;sherry vinegar&apos;: 27, &apos;country style bread&apos;: 7, &apos;teardrop tomatoes&apos;: 2, &apos;ground sausage&apos;: 5, &apos;pita pockets&apos;: 1, &apos;orange juice&apos;: 19, &apos;chili pepper&apos;: 5, &apos;currant&apos;: 7, &apos;small capers, rins and drain&apos;: 2, &apos;filet&apos;: 4, &apos;lettuce leaves&apos;: 8, &apos;vodka&apos;: 27, &apos;stolichnaya&apos;: 1, &apos;romaine lettuce&apos;: 33, &apos;croutons&apos;: 18, &apos;pepperocini&apos;: 2, &apos;cherry peppers&apos;: 3, &apos;dandelion&apos;: 1, &apos;beans&apos;: 6, &apos;hot pepper sauce&apos;: 7, &apos;shredded Monterey Jack cheese&apos;: 13, &apos;cane sugar&apos;: 1, &apos;mixed nuts&apos;: 1, &apos;meatballs&apos;: 17, &apos;plain dry bread crumb&apos;: 11, &apos;spanish onion&apos;: 9, &apos;cuban peppers&apos;: 3, &apos;green tomatoes&apos;: 2, &apos;sesame seeds&apos;: 13, &apos;boneless beef chuck roast&apos;: 3, &apos;hard-boiled egg&apos;: 6, &apos;pork tenderloin&apos;: 24, &apos;bulk italian sausag&apos;: 21, &apos;beef bouillon granules&apos;: 3, &apos;prebaked pizza crusts&apos;: 13, &apos;buttermilk&apos;: 32, &apos;flaked coconut&apos;: 5, &apos;lower sodium chicken broth&apos;: 20, &apos;carnaroli rice&apos;: 13, &apos;fresh oregano leaves&apos;: 24, &apos;cavatappi&apos;: 9, &apos;cooking oil&apos;: 24, &apos;cayenne&apos;: 8, &apos;ground cloves&apos;: 30, &apos;corn syrup&apos;: 4, &apos;small pasta&apos;: 7, &apos;ground fennel&apos;: 7, &apos;low-fat buttermilk&apos;: 9, &apos;ice&apos;: 7, &apos;chicken bouillon granules&apos;: 6, &apos;poultry seasoning&apos;: 10, &apos;roast&apos;: 3, &apos;fusilli&apos;: 43, &apos;Italian herbs&apos;: 7, &apos;diced yellow onion&apos;: 1, &apos;manicotti pasta&apos;: 4, &apos;parsley flakes&apos;: 17, &apos;vinaigrette&apos;: 7, &apos;bread ciabatta&apos;: 16, &apos;kidney beans&apos;: 12, &apos;creole seasoning&apos;: 4, &apos;prepared pasta sauce&apos;: 5, &apos;small curd cottage cheese&apos;: 14, &apos;white sandwich bread&apos;: 9, &apos;mini chocolate chips&apos;: 5, &apos;lean beef&apos;: 2, &apos;breadstick&apos;: 13, &apos;pickled okra&apos;: 2, &apos;fronds&apos;: 15, &apos;thick-cut bacon&apos;: 11, &apos;boiling potatoes&apos;: 13, &apos;ditalini&apos;: 11, &apos;cranberry beans&apos;: 4, &apos;center cut bacon&apos;: 7, &apos;roasting chickens&apos;: 8, &apos;fleur de sel&apos;: 6, &apos;Margherita Pepperoni&apos;: 1, &apos;soft-shell clams&apos;: 1, &apos;liquid&apos;: 5, &apos;fresh chevre&apos;: 4, &apos;pork sausages&apos;: 14, &apos;dried minced onion&apos;: 5, &apos;msg&apos;: 2, &apos;pork stew meat&apos;: 1, &apos;beef stew meat&apos;: 3, &apos;ziti&apos;: 26, &apos;Balsamico Bianco&apos;: 2, &apos;ground mustard&apos;: 1, ...}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line"><span class="comment"># Finally, plot the 10 most used ingredients</span></span><br><span class="line">fig = pd.DataFrame(italian_ingredients, index=[<span class="number">0</span>]).transpose()[<span class="number">0</span>].sort_values(ascending=<span class="keyword">False</span>, inplace=<span class="keyword">False</span>)[:<span class="number">10</span>].plot(kind=<span class="string">'barh'</span>)</span><br><span class="line">fig.invert_yaxis()</span><br><span class="line">fig = fig.get_figure()</span><br><span class="line">fig.tight_layout()</span><br><span class="line">fig.show()</span><br></pre></td></tr></table></figure><pre><code>/opt/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:448: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.  % get_backend())</code></pre><p><img src="/2019/02/15/PredictYourCuisine/output_17_1.png" alt="png"></p><p>若想要对数据分析做更深入的了解，可以参考<a href="https://cn.udacity.com/dand" target="_blank" rel="noopener">数据分析师入门课程</a>或者<a href="https://www.udacity.com/legal/ai-programming" target="_blank" rel="noopener">基于Python语言的人工智能Nano课程</a>.</p><hr><h2 id="第三步-建立模型"><a href="#第三步-建立模型" class="headerlink" title="第三步. 建立模型"></a>第三步. 建立模型</h2><p>在项目的第三步中，你需要了解必要的工具和技巧来让你的模型进行预测。用这些工具和技巧对每一个模型的表现做精确的衡量可以极大地增强你预测的信心。</p><h3 id="3-1-单词清洗"><a href="#3-1-单词清洗" class="headerlink" title="3.1 单词清洗"></a>3.1 单词清洗</h3><p>由于菜品包含的佐料众多，同一种佐料也可能有单复数、时态等变化，为了去除这类差异，我们考虑将<strong>ingredients</strong> 进行过滤</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_clean</span><span class="params">(ingredients)</span>:</span></span><br><span class="line">    <span class="comment">#去除单词的标点符号，只保留 a..z A...Z的单词字符</span></span><br><span class="line">    ingredients= np.array(ingredients).tolist()</span><br><span class="line">    print(<span class="string">"菜品佐料：\n&#123;&#125;"</span>.format(ingredients[<span class="number">9</span>]))</span><br><span class="line">    ingredients=[[re.sub(<span class="string">'[^A-Za-z]'</span>, <span class="string">' '</span>, word) <span class="keyword">for</span> word <span class="keyword">in</span> component]<span class="keyword">for</span> component <span class="keyword">in</span> ingredients]</span><br><span class="line">    print(<span class="string">"去除标点符号之后的结果：\n&#123;&#125;"</span>.format(ingredients[<span class="number">9</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 去除单词的单复数，时态，只保留单词的词干</span></span><br><span class="line">    lemma=WordNetLemmatizer()</span><br><span class="line">    ingredients=[<span class="string">" "</span>.join([ <span class="string">" "</span>.join([lemma.lemmatize(w) <span class="keyword">for</span> w <span class="keyword">in</span> words.split(<span class="string">" "</span>)]) <span class="keyword">for</span> words <span class="keyword">in</span> component])  <span class="keyword">for</span> component <span class="keyword">in</span> ingredients]</span><br><span class="line">    print(<span class="string">"去除时态和单复数之后的结果：\n&#123;&#125;"</span>.format(ingredients[<span class="number">9</span>]))</span><br><span class="line">    <span class="keyword">return</span> ingredients</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\n处理训练集..."</span>)</span><br><span class="line">train_ingredients = text_clean(train_content[<span class="string">'ingredients'</span>])</span><br><span class="line">print(<span class="string">"\n处理测试集..."</span>)</span><br><span class="line">test_ingredients = text_clean(test_content[<span class="string">'ingredients'</span>])</span><br></pre></td></tr></table></figure><pre><code>[nltk_data] Downloading package wordnet to[nltk_data]     /Users/jindongwang/nltk_data...[nltk_data]   Unzipping corpora/wordnet.zip.处理训练集...菜品佐料：[&apos;chopped tomatoes&apos;, &apos;fresh basil&apos;, &apos;garlic&apos;, &apos;extra-virgin olive oil&apos;, &apos;kosher salt&apos;, &apos;flat leaf parsley&apos;]去除标点符号之后的结果：[&apos;chopped tomatoes&apos;, &apos;fresh basil&apos;, &apos;garlic&apos;, &apos;extra virgin olive oil&apos;, &apos;kosher salt&apos;, &apos;flat leaf parsley&apos;]去除时态和单复数之后的结果：chopped tomato fresh basil garlic extra virgin olive oil kosher salt flat leaf parsley处理测试集...菜品佐料：[&apos;eggs&apos;, &apos;cherries&apos;, &apos;dates&apos;, &apos;dark muscovado sugar&apos;, &apos;ground cinnamon&apos;, &apos;mixed spice&apos;, &apos;cake&apos;, &apos;vanilla extract&apos;, &apos;self raising flour&apos;, &apos;sultana&apos;, &apos;rum&apos;, &apos;raisins&apos;, &apos;prunes&apos;, &apos;glace cherries&apos;, &apos;butter&apos;, &apos;port&apos;]去除标点符号之后的结果：[&apos;eggs&apos;, &apos;cherries&apos;, &apos;dates&apos;, &apos;dark muscovado sugar&apos;, &apos;ground cinnamon&apos;, &apos;mixed spice&apos;, &apos;cake&apos;, &apos;vanilla extract&apos;, &apos;self raising flour&apos;, &apos;sultana&apos;, &apos;rum&apos;, &apos;raisins&apos;, &apos;prunes&apos;, &apos;glace cherries&apos;, &apos;butter&apos;, &apos;port&apos;]去除时态和单复数之后的结果：egg cherry date dark muscovado sugar ground cinnamon mixed spice cake vanilla extract self raising flour sultana rum raisin prune glace cherry butter port</code></pre><h3 id="3-2-特征提取"><a href="#3-2-特征提取" class="headerlink" title="3.2 特征提取"></a>3.2 特征提取</h3><p>在该步骤中，我们将菜品的佐料转换成数值特征向量。考虑到绝大多数菜中都包含<code>salt, water, sugar, butter</code>等，采用one-hot的方法提取的向量将不能很好的对菜系作出区分。我们将考虑按照佐料出现的次数对佐料做一定的加权，即：佐料出现次数越多，佐料的区分性就越低。我们采用的特征为TF-IDF，相关介绍内容可以参考：<a href="http://www.ruanyifeng.com/blog/2013/03/tf-idf.html" target="_blank" rel="noopener">TF-IDF与余弦相似性的应用（一）：自动提取关键词</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="comment"># 将佐料转换成特征向量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理 训练集</span></span><br><span class="line">vectorizer = TfidfVectorizer(stop_words=<span class="string">'english'</span>, ngram_range=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                analyzer=<span class="string">'word'</span>, max_df=<span class="number">.57</span>, binary=<span class="keyword">False</span>,</span><br><span class="line">                token_pattern=<span class="string">r"\w+"</span>,sublinear_tf=<span class="keyword">False</span>)</span><br><span class="line">train_tfidf = vectorizer.fit_transform(train_ingredients).todense()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 处理 测试集</span></span><br><span class="line">test_tfidf = vectorizer.transform(test_ingredients)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line">train_targets=np.array(train_content[<span class="string">'cuisine'</span>]).tolist()</span><br><span class="line">train_targets[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><pre><code>[&apos;greek&apos;, &apos;southern_us&apos;, &apos;filipino&apos;, &apos;indian&apos;, &apos;indian&apos;, &apos;jamaican&apos;, &apos;spanish&apos;, &apos;italian&apos;, &apos;mexican&apos;, &apos;italian&apos;]</code></pre><h3 id="编程练习-1"><a href="#编程练习-1" class="headerlink" title="编程练习"></a>编程练习</h3><p>这里我们为了防止前面步骤中累积的错误，导致以下步骤无法正常运行。我们在此检查处理完的实验数据是否正确，请打印<code>train_tfidf</code>和<code>train_targets</code>中前五个数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 你需要预览训练集train_tfidf,train_targets中前5条数据，试试Python的切片语法</span></span><br><span class="line">display(train_tfidf[:<span class="number">5</span>])</span><br><span class="line">display(train_targets[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>matrix([[0., 0., 0., ..., 0., 0., 0.],        [0., 0., 0., ..., 0., 0., 0.],        [0., 0., 0., ..., 0., 0., 0.],        [0., 0., 0., ..., 0., 0., 0.],        [0., 0., 0., ..., 0., 0., 0.]])[&apos;greek&apos;, &apos;southern_us&apos;, &apos;filipino&apos;, &apos;indian&apos;, &apos;indian&apos;]</code></pre><h3 id="3-3-验证集划分"><a href="#3-3-验证集划分" class="headerlink" title="3.3 验证集划分"></a>3.3 验证集划分</h3><p>为了在实验中大致估计模型的精确度我们将从原本的<code>train_ingredients</code> 划分出 <code>20%</code> 的数据用作<code>valid_ingredients</code>。</p><h3 id="编程练习：数据分割与重排"><a href="#编程练习：数据分割与重排" class="headerlink" title="编程练习：数据分割与重排"></a>编程练习：数据分割与重排</h3><p>调用<code>train_test_split</code>函数将训练集划分为新的训练集和验证集，便于之后的模型精度观测。</p><ul><li>从<code>sklearn.model_selection</code>中导入<code>train_test_split</code></li><li>将<code>train_tfidf</code>和<code>train_targets</code>作为<code>train_test_split</code>的输入变量</li><li>设置<code>test_size</code>为0.2，划分出20%的验证集，80%的数据留作新的训练集。</li><li>设置<code>random_state</code>随机种子，以确保每一次运行都可以得到相同划分的结果。（随机种子固定，生成的随机序列就是确定的）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### TODO：划分出验证集</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train , X_valid , y_train, y_valid = train_test_split(train_tfidf, train_targets, test_size=<span class="number">0.2</span>, random_state=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="3-2-建立模型"><a href="#3-2-建立模型" class="headerlink" title="3.2 建立模型"></a>3.2 建立模型</h3><p>调用 <code>sklearn</code> 中的逻辑回归模型（Logistic Regression）。</p><h3 id="编程练习：训练模型"><a href="#编程练习：训练模型" class="headerlink" title="编程练习：训练模型"></a>编程练习：训练模型</h3><ul><li>从<code>sklearn.linear_model</code>导入<code>LogisticRegression</code></li><li>从<code>sklearn.model_selection</code>导入<code>GridSearchCV</code>, 参数自动搜索，只要把参数输进去，就能给出最优的结果和参数，这个方法适合小数据集。</li><li>定义<code>parameters</code>变量：为<code>C</code>参数创造一个字典，它的值是从1至10的数组;</li><li>定义<code>classifier</code>变量: 使用导入的<code>LogisticRegression</code>创建一个分类函数;</li><li>定义<code>grid</code>变量: 使用导入的<code>GridSearchCV</code>创建一个网格搜索对象；将变量’classifier’, ‘parameters’作为参数传至这个对象构造函数中；</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line"><span class="comment">## <span class="doctag">TODO:</span> 建立逻辑回归模型</span></span><br><span class="line">parameters = &#123;<span class="string">'C'</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]&#125;</span><br><span class="line"></span><br><span class="line">classifier = LogisticRegression()</span><br><span class="line"></span><br><span class="line">grid = GridSearchCV(classifier, parameters)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line">grid = grid.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><p>模型训练结束之后，我们计算模型在验证集<code>X_valid</code>上预测结果，并计算模型的预测精度（与<code>y_valid</code>逐个比较）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score <span class="comment">## 计算模型的准确率</span></span><br><span class="line"></span><br><span class="line">valid_predict = grid.predict(X_valid)</span><br><span class="line">valid_score=accuracy_score(y_valid,valid_predict)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"验证集上的得分为：&#123;&#125;"</span>.format(valid_score))</span><br></pre></td></tr></table></figure><pre><code>验证集上的得分为：0.7912005028284098</code></pre><hr><h2 id="第四步-模型预测（可选）"><a href="#第四步-模型预测（可选）" class="headerlink" title="第四步. 模型预测（可选）"></a>第四步. 模型预测（可选）</h2><h3 id="4-1-预测测试集"><a href="#4-1-预测测试集" class="headerlink" title="4.1 预测测试集"></a>4.1 预测测试集</h3><h3 id="编程练习-2"><a href="#编程练习-2" class="headerlink" title="编程练习"></a>编程练习</h3><ul><li>将模型<code>grid</code>对测试集<code>test_tfidf</code>做预测，然后查看预测结果。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### TODO：预测测试结果</span></span><br><span class="line">predictions = grid.predict(test_tfidf)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 请不要修改下方代码</span></span><br><span class="line">print(<span class="string">"预测的测试集个数为：&#123;&#125;"</span>.format(len(predictions)))</span><br><span class="line">test_content[<span class="string">'cuisine'</span>]=predictions</span><br><span class="line">test_content.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>预测的测试集个数为：9944</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>ingredients</th>      <th>cuisine</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>18009</td>      <td>[baking powder, eggs, all-purpose flour, raisins, milk, white sugar]</td>      <td>british</td>    </tr>    <tr>      <th>1</th>      <td>28583</td>      <td>[sugar, egg yolks, corn starch, cream of tartar, bananas, vanilla wafers, milk, vanilla extract, toasted pecans, egg…</td>      <td>southern_us</td>    </tr>    <tr>      <th>2</th>      <td>41580</td>      <td>[sausage links, fennel bulb, fronds, olive oil, cuban peppers, onions]</td>      <td>italian</td>    </tr>    <tr>      <th>3</th>      <td>29752</td>      <td>[meat cuts, file powder, smoked sausage, okra, shrimp, andouille sausage, water, paprika, hot sauce, garlic cloves, …</td>      <td>cajun_creole</td>    </tr>    <tr>      <th>4</th>      <td>35687</td>      <td>[ground black pepper, salt, sausage casings, leeks, parmigiano reggiano cheese, cornmeal, water, extra-virgin olive …</td>      <td>italian</td>    </tr>    <tr>      <th>5</th>      <td>38527</td>      <td>[baking powder, all-purpose flour, peach slices, corn starch, heavy cream, lemon juice, unsalted butter, salt, white…</td>      <td>southern_us</td>    </tr>    <tr>      <th>6</th>      <td>19666</td>      <td>[grape juice, orange, white zinfandel]</td>      <td>french</td>    </tr>    <tr>      <th>7</th>      <td>41217</td>      <td>[ground ginger, white pepper, green onions, orange juice, sugar, Sriracha, vegetable oil, orange zest, chicken broth…</td>      <td>chinese</td>    </tr>    <tr>      <th>8</th>      <td>28753</td>      <td>[diced onions, taco seasoning mix, all-purpose flour, chopped cilantro fresh, ground cumin, ground cinnamon, vegetab…</td>      <td>mexican</td>    </tr>    <tr>      <th>9</th>      <td>22659</td>      <td>[eggs, cherries, dates, dark muscovado sugar, ground cinnamon, mixed spice, cake, vanilla extract, self raising flou…</td>      <td>british</td>    </tr>  </tbody></table></div><h3 id="4-2-提交结果"><a href="#4-2-提交结果" class="headerlink" title="4.2 提交结果"></a>4.2 提交结果</h3><p>为了更好的测试模型的效果，同时比较与其他人的差距，我们将模型的测试集上的结果提交至 <a href="https://www.kaggle.com/c/whats-cooking/submit" target="_blank" rel="noopener">kaggle What’s Cooking?</a> （需要提前注册kaggle账号）。</p><p><strong>注意</strong>：在提交作业时，请将提交排名得分截图，附在压缩包中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 加载结果格式</span></span><br><span class="line">submit_frame = pd.read_csv(<span class="string">"sample_submission.csv"</span>)</span><br><span class="line"><span class="comment">## 保存结果</span></span><br><span class="line">result = pd.merge(submit_frame, test_content, on=<span class="string">"id"</span>, how=<span class="string">'left'</span>)</span><br><span class="line">result = result.rename(index=str, columns=&#123;<span class="string">"cuisine_y"</span>: <span class="string">"cuisine"</span>&#125;)</span><br><span class="line">test_result_name = <span class="string">"tfidf_cuisine_test.csv"</span></span><br><span class="line">result[[<span class="string">'id'</span>,<span class="string">'cuisine'</span>]].to_csv(test_result_name,index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p>将生成的 <strong>tfidf_cuisine_test.csv</strong> 提交至 <a href="https://www.kaggle.com/c/whats-cooking/submit" target="_blank" rel="noopener">https://www.kaggle.com/c/whats-cooking/submit</a> 然后选择 <strong>Upload Submission File</strong>, 点击 <strong>Make submission</strong>即可。稍作等待，就可以看到右上角的评分结果（得分大致为：<code>0.78580</code> 左右）。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;机器学习工程师纳米学位（试学班）&quot;&gt;&lt;a href=&quot;#机器学习工程师纳米学位（试学班）&quot; class=&quot;headerlink&quot; title=&quot;机器学习工程师纳米学位（试学班）&quot;&gt;&lt;/a&gt;机器学习工程师纳米学位（试学班）&lt;/h1&gt;&lt;h2 id=&quot;项目-0-预测你的
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Scorecard</title>
    <link href="http://yoursite.com/2019/02/15/Scorecard/"/>
    <id>http://yoursite.com/2019/02/15/Scorecard/</id>
    <published>2019-02-15T10:40:26.000Z</published>
    <updated>2019-02-15T10:42:13.539Z</updated>
    
    <content type="html"><![CDATA[<h4 id="信用评分卡模型是一种比较成熟的预测方法，广泛应用于信用风险评估以及金融风险控制等领域，其基本原理是：将模型变量以WOE编码方式离散化之后运用-logistic-回归模型进行二分类变量的拟合及预测。"><a href="#信用评分卡模型是一种比较成熟的预测方法，广泛应用于信用风险评估以及金融风险控制等领域，其基本原理是：将模型变量以WOE编码方式离散化之后运用-logistic-回归模型进行二分类变量的拟合及预测。" class="headerlink" title="信用评分卡模型是一种比较成熟的预测方法，广泛应用于信用风险评估以及金融风险控制等领域，其基本原理是：将模型变量以WOE编码方式离散化之后运用 logistic 回归模型进行二分类变量的拟合及预测。"></a>信用评分卡模型是一种比较成熟的预测方法，广泛应用于信用风险评估以及金融风险控制等领域，其基本原理是：将模型变量以WOE编码方式离散化之后运用 logistic 回归模型进行二分类变量的拟合及预测。</h4><p>信用评分卡一般可以分为申请评分卡、行为评分卡、催收评分卡等。本文主要讲述申请评分卡模型的建模分析过程。主要分以下几个步骤：</p><ol><li>目标定义</li><li>数据获取</li><li>数据预处理</li><li>模型开发</li><li>模型评估</li><li>评分系统建立</li></ol><h3 id="1-目标定义"><a href="#1-目标定义" class="headerlink" title="1. 目标定义"></a>1. 目标定义</h3><p>数据来源kaggle project: ‘give-me-some-credit-dataset’，找出关键的特征变量，建立信用评分模型</p><h3 id="2-数据获取"><a href="#2-数据获取" class="headerlink" title="2. 数据获取"></a>2. 数据获取</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入必要的库包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#读取数据</span></span><br><span class="line">data=pd.read_csv(<span class="string">'cs-training.csv'</span>)</span><br><span class="line">data=data.drop(axis=<span class="number">1</span>, columns=[data.columns[<span class="number">0</span>]])</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>SeriousDlqin2yrs</th>      <th>RevolvingUtilizationOfUnsecuredLines</th>      <th>age</th>      <th>NumberOfTime30-59DaysPastDueNotWorse</th>      <th>DebtRatio</th>      <th>MonthlyIncome</th>      <th>NumberOfOpenCreditLinesAndLoans</th>      <th>NumberOfTimes90DaysLate</th>      <th>NumberRealEstateLoansOrLines</th>      <th>NumberOfTime60-89DaysPastDueNotWorse</th>      <th>NumberOfDependents</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>0.766127</td>      <td>45</td>      <td>2</td>      <td>0.802982</td>      <td>9120.0</td>      <td>13</td>      <td>0</td>      <td>6</td>      <td>0</td>      <td>2.0</td>    </tr>    <tr>      <th>1</th>      <td>0</td>      <td>0.957151</td>      <td>40</td>      <td>0</td>      <td>0.121876</td>      <td>2600.0</td>      <td>4</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>1.0</td>    </tr>    <tr>      <th>2</th>      <td>0</td>      <td>0.658180</td>      <td>38</td>      <td>1</td>      <td>0.085113</td>      <td>3042.0</td>      <td>2</td>      <td>1</td>      <td>0</td>      <td>0</td>      <td>0.0</td>    </tr>    <tr>      <th>3</th>      <td>0</td>      <td>0.233810</td>      <td>30</td>      <td>0</td>      <td>0.036050</td>      <td>3300.0</td>      <td>5</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0.0</td>    </tr>    <tr>      <th>4</th>      <td>0</td>      <td>0.907239</td>      <td>49</td>      <td>1</td>      <td>0.024926</td>      <td>63588.0</td>      <td>7</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0.0</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.info()</span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 150000 entries, 0 to 149999Data columns (total 11 columns):SeriousDlqin2yrs                        150000 non-null int64RevolvingUtilizationOfUnsecuredLines    150000 non-null float64age                                     150000 non-null int64NumberOfTime30-59DaysPastDueNotWorse    150000 non-null int64DebtRatio                               150000 non-null float64MonthlyIncome                           120269 non-null float64NumberOfOpenCreditLinesAndLoans         150000 non-null int64NumberOfTimes90DaysLate                 150000 non-null int64NumberRealEstateLoansOrLines            150000 non-null int64NumberOfTime60-89DaysPastDueNotWorse    150000 non-null int64NumberOfDependents                      146076 non-null float64dtypes: float64(4), int64(7)memory usage: 12.6 MB</code></pre><h3 id="3-数据预处理"><a href="#3-数据预处理" class="headerlink" title="3. 数据预处理"></a>3. 数据预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#缺失值处理 </span></span><br><span class="line"><span class="comment">#随机森林法填补MonthlyIncome</span></span><br><span class="line">MI_df=data.iloc[:,<span class="number">0</span>:<span class="number">10</span>]</span><br><span class="line">MI_known=MI_df.loc[MI_df[<span class="string">'MonthlyIncome'</span>].notnull()]</span><br><span class="line">MI_unknown=MI_df.loc[MI_df[<span class="string">'MonthlyIncome'</span>].isnull()]</span><br><span class="line">X_known=MI_known.drop(<span class="string">'MonthlyIncome'</span>,axis=<span class="number">1</span>)</span><br><span class="line">y_known=MI_known[<span class="string">'MonthlyIncome'</span>]</span><br><span class="line">X_unknown=MI_unknown.drop(<span class="string">'MonthlyIncome'</span>,axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">rfr=RandomForestRegressor(random_state=<span class="number">0</span>, n_estimators=<span class="number">100</span>, max_depth=<span class="number">3</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line">rfr.fit(X_known, y_known)</span><br><span class="line"></span><br><span class="line">data.loc[MI_df[<span class="string">'MonthlyIncome'</span>].isnull(), <span class="string">'MonthlyIncome'</span>]=rfr.predict(X_unknown).round(<span class="number">0</span>)</span><br><span class="line">print(<span class="string">'Done'</span>)</span><br></pre></td></tr></table></figure><pre><code>Done</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#NumberOfDependents 缺失值较少，可以直接删除</span></span><br><span class="line">data.dropna(inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment">#去除重复值</span></span><br><span class="line">data.drop_duplicates(inplace=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#异常值处理</span></span><br><span class="line">sns.boxplot(y=<span class="string">'age'</span>, data=data)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x10e034cc0&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/20190215183554743.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data=data[((data[<span class="string">'age'</span>]&gt;<span class="number">0</span>) &amp; (data[<span class="string">'age'</span>]&lt;<span class="number">100</span>))]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#逾期次数</span></span><br><span class="line">sns.boxplot(data=data[[<span class="string">'NumberOfTime30-59DaysPastDueNotWorse'</span>,<span class="string">'NumberOfTimes90DaysLate'</span>,<span class="string">'NumberOfTime60-89DaysPastDueNotWorse'</span>]], palette=<span class="string">'Set2'</span>)</span><br><span class="line">plt.xticks(rotation=<span class="number">20</span>)</span><br></pre></td></tr></table></figure><pre><code>(array([0, 1, 2]), &lt;a list of 3 Text xticklabel objects&gt;)</code></pre><p><img src="https://img-blog.csdnimg.cn/2019021518370423.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data=data[data[<span class="string">'NumberOfTime30-59DaysPastDueNotWorse'</span>]&lt;<span class="number">80</span>]</span><br><span class="line"><span class="comment">#再次检查异常点</span></span><br><span class="line">sns.boxplot(data=data[[<span class="string">'NumberOfTime30-59DaysPastDueNotWorse'</span>,<span class="string">'NumberOfTimes90DaysLate'</span>,<span class="string">'NumberOfTime60-89DaysPastDueNotWorse'</span>]], palette=<span class="string">'Set2'</span>)</span><br><span class="line">plt.xticks(rotation=<span class="number">20</span>)</span><br></pre></td></tr></table></figure><pre><code>(array([0, 1, 2]), &lt;a list of 3 Text xticklabel objects&gt;)</code></pre><p><img src="https://img-blog.csdnimg.cn/20190215183720775.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#月收入和年龄变量的分布</span></span><br><span class="line">sns.boxplot(x=<span class="string">'MonthlyIncome'</span>, data=data)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x10f596b70&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/2019021518373271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'%.2f%% of customers monthly income under 40000.'</span> %(data.loc[data[<span class="string">'MonthlyIncome'</span>]&lt;=<span class="number">40000</span>].shape[<span class="number">0</span>]*<span class="number">100</span>/data.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="comment">#月收入绝大部分集中在40000以下，可画出对应的月收入的分布</span></span><br><span class="line">sns.distplot(data.loc[data[<span class="string">'MonthlyIncome'</span>]&lt;=<span class="number">40000</span>,<span class="string">'MonthlyIncome'</span>], bins=<span class="number">80</span>, label=<span class="string">'MonthlyIncome dist'</span>, kde=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><pre><code>99.69% of customers monthly income under 40000.&lt;matplotlib.axes._subplots.AxesSubplot at 0x107f14048&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/20190215183745111.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.distplot(data[<span class="string">'age'</span>], bins=<span class="number">30</span>, label=<span class="string">'age dist'</span>, kde=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x10e075978&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/20190215183754535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><h4 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#训练集与测试集划分</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">y = data[<span class="string">'SeriousDlqin2yrs'</span>]</span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:]</span><br><span class="line">X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义最优分箱函数</span></span><br><span class="line"><span class="keyword">import</span> scipy.stats <span class="keyword">as</span> stats</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mono_bin</span><span class="params">(Y, X, n = <span class="number">20</span>)</span>:</span></span><br><span class="line">    r = <span class="number">0</span></span><br><span class="line">    total_bad=Y.sum()</span><br><span class="line">    total_good=Y.count()-total_bad</span><br><span class="line">    <span class="comment">#print(total_bad, total_good)</span></span><br><span class="line">    <span class="keyword">while</span> np.abs(r) &lt; <span class="number">1</span>:</span><br><span class="line">        d1 = pd.DataFrame(&#123;<span class="string">"X"</span>: X, <span class="string">"Y"</span>: Y, <span class="string">"Bucket"</span>: pd.qcut(X, n)&#125;)</span><br><span class="line">        d2 = d1.groupby(<span class="string">'Bucket'</span>, as_index = <span class="keyword">True</span>)</span><br><span class="line">        r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)</span><br><span class="line">        n = n - <span class="number">1</span></span><br><span class="line">    d3 = pd.DataFrame(d2.X.min(), columns = [<span class="string">'min'</span>])</span><br><span class="line">    d3[<span class="string">'min'</span>]=d2.min().X</span><br><span class="line">    d3[<span class="string">'max'</span>] = d2.max().X</span><br><span class="line">    d3[<span class="string">'bad'</span>] = d2.sum().Y</span><br><span class="line">    d3[<span class="string">'good'</span>] = d2.count().Y-d3[<span class="string">'bad'</span>]</span><br><span class="line">    d3[<span class="string">'bad_rate'</span>] = d2.mean().Y</span><br><span class="line">    d3[<span class="string">'woe'</span>]=np.log(((d2.count().Y-d2.sum().Y)/total_good)/(d2.sum().Y/total_bad))</span><br><span class="line">    iv = ((d3[<span class="string">'good'</span>]/total_good - d3[<span class="string">'bad'</span>]/total_bad)*d3[<span class="string">'woe'</span>]).sum()</span><br><span class="line">    d4 = (d3.sort_index(by = <span class="string">'min'</span>)).reset_index(drop=<span class="keyword">True</span>)</span><br><span class="line">    woe = list(d4[<span class="string">'woe'</span>].round(<span class="number">3</span>))</span><br><span class="line">    cut=[]</span><br><span class="line">    cut.append(float(<span class="string">'-inf'</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">        qua = X.quantile(i / (n+<span class="number">1</span>))</span><br><span class="line">        cut.append(round(qua, <span class="number">4</span>))</span><br><span class="line">    cut.append(float(<span class="string">'inf'</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> d4, iv, cut, woe</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#举例，将‘age’最优分箱</span></span><br><span class="line">mono_bin(y, data[<span class="string">'age'</span>], n=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>(   min  max   bad   good  bad_rate       woe 0   21   33  1816  14471  0.111500 -0.561845 1   34   40  1664  16073  0.093815 -0.369439 2   41   45  1360  14683  0.084772 -0.258150 3   46   49  1209  13619  0.081535 -0.215683 4   50   54  1298  16516  0.072864 -0.093851 5   55   59   913  15757  0.054769  0.210948 6   60   64   690  15923  0.041534  0.501473 7   65   71   414  14194  0.028341  0.897353 8   72   99   341  14403  0.023128  1.105954, 0.2414021266070617, [-inf, 33.0, 40.0, 45.0, 49.0, 54.0, 59.0, 64.0, 71.0, inf], [-0.562, -0.369, -0.258, -0.216, -0.094, 0.211, 0.501, 0.897, 1.106])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#每个变量的个数，从而确定连续变量与分类变量</span></span><br><span class="line">var_lst=data.columns[<span class="number">1</span>:]</span><br><span class="line">var_num=&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> var_lst:</span><br><span class="line">    var_num[var]=len(data[var].unique())</span><br><span class="line">var_num</span><br></pre></td></tr></table></figure><pre><code>{&apos;RevolvingUtilizationOfUnsecuredLines&apos;: 122950, &apos;age&apos;: 79, &apos;NumberOfTime30-59DaysPastDueNotWorse&apos;: 14, &apos;DebtRatio&apos;: 114065, &apos;MonthlyIncome&apos;: 13592, &apos;NumberOfOpenCreditLinesAndLoans&apos;: 58, &apos;NumberOfTimes90DaysLate&apos;: 17, &apos;NumberRealEstateLoansOrLines&apos;: 28, &apos;NumberOfTime60-89DaysPastDueNotWorse&apos;: 11, &apos;NumberOfDependents&apos;: 13}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将四个连续变量最优分箱</span></span><br><span class="line">x1_df, x1_iv, x1_cut, x1_woe = mono_bin( y_train, X_train[<span class="string">'RevolvingUtilizationOfUnsecuredLines'</span>], n=<span class="number">10</span>)</span><br><span class="line">x2_df, x2_iv, x2_cut, x2_woe = mono_bin( y_train, X_train[<span class="string">'age'</span>], n=<span class="number">10</span>)</span><br><span class="line">x4_df, x4_iv, x4_cut, x4_woe = mono_bin( y_train, X_train[<span class="string">'DebtRatio'</span>], n=<span class="number">10</span>)</span><br><span class="line">x5_df, x5_iv, x5_cut, x5_woe = mono_bin( y_train, X_train[<span class="string">'MonthlyIncome'</span>], n=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#不能最优分箱的变量则进行手动分箱，WOE计算函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">woe_value</span><span class="params">(Y, X, cut)</span>:</span></span><br><span class="line">    total_bad=Y.sum()</span><br><span class="line">    total_good=Y.count()-total_bad</span><br><span class="line">    d1 = pd.DataFrame(&#123;<span class="string">"X"</span>: X, <span class="string">"Y"</span>: Y, <span class="string">"Bucket"</span>: cut&#125;)</span><br><span class="line">    d2 = d1.groupby(<span class="string">'Bucket'</span>, as_index = <span class="keyword">True</span>)</span><br><span class="line">    d3 = pd.DataFrame(d2.X.min(), columns = [<span class="string">'min'</span>])</span><br><span class="line">    d3[<span class="string">'min'</span>]=d2.min().X</span><br><span class="line">    d3[<span class="string">'max'</span>] = d2.max().X</span><br><span class="line">    d3[<span class="string">'bad'</span>] = d2.sum().Y</span><br><span class="line">    d3[<span class="string">'good'</span>] = d2.count().Y-d3[<span class="string">'bad'</span>]</span><br><span class="line">    d3[<span class="string">'bad_rate'</span>] = d2.mean().Y</span><br><span class="line">    d3[<span class="string">'woe'</span>]=np.log(((d2.count().Y-d2.sum().Y)/total_good)/(d2.sum().Y/total_bad))</span><br><span class="line">    iv = ((d3[<span class="string">'good'</span>]/total_good - d3[<span class="string">'bad'</span>]/total_bad)*d3[<span class="string">'woe'</span>]).sum()</span><br><span class="line">    d4 = (d3.sort_index(by = <span class="string">'min'</span>)).reset_index(drop=<span class="keyword">True</span>)</span><br><span class="line">    woe = list(d4[<span class="string">'woe'</span>].round(<span class="number">3</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> d4, iv, woe</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">x3_cut = [-np.Inf, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, np.Inf]</span><br><span class="line">x6_cut = [-np.Inf, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, np.Inf]</span><br><span class="line">x7_cut = [-np.Inf, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, np.Inf]</span><br><span class="line">x8_cut = [-np.Inf, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, np.Inf]</span><br><span class="line">x9_cut = [-np.Inf, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, np.Inf]</span><br><span class="line">x10_cut = [-np.Inf, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, np.Inf]</span><br><span class="line"></span><br><span class="line">x3_bin = pd.cut(X_train[<span class="string">'NumberOfTime30-59DaysPastDueNotWorse'</span>], bins= x3_cut)</span><br><span class="line">x6_bin = pd.cut(X_train[<span class="string">'NumberOfOpenCreditLinesAndLoans'</span>], bins= x6_cut)</span><br><span class="line">x7_bin = pd.cut(X_train[<span class="string">'NumberOfTimes90DaysLate'</span>], bins=x7_cut)</span><br><span class="line">x8_bin = pd.cut(X_train[<span class="string">'NumberRealEstateLoansOrLines'</span>], bins=x8_cut)</span><br><span class="line">x9_bin = pd.cut(X_train[<span class="string">'NumberOfTime60-89DaysPastDueNotWorse'</span>], bins=x9_cut)</span><br><span class="line">x10_bin = pd.cut(X_train[<span class="string">'NumberOfDependents'</span>], bins=x10_cut)</span><br><span class="line"></span><br><span class="line">x3_df, x3_iv, x3_woe = woe_value(y_train, X_train[<span class="string">'NumberOfTime30-59DaysPastDueNotWorse'</span>], x3_bin)</span><br><span class="line">x6_df, x6_iv, x6_woe = woe_value(y_train, X_train[<span class="string">'NumberOfOpenCreditLinesAndLoans'</span>], x6_bin)</span><br><span class="line">x7_df, x7_iv, x7_woe = woe_value(y_train, X_train[<span class="string">'NumberOfTimes90DaysLate'</span>], x7_bin)</span><br><span class="line">x8_df, x8_iv, x8_woe = woe_value(y_train, X_train[<span class="string">'NumberRealEstateLoansOrLines'</span>], x8_bin)</span><br><span class="line">x9_df, x9_iv, x9_woe = woe_value(y_train, X_train[<span class="string">'NumberOfTime60-89DaysPastDueNotWorse'</span>], x9_bin)</span><br><span class="line">x10_df, x10_iv, x10_woe = woe_value(y_train, X_train[<span class="string">'NumberOfDependents'</span>], x10_bin)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#相关性分析</span></span><br><span class="line">corr = data.corr()</span><br><span class="line">xticks = [<span class="string">'x'</span>+str(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">12</span>)]</span><br><span class="line">yticks = list(data.columns)</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">sns.heatmap(corr, cmap=<span class="string">'GnBu'</span>, annot=<span class="keyword">True</span>, ax= ax1, annot_kws=&#123;<span class="string">'size'</span>:<span class="number">6</span>, <span class="string">'color'</span>:<span class="string">'red'</span>&#125;)</span><br><span class="line">ax1.set_xticklabels(xticks, rotation=<span class="number">0</span>, fontsize=<span class="number">10</span>)</span><br><span class="line">ax1.set_yticklabels(yticks, rotation=<span class="number">0</span>, fontsize=<span class="number">10</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190215183807960.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#从相关系数热力图可以看出，自变量之间的线性相关性比较弱</span></span><br><span class="line"><span class="comment">#画出每个变量的IV值</span></span><br><span class="line">iv = [eval(<span class="string">'x'</span>+str(i)+<span class="string">'_iv'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>)]</span><br><span class="line">index=[<span class="string">'x'</span>+str(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>)]</span><br><span class="line">sns.barplot(x=index, y=iv)</span><br><span class="line">plt.ylabel(<span class="string">'IV'</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0,0.5,&apos;IV&apos;)</code></pre><p><img src="https://img-blog.csdnimg.cn/20190215183820781.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#选x1, x2, x3, x7, x9， IV&gt;0.2高预测性</span></span><br><span class="line"><span class="comment">#WOE编码</span></span><br><span class="line">x1= <span class="string">'RevolvingUtilizationOfUnsecuredLines'</span></span><br><span class="line">x2= <span class="string">'age'</span></span><br><span class="line">x3= <span class="string">'NumberOfTime30-59DaysPastDueNotWorse'</span></span><br><span class="line">x7= <span class="string">'NumberOfTimes90DaysLate'</span></span><br><span class="line">x9= <span class="string">'NumberOfTime60-89DaysPastDueNotWorse'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义WOE编码函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">woe_trans</span><span class="params">(data, var, woe, cut)</span>:</span></span><br><span class="line">    woe_name = var+<span class="string">'_woe'</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(woe)):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            data.loc[(data[var]&lt;=cut[i+<span class="number">1</span>]), woe_name] = woe[i]</span><br><span class="line">        <span class="keyword">elif</span> ((i&gt;<span class="number">0</span>) <span class="keyword">and</span> (i&lt;=(len(woe)<span class="number">-2</span>))):</span><br><span class="line">            data.loc[((data[var]&lt;=cut[i+<span class="number">1</span>]) &amp; (data[var]&gt;cut[i])), woe_name] = woe[i]</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            data.loc[(data[var]&gt;cut[i]), woe_name] = woe[i]</span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">9</span>]:</span><br><span class="line">    X_train = woe_trans(X_train, eval(<span class="string">'x'</span>+str(i)), eval(<span class="string">'x'</span>+str(i)+<span class="string">'_woe'</span>), eval(<span class="string">'x'</span>+str(i)+<span class="string">'_cut'</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#选取WOE编码之后的列作为训练数据集</span></span><br><span class="line">X_train = X_train.iloc[:, <span class="number">-5</span>:]</span><br></pre></td></tr></table></figure><h3 id="4-模型开发"><a href="#4-模型开发" class="headerlink" title="4. 模型开发"></a>4. 模型开发</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#建立逻辑回归模型</span></span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line">X1=sm.add_constant(X_train)</span><br><span class="line">logit=sm.Logit(y_train, X1)</span><br><span class="line">result=logit.fit()</span><br><span class="line">print(result.summary())</span><br></pre></td></tr></table></figure><pre><code>Optimization terminated successfully.         Current function value: 0.185840         Iterations 8                           Logit Regression Results                           ==============================================================================Dep. Variable:       SeriousDlqin2yrs   No. Observations:               101740Model:                          Logit   Df Residuals:                   101734Method:                           MLE   Df Model:                            5Date:                Thu, 20 Sep 2018   Pseudo R-squ.:                  0.2382Time:                        21:57:47   Log-Likelihood:                -18907.converged:                       True   LL-Null:                       -24820.                                        LLR p-value:                     0.000============================================================================================================                                               coef    std err          z      P&gt;|z|      [0.025      0.975]------------------------------------------------------------------------------------------------------------const                                       -2.6195      0.015   -171.961      0.000      -2.649      -2.590RevolvingUtilizationOfUnsecuredLines_woe    -0.6441      0.016    -41.387      0.000      -0.675      -0.614age_woe                                     -0.4992      0.033    -15.294      0.000      -0.563      -0.435NumberOfTime30-59DaysPastDueNotWorse_woe    -0.5455      0.016    -34.470      0.000      -0.577      -0.515NumberOfTimes90DaysLate_woe                 -0.5683      0.014    -41.833      0.000      -0.595      -0.542NumberOfTime60-89DaysPastDueNotWorse_woe    -0.4019      0.017    -23.032      0.000      -0.436      -0.368============================================================================================================</code></pre><h3 id="5-模型评估"><a href="#5-模型评估" class="headerlink" title="5. 模型评估"></a>5. 模型评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对测试集进行WOE编码</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">9</span>]:</span><br><span class="line">    X_test = woe_trans(X_test, eval(<span class="string">'x'</span>+str(i)), eval(<span class="string">'x'</span>+str(i)+<span class="string">'_woe'</span>), eval(<span class="string">'x'</span>+str(i)+<span class="string">'_cut'</span>))</span><br><span class="line">X_test = X_test.iloc[:, <span class="number">-5</span>:]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制ROC曲线，计算AUC</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">X2=sm.add_constant(X_test)</span><br><span class="line">y_pred = result.predict(X2)</span><br><span class="line"></span><br><span class="line">fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)</span><br><span class="line">auc = metrics.auc(fpr, tpr)</span><br><span class="line">plt.plot(fpr, tpr, <span class="string">'b'</span>, label=<span class="string">'AUC=%.2f'</span> %auc)</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>,<span class="number">1</span>], [<span class="number">0</span>,<span class="number">1</span>], <span class="string">'r--'</span>)</span><br><span class="line">plt.xlim([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.ylim([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.xlabel(<span class="string">'FPR'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'TPR'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190215183838146.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><h3 id="6-评分系统建立"><a href="#6-评分系统建立" class="headerlink" title="6. 评分系统建立"></a>6. 评分系统建立</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#分数计算函数</span></span><br><span class="line">PDO=<span class="number">20</span></span><br><span class="line">base=<span class="number">600</span></span><br><span class="line"><span class="comment">#all_woe是某一个体所有相关变量woe编码值构成的序列</span></span><br><span class="line"><span class="comment">#total_score = base- PDO*(all_woe.dot(coef))/np.log(2)</span></span><br><span class="line">factor= -PDO/np.log(<span class="number">2</span>)</span><br><span class="line">coef = result.params</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_score</span><span class="params">(coef, woe, factor)</span>:</span></span><br><span class="line">    scores=[round(coef*woe[i]*factor, <span class="number">0</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(woe))]</span><br><span class="line">    <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算各因子每个区间对应的分数</span></span><br><span class="line">x1_scores=get_score(coef[<span class="number">1</span>], x1_woe, factor)</span><br><span class="line">print(x1_scores)</span><br><span class="line">x2_scores=get_score(coef[<span class="number">2</span>], x2_woe, factor)</span><br><span class="line">x3_scores=get_score(coef[<span class="number">3</span>], x3_woe, factor)</span><br><span class="line">x7_scores=get_score(coef[<span class="number">4</span>], x7_woe, factor)</span><br><span class="line">x9_scores=get_score(coef[<span class="number">5</span>], x9_woe, factor)</span><br></pre></td></tr></table></figure><pre><code>[24.0, 23.0, 5.0, -20.0]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.head()</span><br><span class="line">datacopy=data</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data=datacopy</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">9</span>]:</span><br><span class="line">    data = woe_trans(data, eval(<span class="string">'x'</span>+str(i)), eval(<span class="string">'x'</span>+str(i)+<span class="string">'_woe'</span>), eval(<span class="string">'x'</span>+str(i)+<span class="string">'_cut'</span>))</span><br><span class="line">data = data.iloc[:, <span class="number">-5</span>:]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对整个data进行打分计算</span></span><br><span class="line">data_c=sm.add_constant(data)                                         </span><br><span class="line">data[<span class="string">'score'</span>]=<span class="number">500</span> + round(data_c.dot(coef)*factor, <span class="number">0</span>)  </span><br><span class="line">data.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>RevolvingUtilizationOfUnsecuredLines_woe</th>      <th>age_woe</th>      <th>NumberOfTime30-59DaysPastDueNotWorse_woe</th>      <th>NumberOfTimes90DaysLate_woe</th>      <th>NumberOfTime60-89DaysPastDueNotWorse_woe</th>      <th>score</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>-1.097</td>      <td>-0.264</td>      <td>-1.720</td>      <td>0.373</td>      <td>0.267</td>      <td>534.0</td>    </tr>    <tr>      <th>1</th>      <td>-1.097</td>      <td>-0.371</td>      <td>0.515</td>      <td>0.373</td>      <td>0.267</td>      <td>567.0</td>    </tr>    <tr>      <th>2</th>      <td>-1.097</td>      <td>-0.371</td>      <td>-0.878</td>      <td>-1.969</td>      <td>0.267</td>      <td>507.0</td>    </tr>    <tr>      <th>3</th>      <td>0.284</td>      <td>-0.564</td>      <td>0.515</td>      <td>0.373</td>      <td>0.267</td>      <td>590.0</td>    </tr>    <tr>      <th>4</th>      <td>-1.097</td>      <td>-0.184</td>      <td>-0.878</td>      <td>0.373</td>      <td>0.267</td>      <td>548.0</td>    </tr>    <tr>      <th>5</th>      <td>0.284</td>      <td>1.094</td>      <td>0.515</td>      <td>0.373</td>      <td>0.267</td>      <td>614.0</td>    </tr>    <tr>      <th>6</th>      <td>0.284</td>      <td>0.216</td>      <td>0.515</td>      <td>0.373</td>      <td>0.267</td>      <td>601.0</td>    </tr>    <tr>      <th>7</th>      <td>-1.097</td>      <td>-0.371</td>      <td>0.515</td>      <td>0.373</td>      <td>0.267</td>      <td>567.0</td>    </tr>    <tr>      <th>9</th>      <td>0.284</td>      <td>0.216</td>      <td>0.515</td>      <td>0.373</td>      <td>0.267</td>      <td>601.0</td>    </tr>    <tr>      <th>10</th>      <td>-1.097</td>      <td>-0.564</td>      <td>0.515</td>      <td>0.373</td>      <td>0.267</td>      <td>564.0</td>    </tr>  </tbody></table></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;信用评分卡模型是一种比较成熟的预测方法，广泛应用于信用风险评估以及金融风险控制等领域，其基本原理是：将模型变量以WOE编码方式离散化之后运用-logistic-回归模型进行二分类变量的拟合及预测。&quot;&gt;&lt;a href=&quot;#信用评分卡模型是一种比较成熟的预测方法，广泛
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2019/02/15/hello-world/"/>
    <id>http://yoursite.com/2019/02/15/hello-world/</id>
    <published>2019-02-15T08:09:15.599Z</published>
    <updated>2019-02-15T08:09:15.599Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>K-Means</title>
    <link href="http://yoursite.com/2019/02/15/k-Means/"/>
    <id>http://yoursite.com/2019/02/15/k-Means/</id>
    <published>2019-02-15T05:30:45.000Z</published>
    <updated>2019-02-15T08:09:15.599Z</updated>
    
    <content type="html"><![CDATA[<h1 id="电影评分的-k-均值聚类"><a href="#电影评分的-k-均值聚类" class="headerlink" title="电影评分的 k 均值聚类"></a>电影评分的 k 均值聚类</h1><p>假设你是 Netflix 的一名数据分析师，你想要根据用户对不同电影的评分研究用户在电影品位上的相似和不同之处。了解这些评分对用户电影推荐系统有帮助吗？我们来研究下这方面的数据。</p><p>我们将使用的数据来自精彩的 <a href="https://movielens.org/" target="_blank" rel="noopener">MovieLens</a> <a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">用户评分数据集</a>。我们稍后将在 notebook 中查看每个电影评分，先看看不同类型之间的评分比较情况。</p><h2 id="数据集概述"><a href="#数据集概述" class="headerlink" title="数据集概述"></a>数据集概述</h2><p>该数据集有两个文件。我们将这两个文件导入 pandas dataframe 中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> csr_matrix</span><br><span class="line"><span class="keyword">import</span> helper</span><br><span class="line"></span><br><span class="line"><span class="comment"># Import the Movies dataset</span></span><br><span class="line">movies = pd.read_csv(<span class="string">'ml-latest-small/movies.csv'</span>)</span><br><span class="line">movies.head()</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>movieId</th>      <th>title</th>      <th>genres</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>Toy Story (1995)</td>      <td>Adventure|Animation|Children|Comedy|Fantasy</td>    </tr>    <tr>      <th>1</th>      <td>2</td>      <td>Jumanji (1995)</td>      <td>Adventure|Children|Fantasy</td>    </tr>    <tr>      <th>2</th>      <td>3</td>      <td>Grumpier Old Men (1995)</td>      <td>Comedy|Romance</td>    </tr>    <tr>      <th>3</th>      <td>4</td>      <td>Waiting to Exhale (1995)</td>      <td>Comedy|Drama|Romance</td>    </tr>    <tr>      <th>4</th>      <td>5</td>      <td>Father of the Bride Part II (1995)</td>      <td>Comedy</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the ratings dataset</span></span><br><span class="line">ratings = pd.read_csv(<span class="string">'ml-latest-small/ratings.csv'</span>)</span><br><span class="line">ratings.head()</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>userId</th>      <th>movieId</th>      <th>rating</th>      <th>timestamp</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>31</td>      <td>2.5</td>      <td>1260759144</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>1029</td>      <td>3.0</td>      <td>1260759179</td>    </tr>    <tr>      <th>2</th>      <td>1</td>      <td>1061</td>      <td>3.0</td>      <td>1260759182</td>    </tr>    <tr>      <th>3</th>      <td>1</td>      <td>1129</td>      <td>2.0</td>      <td>1260759185</td>    </tr>    <tr>      <th>4</th>      <td>1</td>      <td>1172</td>      <td>4.0</td>      <td>1260759205</td>    </tr>  </tbody></table></div><p>现在我们已经知道数据集的结构，每个表格中有多少条记录。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'The dataset contains: '</span>, len(ratings), <span class="string">' ratings of '</span>, len(movies), <span class="string">' movies.'</span>)</span><br></pre></td></tr></table></figure><pre><code>The dataset contains:  100004  ratings of  9125  movies.</code></pre><h2 id="爱情片与科幻片"><a href="#爱情片与科幻片" class="headerlink" title="爱情片与科幻片"></a>爱情片与科幻片</h2><p>我们先查看一小部分用户，并看看他们喜欢什么类型的电影。我们将大部分数据预处理过程都隐藏在了辅助函数中，并重点研究聚类概念。在完成此 notebook 后，建议你快速浏览下 helper.py，了解这些辅助函数是如何实现的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Calculate the average rating of romance and scifi movies</span></span><br><span class="line"></span><br><span class="line">genre_ratings = helper.get_genre_ratings(ratings, movies, [<span class="string">'Romance'</span>, <span class="string">'Sci-Fi'</span>], [<span class="string">'avg_romance_rating'</span>, <span class="string">'avg_scifi_rating'</span>])</span><br><span class="line">genre_ratings.head()</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>avg_romance_rating</th>      <th>avg_scifi_rating</th>    </tr>    <tr>      <th>userId</th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>1</th>      <td>3.50</td>      <td>2.40</td>    </tr>    <tr>      <th>2</th>      <td>3.59</td>      <td>3.80</td>    </tr>    <tr>      <th>3</th>      <td>3.65</td>      <td>3.14</td>    </tr>    <tr>      <th>4</th>      <td>4.50</td>      <td>4.26</td>    </tr>    <tr>      <th>5</th>      <td>4.08</td>      <td>4.00</td>    </tr>  </tbody></table></div><p>函数 <code>get_genre_ratings</code> 计算了每位用户对所有爱情片和科幻片的平均评分。我们对数据集稍微进行偏倚，删除同时喜欢科幻片和爱情片的用户，使聚类能够将他们定义为更喜欢其中一种类型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">biased_dataset = helper.bias_genre_rating_dataset(genre_ratings, <span class="number">3.2</span>, <span class="number">2.5</span>)</span><br><span class="line"></span><br><span class="line">print( <span class="string">"Number of records: "</span>, len(biased_dataset))</span><br><span class="line">biased_dataset.head()</span><br></pre></td></tr></table></figure><pre><code>Number of records:  183</code></pre><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>userId</th>      <th>avg_romance_rating</th>      <th>avg_scifi_rating</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>3.50</td>      <td>2.40</td>    </tr>    <tr>      <th>1</th>      <td>3</td>      <td>3.65</td>      <td>3.14</td>    </tr>    <tr>      <th>2</th>      <td>6</td>      <td>2.90</td>      <td>2.75</td>    </tr>    <tr>      <th>3</th>      <td>7</td>      <td>2.93</td>      <td>3.36</td>    </tr>    <tr>      <th>4</th>      <td>12</td>      <td>2.89</td>      <td>2.62</td>    </tr>  </tbody></table></div><p>可以看出我们有 183 位用户，对于每位用户，我们都得出了他们对看过的爱情片和科幻片的平均评分。</p><p>我们来绘制该数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">helper.draw_scatterplot(biased_dataset[<span class="string">'avg_scifi_rating'</span>],<span class="string">'Avg scifi rating'</span>, biased_dataset[<span class="string">'avg_romance_rating'</span>], <span class="string">'Avg romance rating'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20181109164524473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>我们可以在此样本中看到明显的偏差（我们故意创建的）。如果使用 k 均值将样本分成两组，效果如何？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's turn our dataset into a list</span></span><br><span class="line">X = biased_dataset[[<span class="string">'avg_scifi_rating'</span>,<span class="string">'avg_romance_rating'</span>]].values</span><br></pre></td></tr></table></figure><ul><li>导入 <a href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" target="_blank" rel="noopener">KMeans</a></li><li>通过 n_clusters = 2 准备 KMeans</li><li>将数据集 <strong>X</strong> 传递给 KMeans 的 fit_predict 方法，并将聚类标签放入 <em>predictions</em></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Import KMeans</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Create an instance of KMeans to find two clusters</span></span><br><span class="line">kmeans_1 = KMeans(n_clusters = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> use fit_predict to cluster the dataset</span></span><br><span class="line">predictions = kmeans_1.fit_predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot</span></span><br><span class="line">helper.draw_clusters(biased_dataset, predictions)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20181109164541299.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>可以看出分组的依据主要是每个人对爱情片的评分高低。如果爱情片的平均评分超过 3 星，则属于第一组，否则属于另一组。</p><p>如果分成三组，会发生什么？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Create an instance of KMeans to find three clusters</span></span><br><span class="line">kmeans_2 = KMeans(n_clusters = <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> use fit_predict to cluster the dataset</span></span><br><span class="line">predictions_2 = kmeans_2.fit_predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot</span></span><br><span class="line">helper.draw_clusters(biased_dataset, predictions_2)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20181109164554198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>现在平均科幻片评分开始起作用了，分组情况如下所示：</p><ul><li>喜欢爱情片但是不喜欢科幻片的用户</li><li>喜欢科幻片但是不喜欢爱情片的用户</li><li>即喜欢科幻片又喜欢爱情片的用户</li></ul><p>再添加一组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Create an instance of KMeans to find four clusters</span></span><br><span class="line">kmeans_3 = KMeans(n_clusters = <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> use fit_predict to cluster the dataset</span></span><br><span class="line">predictions_3 = kmeans_3.fit_predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot</span></span><br><span class="line">helper.draw_clusters(biased_dataset, predictions_3)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20181109164605843.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>可以看出将数据集分成的聚类越多，每个聚类中用户的兴趣就相互之间越相似。</p><h2 id="选择-K"><a href="#选择-K" class="headerlink" title="选择 K"></a>选择 K</h2><p>我们可以将数据点拆分为任何数量的聚类。对于此数据集来说，正确的聚类数量是多少？</p><p>可以通过<a href="https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set" target="_blank" rel="noopener">多种</a>方式选择聚类 k。我们将研究一种简单的方式，叫做“肘部方法”。肘部方法会绘制 k 的上升值与使用该 k 值计算的总误差分布情况。</p><p>如何计算总误差？一种方法是计算平方误差。假设我们要计算 k=2 时的误差。有两个聚类，每个聚类有一个“图心”点。对于数据集中的每个点，我们将其坐标减去所属聚类的图心。然后将差值结果取平方（以便消除负值），并对结果求和。这样就可以获得每个点的误差值。如果将这些误差值求和，就会获得 k=2 时所有点的总误差。</p><p>现在的一个任务是对每个 k（介于 1 到数据集中的元素数量之间）执行相同的操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Choose the range of k values to test.</span></span><br><span class="line"><span class="comment"># We added a stride of 5 to improve performance. We don't need to calculate the error for every k value</span></span><br><span class="line">possible_k_values = range(<span class="number">2</span>, len(X)+<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate error values for all k values we're interested in</span></span><br><span class="line">errors_per_k = [helper.clustering_errors(k, X) <span class="keyword">for</span> k <span class="keyword">in</span> possible_k_values]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Optional: Look at the values of K vs the silhouette score of running K-means with that value of k</span></span><br><span class="line">list(zip(possible_k_values, errors_per_k))</span><br></pre></td></tr></table></figure><pre><code>[(2, 0.35588178764728251), (7, 0.37324118163771741), (12, 0.35650856326047475), (17, 0.3741137698024623), (22, 0.37718217339438476), (27, 0.36071909992215945), (32, 0.37104279808464452), (37, 0.3649882241766923), (42, 0.36895091450195883), (47, 0.37696003940733186), (52, 0.38716548900081571), (57, 0.35079775582937778), (62, 0.34916584233387205), (67, 0.34839937724907), (72, 0.34907390154971468), (77, 0.34837739216196456), (82, 0.3309353056966266), (87, 0.34005916910201761), (92, 0.32494553685658306), (97, 0.32418331059507227), (102, 0.31329160485165003), (107, 0.29407239955320186), (112, 0.27366896911138017), (117, 0.28906341363336779), (122, 0.27342563040040624), (127, 0.25219179857975438), (132, 0.25320773897416415), (137, 0.2412264569953621), (142, 0.21855949198498667), (147, 0.19924498428850082), (152, 0.18722856283659275), (157, 0.16447514022082693), (162, 0.14697529680439808), (167, 0.12609539969216882), (172, 0.096865005870864829), (177, 0.064230120163174503), (182, 0.054644808743169397)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the each value of K vs. the silhouette score at that value</span></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">16</span>, <span class="number">6</span>))</span><br><span class="line">ax.set_xlabel(<span class="string">'K - number of clusters'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Silhouette Score (higher is better)'</span>)</span><br><span class="line">ax.plot(possible_k_values, errors_per_k)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ticks and grid</span></span><br><span class="line">xticks = np.arange(min(possible_k_values), max(possible_k_values)+<span class="number">1</span>, <span class="number">5.0</span>)</span><br><span class="line">ax.set_xticks(xticks, minor=<span class="keyword">False</span>)</span><br><span class="line">ax.set_xticks(xticks, minor=<span class="keyword">True</span>)</span><br><span class="line">ax.xaxis.grid(<span class="keyword">True</span>, which=<span class="string">'both'</span>)</span><br><span class="line">yticks = np.arange(round(min(errors_per_k), <span class="number">2</span>), max(errors_per_k), <span class="number">.05</span>)</span><br><span class="line">ax.set_yticks(yticks, minor=<span class="keyword">False</span>)</span><br><span class="line">ax.set_yticks(yticks, minor=<span class="keyword">True</span>)</span><br><span class="line">ax.yaxis.grid(<span class="keyword">True</span>, which=<span class="string">'both'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20181109164621119.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>看了该图后发现，合适的 k 值包括 7、22、27、32 等（每次运行时稍微不同）。聚类  (k) 数量超过该范围将开始导致糟糕的聚类情况（根据轮廓分数）</p><p>我会选择 k=7，因为更容易可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Create an instance of KMeans to find seven clusters</span></span><br><span class="line">kmeans_4 = KMeans(n_clusters=<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> use fit_predict to cluster the dataset</span></span><br><span class="line">predictions_4 = kmeans_4.fit_predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">helper.draw_clusters(biased_dataset, predictions_4, cmap=<span class="string">'Accent'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20181109164634463.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>注意：当你尝试绘制更大的 k 值（超过 10）时，需要确保你的绘制库没有对聚类重复使用相同的颜色。对于此图，我们需要使用 <a href="https://matplotlib.org/examples/color/colormaps_reference.html" target="_blank" rel="noopener">matplotlib colormap</a> ‘Accent’，因为其他色图要么颜色之间的对比度不强烈，要么在超过 8 个或 10 个聚类后会重复利用某些颜色。</p><h2 id="再加入动作片类型"><a href="#再加入动作片类型" class="headerlink" title="再加入动作片类型"></a>再加入动作片类型</h2><p>到目前为止，我们只查看了用户如何对爱情片和科幻片进行评分。我们再添加另一种类型，看看加入动作片类型后效果如何。</p><p>现在数据集如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">biased_dataset_3_genres = helper.get_genre_ratings(ratings, movies, </span><br><span class="line">                                                     [<span class="string">'Romance'</span>, <span class="string">'Sci-Fi'</span>, <span class="string">'Action'</span>], </span><br><span class="line">                                                     [<span class="string">'avg_romance_rating'</span>, <span class="string">'avg_scifi_rating'</span>, <span class="string">'avg_action_rating'</span>])</span><br><span class="line">biased_dataset_3_genres = helper.bias_genre_rating_dataset(biased_dataset_3_genres, <span class="number">3.2</span>, <span class="number">2.5</span>).dropna()</span><br><span class="line"></span><br><span class="line">print( <span class="string">"Number of records: "</span>, len(biased_dataset_3_genres))</span><br><span class="line">biased_dataset_3_genres.head()</span><br></pre></td></tr></table></figure><pre><code>Number of records:  183</code></pre><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>userId</th>      <th>avg_romance_rating</th>      <th>avg_scifi_rating</th>      <th>avg_action_rating</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>3.50</td>      <td>2.40</td>      <td>2.80</td>    </tr>    <tr>      <th>1</th>      <td>3</td>      <td>3.65</td>      <td>3.14</td>      <td>3.47</td>    </tr>    <tr>      <th>2</th>      <td>6</td>      <td>2.90</td>      <td>2.75</td>      <td>3.27</td>    </tr>    <tr>      <th>3</th>      <td>7</td>      <td>2.93</td>      <td>3.36</td>      <td>3.29</td>    </tr>    <tr>      <th>4</th>      <td>12</td>      <td>2.89</td>      <td>2.62</td>      <td>3.21</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_with_action = biased_dataset_3_genres[[<span class="string">'avg_scifi_rating'</span>,</span><br><span class="line">                                         <span class="string">'avg_romance_rating'</span>, </span><br><span class="line">                                         <span class="string">'avg_action_rating'</span>]].values</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Create an instance of KMeans to find seven clusters</span></span><br><span class="line">kmeans_5 = KMeans(n_clusters=<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> use fit_predict to cluster the dataset</span></span><br><span class="line">predictions_5 = kmeans_5.fit_predict(X_with_action)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">helper.draw_clusters_3d(biased_dataset_3_genres, predictions_5)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20181109164652235.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>我们依然分别用 x 轴和 y 轴表示科幻片和爱情片。并用点的大小大致表示动作片评分情况（更大的点表示平均评分超过 3 颗星，更小的点表示不超过 3 颗星 ）。</p><p>可以看出添加类型后，用户的聚类分布发生了变化。为 k 均值提供的数据越多，每组中用户之间的兴趣越相似。但是如果继续这么绘制，我们将无法可视化二维或三维之外的情形。在下个部分，我们将使用另一种图表，看看多达 50 个维度的聚类情况。</p><h2 id="电影级别的聚类"><a href="#电影级别的聚类" class="headerlink" title="电影级别的聚类"></a>电影级别的聚类</h2><p>现在我们已经知道 k 均值会如何根据用户的类型品位对用户进行聚类，我们再进一步分析，看看用户对单个影片的评分情况。为此，我们将数据集构建成 userId 与用户对每部电影的评分形式。例如，我们来看看以下数据集子集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Merge the two tables then pivot so we have Users X Movies dataframe</span></span><br><span class="line">ratings_title = pd.merge(ratings, movies[[<span class="string">'movieId'</span>, <span class="string">'title'</span>]], on=<span class="string">'movieId'</span> )</span><br><span class="line">user_movie_ratings = pd.pivot_table(ratings_title, index=<span class="string">'userId'</span>, columns= <span class="string">'title'</span>, values=<span class="string">'rating'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'dataset dimensions: '</span>, user_movie_ratings.shape, <span class="string">'\n\nSubset example:'</span>)</span><br><span class="line">user_movie_ratings.iloc[:<span class="number">6</span>, :<span class="number">10</span>]</span><br></pre></td></tr></table></figure><pre><code>dataset dimensions:  (671, 9064) Subset example:</code></pre><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th>title</th>      <th>“Great Performances” Cats (1998)</th>      <th>$9.99 (2008)</th>      <th>‘Hellboy’: The Seeds of Creation (2004)</th>      <th>‘Neath the Arizona Skies (1934)</th>      <th>‘Round Midnight (1986)</th>      <th>‘Salem’s Lot (2004)</th>      <th>‘Til There Was You (1997)</th>      <th>‘burbs, The (1989)</th>      <th>‘night Mother (1986)</th>      <th>(500) Days of Summer (2009)</th>    </tr>    <tr>      <th>userId</th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>1</th>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>2</th>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>3</th>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>4</th>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>5</th>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>6</th>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>4.0</td>      <td>NaN</td>      <td>NaN</td>    </tr>  </tbody></table></div><p>NaN 值的优势表明了第一个问题。大多数用户没有看过大部分电影，并且没有为这些电影评分。这种数据集称为“稀疏”数据集，因为只有少数单元格有值。</p><p>为了解决这一问题，我们按照获得评分次数最多的电影和对电影评分次数最多的用户排序。这样可以形成更“密集”的区域，使我们能够查看数据集的顶部数据。</p><p>如果我们要选择获得评分次数最多的电影和对电影评分次数最多的用户，则如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">n_movies = <span class="number">30</span></span><br><span class="line">n_users = <span class="number">18</span></span><br><span class="line">most_rated_movies_users_selection = helper.sort_by_rating_density(user_movie_ratings, n_movies, n_users)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'dataset dimensions: '</span>, most_rated_movies_users_selection.shape)</span><br><span class="line">most_rated_movies_users_selection.head()</span><br></pre></td></tr></table></figure><pre><code>dataset dimensions:  (18, 30)</code></pre><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th>title</th>      <th>Forrest Gump (1994)</th>      <th>Pulp Fiction (1994)</th>      <th>Shawshank Redemption, The (1994)</th>      <th>Silence of the Lambs, The (1991)</th>      <th>Star Wars: Episode IV - A New Hope (1977)</th>      <th>Jurassic Park (1993)</th>      <th>Matrix, The (1999)</th>      <th>Toy Story (1995)</th>      <th>Schindler’s List (1993)</th>      <th>Terminator 2: Judgment Day (1991)</th>      <th>…</th>      <th>Dances with Wolves (1990)</th>      <th>Fight Club (1999)</th>      <th>Usual Suspects, The (1995)</th>      <th>Seven (a.k.a. Se7en) (1995)</th>      <th>Lion King, The (1994)</th>      <th>Godfather, The (1972)</th>      <th>Lord of the Rings: The Fellowship of the Ring, The (2001)</th>      <th>Apollo 13 (1995)</th>      <th>True Lies (1994)</th>      <th>Twelve Monkeys (a.k.a. 12 Monkeys) (1995)</th>    </tr>  </thead>  <tbody>    <tr>      <th>29</th>      <td>5.0</td>      <td>5.0</td>      <td>5.0</td>      <td>4.0</td>      <td>4.0</td>      <td>4.0</td>      <td>3.0</td>      <td>4.0</td>      <td>5.0</td>      <td>4.0</td>      <td>…</td>      <td>5.0</td>      <td>4.0</td>      <td>5.0</td>      <td>4.0</td>      <td>3.0</td>      <td>5.0</td>      <td>3.0</td>      <td>5.0</td>      <td>4.0</td>      <td>2.0</td>    </tr>    <tr>      <th>508</th>      <td>4.0</td>      <td>5.0</td>      <td>4.0</td>      <td>4.0</td>      <td>5.0</td>      <td>3.0</td>      <td>4.5</td>      <td>3.0</td>      <td>5.0</td>      <td>2.0</td>      <td>…</td>      <td>5.0</td>      <td>4.0</td>      <td>5.0</td>      <td>4.0</td>      <td>3.5</td>      <td>5.0</td>      <td>4.5</td>      <td>3.0</td>      <td>2.0</td>      <td>4.0</td>    </tr>    <tr>      <th>14</th>      <td>1.0</td>      <td>5.0</td>      <td>2.0</td>      <td>5.0</td>      <td>5.0</td>      <td>3.0</td>      <td>5.0</td>      <td>2.0</td>      <td>4.0</td>      <td>4.0</td>      <td>…</td>      <td>3.0</td>      <td>5.0</td>      <td>5.0</td>      <td>5.0</td>      <td>4.0</td>      <td>5.0</td>      <td>5.0</td>      <td>3.0</td>      <td>4.0</td>      <td>4.0</td>    </tr>    <tr>      <th>72</th>      <td>5.0</td>      <td>5.0</td>      <td>5.0</td>      <td>4.5</td>      <td>4.5</td>      <td>4.0</td>      <td>4.5</td>      <td>5.0</td>      <td>5.0</td>      <td>3.0</td>      <td>…</td>      <td>4.5</td>      <td>5.0</td>      <td>5.0</td>      <td>5.0</td>      <td>5.0</td>      <td>5.0</td>      <td>5.0</td>      <td>3.5</td>      <td>3.0</td>      <td>5.0</td>    </tr>    <tr>      <th>653</th>      <td>4.0</td>      <td>5.0</td>      <td>5.0</td>      <td>4.5</td>      <td>5.0</td>      <td>4.5</td>      <td>5.0</td>      <td>5.0</td>      <td>5.0</td>      <td>5.0</td>      <td>…</td>      <td>4.5</td>      <td>5.0</td>      <td>5.0</td>      <td>4.5</td>      <td>5.0</td>      <td>4.5</td>      <td>5.0</td>      <td>5.0</td>      <td>4.0</td>      <td>5.0</td>    </tr>  </tbody></table><p>5 rows × 30 columns</p></div><p>这样更好分析。我们还需要指定一个可视化这些评分的良好方式，以便在查看更庞大的子集时能够直观地识别这些评分（稍后变成聚类）。</p><p>我们使用颜色代替评分数字：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helper.draw_movies_heatmap(most_rated_movies_users_selection)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2018110916471948.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>每列表示一部电影。每行表示一位用户。单元格的颜色根据图表右侧的刻度表示用户对该电影的评分情况。</p><p>注意到某些单元格是白色吗？表示相应用户没有对该电影进行评分。在现实中进行聚类时就会遇到这种问题。与一开始经过整理的示例不同，现实中的数据集经常比较稀疏，数据集中的部分单元格没有值。这样的话，直接根据电影评分对用户进行聚类不太方便，因为 k 均值通常不喜欢缺失值。</p><p>为了提高性能，我们将仅使用 1000 部电影的评分（数据集中一共有 9000 部以上）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">user_movie_ratings =  pd.pivot_table(ratings_title, index=<span class="string">'userId'</span>, columns= <span class="string">'title'</span>, values=<span class="string">'rating'</span>)</span><br><span class="line">most_rated_movies_1k = helper.get_most_rated_movies(user_movie_ratings, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure><p>为了使 sklearn 对像这样缺少值的数据集运行 k 均值聚类，我们首先需要将其转型为<a href="https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.sparse.csr_matrix.html" target="_blank" rel="noopener">稀疏 csr 矩阵</a>类型（如 SciPi 库中所定义）。</p><p>要从 pandas dataframe 转换为稀疏矩阵，我们需要先转换为 SparseDataFrame，然后使用 pandas 的 <code>to_coo()</code> 方法进行转换。</p><p>注意：只有较新版本的 pandas 具有<code>to_coo()</code>。如果你在下个单元格中遇到问题，确保你的 pandas 是最新版本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sparse_ratings = csr_matrix(pd.SparseDataFrame(most_rated_movies_1k).to_coo())</span><br></pre></td></tr></table></figure><h2 id="我们来聚类吧！"><a href="#我们来聚类吧！" class="headerlink" title="我们来聚类吧！"></a>我们来聚类吧！</h2><p>对于 k 均值，我们需要指定 k，即聚类数量。我们随意地尝试 k=20（选择 k 的更佳方式如上述肘部方法所示。但是，该方法需要一定的运行时间。):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 20 clusters</span></span><br><span class="line">predictions = KMeans(n_clusters=<span class="number">20</span>, algorithm=<span class="string">'full'</span>).fit_predict(sparse_ratings)</span><br></pre></td></tr></table></figure><p>为了可视化其中一些聚类，我们需要将每个聚类绘制成热图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">max_users = <span class="number">70</span></span><br><span class="line">max_movies = <span class="number">50</span></span><br><span class="line"></span><br><span class="line">clustered = pd.concat([most_rated_movies_1k.reset_index(), pd.DataFrame(&#123;<span class="string">'group'</span>:predictions&#125;)], axis=<span class="number">1</span>)</span><br><span class="line">helper.draw_movie_clusters(clustered, max_users, max_movies)</span><br></pre></td></tr></table></figure><pre><code>cluster # 7# of users in cluster: 276. # of users in plot: 70</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164732568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 16# of users in cluster: 64. # of users in plot: 64</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164744447.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 0# of users in cluster: 26. # of users in plot: 26</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164755257.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 2# of users in cluster: 72. # of users in plot: 70</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164807895.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 6# of users in cluster: 17. # of users in plot: 17</code></pre><p><img src="https://img-blog.csdnimg.cn/2018110916482043.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 3# of users in cluster: 37. # of users in plot: 37</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164831735.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 11# of users in cluster: 12. # of users in plot: 12</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164842611.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 18# of users in cluster: 35. # of users in plot: 35</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164853952.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 9# of users in cluster: 55. # of users in plot: 55</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164905955.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 8# of users in cluster: 27. # of users in plot: 27</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164919664.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><pre><code>cluster # 15# of users in cluster: 15. # of users in plot: 15</code></pre><p><img src="https://img-blog.csdnimg.cn/20181109164944204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>需要注意以下几个事项：</p><ul><li>聚类中的评分越相似，你在该聚类中就越能发现颜色相似的<strong>垂直</strong>线。</li><li>在聚类中发现了非常有趣的规律：<ul><li>某些聚类比其他聚类更稀疏，其中的用户可能比其他聚类中的用户看的电影更少，评分的电影也更少。</li><li>某些聚类主要是黄色，汇聚了非常喜欢特定类型电影的用户。其他聚类主要是绿色或海蓝色，表示这些用户都认为某些电影可以评 2-3 颗星。</li><li>注意每个聚类中的电影有何变化。图表对数据进行了过滤，仅显示评分最多的电影，然后按照平均评分排序。</li><li>能找到《指环王》在每个聚类中位于哪个位置吗？《星球大战》呢？</li></ul></li><li>很容易发现具有相似颜色的<strong>水平</strong>线，表示评分变化不大的用户。这可能是 Netflix 从基于星级的评分切换到喜欢/不喜欢评分的原因之一。四颗星评分对不同的人来说，含义不同。</li><li>我们在可视化聚类时，采取了一些措施（过滤/排序/切片）。因为这种数据集比较“稀疏”，大多数单元格没有值（因为大部分用户没有看过大部分电影）。</li></ul><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>我们选择一个聚类和一位特定的用户，看看该聚类可以使我们执行哪些实用的操作。</p><p>首先选择一个聚类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Pick a cluster ID from the clusters above</span></span><br><span class="line">cluster_number = <span class="number">11</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Let's filter to only see the region of the dataset with the most number of values </span></span><br><span class="line">n_users = <span class="number">75</span></span><br><span class="line">n_movies = <span class="number">300</span></span><br><span class="line">cluster = clustered[clustered.group == cluster_number].drop([<span class="string">'index'</span>, <span class="string">'group'</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">cluster = helper.sort_by_rating_density(cluster, n_movies, n_users)</span><br><span class="line">helper.draw_movies_heatmap(cluster, axis_labels=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20181109164957936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTM0OTQw,size_16,color_FFFFFF,t_70" alt="png"></p><p>聚类中的实际评分如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cluster.fillna(<span class="string">''</span>).head()</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Amadeus (1984)</th>      <th>Annie Hall (1977)</th>      <th>One Flew Over the Cuckoo’s Nest (1975)</th>      <th>Fargo (1996)</th>      <th>Cool Hand Luke (1967)</th>      <th>Chinatown (1974)</th>      <th>North by Northwest (1959)</th>      <th>Citizen Kane (1941)</th>      <th>Wizard of Oz, The (1939)</th>      <th>Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)</th>      <th>…</th>      <th>Sense and Sensibility (1995)</th>      <th>Top Gun (1986)</th>      <th>Flashdance (1983)</th>      <th>Jerry Maguire (1996)</th>      <th>Superman (1978)</th>      <th>Abyss, The (1989)</th>      <th>Devil in a Blue Dress (1995)</th>      <th>Beetlejuice (1988)</th>      <th>Dial M for Murder (1954)</th>      <th>Broken Arrow (1996)</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>5.0</td>      <td>4.0</td>      <td>4.0</td>      <td>5</td>      <td>4</td>      <td>4</td>      <td>4</td>      <td>5</td>      <td></td>      <td>4</td>      <td>…</td>      <td></td>      <td>3</td>      <td></td>      <td>3</td>      <td></td>      <td>3</td>      <td></td>      <td></td>      <td>4</td>      <td>3</td>    </tr>    <tr>      <th>1</th>      <td>4.0</td>      <td>4.0</td>      <td>4.0</td>      <td>4</td>      <td>5</td>      <td>5</td>      <td>3</td>      <td>5</td>      <td>4</td>      <td>3</td>      <td>…</td>      <td></td>      <td>2</td>      <td></td>      <td>3</td>      <td>2</td>      <td></td>      <td>4</td>      <td>2</td>      <td></td>      <td>3</td>    </tr>    <tr>      <th>2</th>      <td>5.0</td>      <td>4.0</td>      <td>5.0</td>      <td>5</td>      <td>5</td>      <td>5</td>      <td>5</td>      <td>5</td>      <td>5</td>      <td>5</td>      <td>…</td>      <td></td>      <td></td>      <td>3</td>      <td>4</td>      <td></td>      <td></td>      <td>5</td>      <td></td>      <td></td>      <td>4</td>    </tr>    <tr>      <th>8</th>      <td>2.0</td>      <td>5.0</td>      <td>2.0</td>      <td>5</td>      <td>3</td>      <td>5</td>      <td>3</td>      <td>4</td>      <td>5</td>      <td>3</td>      <td>…</td>      <td>4.5</td>      <td></td>      <td>2</td>      <td>4</td>      <td>3</td>      <td></td>      <td>3</td>      <td>3</td>      <td></td>      <td></td>    </tr>    <tr>      <th>10</th>      <td>3.0</td>      <td>4.0</td>      <td>3.0</td>      <td>4</td>      <td>5</td>      <td>4</td>      <td>4</td>      <td></td>      <td>4</td>      <td>5</td>      <td>…</td>      <td>5</td>      <td></td>      <td></td>      <td></td>      <td>4</td>      <td>3</td>      <td></td>      <td>2</td>      <td></td>      <td></td>    </tr>  </tbody></table><p>5 rows × 300 columns</p></div><p>从表格中选择一个空白单元格。因为用户没有对该电影评分，所以是空白状态。能够预测她是否喜欢该电影吗？因为该用户属于似乎具有相似品位的用户聚类，我们可以计算该电影在此聚类中的平均评分，结果可以作为她是否喜欢该电影的合理预测依据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Fill in the name of the column/movie. e.g. 'Forrest Gump (1994)'</span></span><br><span class="line"><span class="comment"># Pick a movie from the table above since we're looking at a subset</span></span><br><span class="line">movie_name = <span class="string">'Forrest Gump (1994)'</span></span><br><span class="line"></span><br><span class="line">cluster[movie_name].mean()</span><br></pre></td></tr></table></figure><pre><code>3.6666666666666665</code></pre><p>这就是我们关于她会如何对该电影进行评分的预测。</p><h2 id="推荐"><a href="#推荐" class="headerlink" title="推荐"></a>推荐</h2><p>我们回顾下上一步的操作。我们使用 k 均值根据用户的评分对用户进行聚类。这样就形成了具有相似评分的用户聚类，因此通常具有相似的电影品位。基于这一点，当某个用户对某部电影没有评分时，我们对该聚类中所有其他用户的评分取平均值，该平均值就是我们猜测该用户对该电影的喜欢程度。</p><p>根据这一逻辑，如果我们计算该聚类中每部电影的平均分数，就可以判断该“品位聚类”对数据集中每部电影的喜欢程度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The average rating of 20 movies as rated by the users in the cluster</span></span><br><span class="line">cluster.mean().head(<span class="number">20</span>)</span><br></pre></td></tr></table></figure><pre><code>Amadeus (1984)                                                                    3.833333Annie Hall (1977)                                                                 4.291667One Flew Over the Cuckoo&apos;s Nest (1975)                                            4.208333Fargo (1996)                                                                      4.454545Cool Hand Luke (1967)                                                             4.636364Chinatown (1974)                                                                  4.454545North by Northwest (1959)                                                         4.409091Citizen Kane (1941)                                                               4.681818Wizard of Oz, The (1939)                                                          4.500000Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)    4.272727Butch Cassidy and the Sundance Kid (1969)                                         4.045455Star Wars: Episode V - The Empire Strikes Back (1980)                             4.090909Groundhog Day (1993)                                                              3.727273Gone with the Wind (1939)                                                         4.272727It&apos;s a Wonderful Life (1946)                                                      4.2727272001: A Space Odyssey (1968)                                                      4.272727Shawshank Redemption, The (1994)                                                  4.363636Philadelphia Story, The (1940)                                                    4.409091Bonnie and Clyde (1967)                                                           4.150000To Kill a Mockingbird (1962)                                                      4.400000dtype: float64</code></pre><p>这对我们来说变得非常实用，因为现在我们可以使用它作为推荐引擎，使用户能够发现他们可能喜欢的电影。</p><p>当用户登录我们的应用时，现在我们可以向他们显示符合他们的兴趣品位的电影。推荐方式是选择聚类中该用户尚未评分的最高评分的电影。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Pick a user ID from the dataset</span></span><br><span class="line"><span class="comment"># Look at the table above outputted by the command "cluster.fillna('').head()" </span></span><br><span class="line"><span class="comment"># and pick one of the user ids (the first column in the table)</span></span><br><span class="line">user_id = <span class="number">11</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get all this user's ratings</span></span><br><span class="line">user_2_ratings  = cluster.loc[user_id, :]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Which movies did they not rate? (We don't want to recommend movies they've already rated)</span></span><br><span class="line">user_2_unrated_movies =  user_2_ratings[user_2_ratings.isnull()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># What are the ratings of these movies the user did not rate?</span></span><br><span class="line">avg_ratings = pd.concat([user_2_unrated_movies, cluster.mean()], axis=<span class="number">1</span>, join=<span class="string">'inner'</span>).loc[:,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Let's sort by rating so the highest rated movies are presented first</span></span><br><span class="line">avg_ratings.sort_values(ascending=<span class="keyword">False</span>)[:<span class="number">20</span>]</span><br></pre></td></tr></table></figure><pre><code>Remains of the Day, The (1993)    4.666667Saving Private Ryan (1998)        4.642857African Queen, The (1951)         4.625000Lone Star (1996)                  4.600000Godfather: Part II, The (1974)    4.500000Singin&apos; in the Rain (1952)        4.500000My Cousin Vinny (1992)            4.500000Raising Arizona (1987)            4.500000Fargo (1996)                      4.454545Rain Man (1988)                   4.400000Full Metal Jacket (1987)          4.400000Sense and Sensibility (1995)      4.375000Fried Green Tomatoes (1991)       4.333333Room with a View, A (1986)        4.300000It&apos;s a Wonderful Life (1946)      4.272727Dial M for Murder (1954)          4.250000Laura (1944)                      4.250000American Graffiti (1973)          4.250000Much Ado About Nothing (1993)     4.250000Ordinary People (1980)            4.250000Name: 0, dtype: float64</code></pre><p>这些是向用户推荐的前 20 部电影！</p><h3 id="练习："><a href="#练习：" class="headerlink" title="练习："></a>练习：</h3><ul><li>如果聚类中有一部电影只有一个评分，评分是 5 颗星。该电影在该聚类中的平均评分是多少？这会对我们的简单推荐引擎有何影响？你会如何调整推荐系统，以解决这一问题？</li></ul><h2 id="关于协同过滤的更多信息"><a href="#关于协同过滤的更多信息" class="headerlink" title="关于协同过滤的更多信息"></a>关于协同过滤的更多信息</h2><ul><li>这是一个简单的推荐引擎，展示了“协同过滤”的最基本概念。有很多可以改进该引擎的启发法和方法。为了推动在这一领域的发展，Netflix 设立了 <a href="https://en.wikipedia.org/wiki/Netflix_Prize" target="_blank" rel="noopener">Netflix 奖项</a> ，他们会向对 Netflix 的推荐算法做出最大改进的算法奖励 1,000,000 美元。</li><li>在 2009 年，“BellKor’s Pragmatic Chaos”团队获得了这一奖项。<a href="http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf" target="_blank" rel="noopener">这篇论文</a>介绍了他们采用的方式，其中包含大量方法。</li><li><a href="https://thenextweb.com/media/2012/04/13/remember-netflixs-1m-algorithm-contest-well-heres-why-it-didnt-use-the-winning-entry/" target="_blank" rel="noopener">Netflix 最终并没有使用这个荣获 1,000,000 美元奖励的算法</a>，因为他们采用了流式传输的方式，并产生了比电影评分要庞大得多的数据集——用户搜索了哪些内容？用户在此会话中试看了哪些其他电影？他们是否先看了一部电影，然后切换到了其他电影？这些新的数据点可以提供比评分本身更多的线索。</li></ul><h2 id="深入研究"><a href="#深入研究" class="headerlink" title="深入研究"></a>深入研究</h2><ul><li>该 notebook 显示了用户级推荐系统。我们实际上可以使用几乎一样的代码进行商品级推荐。例如亚马逊的“购买（评价或喜欢）此商品的客户也购买了（评价了或喜欢）以下商品：” 。我们可以在应用的每个电影页面显示这种推荐。为此，我们只需将数据集转置为“电影 X 用户”形状，然后根据评分之间的联系对电影（而不是用户）进行聚类。</li><li>我们从数据集 Movie Lens 中抽取了最小的子集，只包含 100,000 个评分。如果你想深入了解电影评分数据，可以查看他们的<a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">完整数据集</a>，其中包含 2400 万个评分。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;电影评分的-k-均值聚类&quot;&gt;&lt;a href=&quot;#电影评分的-k-均值聚类&quot; class=&quot;headerlink&quot; title=&quot;电影评分的 k 均值聚类&quot;&gt;&lt;/a&gt;电影评分的 k 均值聚类&lt;/h1&gt;&lt;p&gt;假设你是 Netflix 的一名数据分析师，你想要根据用户对
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>CNN网络图像识别</title>
    <link href="http://yoursite.com/2019/02/15/My-First-Post/"/>
    <id>http://yoursite.com/2019/02/15/My-First-Post/</id>
    <published>2019-02-15T05:05:16.000Z</published>
    <updated>2019-02-15T10:19:04.357Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p><strong>本文使用keras(2.1.4)—-其他版本有坑. 网络框架搭建CNN网络，对cifar10数据集进行图像识别，cifar10是一种自带label的图像数据集，数据集种类十分丰富可以很好的检验网络性能，话不多说直接进入正题 </strong></p><h2 id="第一步获取数据集"><a href="#第一步获取数据集" class="headerlink" title="第一步获取数据集"></a>第一步获取数据集</h2><p>通过keras可以直接下载cifar10数据集(数据集比较大可能需要一些时间)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="comment">#使用cifar10数据集</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> cifar10</span><br><span class="line">(x_train, y_train), (x_test, y_test) = cifar10.load_data()</span><br></pre></td></tr></table></figure></p><h2 id="展示前24张图片"><a href="#展示前24张图片" class="headerlink" title="展示前24张图片"></a>展示前24张图片</h2><p>观察数据集的部分样本别问为什么，要有一个程序员的严谨！！严谨！！严谨！！(重要的事说3遍)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">36</span>):</span><br><span class="line">    ax = fig.add_subplot(<span class="number">3</span>, <span class="number">12</span>, i + <span class="number">1</span>, xticks=[], yticks=[])</span><br><span class="line">    ax.imshow(np.squeeze(x_train[i]))</span><br></pre></td></tr></table></figure></p><h2 id="所有数据集除以255重构图像"><a href="#所有数据集除以255重构图像" class="headerlink" title="所有数据集除以255重构图像"></a>所有数据集除以255重构图像</h2><p>因为图像单个像素中最大值为255，将其除以255是将每一个像素缩放到0-1之间，类似于标准化<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_train = x_train.astype(<span class="string">'float32'</span>)/<span class="number">255</span></span><br><span class="line">x_test = x_test.astype(<span class="string">'float32'</span>)/<span class="number">255</span></span><br></pre></td></tr></table></figure></p><h2 id="将数据分解为测试集、训练集、验证集"><a href="#将数据分解为测试集、训练集、验证集" class="headerlink" title="将数据分解为测试集、训练集、验证集"></a>将数据分解为测试集、训练集、验证集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将标签转化为one-hot</span></span><br><span class="line">num_classes = len(np.unique(y_train))</span><br><span class="line">y_train = keras.utils.to_categorical(y_train, num_classes)</span><br><span class="line">y_test = keras.utils.to_categorical(y_test, num_classes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据分解为训练集和测试集</span></span><br><span class="line">(x_train, x_valid) = x_train[<span class="number">5000</span>:], x_train[:<span class="number">5000</span>]</span><br><span class="line">(y_train, y_valid) = y_train[<span class="number">5000</span>:], y_train[:<span class="number">5000</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出训练集形状</span></span><br><span class="line">print(<span class="string">'x_train shape:'</span>, x_train.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出每一个集合的长度</span></span><br><span class="line">print(x_train.shape[<span class="number">0</span>], <span class="string">'train samples'</span>)</span><br><span class="line">print(x_test.shape[<span class="number">0</span>], <span class="string">'test samples'</span>)</span><br><span class="line">print(x_valid.shape[<span class="number">0</span>], <span class="string">'validation samples'</span>)</span><br></pre></td></tr></table></figure><h2 id="开始构建卷积神经网络"><a href="#开始构建卷积神经网络" class="headerlink" title="开始构建卷积神经网络"></a>开始构建卷积神经网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv2D, MaxPooling2D, Flatten, Dense, Dropout</span><br><span class="line"><span class="comment">#初始化网络类型，选择顺序网络</span></span><br><span class="line">model = Sequential()</span><br><span class="line"><span class="comment">#添加卷积层，使用same填充，relu激活</span></span><br><span class="line">model.add(Conv2D(filters=<span class="number">16</span>, kernel_size=<span class="number">2</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, </span><br><span class="line">                        input_shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)))</span><br><span class="line"><span class="comment">#添加池化层</span></span><br><span class="line">model.add(MaxPooling2D(pool_size=<span class="number">2</span>))</span><br><span class="line">model.add(Conv2D(filters=<span class="number">32</span>, kernel_size=<span class="number">2</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(MaxPooling2D(pool_size=<span class="number">2</span>))</span><br><span class="line">model.add(Conv2D(filters=<span class="number">64</span>, kernel_size=<span class="number">2</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(MaxPooling2D(pool_size=<span class="number">2</span>))</span><br><span class="line"><span class="comment">#舍弃部分神经元，避免过拟合</span></span><br><span class="line">model.add(Dropout(<span class="number">0.3</span>))</span><br><span class="line"><span class="comment">#数据扁平化</span></span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">500</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.4</span>))</span><br><span class="line">model.add(Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"><span class="comment">#模型确认</span></span><br><span class="line">model.summary()</span><br><span class="line"><span class="comment">#模型启动，定义损失函数，优化器，评分标准</span></span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'rmsprop'</span>, </span><br><span class="line">                  metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure><h2 id="模型训练开始！"><a href="#模型训练开始！" class="headerlink" title="模型训练开始！"></a>模型训练开始！</h2><p>心疼一波没有GPU的小伙伴。。。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint   </span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">checkpointer = ModelCheckpoint(filepath=<span class="string">'model.weights.best.hdf5'</span>, verbose=<span class="number">1</span>, </span><br><span class="line">                               save_best_only=<span class="keyword">True</span>)</span><br><span class="line">hist = model.fit(x_train, y_train, batch_size=<span class="number">32</span>, epochs=<span class="number">100</span>,</span><br><span class="line">          validation_data=(x_valid, y_valid), callbacks=[checkpointer], </span><br><span class="line">          verbose=<span class="number">2</span>, shuffle=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></p><h2 id="测试集预测"><a href="#测试集预测" class="headerlink" title="测试集预测"></a>测试集预测</h2><p>终于到了激动人心的时刻，想不想知道自己搭建的模型的性能? 等着吧！<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取训练集预测</span></span><br><span class="line">y_hat = model.predict(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义文本标签--来源:(source: https://www.cs.toronto.edu/~kriz/cifar.html)</span></span><br><span class="line">cifar10_labels = [<span class="string">'airplane'</span>, <span class="string">'automobile'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>, <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>]</span><br></pre></td></tr></table></figure></p><h2 id="结果展示！！！！！"><a href="#结果展示！！！！！" class="headerlink" title="结果展示！！！！！"></a>结果展示！！！！！</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 展示样本训练结果</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">20</span>, <span class="number">8</span>))</span><br><span class="line"><span class="keyword">for</span> i, idx <span class="keyword">in</span> enumerate(np.random.choice(x_test.shape[<span class="number">0</span>], size=<span class="number">32</span>, replace=<span class="keyword">False</span>)):</span><br><span class="line">    ax = fig.add_subplot(<span class="number">4</span>, <span class="number">8</span>, i + <span class="number">1</span>, xticks=[], yticks=[])</span><br><span class="line">    ax.imshow(np.squeeze(x_test[idx]))</span><br><span class="line">    pred_idx = np.argmax(y_hat[idx])</span><br><span class="line">    true_idx = np.argmax(y_test[idx])</span><br><span class="line">    ax.set_title(<span class="string">"&#123;&#125; (&#123;&#125;)"</span>.format(cifar10_labels[pred_idx], cifar10_labels[true_idx]),</span><br><span class="line">                 color=(<span class="string">"green"</span> <span class="keyword">if</span> pred_idx == true_idx <span class="keyword">else</span> <span class="string">"red"</span>))</span><br></pre></td></tr></table></figure><h2 id="感言"><a href="#感言" class="headerlink" title="感言:"></a>感言:</h2><p>说实话图像识别的发展是一个很漫长的过程，通过结果可以发现有时候我们确实有点为难机器了，不信你们自己看看那训练结果。。 有些图片你自己都不知道是什么东西。。 还有一点 感谢各位的支持 ！拜拜👋！</p><p>还没完。 没有GPU的小伙伴可以去亚马逊申请免费的GPU服务器后 嘿嘿😁最后像提供数据集的前辈们致敬！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;本文使用keras(2.1.4)—-其他版本有坑. 网络框架搭建CNN网络，对cifar10数据集进行图像识别，cifar1
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
</feed>
