{"meta":{"title":"Jindong","subtitle":null,"description":"博客","author":"Jindong Wang","url":"http://jinyaxuan.github.io"},"pages":[{"title":"Tags","date":"2019-02-16T18:52:11.000Z","updated":"2019-02-16T18:53:16.344Z","comments":true,"path":"Tags/index.html","permalink":"http://jinyaxuan.github.io/Tags/index.html","excerpt":"","text":""}],"posts":[{"title":"robot maze","slug":"robot-maze","date":"2019-02-16T17:41:54.000Z","updated":"2019-02-16T18:17:28.631Z","comments":false,"path":"2019/02/17/robot-maze/","link":"","permalink":"http://jinyaxuan.github.io/2019/02/17/robot-maze/","excerpt":"","text":"Section 0 问题描述与完成项目流程1. 问题描述 在该项目中，你将使用强化学习算法，实现一个自动走迷宫机器人。 如上图所示，智能机器人显示在右上角。在我们的迷宫中，有陷阱（红色炸弹）及终点（蓝色的目标点）两种情景。机器人要尽量避开陷阱、尽快到达目的地。 小车可执行的动作包括：向上走 u、向右走 r、向下走 d、向左走 l。 执行不同的动作后，根据不同的情况会获得不同的奖励，具体而言，有以下几种情况。 撞到墙壁：-10 走到终点：50 走到陷阱：-30 其余情况：-0.1 我们需要通过修改 robot.py 中的代码，来实现一个 Q Learning 机器人，实现上述的目标。 2. 完成项目流程 配置环境，使用 envirnment.yml 文件配置名为 robot-env 的 conda 环境，具体而言，你只需转到当前的目录，在命令行/终端中运行如下代码，稍作等待即可。conda env create -f envirnment.yml 安装完毕后，在命令行/终端中运行 source activate robot-env（Mac/Linux 系统）或 activate robot-env（Windows 系统）激活该环境。 阅读 main.ipynb 中的指导完成项目，并根据指导修改对应的代码，生成、观察结果。 导出代码与报告，上传文件，提交审阅并优化。 Section 1 算法理解1. 1 强化学习总览强化学习作为机器学习算法的一种，其模式也是让智能体在“训练”中学到“经验”，以实现给定的任务。但不同于监督学习与非监督学习，在强化学习的框架中，我们更侧重通过智能体与环境的交互来学习。通常在监督学习和非监督学习任务中，智能体往往需要通过给定的训练集，辅之以既定的训练目标（如最小化损失函数），通过给定的学习算法来实现这一目标。然而在强化学习中，智能体则是通过其与环境交互得到的奖励进行学习。这个环境可以是虚拟的（如虚拟的迷宫），也可以是真实的（自动驾驶汽车在真实道路上收集数据）。 在强化学习中有五个核心组成部分，它们分别是：环境（Environment）、智能体（Agent）、状态（State）、动作（Action）和奖励（Reward）。在某一时间节点 $t$： 智能体在从环境中感知其所处的状态 $s_t$ 智能体根据某些准则选择动作 $a_t$ 环境根据智能体选择的动作，向智能体反馈奖励 $r_{t+1}$ 通过合理的学习算法，智能体将在这样的问题设置下，成功学到一个在状态 $s_t$ 选择动作 $a_t$ 的策略 $\\pi (s_t) = a_t$。 问题 1：请参照如上的定义，描述出 “机器人走迷宫这个问题” 中强化学习五个组成部分对应的实际对象： 环境 : 某一时间点t 状态 : 从环境中获取其所出状态s_{t} 动作 : 依据某些准则选择动作a_{t} 奖励 : 依据智能体选择的动作反馈奖励r_{t+1} T(s^{'}, a, s) = P(s^{'}|a,s) 1.2 计算 Q 值在我们的项目中，我们要实现基于 Q-Learning 的强化学习算法。Q-Learning 是一个值迭代（Value Iteration）算法。与策略迭代（Policy Iteration）算法不同，值迭代算法会计算每个”状态“或是”状态-动作“的值（Value）或是效用（Utility），然后在执行动作的时候，会设法最大化这个值。因此，对每个状态值的准确估计，是我们值迭代算法的核心。通常我们会考虑最大化动作的长期奖励，即不仅考虑当前动作带来的奖励，还会考虑动作长远的奖励。 在 Q-Learning 算法中，我们把这个长期奖励记为 Q 值，我们会考虑每个 ”状态-动作“ 的 Q 值，具体而言，它的计算公式为： q(s_{t},a) = R_{t+1} + \\gamma \\times\\max_a q(a,s_{t+1})也就是对于当前的“状态-动作” $(s{t},a)$，我们考虑执行动作 $a$ 后环境给我们的奖励 $R{t+1}$，以及执行动作 $a$ 到达 $s{t+1}$后，执行任意动作能够获得的最大的Q值 $\\max_a q(a,s{t+1})$，$\\gamma$ 为折扣因子。 不过一般地，我们使用更为保守地更新 Q 表的方法，即引入松弛变量 $alpha$，按如下的公式进行更新，使得 Q 表的迭代变化更为平缓。 q(s_{t},a) = (1-\\alpha) \\times q(s_{t},a) + \\alpha \\times(R_{t+1} + \\gamma \\times\\max_a q(a,s_{t+1})) 问题 2：根据已知条件求 $q(s_{t},a)$，在如下模板代码中的空格填入对应的数字即可。 已知：如上图，机器人位于 $s_1$，行动为 u，行动获得的奖励与题目的默认设置相同。在 $s_2$ 中执行各动作的 Q 值为：u: -24，r: -13，d: -0.29、l: +40，$\\gamma$ 取0.9。 \\begin{align} q(s_{t},a) & = R_{t+1} + \\gamma \\times\\max_a q(a,s_{t+1}) \\\\ & =(-0.1) + (0.9)*(40) \\\\ & =(35.9) \\end{align} 1.3 如何选择动作在强化学习中，「探索-利用」问题是非常重要的问题。具体来说，根据上面的定义，我们会尽可能地让机器人在每次选择最优的决策，来最大化长期奖励。但是这样做有如下的弊端： 在初步的学习中，我们的 Q 值会不准确，如果在这个时候都按照 Q 值来选择，那么会造成错误。 学习一段时间后，机器人的路线会相对固定，则机器人无法对环境进行有效的探索。 因此我们需要一种办法，来解决如上的问题，增加机器人的探索。由此我们考虑使用 epsilon-greedy 算法，即在小车选择动作的时候，以一部分的概率随机选择动作，以一部分的概率按照最优的 Q 值选择动作。同时，这个选择随机动作的概率应当随着训练的过程逐步减小。 问题 3：在如下的代码块中，实现 epsilon-greedy 算法的逻辑，并运行测试代码。 import randomimport operatoractions = ['u','r','d','l']qline = &#123;'u':1.2, 'r':-2.1, 'd':-24.5, 'l':27&#125;epsilon = 0.3 # 以0.3的概率进行随机选择def choose_action(epsilon): action = None if random.uniform(0,1.0) &lt;= epsilon: # 以某一概率 action = random.choice(actions) # 实现对动作的随机选择 else: action = max(qline.items(), key=operator.itemgetter(1))[0] # 否则选择具有最大 Q 值的动作 return actionresult = ''for i in range(10000): result += choose_action(epsilon)result &#39;llldllllllldlllllllllrllllldllllluldllrrlllllllllllllllrlllldllllrlrllrlulllllulllrlldlllllrlllllllrlllllrllllllllllldlllludlu...&#39; Section 2 代码实现2.1. Maze 类理解我们首先引入了迷宫类 Maze，这是一个非常强大的函数，它能够根据你的要求随机创建一个迷宫，或者根据指定的文件，读入一个迷宫地图信息。 使用 Maze(&quot;file_name&quot;) 根据指定文件创建迷宫，或者使用 Maze(maze_size=(height,width)) 来随机生成一个迷宫。 使用 trap_number 参数，在创建迷宫的时候，设定迷宫中陷阱的数量。 直接键入迷宫变量的名字按回车，展示迷宫图像（如 g=Maze(&quot;xx.txt&quot;)，那么直接输入 g 即可。 建议生成的迷宫尺寸，长在 6~12 之间，宽在 10～12 之间。 问题 4：在如下的代码块中，创建你的迷宫并展示。 from Maze import Maze%matplotlib inline%config InlineBackend.figure_format = 'retina'## todo: 创建迷宫并展示g = Maze('test_world/maze_01.txt')g Maze of size (12, 12) 你可能已经注意到，在迷宫中我们已经默认放置了一个机器人。实际上，我们为迷宫配置了相应的 API，来帮助机器人的移动与感知。其中你随后会使用的两个 API 为 maze.sense_robot() 及 maze.move_robot()。 maze.sense_robot() 为一个无参数的函数，输出机器人在迷宫中目前的位置。 maze.move_robot(direction) 对输入的移动方向，移动机器人，并返回对应动作的奖励值。 问题 5：随机移动机器人，并记录下获得的奖励，展示出机器人最后的位置。 rewards = []## 循环、随机移动机器人10次，记录下奖励for i in range(10): result = g.move_robot(random.choice(actions)) rewards.append(result)## 输出机器人最后的位置print(g.sense_robot)## 打印迷宫，观察机器人位置g &lt;bound method Maze.sense_robot of Maze of size (12, 12)&gt; Maze of size (12, 12) 2.2. Robot 类实现Robot 类是我们需要重点实现的部分。在这个类中，我们需要实现诸多功能，以使得我们成功实现一个强化学习智能体。总体来说，之前我们是人为地在环境中移动了机器人，但是现在通过实现 Robot 这个类，机器人将会自己移动。通过实现学习函数，Robot 类将会学习到如何选择最优的动作，并且更新强化学习中对应的参数。 首先 Robot 有多个输入，其中 alpha=0.5, gamma=0.9, epsilon0=0.5 表征强化学习相关的各个参数的默认值，这些在之前你已经了解到，Maze 应为机器人所在迷宫对象。 随后观察 Robot.update 函数，它指明了在每次执行动作时，Robot 需要执行的程序。按照这些程序，各个函数的功能也就明了了。 最后你需要实现 Robot.py 代码中的8段代码，他们都在代码中以 #TODO 进行标注，你能轻松地找到他们。 问题 6：实现 Robot.py 中的8段代码，并运行如下代码检查效果（记得将 maze 变量修改为你创建迷宫的变量名）。 import randomfrom Robot import Robotrobot = Robot(g) # 记得将 maze 变量修改为你创建迷宫的变量名robot.set_status(learning=True,testing=False)print(robot.update())g (&#39;u&#39;, -0.1) Maze of size (12, 12) 2.3 用 Runner 类训练 Robot在实现了上述内容之后，我们就可以开始对我们 Robot 进行训练并调参了。我们为你准备了又一个非常棒的类 Runner，来实现整个训练过程及可视化。使用如下的代码，你可以成功对机器人进行训练。并且你会在当前文件夹中生成一个名为 filename 的视频，记录了整个训练的过程。通过观察该视频，你能够发现训练过程中的问题，并且优化你的代码及参数。 问题 7：尝试利用下列代码训练机器人，并进行调参。可选的参数包括： 训练参数 训练次数 epoch 机器人参数： epsilon0 (epsilon 初值) epsilon衰减（可以是线性、指数衰减，可以调整衰减的速度），你需要在 Robot.py 中调整 alpha gamma 迷宫参数: 迷宫大小 迷宫中陷阱的数量 ## 可选的参数：epoch = 30epsilon0 = 0.6alpha = 0.5gamma = 0.9maze_size = (6,8)trap_number = 2 from Runner import Runnerg = Maze(maze_size=maze_size,trap_number=trap_number)r = Robot(g,alpha=alpha, epsilon0=epsilon0, gamma=gamma)r.set_status(learning=True)runner = Runner(r, g)runner.run_training(epoch, display_direction=True)runner.generate_movie(filename = \"final1.mp4\") # 你可以注释该行代码，加快运行速度，不过你就无法观察到视频了。 Generate Movies: 100%|██████████| 892/892 [00:10&lt;00:00, 87.02it/s] 使用 runner.plot_results() 函数，能够打印机器人在训练过程中的一些参数信息。 Success Times 代表机器人在训练过程中成功的累计次数，这应当是一个累积递增的图像。 Accumulated Rewards 代表机器人在每次训练 epoch 中，获得的累积奖励的值，这应当是一个逐步递增的图像。 Running Times per Epoch 代表在每次训练 epoch 中，小车训练的次数（到达终点就会停止该 epoch 转入下次训练），这应当是一个逐步递减的图像。 问题 8：使用 runner.plot_results() 输出训练结果，根据该结果对你的机器人进行分析。 指出你选用的参数如何，选用参数的原因。 建议你比较不同参数下机器人的训练的情况。 训练的结果是否满意，有何改进的计划。 runner.plot_results()","categories":[{"name":"强化学习","slug":"强化学习","permalink":"http://jinyaxuan.github.io/categories/强化学习/"}],"tags":[{"name":"Reinforcement learning","slug":"Reinforcement-learning","permalink":"http://jinyaxuan.github.io/tags/Reinforcement-learning/"}],"keywords":[{"name":"强化学习","slug":"强化学习","permalink":"http://jinyaxuan.github.io/categories/强化学习/"}]},{"title":"Explore Movie Dataset","slug":"Explore-Movie-Dataset","date":"2019-02-16T15:51:36.000Z","updated":"2019-02-16T15:52:11.776Z","comments":false,"path":"2019/02/16/Explore-Movie-Dataset/","link":"","permalink":"http://jinyaxuan.github.io/2019/02/16/Explore-Movie-Dataset/","excerpt":"","text":"探索电影数据集在这个项目中，你将尝试使用所学的知识，使用 NumPy、Pandas、matplotlib、seaborn 库中的函数，来对电影数据集进行探索。 下载数据集：TMDb电影数据 数据集各列名称的含义： 列名称idimdb_idpopularitybudgetrevenueoriginal_titlecasthomepagedirectortaglinekeywordsoverviewruntimegenresproduction_companiesrelease_datevote_countvote_averagerelease_yearbudget_adjrevenue_adj 含义编号IMDB 编号知名度预算票房名称主演网站导演宣传词关键词简介时常类别发行公司发行日期投票总数投票均值发行年份预算（调整后）票房（调整后） 请注意，你需要提交该报告导出的 .html、.ipynb 以及 .py 文件。 第一节 数据的导入与处理在这一部分，你需要编写代码，使用 Pandas 读取数据，并进行预处理。 任务1.1： 导入库以及数据 载入需要的库 NumPy、Pandas、matplotlib、seaborn。 利用 Pandas 库，读取 tmdb-movies.csv 中的数据，保存为 movie_data。 提示：记得使用 notebook 中的魔法指令 %matplotlib inline，否则会导致你接下来无法打印出图像。 import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsmovie_data = pd.read_csv('/Users/jindongwang/MachingLearning/AIPND-cn/tmdb-movies.csv')%matplotlib inline 任务1.2: 了解数据 你会接触到各种各样的数据表，因此在读取之后，我们有必要通过一些简单的方法，来了解我们数据表是什么样子的。 获取数据表的行列，并打印。 使用 .head()、.tail()、.sample() 方法，观察、了解数据表的情况。 使用 .dtypes 属性，来查看各列数据的数据类型。 使用 isnull() 配合 .any() 等方法，来查看各列是否存在空值。 使用 .describe() 方法，看看数据表中数值型的数据是怎么分布的。 movie_data.head()movie_data.tail()movie_data.sample()movie_data.dtypesmovie_data.isnull().any()movie_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id popularity budget revenue runtime vote_count vote_average release_year budget_adj revenue_adj profit count 10820.000000 10820.000000 1.082000e+04 1.082000e+04 10820.000000 10820.000000 10820.000000 10820.000000 1.082000e+04 1.082000e+04 1.082000e+04 mean 66274.834381 0.647896 1.468256e+07 3.998479e+07 102.050370 218.179020 5.974270 2001.472828 1.758992e+07 5.153012e+07 2.530223e+07 std 92265.282197 1.001940 3.096573e+07 1.172250e+08 31.377214 576.706535 0.935026 12.630994 3.434136e+07 1.448923e+08 9.677916e+07 min 5.000000 0.000065 0.000000e+00 0.000000e+00 0.000000 10.000000 1.500000 1960.000000 0.000000e+00 0.000000e+00 -4.139124e+08 25% 10605.750000 0.207988 0.000000e+00 0.000000e+00 90.000000 17.000000 5.400000 1995.000000 0.000000e+00 0.000000e+00 0.000000e+00 50% 20695.500000 0.384556 0.000000e+00 0.000000e+00 99.000000 38.000000 6.000000 2006.000000 0.000000e+00 0.000000e+00 0.000000e+00 75% 75789.250000 0.715897 1.510000e+07 2.427926e+07 111.000000 146.000000 6.600000 2011.000000 2.093530e+07 3.388119e+07 9.333081e+06 max 417859.000000 32.985763 4.250000e+08 2.781506e+09 900.000000 9767.000000 9.200000 2015.000000 4.250000e+08 2.827124e+09 2.544506e+09 任务1.3: 清理数据 在真实的工作场景中，数据处理往往是最为费时费力的环节。但是幸运的是，我们提供给大家的 tmdb 数据集非常的「干净」，不需要大家做特别多的数据清洗以及处理工作。在这一步中，你的核心的工作主要是对数据表中的空值进行处理。你可以使用 .fillna() 来填补空值，当然也可以使用 .dropna() 来丢弃数据表中包含空值的某些行或者列。 任务：使用适当的方法来清理空值，并将得到的数据保存。 # movie_data.fillna(value=movie_data.mean(), inplace=True)movie_data.fillna(method='bfill', inplace=True)movie_data.dropna(axis=0, inplace=True)movie_data.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 10820 entries, 0 to 6181 Data columns (total 22 columns): id 10820 non-null int64 imdb_id 10820 non-null object popularity 10820 non-null float64 budget 10820 non-null int64 revenue 10820 non-null int64 original_title 10820 non-null object cast 10820 non-null object homepage 10820 non-null object director 10820 non-null object tagline 10820 non-null object keywords 10820 non-null object overview 10820 non-null object runtime 10820 non-null int64 genres 10820 non-null object production_companies 10820 non-null object release_date 10820 non-null object vote_count 10820 non-null int64 vote_average 10820 non-null float64 release_year 10820 non-null int64 budget_adj 10820 non-null float64 revenue_adj 10820 non-null float64 profit 10820 non-null int64 dtypes: float64(4), int64(7), object(11) memory usage: 1.9+ MB 第二节 根据指定要求读取数据相比 Excel 等数据分析软件，Pandas 的一大特长在于，能够轻松地基于复杂的逻辑选择合适的数据。因此，如何根据指定的要求，从数据表当获取适当的数据，是使用 Pandas 中非常重要的技能，也是本节重点考察大家的内容。 任务2.1: 简单读取 读取数据表中名为 id、popularity、budget、runtime、vote_average 列的数据。 读取数据表中前1～20行以及48、49行的数据。 读取数据表中第50～60行的 popularity 那一列的数据。 要求：每一个语句只能用一行代码实现。 title_list = ['id', 'popularity', 'budget', 'runtime', 'vote_average']movie_data[title_list]movie_data.iloc[[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,47,48]]#单个我会 这组合起来就 。。movie_data[50:61]['popularity'] 1387 6.098027 6081 6.095293 643 6.052479 3912 6.012584 13 5.984995 644 5.947136 14 5.944927 4364 5.944518 6190 5.939927 3373 5.903353 15 5.898400 Name: popularity, dtype: float64 任务2.2: 逻辑读取（Logical Indexing） 读取数据表中 popularity 大于5 的所有数据。 读取数据表中 popularity 大于5 的所有数据且发行年份在1996年之后的所有数据。 提示：Pandas 中的逻辑运算符如 &amp;、|，分别代表且以及或。 要求：请使用 Logical Indexing实现。 movie_data[movie_data['popularity']&gt;5]movie_data[(movie_data['popularity']&gt;5) &amp; (movie_data['release_year']&gt;1996)] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id imdb_id popularity budget revenue original_title cast homepage director tagline ... runtime genres production_companies release_date vote_count vote_average release_year budget_adj revenue_adj profit 0 135397 tt0369610 32.985763 150000000 1513528810 Jurassic World Chris Pratt|Bryce Dallas Howard|Irrfan Khan|Vi... http://www.jurassicworld.com/ Colin Trevorrow The park is open. ... 124 Action|Adventure|Science Fiction|Thriller Universal Studios|Amblin Entertainment|Legenda... 6/9/15 5562 6.5 2015 1.379999e+08 1.392446e+09 1363528810 1 76341 tt1392190 28.419936 150000000 378436354 Mad Max: Fury Road Tom Hardy|Charlize Theron|Hugh Keays-Byrne|Nic... http://www.madmaxmovie.com/ George Miller What a Lovely Day. ... 120 Action|Adventure|Science Fiction|Thriller Village Roadshow Pictures|Kennedy Miller Produ... 5/13/15 6185 7.1 2015 1.379999e+08 3.481613e+08 228436354 629 157336 tt0816692 24.949134 165000000 621752480 Interstellar Matthew McConaughey|Jessica Chastain|Anne Hath... http://www.interstellarmovie.net/ Christopher Nolan Mankind was born on Earth. It was never meant ... ... 169 Adventure|Drama|Science Fiction Paramount Pictures|Legendary Pictures|Warner B... 11/5/14 6498 8.0 2014 1.519800e+08 5.726906e+08 456752480 630 118340 tt2015381 14.311205 170000000 773312399 Guardians of the Galaxy Chris Pratt|Zoe Saldana|Dave Bautista|Vin Dies... http://marvel.com/guardians James Gunn All heroes start somewhere. ... 121 Action|Science Fiction|Adventure Marvel Studios|Moving Picture Company (MPC)|Bu... 7/30/14 5612 7.9 2014 1.565855e+08 7.122911e+08 603312399 2 262500 tt2908446 13.112507 110000000 295238201 Insurgent Shailene Woodley|Theo James|Kate Winslet|Ansel... http://www.thedivergentseries.movie/#insurgent Robert Schwentke One Choice Can Destroy You ... 119 Adventure|Science Fiction|Thriller Summit Entertainment|Mandeville Films|Red Wago... 3/18/15 2480 6.3 2015 1.012000e+08 2.716190e+08 185238201 631 100402 tt1843866 12.971027 170000000 714766572 Captain America: The Winter Soldier Chris Evans|Scarlett Johansson|Sebastian Stan|... http://www.captainamericathewintersoldiermovie... Joe Russo|Anthony Russo In heroes we trust. ... 136 Action|Adventure|Science Fiction Marvel Studios 3/20/14 3848 7.6 2014 1.565855e+08 6.583651e+08 544766572 632 245891 tt2911666 11.422751 20000000 78739897 John Wick Keanu Reeves|Michael Nyqvist|Alfie Allen|Wille... http://www.johnwickthemovie.com/ Chad Stahelski|David Leitch Don't set him off. ... 101 Action|Thriller Thunder Road Pictures|Warner Bros.|87Eleven|De... 10/22/14 2712 7.0 2014 1.842182e+07 7.252661e+07 58739897 3 140607 tt2488496 11.173104 200000000 2068178225 Star Wars: The Force Awakens Harrison Ford|Mark Hamill|Carrie Fisher|Adam D... http://www.starwars.com/films/star-wars-episod... J.J. Abrams Every generation has a story. ... 136 Action|Adventure|Science Fiction|Fantasy Lucasfilm|Truenorth Productions|Bad Robot 12/15/15 5292 7.5 2015 1.839999e+08 1.902723e+09 1868178225 633 131631 tt1951265 10.739009 125000000 752100229 The Hunger Games: Mockingjay - Part 1 Jennifer Lawrence|Josh Hutcherson|Liam Hemswor... http://www.thehungergames.movie/ Francis Lawrence Fire burns brighter in the darkness ... 123 Science Fiction|Adventure|Thriller Lionsgate|Color Force 11/18/14 3590 6.6 2014 1.151364e+08 6.927528e+08 627100229 634 122917 tt2310332 10.174599 250000000 955119788 The Hobbit: The Battle of the Five Armies Martin Freeman|Ian McKellen|Richard Armitage|K... http://www.thehobbit.com/ Peter Jackson Witness the defining chapter of the Middle-Ear... ... 144 Adventure|Fantasy WingNut Films|New Line Cinema|3Foot7|Metro-Gol... 12/10/14 3110 7.1 2014 2.302728e+08 8.797523e+08 705119788 1386 19995 tt0499549 9.432768 237000000 2781505847 Avatar Sam Worthington|Zoe Saldana|Sigourney Weaver|S... http://www.avatarmovie.com/ James Cameron Enter the World of Pandora. ... 162 Action|Adventure|Fantasy|Science Fiction Ingenious Film Partners|Twentieth Century Fox ... 12/10/09 8458 7.1 2009 2.408869e+08 2.827124e+09 2544505847 1919 27205 tt1375666 9.363643 160000000 825500000 Inception Leonardo DiCaprio|Joseph Gordon-Levitt|Ellen P... http://inceptionmovie.warnerbros.com/ Christopher Nolan Your mind is the scene of the crime. ... 148 Action|Thriller|Science Fiction|Mystery|Adventure Legendary Pictures|Warner Bros.|Syncopy 7/14/10 9767 7.9 2010 1.600000e+08 8.255000e+08 665500000 4 168259 tt2820852 9.335014 190000000 1506249360 Furious 7 Vin Diesel|Paul Walker|Jason Statham|Michelle ... http://www.furious7.com/ James Wan Vengeance Hits Home ... 137 Action|Crime|Thriller Universal Pictures|Original Film|Media Rights ... 4/1/15 2947 7.3 2015 1.747999e+08 1.385749e+09 1316249360 5 281957 tt1663202 9.110700 135000000 532950503 The Revenant Leonardo DiCaprio|Tom Hardy|Will Poulter|Domhn... http://www.foxmovies.com/movies/the-revenant Alejandro GonzÃ¡lez IÃ±Ã¡rritu (n. One who has returned, as if from the dead.) ... 156 Western|Drama|Adventure|Thriller Regency Enterprises|Appian Way|CatchPlay|Anony... 12/25/15 3929 7.2 2015 1.241999e+08 4.903142e+08 397950503 2409 550 tt0137523 8.947905 63000000 100853753 Fight Club Edward Norton|Brad Pitt|Meat Loaf|Jared Leto|H... http://www.foxmovies.com/movies/fight-club David Fincher How much can you know about yourself if you've... ... 139 Drama Regency Enterprises|Fox 2000 Pictures|Taurus F... 10/14/99 5923 8.1 1999 8.247033e+07 1.320229e+08 37853753 635 177572 tt2245084 8.691294 165000000 652105443 Big Hero 6 Scott Adsit|Ryan Potter|Daniel Henney|T.J. Mil... http://movies.disney.com/big-hero-6 Don Hall|Chris Williams From the creators of Wreck-it Ralph and Frozen ... 102 Adventure|Family|Animation|Action|Comedy Walt Disney Pictures|Walt Disney Animation Stu... 10/24/14 4185 7.8 2014 1.519800e+08 6.006485e+08 487105443 6 87101 tt1340138 8.654359 155000000 440603537 Terminator Genisys Arnold Schwarzenegger|Jason Clarke|Emilia Clar... http://www.terminatormovie.com/ Alan Taylor Reset the future ... 125 Science Fiction|Action|Thriller|Adventure Paramount Pictures|Skydance Productions 6/23/15 2598 5.8 2015 1.425999e+08 4.053551e+08 285603537 2633 120 tt0120737 8.575419 93000000 871368364 The Lord of the Rings: The Fellowship of the Ring Elijah Wood|Ian McKellen|Viggo Mortensen|Liv T... http://www.lordoftherings.net/ Peter Jackson One ring to rule them all ... 178 Adventure|Fantasy|Action WingNut Films|New Line Cinema|The Saul Zaentz ... 12/18/01 6079 7.8 2001 1.145284e+08 1.073080e+09 778368364 2875 155 tt0468569 8.466668 185000000 1001921825 The Dark Knight Christian Bale|Michael Caine|Heath Ledger|Aaro... http://thedarkknight.warnerbros.com/dvdsite/ Christopher Nolan Why So Serious? ... 152 Drama|Action|Crime|Thriller DC Comics|Legendary Pictures|Warner Bros.|Syncopy 7/16/08 8432 8.1 2008 1.873655e+08 1.014733e+09 816921825 3371 161337 tt2381375 8.411577 0 0 Underworld: Endless War Trevor Devall|Brian Dobson|Paul Dobson|Laura H... http://captainamerica.marvel.com/ Juno John Lee When patriots become heroes ... 18 Action|Animation|Horror Marvel Studios 10/19/11 21 5.9 2011 0.000000e+00 0.000000e+00 0 636 205596 tt2084970 8.110711 14000000 233555708 The Imitation Game Benedict Cumberbatch|Keira Knightley|Matthew G... http://theimitationgamemovie.com/ Morten Tyldum The true enigma was the man who cracked the code. ... 113 History|Drama|Thriller|War Black Bear Pictures|Bristol Automotive 11/14/14 3478 8.0 2014 1.289527e+07 2.151261e+08 219555708 3911 121 tt0167261 8.095275 79000000 926287400 The Lord of the Rings: The Two Towers Elijah Wood|Ian McKellen|Viggo Mortensen|Liv T... http://www.lordoftherings.net/ Peter Jackson A New Power Is Rising. ... 179 Adventure|Fantasy|Action WingNut Films|New Line Cinema|The Saul Zaentz ... 12/18/02 5114 7.8 2002 9.576865e+07 1.122902e+09 847287400 2634 671 tt0241527 8.021423 125000000 976475550 Harry Potter and the Philosopher's Stone Daniel Radcliffe|Rupert Grint|Emma Watson|John... http://harrypotter.warnerbros.com/harrypottera... Chris Columbus Let the Magic Begin. ... 152 Adventure|Fantasy|Family 1492 Pictures|Warner Bros.|Heyday Films 11/16/01 4265 7.2 2001 1.539360e+08 1.202518e+09 851475550 3372 1771 tt0458339 7.959228 140000000 370569774 Captain America: The First Avenger Chris Evans|Hugo Weaving|Tommy Lee Jones|Hayle... http://captainamerica.marvel.com/ Joe Johnston When patriots become heroes ... 124 Action|Adventure|Science Fiction Marvel Studios 7/22/11 5025 6.5 2011 1.357157e+08 3.592296e+08 230569774 2410 603 tt0133093 7.753899 63000000 463517383 The Matrix Keanu Reeves|Laurence Fishburne|Carrie-Anne Mo... http://www.warnerbros.com/matrix Lilly Wachowski|Lana Wachowski Welcome to the Real World. ... 136 Action|Science Fiction Village Roadshow Pictures|Groucho II Film Part... 3/30/99 6351 7.8 1999 8.247033e+07 6.067687e+08 400517383 7 286217 tt3659388 7.667400 108000000 595380321 The Martian Matt Damon|Jessica Chastain|Kristen Wiig|Jeff ... http://www.foxmovies.com/movies/the-martian Ridley Scott Bring Him Home ... 141 Drama|Adventure|Science Fiction Twentieth Century Fox Film Corporation|Scott F... 9/30/15 4572 7.6 2015 9.935996e+07 5.477497e+08 487380321 4361 24428 tt0848228 7.637767 220000000 1519557910 The Avengers Robert Downey Jr.|Chris Evans|Mark Ruffalo|Chr... http://marvel.com/avengers_movie/ Joss Whedon Some assembly required. ... 143 Science Fiction|Action|Adventure Marvel Studios 4/25/12 8903 7.3 2012 2.089437e+08 1.443191e+09 1299557910 8 211672 tt2293640 7.404165 74000000 1156730962 Minions Sandra Bullock|Jon Hamm|Michael Keaton|Allison... http://www.minionsmovie.com/ Kyle Balda|Pierre Coffin Before Gru, they had a history of bad bosses ... 91 Family|Animation|Adventure|Comedy Universal Pictures|Illumination Entertainment 6/17/15 2893 6.5 2015 6.807997e+07 1.064192e+09 1082730962 637 198663 tt1790864 7.137273 34000000 348319861 The Maze Runner Dylan O'Brien|Ki Hong Lee|Kaya Scodelario|Aml ... http://themazerunnermovie.com/ Wes Ball Run - Remember - Survive ... 113 Action|Mystery|Science Fiction|Thriller Ingenious Media|Twentieth Century Fox Film Cor... 9/10/14 3425 7.0 2014 3.131710e+07 3.208343e+08 314319861 4949 122 tt0167260 7.122455 94000000 1118888979 The Lord of the Rings: The Return of the King Elijah Wood|Ian McKellen|Viggo Mortensen|Liv T... http://www.lordoftherings.net Peter Jackson The eye of the enemy is moving. ... 201 Adventure|Fantasy|Action WingNut Films|New Line Cinema 12/1/03 5636 7.9 2003 1.114231e+08 1.326278e+09 1024888979 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 3912 672 tt0295297 6.012584 100000000 876688482 Harry Potter and the Chamber of Secrets Daniel Radcliffe|Rupert Grint|Emma Watson|Kenn... http://www.iceagemovies.com/films/ice-age Chris Columbus Hogwarts is back in session. ... 161 Adventure|Fantasy|Family 1492 Pictures|Warner Bros.|Heyday Films|MIRACL... 11/13/02 3458 7.2 2002 1.212261e+08 1.062776e+09 776688482 13 257344 tt2120120 5.984995 88000000 243637091 Pixels Adam Sandler|Michelle Monaghan|Peter Dinklage|... http://www.pixels-movie.com/ Chris Columbus Game On. ... 105 Action|Comedy|Science Fiction Columbia Pictures|Happy Madison Productions 7/16/15 1575 5.8 2015 8.095996e+07 2.241460e+08 155637091 644 240832 tt2872732 5.947136 40000000 463360063 Lucy Scarlett Johansson|Morgan Freeman|Choi Min-sik... http://lucymovie.com/ Luc Besson The average person uses 10% of their brain cap... ... 89 Action|Science Fiction Universal Pictures|TF1 Films Production|Canal+... 7/14/14 3699 6.3 2014 3.684364e+07 4.267968e+08 423360063 14 99861 tt2395427 5.944927 280000000 1405035767 Avengers: Age of Ultron Robert Downey Jr.|Chris Hemsworth|Mark Ruffalo... http://marvel.com/movies/movie/193/avengers_ag... Joss Whedon A New Age Has Come. ... 141 Action|Adventure|Science Fiction Marvel Studios|Prime Focus|Revolution Sun Studios 4/22/15 4304 7.4 2015 2.575999e+08 1.292632e+09 1125035767 4364 68718 tt1853728 5.944518 100000000 425368238 Django Unchained Jamie Foxx|Christoph Waltz|Leonardo DiCaprio|K... http://unchainedmovie.com/ Quentin Tarantino Life, liberty and the pursuit of vengeance. ... 165 Drama|Western Columbia Pictures|The Weinstein Company 12/25/12 7375 7.7 2012 9.497443e+07 4.039911e+08 325368238 6190 674 tt0330373 5.939927 150000000 895921036 Harry Potter and the Goblet of Fire Daniel Radcliffe|Rupert Grint|Emma Watson|Ralp... http://harrypotter.warnerbros.com/ Mike Newell Dark And Difficult Times Lie Ahead. ... 157 Adventure|Fantasy|Family Patalex IV Productions Limited|Warner Bros.|He... 11/5/05 3406 7.3 2005 1.674845e+08 1.000353e+09 745921036 3373 64690 tt0780504 5.903353 15000000 76175166 Drive Ryan Gosling|Carey Mulligan|Christina Hendrick... http://www.harrypotter.com Nicolas Winding Refn There are no clean getaways. ... 100 Drama|Action|Thriller|Crime Bold Films|Marc Platt Productions|Odd Lot Ente... 1/10/11 2347 7.3 2011 1.454097e+07 7.384406e+07 61175166 15 273248 tt3460252 5.898400 44000000 155760117 The Hateful Eight Samuel L. Jackson|Kurt Russell|Jennifer Jason ... http://thehatefuleight.com/ Quentin Tarantino No one comes up here without a damn good reason. ... 167 Crime|Drama|Mystery|Western Double Feature Films|The Weinstein Company|Fil... 12/25/15 2389 7.4 2015 4.047998e+07 1.432992e+08 111760117 6554 834 tt0401855 5.838503 50000000 111340801 Underworld: Evolution Kate Beckinsale|Scott Speedman|Tony Curran|Sha... http://www.sonypictures.com/movies/underworlda... Len Wiseman My God. Brother, what have you done? ... 106 Fantasy|Action|Science Fiction|Thriller Lakeshore Entertainment|Screen Gems 1/12/06 1015 6.3 2006 5.408346e+07 1.204339e+08 61340801 6962 673 tt0304141 5.827781 130000000 789804554 Harry Potter and the Prisoner of Azkaban Daniel Radcliffe|Rupert Grint|Emma Watson|Gary... http://www.eternalsunshine.com Alfonso CuarÃ³n Something wicked this way comes. ... 141 Adventure|Fantasy|Family 1492 Pictures|Warner Bros.|Heyday Films|P of A... 5/31/04 3550 7.4 2004 1.500779e+08 9.117862e+08 659804554 1388 12437 tt0834001 5.806897 35000000 91327197 Underworld: Rise of the Lycans Bill Nighy|Michael Sheen|Rhona Mitra|Shane Bro... http://harrypotter.warnerbros.com/harrypottera... Patrick Tatopoulos Every war has a beginning. ... 92 Fantasy|Action|Adventure|Science Fiction|Thriller Lakeshore Entertainment|Screen Gems|Sketch Fil... 1/22/09 979 6.2 2009 3.557402e+07 9.282500e+07 56327197 645 98566 tt1291150 5.787396 125000000 477200000 Teenage Mutant Ninja Turtles Megan Fox|Will Arnett|William Fichtner|Alan Ri... http://www.teenagemutantninjaturtlesmovie.com Jonathan Liebesman Mysterious. Dangerous. Reptilious. You've neve... ... 101 Science Fiction|Action|Adventure|Fantasy|Comedy Paramount Pictures|Nickelodeon Movies|Platinum... 8/7/14 1836 5.8 2014 1.151364e+08 4.395446e+08 352200000 16 260346 tt2446042 5.749758 48000000 325771424 Taken 3 Liam Neeson|Forest Whitaker|Maggie Grace|Famke... http://www.taken3movie.com/ Olivier Megaton It Ends Here ... 109 Crime|Action|Thriller Twentieth Century Fox Film Corporation|M6 Film... 1/1/15 1578 6.1 2015 4.415998e+07 2.997096e+08 277771424 3374 12445 tt1201607 5.711315 125000000 1327817822 Harry Potter and the Deathly Hallows: Part 2 Daniel Radcliffe|Rupert Grint|Emma Watson|Alan... http://www.harrypotter.com David Yates It all ends here. ... 130 Adventure|Family|Fantasy Warner Bros.|Heyday Films|Moving Picture Compa... 7/7/11 3750 7.7 2011 1.211748e+08 1.287184e+09 1202817822 1920 10138 tt1228705 5.704860 200000000 623933331 Iron Man 2 Robert Downey Jr.|Gwyneth Paltrow|Don Cheadle|... http://www.ironmanmovie.com/ Jon Favreau It's not the armor that makes the hero, but th... ... 124 Adventure|Action|Science Fiction Marvel Studios 4/28/10 4920 6.6 2010 2.000000e+08 6.239333e+08 423933331 646 225886 tt1956620 5.701683 40000000 126069509 Sex Tape Cameron Diaz|Jason Segel|Rob Corddry|Ellie Kem... http://nightcrawlerfilm.com/ Jake Kasdan A movie about a movie they don't want you to see. ... 97 Comedy Escape Artists|Media Rights Capital|Sony Pictu... 7/17/14 1150 5.3 2014 3.684364e+07 1.161215e+08 86069509 2876 10681 tt0910970 5.678119 180000000 521311860 WALLÂ·E Ben Burtt|Elissa Knight|Jeff Garlin|Fred Willa... http://disney.go.com/disneypictures/wall-e/ Andrew Stanton An adventure beyond the ordinar-E. ... 98 Animation|Family Walt Disney Pictures|Pixar Animation Studios 6/22/08 4209 7.6 2008 1.823016e+08 5.279777e+08 341311860 4365 37724 tt1074638 5.603587 200000000 1108561013 Skyfall Daniel Craig|Judi Dench|Javier Bardem|Ralph Fi... http://www.skyfall-movie.com Sam Mendes Think on your sins. ... 143 Action|Adventure|Thriller Columbia Pictures 10/25/12 6137 6.8 2012 1.899489e+08 1.052849e+09 908561013 17 102899 tt0478970 5.573184 130000000 518602163 Ant-Man Paul Rudd|Michael Douglas|Evangeline Lilly|Cor... http://marvel.com/movies/movie/180/ant-man Peyton Reed Heroes Don't Get Any Bigger ... 115 Science Fiction|Action|Adventure Marvel Studios 7/14/15 3779 7.0 2015 1.195999e+08 4.771138e+08 388602163 1921 12155 tt1014759 5.572950 200000000 1025467110 Alice in Wonderland Mia Wasikowska|Johnny Depp|Anne Hathaway|Helen... http://disney.go.com/wonderland/ Tim Burton You're invited to a very important date. ... 108 Family|Fantasy|Adventure Walt Disney Pictures|Team Todd|Tim Burton Prod... 3/3/10 2853 6.3 2010 2.000000e+08 1.025467e+09 825467110 18 150689 tt1661199 5.556818 95000000 542351353 Cinderella Lily James|Cate Blanchett|Richard Madden|Helen... http://www.thehungergames.movie/ Kenneth Branagh Midnight is just the beginning. ... 112 Romance|Fantasy|Family|Drama Walt Disney Pictures|Genre Films|Beagle Pug Fi... 3/12/15 1495 6.8 2015 8.739996e+07 4.989630e+08 447351353 647 242582 tt2872718 5.522641 8500000 38697217 Nightcrawler Jake Gyllenhaal|Rene Russo|Riz Ahmed|Bill Paxt... http://nightcrawlerfilm.com/ Dan Gilroy The city shines brightest at night ... 117 Crime|Drama|Thriller Bold Films|Sierra / Affinity 10/23/14 2087 7.6 2014 7.829274e+06 3.564366e+07 30197217 19 131634 tt1951266 5.476958 160000000 650523427 The Hunger Games: Mockingjay - Part 2 Jennifer Lawrence|Josh Hutcherson|Liam Hemswor... http://www.thehungergames.movie/ Francis Lawrence The fire will burn forever. ... 136 War|Adventure|Science Fiction Studio Babelsberg|StudioCanal|Lionsgate|Walt D... 11/18/15 2380 6.5 2015 1.471999e+08 5.984813e+08 490523427 20 158852 tt1964418 5.462138 190000000 209035668 Tomorrowland Britt Robertson|George Clooney|Raffey Cassidy|... http://movies.disney.com/tomorrowland Brad Bird Imagine a world where nothing is impossible. ... 130 Action|Family|Science Fiction|Adventure|Mystery Walt Disney Pictures|Babieka|A113 5/19/15 1899 6.2 2015 1.747999e+08 1.923127e+08 19035668 6191 272 tt0372784 5.400826 150000000 374218673 Batman Begins Christian Bale|Michael Caine|Liam Neeson|Katie... http://www2.warnerbros.com/batmanbegins/index.... Christopher Nolan Evil fears the knight. ... 140 Action|Crime|Drama DC Comics|Legendary Pictures|Warner Bros.|DC E... 6/14/05 4914 7.3 2005 1.674845e+08 4.178388e+08 224218673 21 307081 tt1798684 5.337064 30000000 91709827 Southpaw Jake Gyllenhaal|Rachel McAdams|Forest Whitaker... http://www.sanandreasmovie.com/ Antoine Fuqua Believe in Hope. ... 123 Action|Drama Escape Artists|Riche-Ludwig Productions 6/15/15 1386 7.3 2015 2.759999e+07 8.437300e+07 61709827 1922 44214 tt0947798 5.293180 13000000 327803731 Black Swan Natalie Portman|Mila Kunis|Vincent Cassel|Barb... http://www.foxsearchlight.com/blackswan/ Darren Aronofsky In the era of personal branding, the scariest ... ... 108 Drama|Mystery|Thriller Fox Searchlight Pictures|Dune Entertainment|Pr... 12/2/10 2597 7.1 2010 1.300000e+07 3.278037e+08 314803731 5423 49047 tt1454468 5.242753 105000000 716392705 Gravity Sandra Bullock|George Clooney|Ed Harris|Orto I... http://gravitymovie.warnerbros.com/ Alfonso CuarÃ³n Don't Let Go ... 91 Science Fiction|Thriller|Drama Warner Bros.|Heyday Films|Esperanto Filmoj 9/27/13 3775 7.4 2013 9.828350e+07 6.705675e+08 611392705 5424 76338 tt1981115 5.111900 170000000 479765000 Thor: The Dark World Chris Hemsworth|Natalie Portman|Tom Hiddleston... http://marvel.com/thor Alan Taylor Delve into the darkness ... 112 Action|Adventure|Fantasy Marvel Studios 10/29/13 3025 6.8 2013 1.591257e+08 4.490760e+08 309765000 1389 767 tt0417741 5.076472 250000000 933959197 Harry Potter and the Half-Blood Prince Daniel Radcliffe|Rupert Grint|Emma Watson|Tom ... http://harrypotter.warnerbros.com/harrypottera... David Yates Dark Secrets Revealed ... 153 Adventure|Fantasy|Family Warner Bros.|Heyday Films 7/7/09 3220 7.3 2009 2.541001e+08 9.492765e+08 683959197 78 rows × 22 columns 任务2.3: 分组读取 对 release_year 进行分组，使用 .agg 获得 revenue 的均值。 对 director 进行分组，使用 .agg 获得 popularity 的均值，从高到低排列。 要求：使用 Groupby 命令实现。 movie_data.groupby('release_year')['revenue'].agg(np.mean)movie_data.groupby('director')['popularity'].agg(np.mean).sort_values(ascending=False) director Colin Trevorrow 16.696886 Joe Russo|Anthony Russo 12.971027 Chad Stahelski|David Leitch 11.422751 Don Hall|Chris Williams 8.691294 Juno John Lee 8.411577 Kyle Balda|Pierre Coffin 7.404165 Alan Taylor 6.883129 Peter Richardson 6.668990 Pete Docter 6.326804 Christopher Nolan 6.195521 Alex Garland 6.118847 Patrick Tatopoulos 5.806897 Wes Ball 5.553082 Dan Gilroy 5.522641 Lilly Wachowski|Lana Wachowski 5.331930 James Gunn 5.225378 Bob Peterson|Pete Docter 4.908902 J.J. Abrams 4.800957 Alejandro GonzÃ¡lez IÃ±Ã¡rritu 4.793536 Roger Allers|Rob Minkoff 4.782688 Damien Chazelle 4.780419 Morten Tyldum 4.485181 George Miller 4.450001 Francis Lawrence 4.437604 Len Wiseman 4.380968 Quentin Tarantino 4.187272 David Yates 4.163195 Evan Goldberg|Seth Rogen 3.989231 John Lasseter|Joe Ranft 3.941265 Chris Buck|Jennifer Lee 3.918739 ... Venus Keung Kwok-Man|Wong Jing 0.004433 Connor McGuire|Colin McGuire 0.004323 Shashank Khaitan 0.004282 Alex Holdridge 0.004196 Karen Disher|Guy Moore 0.004010 Kristopher Belman 0.003933 Eldar Ryazanov 0.003659 Laurent Malaquais 0.003611 James Erskine 0.003504 Walter Carvalho|Sandra Werneck 0.003461 Katie Graham|Andrew Matthews 0.003066 Nicolas Castro 0.002922 David Higby 0.002838 Marv Newland 0.002757 David Thorpe 0.002599 Oliver Irving 0.002514 Barbara Schroeder 0.002460 Charles B. Pierce 0.002381 Norman Buckley 0.002262 Beth McCarthy-Miller|Rob Ashford 0.002165 Amol Palekar 0.001983 Sarah Burns|Ken Burns 0.001783 Enrico Oldoini 0.001567 Russ Malkin|David Alexanian 0.001531 Nacho G. Velilla 0.001499 Stephen Cragg 0.001423 Jean-Xavier de Lestrade 0.001315 Zana Briski|Ross Kauffman 0.001117 Dibakar Banerjee 0.001115 Pascal Thomas 0.000973 Name: popularity, Length: 5056, dtype: float64 第三节 绘图与可视化接着你要尝试对你的数据进行图像的绘制以及可视化。这一节最重要的是，你能够选择合适的图像，对特定的可视化目标进行可视化。所谓可视化的目标，是你希望从可视化的过程中，观察到怎样的信息以及变化。例如，观察票房随着时间的变化、哪个导演最受欢迎等。 可视化的目标可以使用的图像 表示某一属性数据的分布饼图、直方图、散点图 表示某一属性数据随着某一个变量变化条形图、折线图、热力图 比较多个属性的数据之间的关系散点图、小提琴图、堆积条形图、堆积折线图 在这个部分，你需要根据题目中问题，选择适当的可视化图像进行绘制，并进行相应的分析。对于选做题，他们具有一定的难度，你可以尝试挑战一下～ 任务3.1：对 popularity 最高的20名电影绘制其 popularity 值。 movie_data.sort_values(by='popularity', ascending=False, inplace=True)plt.xlabel('popularity')plt.ylabel('original_title')plt.barh(movie_data['original_title'][:20], width=movie_data['popularity'][:20], align='center') &lt;BarContainer object of 20 artists&gt; 任务3.2：分析电影净利润（票房-成本）随着年份变化的情况，并简单进行分析。 movie_data['profit'] = movie_data['revenue'] - movie_data['budget']pd.Series.value_counts(movie_data['release_year'])data = movie_data.groupby(by='release_year')['profit'].agg([np.mean,np.std])x = list(data.index)y = data['mean']plt.xlabel('release_year')plt.ylabel('profit')plt.errorbar(x, y) &lt;ErrorbarContainer object of 3 artists&gt; [选做]任务3.3：选择最多产的10位导演（电影数量最多的），绘制他们排行前3的三部电影的票房情况，并简要进行分析。 tmp = movie_data['director'].str.split('|', expand=True).stack().reset_index(level=1, drop=True).rename('director')movie_data_split = movie_data[['original_title', 'revenue']].join(tmp)movie_data_split_direction = list(movie_data_split['director'].value_counts().isin([45,34,31,30,23,22,21,20]))movie_data_split_direction --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-74-a6f6663f8cbb&gt; in &lt;module&gt;() 2 movie_data_split = movie_data[[&#39;original_title&#39;, &#39;revenue&#39;]].join(tmp) 3 ----&gt; 4 movie_data_split_direction = list(movie_data_split[&#39;director&#39;].value_counts().isin([45,34,31,30,23,22,21,20]).index()) 5 movie_data_split_direction TypeError: &#39;Index&#39; object is not callable [选做]任务3.4：分析1968年~2015年六月电影的数量的变化。 [选做]任务3.5：分析1968年~2015年六月电影 Comedy 和 Drama 两类电影的数量的变化。 注意: 当你写完了所有的代码，并且回答了所有的问题。你就可以把你的 iPython Notebook 导出成 HTML 文件。你可以在菜单栏，这样导出File -&gt; Download as -&gt; HTML (.html)、Python (.py) 把导出的 HTML、python文件 和这个 iPython notebook 一起提交给审阅者。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://jinyaxuan.github.io/categories/机器学习/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://jinyaxuan.github.io/tags/Machine-Learning/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://jinyaxuan.github.io/categories/机器学习/"}]},{"title":"Linear Algebra","slug":"Linear-Algebra","date":"2019-02-16T15:37:33.000Z","updated":"2019-02-16T15:42:47.803Z","comments":false,"path":"2019/02/16/Linear-Algebra/","link":"","permalink":"http://jinyaxuan.github.io/2019/02/16/Linear-Algebra/","excerpt":"","text":"线性代数：机器学习背后的优化原理线性代数作为数学的一个分支，广泛应用于科学和工程中，掌握好线性代数对于理解和从事机器学习算法相关工作是很有必要的，尤其对于深度学习算法而言。因此，这个项目会从浅入深更好的帮助你学习与积累一些跟人工智能强相关的线性代数的知识。 本项目内容理论知识部分参考《DeepLearning》又名花书第二章，希望大家支持正版购买图书。 若项目中的题目有困难没完成也没关系，我们鼓励你带着问题提交项目，评审人会给予你诸多帮助。 所有选做题都可以不做，不影响项目通过。如果你做了，那么项目评审会帮你批改，也会因为选做部分做错而判定为不通过。 准备工作我们将讲解常用的线性代数知识，而学员需使用numpy来实现这些知识点（当然也可以自己写算法实现），还需要使用matplotlib完成规定图像习题，当然，本项目用到的python代码(或numpy的使用)课程中并未完全教授，所以需要学员对相应操作进行学习与查询，这在我们往后的人工智能学习之旅中是必不可少的一个技能，请大家珍惜此项目的练习机会。 当然，这里提供官方的numpy Quickstart来帮助你更好的完成项目。 本项目还需要使用LaTeX公式，以下两个链接供学习与使用： Latex cheatsheet aTeX Cookbook 首先，导入你所需的软件包。一般我们建议在工程开头导入所有需要的软件包。 # TODO: import相关库import numpy as npimport matplotlib.pyplot as plt 1、标量，向量，矩阵，张量首先，让我们回顾下基本的定义： 标量（scalar）：形式而言，一个标量是一个单独的数，常用斜体的小写变量名称来表示。v 向量（vector）：形式而言，一个向量是一列有序数，常用粗体的小写变量名称表示v，或者上面标记剪头$\\vec{v}$ 矩阵（matrix）：形式而言，一个矩阵是一个二维数组，常用大写变量名称表示A，表示内部的元素则会使用$A_{i,j}$ 张量（tensor）：形式而言，一个张量是一个多维数组，常用粗体的大写字母变量名称表示T，表示内部的元素则会使用$A_{i,j,z}$ 等等 用图片直观的显示区别如下 接下来让我们回顾下基本的运算： 加法 标量乘法 转置 矩阵向量乘法（内积，人工智能中常见的拼写：matrix product 或者 dot product） 线性方程组： 由矩阵乘法也演变出了我们最常见的线性方程组，已知矩阵与未知向量的乘积，等于另一个已知向量，通过此方程组可求解那个未知向量，一般写为x，具体如下表示。等式左侧可以这么来理解：列为具体的矩阵来看： \\begin{bmatrix} A_{1,1} & A_{1,2} & \\cdots & A_{1,n} \\\\\\\\ A_{2,1} & A_{2,2} & \\cdots & A_{2,n} \\\\\\\\ \\cdots & \\cdots & \\cdots & \\cdots \\\\\\\\ A_{m,1} & A_{m,2} & \\cdots & A_{m,n} \\end{bmatrix} \\times \\begin{bmatrix} x_1 \\\\\\\\ x_2 \\\\\\\\ \\cdots \\\\\\\\ x_n \\end{bmatrix} = \\begin{bmatrix} b_1 \\\\\\\\ b_2 \\\\\\\\ \\cdots \\\\\\\\ b_m \\end{bmatrix}或者更简单的表示为 Ax=b既然有未知数，那么自然需要求解未知数，而我们的未知数需要满足所有方程，也不是一直都有解的，下面来列我们二维矩阵所组成的方程解的情况,若两条线平行不存在焦点，那么说明没有一个$x_1$, $x_2$同时满足两个方程，则此方程组无解，同理，若相交，则有一个解，若完全相等，则有无穷个解。 1.1、基本运算并绘图例题 $\\vec{v}$ + $\\vec{w}$ $\\hspace{1cm}\\vec{v} = \\begin{bmatrix} 1\\ 1\\end{bmatrix}$ $\\hspace{1cm}\\vec{w} = \\begin{bmatrix} -2\\ 2\\end{bmatrix}$ 结果需要先使用numpy计算向量运算结果，并用LaTeX公式表示： $\\hspace{1cm}\\vec{v}+\\vec{w} = \\begin{bmatrix} -1\\ 3\\end{bmatrix}$ 并使用matlibplot绘制出(图表颜色样式不要求) 1.1.1根据上面例题展示，计算并绘制 $2\\vec{v}$ - $\\vec{w}$ 的结果 $\\hspace{1cm}\\vec{v} = \\begin{bmatrix} 4\\ 1\\end{bmatrix}$ $\\hspace{1cm}\\vec{w} = \\begin{bmatrix} -1\\ 2\\end{bmatrix}$ $\\hspace{1cm}\\vec{v}+\\vec{w} = \\begin{bmatrix} -1\\ 3\\end{bmatrix}$ # 1.1.1 TODO：a = 2v = np.array([4,1])av = v*aw = np.array([-1,2])v_2_w = 2 * v - wax = plt.axes()ax.arrow(0, 0, *av, color='b', linewidth=2.0, head_width=0.20, head_length=0.25)ax.arrow(v_2_w[0], v_2_w[1], *w, color='r', linewidth=2.0, head_width=0.20, head_length=0.25)ax.arrow(0, 0, *v_2_w, color='k', linewidth=2.0, head_width=0.20, head_length=0.25)plt.xlim(-1,10)major_xticks = np.arange(-1,4)plt.ylim(-1,4)major_yticks = np.arange(-1, 10)ax.set_yticks(major_yticks)plt.grid(b=True, which='major')plt.show() \\begin{bmatrix} 4\\\\\\\\ 1 \\end{bmatrix} \\times \\begin{bmatrix} 2 \\end{bmatrix}- \\begin{bmatrix} -1\\\\\\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 9\\\\\\\\ 0 \\end{bmatrix}例题，方程组求解： \\begin{cases} y = 2x + 1\\\\\\\\ y = 6x - 2 \\end{cases}用matplotlib绘制图表（图表样式不要求）由上可知此方程组有且仅有一个解 需使用numpy（或自写算法）计算该解的结果,并用LaTeX公式表示出来(结果可以用小数或者分数展示) \\begin{cases} x = \\frac{3}{4} \\\\\\\\ y = \\frac{5}{2} \\end{cases}1.1.2根据上面例题展示，绘制方程组，说明是否有解是否为唯一解，若有解需计算出方程组的解 \\begin{cases} y = 2x + 1\\\\\\\\ y = \\frac{1}{10}x+6 \\end{cases}# 1.1.2 TODOX = np.mat([[-1,2],[-1,0.1]])y = np.mat([-1,-6]).Tx = np.arange(-10,10)plt.plot(2*x+1)plt.plot(0.1*x+6)plt.showr = np.linalg.solve(X,y)print(r)#有唯一解 函数相交 [[6.26315789] [2.63157895]] 1.2、说明题1.2.1使用numpy（或自写算法）说明$(AB)^{\\text{T}} = B^\\text{T}A^\\text{T}$ 其中 A=\\begin{bmatrix} 21 & 7 \\\\\\\\ 15 & 42 \\\\\\\\ 9 & 6 \\end{bmatrix}, B=\\begin{bmatrix} 4 \\\\\\\\ 33 \\end{bmatrix}# 1.2.1 TODOA = np.array([[21,7],[15,42],[9,6]],dtype=int)B = np.array([4,33],dtype=int)display(A.dot(B).T)display(B.T.dot(A.T))#结果证明(AB)T=BTAT array([ 315, 1446, 234]) array([ 315, 1446, 234]) 1.2.2使用numpy（或自写算法）说明 $A ( B + C ) = AB + AC$ 其中 A=\\begin{bmatrix} 9 & 3 \\\\\\\\ 8 & 4 \\\\\\\\ 7 & 6 \\end{bmatrix}, B=\\begin{bmatrix} 5 \\\\\\\\ 2 \\end{bmatrix}, C=\\begin{bmatrix} 5 \\\\\\\\ 7 \\end{bmatrix}# 1.2.2 TODOA = np.array([[9,3],[8,4],[7,6]])B = np.array([[5,2]])C = np.array([[5,7]])display(A*(B+C))display(A*B+A*C)# 据结果A(B+C)=AB+AC array([[90, 27], [80, 36], [70, 54]]) array([[90, 27], [80, 36], [70, 54]]) 2、特殊矩阵 单位矩阵 如果选取任意一个向量和某矩阵相乘，该向量都不会改变，我们将这种保持n维向量不变的矩阵记为单位矩阵$I_n$ 逆矩阵 如果存在一个矩阵，使$A^{-1} A = I_n$，那么$A^{-1}$就是A的逆矩阵。 对角矩阵 如果一个矩阵只有主对角线上还有非零元素，其他位置都是零，这个矩阵就是对角矩阵 对称矩阵 如果一个矩阵的转置是和它自己相等的矩阵，即$A=A^{T}$，那么这个矩阵就是对称矩阵 正交矩阵 行向量和列向量是分别标准正交(90度)的方阵，即$A^{T}A = AA^{T} = I_n$，又即$A^{-1} = A^{T}$，那么这种方阵就是正交矩阵 2.1、证明题通过LaTeX公式，结合上面所述概念，假设$A^{-1}$存在的情况下，证明$Ax=b$的解$x={A}^{-1}{b}$ 回答： 因为$A^{-1}$存在 所以 等式两边同乘 $A^{-1}$ $A^{-1}Ax = A^{-1}b$ 又因为$A^{-1} A = I_n$ 所以 等式为 $I_nx = A^{-1}b$ 同时 $I_n$为单位矩阵所以 $x = A^{-1}b$ $a^2$ 2.2、 计算题2.2.1通过numpy计算，再次验证2.1证明题 \\begin{cases} y = 2x + 1\\\\\\\\ y = \\frac{1}{10}x+6 \\end{cases}并用LaTeX公式写出$A^{-1}$是多少（小数分数皆可） # 2.2.1 TODOx = np.mat([[-1,2],[-1,0.1]])y = np.mat([-1,-6]).Tr = np.linalg.solve(x,y) 2.2.21、请用numpy（或自写算法）实现一个6x6的对角矩阵，矩阵的对角线由3至8（含8）组成。 2、计算第一问生成的对角矩阵与向量$[6,7,1,2,5,9]^{T}$的乘积 # 2.2.2 TODOdiagonal = np.diag(np.arange(3,9))vector = np.array([6,7,1,2,5,9])diagonal.dot(vector.T) array([18, 28, 5, 12, 35, 72]) 3、迹运算迹运算返回的是矩阵对角元素的和，如图所示写成数学公式为： \\large Tr(A) = \\sum_{i}A_{i,i}说明题： 使用numpy验证 \\large Tr(ABC) = Tr(CAB) = Tr(BCA)其中 A= \\begin{bmatrix} 7 & 6 \\\\\\\\ 29 & 3 \\end{bmatrix} B= \\begin{bmatrix} 2 & -8 \\\\\\\\ 9 & 10 \\end{bmatrix} C= \\begin{bmatrix} 2 & 17 \\\\\\\\ 1 & 5 \\end{bmatrix}# 3 TODOA = np.array([[7,6],[29,3]])B = np.array([[2,-8],[9,10]])C = np.array([[2,17],[1,5]])ABC = A.dot(B).dot(C)CAB = C.dot(A).dot(B)BCA = B.dot(C).dot(A)print('ABC:&#123;0&#125;,CAB:&#123;1&#125;,BCA:&#123;2&#125;'.format(np.trace(ABC),np.trace(CAB),np.trace(BCA))) ABC:575,CAB:575,BCA:575 4、衡量向量以及矩阵的大小：范数与条件数范数的定义在线性代数等数学分支中，范数（Norm）是一个函数，其给予某向量空间（或矩阵）中的每个向量以长度或称之为大小。对于零向量，其长度为零。直观的说，向量或矩阵的范数越大，则我们可以说这个向量或矩阵也就越大。有时范数有很多更为常见的叫法，如绝对值其实便是一维向量空间中实数或复数的范数，范数的一般化定义：设$p\\ge 1$，p-norm用以下来表示 \\large {\\Vert x \\Vert}_{p} = \\lgroup {\\sum_{i}{\\vert x_i \\vert}^p }\\rgroup ^{\\frac{1}{p}}此处，当p=1时，我们称之曼哈顿范数(Manhattan Norm)。其来源是曼哈顿的出租车司机在四四方方的曼哈顿街道中从一点到另一点所需要走过的距离。也即我们所要讨论的L1范数。其表示某个向量中所有元素绝对值的和。 而当p=2时，则是我们最为常见的Euclidean norm。也称为Euclidean distance，中文叫欧几里得范数，也即我们要讨论的L2范数，他也经常被用来衡量向量的大小。 而当p=0时，严格的说此时p已不算是范数了，L0范数是指向量中非0的元素的个数，但很多人仍然称之为L0范数（Zero norm零范数）。 这三个范数有很多非常有意思的特征，尤其是在机器学习中的正则化（Regularization）以及稀疏编码（Sparse Coding）有非常有趣的应用，这个在进阶课程可以做更深入的了解。 L0 范数 \\large \\Vert x \\Vert = \\sqrt[0]{\\sum_i x_i^0} = \\#(i|x_i \\neq0)L1 范数 \\large {\\Vert x \\Vert}_{1} = \\lgroup {\\sum_{i}{\\vert x_i \\vert} }\\rgroupL2 范数 \\large {\\Vert x \\Vert}_{2} = \\lgroup {\\sum_{i}{\\vert x_i \\vert}^2 }\\rgroup ^{\\frac{1}{2}}另外这里还存在特例： 当 $ p -&gt; \\infty $ 时，我们称之为 $ L^{\\infty} $范数，也被称为“maximum norm（max范数）”，这个范数表示向量中具有最大幅度的元素的绝对值： \\large {\\Vert x \\Vert}^{\\infty} = \\max_{i}{\\vert x_i \\vert}以上资料部分参考wiki 4.1、计算向量的范数编写一个函数来计算一下向量的各种范数。 # TODO 实现这里向量范数计算的函数，要求可以计算p = 0,1,2,3 ... 无穷 情况下的范数\"\"\" 计算向量的范数 参数 x: 向量 numpy数组 或者list数组 p: 范数的阶，int型整数或者None infty: 是否计算max范数，bool型变量，True的时候表示计算max范数，False的时候计算p范数 返回 向量的范数，float类型数值 hint: 1.你需要首先判断infty是True or False, 然后判断p 是否为零 2.注意int类型变量在计算时候需要规整为float类型 \"\"\"def calc_Norm(x, p = 2, infty = False): if infty == False: answer = np.linalg.norm(x,float(p)) else: answer = np.linalg.norm(x,np.Inf) return answer %run -i -e test.py LinearRegressionTestCase.test_calc_Norm . ---------------------------------------------------------------------- Ran 1 test in 0.004s OK 4.2、计算矩阵的范数我们也需要衡量矩阵的大小，对于矩阵大小的衡量在很多优化问题中是非常重要的。而在深度学习中，最常见的做法是使用Frobenius 范数(Frobenius norm)，也称作矩阵的F范数，其定义如下： \\large {\\Vert A \\Vert}_{F} = \\sqrt {\\sum_{i,j}{\\vert A_{i,j} \\vert}^2 }我们这里继续来计算一下F范数 # TODO 实现这里矩阵Frobenius范数计算的函数\"\"\" 计算向量的范数 参数 A: 给定的任意二维矩阵 list或者numpy数组形式 返回 矩阵的Frobenius范数，float类型数值 \"\"\"def calc_Frobenius_Norm(A): return np.linalg.norm(A,'fro') %run -i -e test.py LinearRegressionTestCase.test_calc_Frobenius_Norm . ---------------------------------------------------------------------- Ran 1 test in 0.001s OK 4.3、计算矩阵的条件数矩阵的条件数(condition number)是矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，我们这里为了简化条件，这里只考虑矩阵是奇异矩阵的时候，如何计算以及理解条件数(condition number): 当矩阵A为奇异矩阵的时候，condition number为无限大；当矩阵A非奇异的时候，我们定义condition number如下： \\large \\kappa{(A)} = {\\Vert A \\Vert}_F {\\Vert A^{-1} \\Vert}_F奇异矩阵，非奇异矩阵 计算矩阵的条件数 \"\"\" 计算矩阵的条件数 参数 A: 给定的任意二维矩阵 list或者numpy数组形式 返回 矩阵的condition number, \"\"\"def calc_Condition_Number(A): A = np.mat(A) return np.dot(np.linalg.norm(A,'fro'),np.linalg.norm(A.I,'fro')) %run -i -e test.py LinearRegressionTestCase.test_calc_Condition_Number /opt/anaconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py:68: PendingDeprecationWarning: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray. return matrix(data, dtype=dtype, copy=False) . ---------------------------------------------------------------------- Ran 1 test in 0.017s OK (选做)4.4、条件数的理解与应用a. 有如下两个2*2的非奇异矩阵A和B: $ A = \\begin{bmatrix} 1 &amp;2 \\ 3 &amp;4 \\\\end{bmatrix} $ $ B = \\begin{bmatrix} 1 &amp;2 \\ 2 &amp;4.0001 \\\\end{bmatrix}$ 计算condition number(A), condition number(B); b. 根据上面构造的矩阵A,B分别计算线性系统方程组的解$w$: A $ \\begin{bmatrix}w{a1}\\w{a2}\\ \\end{bmatrix} $ = $ \\begin{bmatrix}1\\2\\ \\end{bmatrix} $, B $ \\begin{bmatrix}w{b1}\\w{b2}\\ \\end{bmatrix} $ = $ \\begin{bmatrix}1\\2\\ \\end{bmatrix} $, A $ \\begin{bmatrix}w{a1}\\w{a2}\\ \\end{bmatrix} $ = $ \\begin{bmatrix}{1.0001}\\{2.0001}\\ \\end{bmatrix} $, B $ \\begin{bmatrix}w{b1}\\w{b2}\\ \\end{bmatrix} $ = $ \\begin{bmatrix}{1.0001}\\{2.0001}\\ \\end{bmatrix} $. c. 计算完成之后，比较condition number大小与线性系统稳定性之间的关系，并且给出规律性的总结； d. 阅读与思考: 考虑更为通用的一种情况，我们计算一个典型的线性回归系统: Xw = b可以简单推导得出其闭式解为：$ w=(X^TX)^{−1}X^Tb $ ，如果 $X^TX$可逆 推导过程： 1.等式两边乘以$X^T$ X^TXw = X^Tb2.等式两边乘以$(X^TX)^{-1}$ (X^TX)^{-1}X^TXw = (X^TX)^{−1}X^Tb3.因为$A^{-1}A = I$，两边约去即可得： w=(X^TX)^{−1}X^Tb当我们需要拟合的数据X满足数据量远远小于特征数目的时候，也就是X矩阵的行数 &lt;&lt; X矩阵的列数的时候，因为$X^TX$不是奇异矩阵，此时方程组不存在闭式解；那么我们该如何重新构造$X^TX$，使得该闭式解成立？ hint1. 单位矩阵的condition number是最低的，是最为稳定的； hint2. 如果要使得该系统存在闭式解，那么就必须使得求逆运算是可以进行的，也就是说重新构造的$X^TX$必须是可逆的方阵； hint3. 重新构造的方式可以是在$X^TX$基础上进行加或者减或者乘除相关矩阵的操作； 一种可行的方式就是： w = (X^TX+\\lambda I)^{−1}X^Tb实际上我们最为常用的Ridge Regression和 L2范数以及condition number之间某种程度上是可以相互推导的： 首先，Ridge Regression的损失函数为： J_w = min({\\Vert Xw -y \\Vert}^2 + \\alpha {\\Vert w \\Vert}^2)我们展开这个损失函数： {\\Vert Xw -y \\Vert}^2 + \\alpha {\\Vert w \\Vert}^2 = (Xw -y)^T (Xw-y) + \\alpha w^Tw由于这里是一个凸函数，我们令导数等于零，即为最小值的解，求导可得： X^T (Xw-y) + \\alpha w = 0整理即可得到： w = (X^TX+\\lambda I)^{−1}X^Tb5、SVDSVD是Singular value decomposition的缩写，称为奇异值分解，是分解矩阵的一种方式，会将矩阵分解为奇异向量（singular vector）和奇异值（singular value），分解的意义其实很明确，就是想将一个很大很复杂的矩阵，用更小更简单的几个子矩阵的相乘来表示，这些小矩阵描述的是矩阵的重要的特性。 那么SVD具体的数学表达是什么呢？ 假设有一个矩阵C，我们可以将矩阵C分解为三个矩阵的乘积： \\large C = UDV^{T}如果C是一个m x n的矩阵，那么U是一个m x m的矩阵，D是一个m x n的矩阵，V是一个n x n的矩阵，这些小矩阵并不是普普通通的矩阵，U和V都定义为正交矩阵，而D定义为对角矩阵。 SVD最常用的做法就是用来进行特征的降维以及矩阵的低秩重构，例如这里分别取矩阵U、D、VT的前k列，如图示中的白色部分，然后重新计算新的C矩阵，即为k维度下的矩阵重构，这种方法被广泛应用于自然语言处理LSA、推荐系统SVD++,FM,FFM等领域，如有兴趣可以继续参考链接相关资料。 具体计算UDV的算法不是我们这个项目的关键，我们只需使用numpy得出结果即可，下面的习题，将会带你体会SVD的某一应用场景。 提示：我们会需要使用numpy.linalg相关函数。 5.1、使用numpy去计算任意矩阵的奇异值分解：\"\"\" 计算任意矩阵的奇异值分解 参数 A: 给定的任意二维矩阵 list或者numpy数组形式 返回 使用numpy.linalg相关函数，直接返回分解之后的矩阵U,D,V （可以尝试一下使用np.shape一下分解出来的U，D，VT，会发现维度跟我们上面讲解所描述的不同， 暂时不用管他直接返回np求解出的U，D，VT即可） \"\"\"def calc_svd(A): return np.linalg.svd(A) %run -i -e test.py LinearRegressionTestCase.test_calc_svd . ---------------------------------------------------------------------- Ran 1 test in 0.004s OK (选做) 5.2、利用奇异值分解对矩阵进行降维# TODO 利用SVD进行对于矩阵进行降维\"\"\" 利用SVD进行对于矩阵进行降维 参数 A: 给定的任意二维矩阵 list或者numpy数组形式 shape为(m,n) topk: 降维的维度 (m,n) -&gt; (m,topk) 返回 降维后的矩阵 (m, topk) hint 1. 对角矩阵D存在一个较为明显的特性，就是D的对角线元素是递减的，这些元素实际上是衡量了所分解的矩阵U,V的列向量的重要性 2. 因此我们常说的svd降维就是利用选取的前topk大的对角线矩阵元素进行构造新的降维矩阵 3. U的按照前topk截取的列向量 * topk截取的对角矩阵 即为新的降维后的矩阵 \"\"\"def calc_svd_decompostion(A, topk = 2): U,Sig,V = np.linalg.svd(A) Sig[1] = topk return Sig %run -i -e test.py LinearRegressionTestCase.test_calc_svd_decompostion . ---------------------------------------------------------------------- Ran 1 test in 0.004s OK (选做) 5.3、利用奇异值分解对矩阵进行降维后重构\"\"\" 利用SVD进行对于矩阵进行降维 参数 A: 给定的任意二维矩阵 list或者numpy数组形式 shape为(m,n) topk: 降维的维度 (m,n) -&gt; (m,topk) 返回 降维重构后的矩阵 (m, n) hint 这里除了降维矩阵外，另外一个较为常见的应用就是对矩阵进行重构，具体的做法类似前面的思路 1. 选取对应的U，D，V的topk向量 2. U的按照前topk截取的列向量 * topk截取的对角矩阵 * V^T按照前topk截取的行向量(注意这里是V的转置,因为分解得到的是V^T) \"\"\"def calc_svd_reconsitution(A, topk = 2): A = np.array(A) U,Sig,V = np.linalg.svd(A) Sig[1] = topk S = np.zeros((5,8)) S[:5,:5] = np.diag(Sig) A_conv = np.dot(np.dot(A.T,U),S) A = np.dot(np.dot(U,S),V) return A %run -i -e test.py LinearRegressionTestCase.test_calc_svd_reconsitution . ---------------------------------------------------------------------- Ran 1 test in 0.001s OK (选做) 5.4、计算不同降维大小重构矩阵的Frobenius范数损失定义矩阵$A$以及使用SVD降维（降维大小为k)分解后的重构矩阵$A_k$，则这里的F范数损失定义如下： \\Large Loss_{F} = {\\Vert A - A_k \\Vert}_F这里需要编码求出对于给定的矩阵A 分别在不同的降维幅度下重构后的F范数损失，并且作出损失大小随着降维大小的变化图： ## 不要修改这里！import numpy as npfrom sklearn.datasets import load_boston import matplotlib.pyplot as plt%matplotlib inline A = load_boston()['data'] # 载入boston house 数据集print(A.shape) (506, 13) loss_hist = []for topk in range(1,13): # 5.4 TODO ### 1.计算相应的SVD topk降维后的重构矩阵，需实现calc_svd_reconsitution ### 2.计算对应的F范数损失，并存储loss放入loss_hist列表中### 画出F损失随着降维大小的变化图### x坐标为对应的降维大小，y坐标为对应的F损失plt.plot(range(1,13),loss_hist,'r--')plt.xlabel('decomposition size')plt.ylabel('F Loss') File &quot;&lt;ipython-input-24-309fc4d50cf4&gt;&quot;, line 10 plt.plot(range(1,13),loss_hist,&#39;r&#39;) ^ IndentationError: expected an indented block 5.5、SVD的有趣应用为了这个习题我准备了两张图，参见项目文件夹下的test_girl.jpg和test_boy.jpeg，自选一张，你需要 需要使用 PIL 加载你所选择的图像（文档）,所以记得导入需要的包（模块） 使用Image的convert方法将图像变为灰度图 将convert后的结果转换成np.array,需用到Image.getdata方法来读取图片每个pixel的数据，特别注意一下，对于彩色的图来说，即使我们转为了灰度图，但每一个pixel还是由RGB三个维度组成，所以在getdata时，band需要设定为某一个颜色index，比如band = 0，这样只用R这个维度的数据。用这个方法来保证图片的每个pixel只占有一个单元的空间。 因为我们转np.array时破坏了原有图形的样子，变成了一个一维数据，我们要将转换后的np.array恢复到图片应有的size，转换后，可以shape确认下是否与最开始转出的灰度图的size一致，注意图的size是（宽，高），而宽对应array.shape的应该是列，而高对应的是行，别弄反了。 使用上方实现的calc_svd函数计算上一步计算出的np.array数据，赋值给变量：U,D,VT 打印出U,D,VT的shape形状，尤其注意观察D的shape 在U，VT，D变量成功实现的情况下，运行测试程序看效果 # 5.5 TODOfrom PIL import Imageim = Image.open('test_girl.jpg').convert('LA')im_array = np.array(im.getdata(band=0))im_array = im_array.reshape(im.size[1],im.size[0])U,D,VT = calc_svd(im_array)display(U.shape)display(D.shape)display(VT.shape) (600, 600) (600,) (750, 750) #请在U，D，V变量完成的情况下调用此测试程序，不要修改此处plt.figure(figsize=(16,6))for i,topk in enumerate([5, 10, 15, 20, 30, 50]): reconstimg = np.matrix(U[:, :topk]) * np.diag(D[:topk]) * np.matrix(VT[:topk, :]) plt.subplot(231+i) plt.imshow(reconstimg, cmap='gray') title = \"n = %s\" % ((i+1)*5) plt.title(title)plt.show() 相关继续深入学习的资料： 机器学习与优化 PCA与SVD的区别 SVD在降维中的应用 SVD在自然语言处理中的应用 SVD在推荐系统中的应用 《Elements of Statistical Learning》Trevor Hastie, Robert Tibshirani, and Jerome Friedman","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://jinyaxuan.github.io/categories/机器学习/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://jinyaxuan.github.io/tags/Machine-Learning/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://jinyaxuan.github.io/categories/机器学习/"}]},{"title":"image classifier project","slug":"image-classifier-project","date":"2019-02-16T15:35:55.000Z","updated":"2019-02-16T15:49:40.440Z","comments":false,"path":"2019/02/16/image-classifier-project/","link":"","permalink":"http://jinyaxuan.github.io/2019/02/16/image-classifier-project/","excerpt":"","text":"开发 AI 应用未来，AI 算法在日常生活中的应用将越来越广泛。例如，你可能想要在智能手机应用中包含图像分类器。为此，在整个应用架构中，你将使用一个用成百上千个图像训练过的深度学习模型。未来的软件开发很大一部分将是使用这些模型作为应用的常用部分。 在此项目中，你将训练一个图像分类器来识别不同的花卉品种。可以想象有这么一款手机应用，当你对着花卉拍摄时，它能够告诉你这朵花的名称。在实际操作中，你会训练此分类器，然后导出它以用在你的应用中。我们将使用此数据集，其中包含 102 个花卉类别。你可以在下面查看几个示例。 该项目分为多个步骤： 加载和预处理图像数据集 用数据集训练图像分类器 使用训练的分类器预测图像内容 我们将指导你完成每一步，你将用 Python 实现这些步骤。 完成此项目后，你将拥有一个可以用任何带标签图像的数据集进行训练的应用。你的网络将学习花卉，并成为一个命令行应用。但是，你对新技能的应用取决于你的想象力和构建数据集的精力。例如，想象有一款应用能够拍摄汽车，告诉你汽车的制造商和型号，然后查询关于该汽车的信息。构建你自己的数据集并开发一款新型应用吧。 首先，导入你所需的软件包。建议在代码开头导入所有软件包。当你创建此 notebook 时，如果发现你需要导入某个软件包，确保在开头导入该软件包。 # Imports here%matplotlib inline%config InlineBackend.figure_format = 'retina'import matplotlib.pyplot as pltimport torchfrom torch import nnfrom torch import optimimport torch.nn.functional as Ffrom torch.autograd import Variablefrom torchvision import datasets, transforms, models 加载数据在此项目中，你将使用 torchvision 加载数据（文档）。数据应该和此 notebook 一起包含在内，否则你可以在此处下载数据。数据集分成了三部分：训练集、验证集和测试集。对于训练集，你需要变换数据，例如随机缩放、剪裁和翻转。这样有助于网络泛化，并带来更好的效果。你还需要确保将输入数据的大小调整为 224x224 像素，因为预训练的网络需要这么做。 验证集和测试集用于衡量模型对尚未见过的数据的预测效果。对此步骤，你不需要进行任何缩放或旋转变换，但是需要将图像剪裁到合适的大小。 对于所有三个数据集，你都需要将均值和标准差标准化到网络期望的结果。均值为 [0.485, 0.456, 0.406]，标准差为 [0.229, 0.224, 0.225]。这样使得每个颜色通道的值位于 -1 到 1 之间，而不是 0 到 1 之间。 train_dir = 'train'valid_dir = 'valid'test_dir = 'test' # TODO: Define your transforms for the training, validation, and testing setsdata_dir = 'flowers/'data_transforms = transforms.Compose([ transforms.RandomRotation(30), transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])test_transforms = transforms.Compose([ transforms.Resize(255), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])# TODO: Load the datasets with ImageFolderimage_datasets = datasets.ImageFolder(data_dir + train_dir, transform=data_transforms)image_datasets_test = datasets.ImageFolder(data_dir + test_dir, transform=test_transforms)# TODO: Using the image datasets and the trainforms, define the dataloadersdataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=64, shuffle=True)test_dataloaders = torch.utils.data.DataLoader(image_datasets_test, batch_size=32) 标签映射你还需要加载从类别标签到类别名称的映射。你可以在文件 cat_to_name.json 中找到此映射。它是一个 JSON 对象，可以使用 json 模块读取它。这样可以获得一个从整数编码的类别到实际花卉名称的映射字典。 import jsonwith open('cat_to_name.json', 'r') as f: cat_to_name = json.load(f) 构建和训练分类器数据准备好后，就开始构建和训练分类器了。和往常一样，你应该使用 torchvision.models 中的某个预训练模型获取图像特征。使用这些特征构建和训练新的前馈分类器。 这部分将由你来完成。如果你想与他人讨论这部分，欢迎与你的同学讨论！你还可以在论坛上提问或在工作时间内咨询我们的课程经理和助教导师。 请参阅审阅标准，了解如何成功地完成此部分。你需要执行以下操作： 加载预训练的网络（如果你需要一个起点，推荐使用 VGG 网络，它简单易用） 使用 ReLU 激活函数和丢弃定义新的未训练前馈网络作为分类器 使用反向传播训练分类器层，并使用预训练的网络获取特征 跟踪验证集的损失和准确率，以确定最佳超参数 我们在下面为你留了一个空的单元格，但是你可以使用多个单元格。建议将问题拆分为更小的部分，并单独运行。检查确保每部分都达到预期效果，然后再完成下个部分。你可能会发现，当你实现每部分时，可能需要回去修改之前的代码，这很正常！ 训练时，确保仅更新前馈网络的权重。如果一切构建正确的话，验证准确率应该能够超过 70%。确保尝试不同的超参数（学习速率、分类器中的单元、周期等），寻找最佳模型。保存这些超参数并用作项目下个部分的默认值。 # TODO: Build and train your networkmodel = models.vgg16(pretrained=True)modelfor param in model.parameters(): param.requires_grad = Falsefrom collections import OrderedDictclassifier = nn.Sequential(OrderedDict([ ('fc1', nn.Linear(25088,1024)), ('relu', nn.ReLU()), ('fc2', nn.Linear(1024,102)), ('output', nn.LogSoftmax(dim=1))]))model.classifier = classifiercriterion = nn.NLLLoss()optimizer = optim.Adam(model.classifier.parameters(), lr=0.001) Downloading: &quot;https://download.pytorch.org/models/vgg16-397923af.pth&quot; to /root/.torch/models/vgg16-397923af.pth 100%|██████████| 553433881/553433881 [00:34&lt;00:00, 16147158.40it/s] 测试网络建议使用网络在训练或验证过程中从未见过的测试数据测试训练的网络。这样，可以很好地判断模型预测全新图像的效果。用网络预测测试图像，并测量准确率，就像验证过程一样。如果模型训练良好的话，你应该能够达到大约 70% 的准确率。 # TODO: Do validation on the test setepochs = 3print_every = 40steps = 0model.to('cuda')for i in range(epochs): model.train() running_loss = 0 for li, (images, labels) in enumerate(dataloaders): steps += 1 images, labels = images.to('cuda'), labels.to('cuda') optimizer.zero_grad() outputs = model.forward(images) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() if print_every % steps == 0: model.eval() accuracy = 0 for ii, (images, labels) in enumerate(test_dataloaders): images, labels = images.to('cuda'), labels.to('cuda') predicted = model(images) equality = (labels == predicted.max(1)[1]) accuracy += equality.type_as(torch.FloatTensor()).mean() print(\"Epoch: &#123;&#125;/&#123;&#125;\".format(i+1, epochs), \"Loss: &#123;:.4f&#125;\".format(running_loss/print_every), \"Test accuracy: &#123;:.4f&#125;\".format(accuracy/(ii+1))) model.train() steps = 0 running_loss = 0 Epoch: 1/3 Loss: 0.1159 Test accuracy: 0.0613 Epoch: 1/3 Loss: 0.1666 Test accuracy: 0.0481 Epoch: 1/3 Loss: 0.1464 Test accuracy: 0.1146 Epoch: 1/3 Loss: 0.1557 Test accuracy: 0.1070 Epoch: 1/3 Loss: 0.1142 Test accuracy: 0.1154 Epoch: 1/3 Loss: 0.1290 Test accuracy: 0.1619 Epoch: 1/3 Loss: 0.1171 Test accuracy: 0.2007 Epoch: 1/3 Loss: 0.1080 Test accuracy: 0.1863 Epoch: 1/3 Loss: 0.1133 Test accuracy: 0.2224 Epoch: 1/3 Loss: 0.1019 Test accuracy: 0.2841 Epoch: 1/3 Loss: 0.0866 Test accuracy: 0.3153 Epoch: 1/3 Loss: 0.0825 Test accuracy: 0.3322 Epoch: 1/3 Loss: 0.0917 Test accuracy: 0.3346 Epoch: 1/3 Loss: 0.0864 Test accuracy: 0.3531 Epoch: 1/3 Loss: 0.0782 Test accuracy: 0.3771 Epoch: 1/3 Loss: 0.0754 Test accuracy: 0.3847 Epoch: 1/3 Loss: 0.0702 Test accuracy: 0.3875 Epoch: 1/3 Loss: 0.0790 Test accuracy: 0.3971 Epoch: 1/3 Loss: 0.0797 Test accuracy: 0.4152 Epoch: 1/3 Loss: 0.0707 Test accuracy: 0.4292 Epoch: 1/3 Loss: 0.0616 Test accuracy: 0.4248 Epoch: 1/3 Loss: 0.0624 Test accuracy: 0.4429 Epoch: 1/3 Loss: 0.0766 Test accuracy: 0.4593 Epoch: 1/3 Loss: 0.0594 Test accuracy: 0.4701 Epoch: 1/3 Loss: 0.0612 Test accuracy: 0.4953 Epoch: 1/3 Loss: 0.0680 Test accuracy: 0.4989 Epoch: 1/3 Loss: 0.0605 Test accuracy: 0.5077 Epoch: 1/3 Loss: 0.0618 Test accuracy: 0.5153 Epoch: 1/3 Loss: 0.0606 Test accuracy: 0.5157 Epoch: 1/3 Loss: 0.0542 Test accuracy: 0.5493 Epoch: 1/3 Loss: 0.0562 Test accuracy: 0.5385 Epoch: 1/3 Loss: 0.0484 Test accuracy: 0.5389 Epoch: 1/3 Loss: 0.0390 Test accuracy: 0.5638 Epoch: 1/3 Loss: 0.0559 Test accuracy: 0.5786 Epoch: 1/3 Loss: 0.0445 Test accuracy: 0.5758 Epoch: 1/3 Loss: 0.0487 Test accuracy: 0.5682 Epoch: 1/3 Loss: 0.0436 Test accuracy: 0.5819 Epoch: 1/3 Loss: 0.0566 Test accuracy: 0.5731 Epoch: 1/3 Loss: 0.0488 Test accuracy: 0.5879 Epoch: 1/3 Loss: 0.0469 Test accuracy: 0.6104 Epoch: 1/3 Loss: 0.0519 Test accuracy: 0.6116 Epoch: 1/3 Loss: 0.0413 Test accuracy: 0.6144 Epoch: 1/3 Loss: 0.0532 Test accuracy: 0.6300 Epoch: 1/3 Loss: 0.0380 Test accuracy: 0.6480 Epoch: 1/3 Loss: 0.0592 Test accuracy: 0.6673 Epoch: 1/3 Loss: 0.0497 Test accuracy: 0.6805 Epoch: 1/3 Loss: 0.0449 Test accuracy: 0.6925 Epoch: 1/3 Loss: 0.0509 Test accuracy: 0.6969 Epoch: 1/3 Loss: 0.0479 Test accuracy: 0.6933 Epoch: 1/3 Loss: 0.0409 Test accuracy: 0.6912 Epoch: 1/3 Loss: 0.0470 Test accuracy: 0.6952 Epoch: 1/3 Loss: 0.0369 Test accuracy: 0.6832 Epoch: 1/3 Loss: 0.0432 Test accuracy: 0.6696 Epoch: 1/3 Loss: 0.0372 Test accuracy: 0.6821 Epoch: 1/3 Loss: 0.0347 Test accuracy: 0.6781 Epoch: 1/3 Loss: 0.0302 Test accuracy: 0.6777 Epoch: 1/3 Loss: 0.0433 Test accuracy: 0.6757 Epoch: 1/3 Loss: 0.0433 Test accuracy: 0.6869 Epoch: 1/3 Loss: 0.0454 Test accuracy: 0.7022 Epoch: 1/3 Loss: 0.0380 Test accuracy: 0.7130 Epoch: 1/3 Loss: 0.0339 Test accuracy: 0.7154 Epoch: 1/3 Loss: 0.0346 Test accuracy: 0.7094 Epoch: 1/3 Loss: 0.0343 Test accuracy: 0.6914 Epoch: 1/3 Loss: 0.0469 Test accuracy: 0.6998 Epoch: 1/3 Loss: 0.0324 Test accuracy: 0.7118 Epoch: 1/3 Loss: 0.0347 Test accuracy: 0.7130 Epoch: 1/3 Loss: 0.0314 Test accuracy: 0.7174 Epoch: 1/3 Loss: 0.0408 Test accuracy: 0.7222 Epoch: 1/3 Loss: 0.0355 Test accuracy: 0.7382 Epoch: 1/3 Loss: 0.0421 Test accuracy: 0.7603 Epoch: 1/3 Loss: 0.0414 Test accuracy: 0.7747 Epoch: 1/3 Loss: 0.0332 Test accuracy: 0.7771 Epoch: 1/3 Loss: 0.0318 Test accuracy: 0.7755 Epoch: 1/3 Loss: 0.0281 Test accuracy: 0.7666 Epoch: 1/3 Loss: 0.0430 Test accuracy: 0.7718 Epoch: 1/3 Loss: 0.0384 Test accuracy: 0.7690 Epoch: 1/3 Loss: 0.0350 Test accuracy: 0.7718 Epoch: 1/3 Loss: 0.0353 Test accuracy: 0.7582 Epoch: 1/3 Loss: 0.0390 Test accuracy: 0.7618 Epoch: 1/3 Loss: 0.0271 Test accuracy: 0.7522 Epoch: 1/3 Loss: 0.0253 Test accuracy: 0.7594 Epoch: 1/3 Loss: 0.0417 Test accuracy: 0.7622 Epoch: 1/3 Loss: 0.0258 Test accuracy: 0.7590 Epoch: 1/3 Loss: 0.0358 Test accuracy: 0.7590 Epoch: 1/3 Loss: 0.0314 Test accuracy: 0.7566 Epoch: 1/3 Loss: 0.0260 Test accuracy: 0.7386 Epoch: 1/3 Loss: 0.0384 Test accuracy: 0.7278 Epoch: 1/3 Loss: 0.0281 Test accuracy: 0.7218 Epoch: 1/3 Loss: 0.0323 Test accuracy: 0.7126 Epoch: 1/3 Loss: 0.0287 Test accuracy: 0.7077 Epoch: 1/3 Loss: 0.0287 Test accuracy: 0.7142 Epoch: 1/3 Loss: 0.0206 Test accuracy: 0.7355 Epoch: 1/3 Loss: 0.0277 Test accuracy: 0.7523 Epoch: 1/3 Loss: 0.0314 Test accuracy: 0.7667 Epoch: 1/3 Loss: 0.0339 Test accuracy: 0.7823 Epoch: 1/3 Loss: 0.0322 Test accuracy: 0.7967 Epoch: 1/3 Loss: 0.0248 Test accuracy: 0.8064 Epoch: 1/3 Loss: 0.0267 Test accuracy: 0.7995 Epoch: 1/3 Loss: 0.0368 Test accuracy: 0.7815 Epoch: 1/3 Loss: 0.0378 Test accuracy: 0.7747 Epoch: 1/3 Loss: 0.0312 Test accuracy: 0.7567 Epoch: 1/3 Loss: 0.0221 Test accuracy: 0.7519 Epoch: 1/3 Loss: 0.0260 Test accuracy: 0.7386 Epoch: 2/3 Loss: 0.0254 Test accuracy: 0.7378 Epoch: 2/3 Loss: 0.0358 Test accuracy: 0.7514 Epoch: 2/3 Loss: 0.0297 Test accuracy: 0.7574 Epoch: 2/3 Loss: 0.0246 Test accuracy: 0.7510 Epoch: 2/3 Loss: 0.0277 Test accuracy: 0.7602 Epoch: 2/3 Loss: 0.0300 Test accuracy: 0.7723 Epoch: 2/3 Loss: 0.0331 Test accuracy: 0.7803 Epoch: 2/3 Loss: 0.0217 Test accuracy: 0.7739 Epoch: 2/3 Loss: 0.0230 Test accuracy: 0.7727 Epoch: 2/3 Loss: 0.0229 Test accuracy: 0.7655 Epoch: 2/3 Loss: 0.0195 Test accuracy: 0.7583 Epoch: 2/3 Loss: 0.0281 Test accuracy: 0.7571 Epoch: 2/3 Loss: 0.0306 Test accuracy: 0.7655 Epoch: 2/3 Loss: 0.0194 Test accuracy: 0.7819 Epoch: 2/3 Loss: 0.0213 Test accuracy: 0.7916 Epoch: 2/3 Loss: 0.0231 Test accuracy: 0.7976 Epoch: 2/3 Loss: 0.0173 Test accuracy: 0.7916 Epoch: 2/3 Loss: 0.0161 Test accuracy: 0.7856 Epoch: 2/3 Loss: 0.0144 Test accuracy: 0.7771 Epoch: 2/3 Loss: 0.0289 Test accuracy: 0.7739 Epoch: 2/3 Loss: 0.0204 Test accuracy: 0.7695 Epoch: 2/3 Loss: 0.0260 Test accuracy: 0.7815 Epoch: 2/3 Loss: 0.0271 Test accuracy: 0.8031 Epoch: 2/3 Loss: 0.0237 Test accuracy: 0.8120 Epoch: 2/3 Loss: 0.0228 Test accuracy: 0.8084 Epoch: 2/3 Loss: 0.0238 Test accuracy: 0.8084 Epoch: 2/3 Loss: 0.0208 Test accuracy: 0.7980 Epoch: 2/3 Loss: 0.0237 Test accuracy: 0.7919 Epoch: 2/3 Loss: 0.0181 Test accuracy: 0.7976 Epoch: 2/3 Loss: 0.0361 Test accuracy: 0.7996 Epoch: 2/3 Loss: 0.0228 Test accuracy: 0.7828 Epoch: 2/3 Loss: 0.0252 Test accuracy: 0.7852 Epoch: 2/3 Loss: 0.0246 Test accuracy: 0.7880 Epoch: 2/3 Loss: 0.0157 Test accuracy: 0.7856 Epoch: 2/3 Loss: 0.0221 Test accuracy: 0.7847 Epoch: 2/3 Loss: 0.0241 Test accuracy: 0.7919 Epoch: 2/3 Loss: 0.0220 Test accuracy: 0.7959 Epoch: 2/3 Loss: 0.0158 Test accuracy: 0.8055 Epoch: 2/3 Loss: 0.0233 Test accuracy: 0.8164 Epoch: 2/3 Loss: 0.0267 Test accuracy: 0.8304 Epoch: 2/3 Loss: 0.0331 Test accuracy: 0.8324 Epoch: 2/3 Loss: 0.0232 Test accuracy: 0.8312 Epoch: 2/3 Loss: 0.0169 Test accuracy: 0.8132 Epoch: 2/3 Loss: 0.0201 Test accuracy: 0.8036 Epoch: 2/3 Loss: 0.0197 Test accuracy: 0.7827 Epoch: 2/3 Loss: 0.0181 Test accuracy: 0.7899 Epoch: 2/3 Loss: 0.0277 Test accuracy: 0.8043 Epoch: 2/3 Loss: 0.0124 Test accuracy: 0.8176 Epoch: 2/3 Loss: 0.0211 Test accuracy: 0.8236 Epoch: 2/3 Loss: 0.0203 Test accuracy: 0.8284 Epoch: 2/3 Loss: 0.0177 Test accuracy: 0.8357 Epoch: 2/3 Loss: 0.0196 Test accuracy: 0.8272 Epoch: 2/3 Loss: 0.0251 Test accuracy: 0.8236 Epoch: 2/3 Loss: 0.0212 Test accuracy: 0.8140 Epoch: 2/3 Loss: 0.0180 Test accuracy: 0.8056 Epoch: 2/3 Loss: 0.0213 Test accuracy: 0.7984 Epoch: 2/3 Loss: 0.0292 Test accuracy: 0.7964 Epoch: 2/3 Loss: 0.0195 Test accuracy: 0.8060 Epoch: 2/3 Loss: 0.0317 Test accuracy: 0.8036 Epoch: 2/3 Loss: 0.0245 Test accuracy: 0.8168 Epoch: 2/3 Loss: 0.0229 Test accuracy: 0.8144 Epoch: 2/3 Loss: 0.0170 Test accuracy: 0.8168 Epoch: 2/3 Loss: 0.0243 Test accuracy: 0.8048 Epoch: 2/3 Loss: 0.0174 Test accuracy: 0.8068 Epoch: 2/3 Loss: 0.0205 Test accuracy: 0.8008 Epoch: 2/3 Loss: 0.0250 Test accuracy: 0.8056 Epoch: 2/3 Loss: 0.0168 Test accuracy: 0.8104 Epoch: 2/3 Loss: 0.0174 Test accuracy: 0.8116 Epoch: 2/3 Loss: 0.0151 Test accuracy: 0.8104 Epoch: 2/3 Loss: 0.0247 Test accuracy: 0.8056 Epoch: 2/3 Loss: 0.0254 Test accuracy: 0.8092 Epoch: 2/3 Loss: 0.0323 Test accuracy: 0.8272 Epoch: 2/3 Loss: 0.0241 Test accuracy: 0.8332 Epoch: 2/3 Loss: 0.0187 Test accuracy: 0.8369 Epoch: 2/3 Loss: 0.0249 Test accuracy: 0.8372 Epoch: 2/3 Loss: 0.0256 Test accuracy: 0.8396 Epoch: 2/3 Loss: 0.0196 Test accuracy: 0.8280 Epoch: 2/3 Loss: 0.0201 Test accuracy: 0.8280 Epoch: 2/3 Loss: 0.0243 Test accuracy: 0.8324 Epoch: 2/3 Loss: 0.0303 Test accuracy: 0.8280 Epoch: 2/3 Loss: 0.0196 Test accuracy: 0.8300 Epoch: 2/3 Loss: 0.0188 Test accuracy: 0.8132 Epoch: 2/3 Loss: 0.0207 Test accuracy: 0.8152 Epoch: 2/3 Loss: 0.0300 Test accuracy: 0.8068 Epoch: 2/3 Loss: 0.0179 Test accuracy: 0.8080 Epoch: 2/3 Loss: 0.0353 Test accuracy: 0.8068 Epoch: 2/3 Loss: 0.0162 Test accuracy: 0.8236 Epoch: 2/3 Loss: 0.0130 Test accuracy: 0.8256 Epoch: 2/3 Loss: 0.0252 Test accuracy: 0.8256 Epoch: 2/3 Loss: 0.0199 Test accuracy: 0.8284 Epoch: 2/3 Loss: 0.0217 Test accuracy: 0.8360 Epoch: 2/3 Loss: 0.0212 Test accuracy: 0.8492 Epoch: 2/3 Loss: 0.0222 Test accuracy: 0.8516 Epoch: 2/3 Loss: 0.0216 Test accuracy: 0.8492 Epoch: 2/3 Loss: 0.0164 Test accuracy: 0.8528 Epoch: 2/3 Loss: 0.0171 Test accuracy: 0.8504 Epoch: 2/3 Loss: 0.0170 Test accuracy: 0.8464 Epoch: 2/3 Loss: 0.0197 Test accuracy: 0.8464 Epoch: 2/3 Loss: 0.0249 Test accuracy: 0.8404 Epoch: 2/3 Loss: 0.0329 Test accuracy: 0.8348 Epoch: 3/3 Loss: 0.0149 Test accuracy: 0.8364 Epoch: 3/3 Loss: 0.0172 Test accuracy: 0.8316 Epoch: 3/3 Loss: 0.0240 Test accuracy: 0.8292 Epoch: 3/3 Loss: 0.0231 Test accuracy: 0.8228 Epoch: 3/3 Loss: 0.0250 Test accuracy: 0.8312 Epoch: 3/3 Loss: 0.0110 Test accuracy: 0.8312 Epoch: 3/3 Loss: 0.0179 Test accuracy: 0.8300 Epoch: 3/3 Loss: 0.0157 Test accuracy: 0.8296 Epoch: 3/3 Loss: 0.0201 Test accuracy: 0.8345 Epoch: 3/3 Loss: 0.0181 Test accuracy: 0.8308 Epoch: 3/3 Loss: 0.0270 Test accuracy: 0.8284 Epoch: 3/3 Loss: 0.0233 Test accuracy: 0.8264 Epoch: 3/3 Loss: 0.0161 Test accuracy: 0.8268 Epoch: 3/3 Loss: 0.0189 Test accuracy: 0.8172 Epoch: 3/3 Loss: 0.0272 Test accuracy: 0.8152 Epoch: 3/3 Loss: 0.0241 Test accuracy: 0.8107 Epoch: 3/3 Loss: 0.0175 Test accuracy: 0.8119 Epoch: 3/3 Loss: 0.0187 Test accuracy: 0.8167 Epoch: 3/3 Loss: 0.0137 Test accuracy: 0.8320 Epoch: 3/3 Loss: 0.0210 Test accuracy: 0.8344 Epoch: 3/3 Loss: 0.0156 Test accuracy: 0.8400 Epoch: 3/3 Loss: 0.0179 Test accuracy: 0.8380 Epoch: 3/3 Loss: 0.0126 Test accuracy: 0.8320 Epoch: 3/3 Loss: 0.0169 Test accuracy: 0.8236 Epoch: 3/3 Loss: 0.0161 Test accuracy: 0.8304 Epoch: 3/3 Loss: 0.0160 Test accuracy: 0.8376 Epoch: 3/3 Loss: 0.0187 Test accuracy: 0.8412 Epoch: 3/3 Loss: 0.0231 Test accuracy: 0.8448 Epoch: 3/3 Loss: 0.0206 Test accuracy: 0.8393 Epoch: 3/3 Loss: 0.0248 Test accuracy: 0.8332 Epoch: 3/3 Loss: 0.0169 Test accuracy: 0.8429 Epoch: 3/3 Loss: 0.0158 Test accuracy: 0.8364 Epoch: 3/3 Loss: 0.0172 Test accuracy: 0.8320 Epoch: 3/3 Loss: 0.0199 Test accuracy: 0.8320 Epoch: 3/3 Loss: 0.0167 Test accuracy: 0.8352 Epoch: 3/3 Loss: 0.0220 Test accuracy: 0.8340 Epoch: 3/3 Loss: 0.0151 Test accuracy: 0.8316 Epoch: 3/3 Loss: 0.0198 Test accuracy: 0.8316 Epoch: 3/3 Loss: 0.0119 Test accuracy: 0.8376 Epoch: 3/3 Loss: 0.0177 Test accuracy: 0.8352 Epoch: 3/3 Loss: 0.0188 Test accuracy: 0.8308 Epoch: 3/3 Loss: 0.0187 Test accuracy: 0.8332 Epoch: 3/3 Loss: 0.0193 Test accuracy: 0.8360 Epoch: 3/3 Loss: 0.0135 Test accuracy: 0.8356 Epoch: 3/3 Loss: 0.0173 Test accuracy: 0.8380 Epoch: 3/3 Loss: 0.0154 Test accuracy: 0.8356 Epoch: 3/3 Loss: 0.0187 Test accuracy: 0.8340 Epoch: 3/3 Loss: 0.0203 Test accuracy: 0.8360 Epoch: 3/3 Loss: 0.0242 Test accuracy: 0.8396 Epoch: 3/3 Loss: 0.0175 Test accuracy: 0.8260 Epoch: 3/3 Loss: 0.0140 Test accuracy: 0.8212 Epoch: 3/3 Loss: 0.0127 Test accuracy: 0.8116 Epoch: 3/3 Loss: 0.0303 Test accuracy: 0.8056 Epoch: 3/3 Loss: 0.0125 Test accuracy: 0.7996 Epoch: 3/3 Loss: 0.0148 Test accuracy: 0.8020 Epoch: 3/3 Loss: 0.0168 Test accuracy: 0.8080 Epoch: 3/3 Loss: 0.0129 Test accuracy: 0.8120 Epoch: 3/3 Loss: 0.0202 Test accuracy: 0.8156 Epoch: 3/3 Loss: 0.0230 Test accuracy: 0.8144 Epoch: 3/3 Loss: 0.0111 Test accuracy: 0.8180 Epoch: 3/3 Loss: 0.0242 Test accuracy: 0.8184 Epoch: 3/3 Loss: 0.0158 Test accuracy: 0.8256 Epoch: 3/3 Loss: 0.0155 Test accuracy: 0.8184 Epoch: 3/3 Loss: 0.0133 Test accuracy: 0.8228 Epoch: 3/3 Loss: 0.0142 Test accuracy: 0.8216 Epoch: 3/3 Loss: 0.0189 Test accuracy: 0.8300 Epoch: 3/3 Loss: 0.0184 Test accuracy: 0.8481 Epoch: 3/3 Loss: 0.0163 Test accuracy: 0.8505 Epoch: 3/3 Loss: 0.0066 Test accuracy: 0.8444 Epoch: 3/3 Loss: 0.0141 Test accuracy: 0.8364 Epoch: 3/3 Loss: 0.0192 Test accuracy: 0.8260 Epoch: 3/3 Loss: 0.0196 Test accuracy: 0.8248 Epoch: 3/3 Loss: 0.0177 Test accuracy: 0.8264 Epoch: 3/3 Loss: 0.0183 Test accuracy: 0.8227 Epoch: 3/3 Loss: 0.0220 Test accuracy: 0.8315 Epoch: 3/3 Loss: 0.0182 Test accuracy: 0.8391 Epoch: 3/3 Loss: 0.0240 Test accuracy: 0.8391 Epoch: 3/3 Loss: 0.0275 Test accuracy: 0.8503 Epoch: 3/3 Loss: 0.0233 Test accuracy: 0.8463 Epoch: 3/3 Loss: 0.0228 Test accuracy: 0.8463 Epoch: 3/3 Loss: 0.0137 Test accuracy: 0.8379 Epoch: 3/3 Loss: 0.0231 Test accuracy: 0.8376 Epoch: 3/3 Loss: 0.0134 Test accuracy: 0.8308 Epoch: 3/3 Loss: 0.0292 Test accuracy: 0.8324 Epoch: 3/3 Loss: 0.0212 Test accuracy: 0.8389 Epoch: 3/3 Loss: 0.0101 Test accuracy: 0.8405 Epoch: 3/3 Loss: 0.0216 Test accuracy: 0.8372 Epoch: 3/3 Loss: 0.0202 Test accuracy: 0.8396 Epoch: 3/3 Loss: 0.0227 Test accuracy: 0.8420 Epoch: 3/3 Loss: 0.0194 Test accuracy: 0.8400 Epoch: 3/3 Loss: 0.0221 Test accuracy: 0.8400 Epoch: 3/3 Loss: 0.0159 Test accuracy: 0.8420 Epoch: 3/3 Loss: 0.0198 Test accuracy: 0.8444 Epoch: 3/3 Loss: 0.0214 Test accuracy: 0.8372 Epoch: 3/3 Loss: 0.0144 Test accuracy: 0.8324 Epoch: 3/3 Loss: 0.0148 Test accuracy: 0.8228 Epoch: 3/3 Loss: 0.0187 Test accuracy: 0.8296 Epoch: 3/3 Loss: 0.0175 Test accuracy: 0.8312 Epoch: 3/3 Loss: 0.0175 Test accuracy: 0.8300 Epoch: 3/3 Loss: 0.0280 Test accuracy: 0.8340 Epoch: 3/3 Loss: 0.0187 Test accuracy: 0.8412 Epoch: 3/3 Loss: 0.0215 Test accuracy: 0.8501 Epoch: 3/3 Loss: 0.0249 Test accuracy: 0.8537 保存检查点训练好网络后，保存模型，以便稍后加载它并进行预测。你可能还需要保存其他内容，例如从类别到索引的映射，索引是从某个图像数据集中获取的：image_datasets[&#39;train&#39;].class_to_idx。你可以将其作为属性附加到模型上，这样稍后推理会更轻松。 '''注意，稍后你需要完全重新构建模型，以便用模型进行推理。确保在检查点中包含你所需的任何信息。如果你想加载模型并继续训练，则需要保存周期数量和优化器状态 `optimizer.state_dict`。你可能需要在下面的下个部分使用训练的模型，因此建议立即保存它。 ```python'''# TODO: Save the checkpoint # model_dict = image_datasets.class_to_idxtorch.save(model.state_dict(), 'checkpoint.pth') 加载检查点此刻，建议写一个可以加载检查点并重新构建模型的函数。这样的话，你可以回到此项目并继续完善它，而不用重新训练网络。 # TODO: Write a function that loads a checkpoint and rebuilds the modeldef load_checkpoint(filepath): model_res = torch.load(filepath) model.load_state_dict(model_res) return model model = load_checkpoint('checkpoint.pth')print(model) VGG( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU(inplace) (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU(inplace) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU(inplace) (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (15): ReLU(inplace) (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (18): ReLU(inplace) (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (20): ReLU(inplace) (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (22): ReLU(inplace) (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (25): ReLU(inplace) (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (27): ReLU(inplace) (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (29): ReLU(inplace) (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (classifier): Sequential( (fc1): Linear(in_features=25088, out_features=1024, bias=True) (relu): ReLU() (fc2): Linear(in_features=1024, out_features=102, bias=True) (output): LogSoftmax() ) ) 类别推理现在，你需要写一个使用训练的网络进行推理的函数。即你将向网络中传入一个图像，并预测图像中的花卉类别。写一个叫做 predict 的函数，该函数会接受图像和模型，然后返回概率在前 $K$ 的类别及其概率。应该如下所示： def predict(image_path, model, topk=5): ''' Predict the class (or classes) of an image using a trained deep learning model. ''' model.eval() dataiter = iter(valid_dataloaders) images, labels = dataiter.next() outputs = model(Variable(images)) _,predicted = torch.max(outputs.data, 1) return _,predicted # TODO: Implement the code to predict the class from an image file image_path = data_dir + valid_dirprobs, classes = predict(image_path, model)print(probs)print(classes)# &gt; [ 0.01558163 0.01541934 0.01452626 0.01443549 0.01407339]# &gt; ['70', '3', '45', '62', '55'] tensor([-5.6533e-01, -1.0756e+00, -3.3086e-02, -6.0575e-01, -1.6227e+00, -1.2889e+00, -9.1778e-01, -1.0155e-01, -9.0205e-03, -2.6388e-02, -1.1171e-01, -2.5127e-05, -8.6765e-06, -1.2372e-05, -3.3515e-03, -4.3414e-04, -2.2719e-02, -3.1610e-02, -1.4661e-01, -6.8172e-03, -3.0326e-03, -4.6300e-04, -5.8895e-01, -7.0555e-03, -4.4683e-03, -1.2083e-06, -7.6587e-02, -2.2083e-05, -7.6721e-05, -5.5256e-02, -9.8522e-01, -1.8399e-01]) tensor([ 0, 95, 0, 27, 99, 0, 49, 0, 94, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 5, 4, 4, 4, 4, 4, 4, 5, 40, 5]) 首先，你需要处理输入图像，使其可以用于你的网络。 图像处理你需要使用 PIL 加载图像（文档）。建议写一个函数来处理图像，使图像可以作为模型的输入。该函数应该按照训练的相同方式处理图像。 首先，调整图像大小，使最小的边为 256 像素，并保持宽高比。为此，可以使用 thumbnail 或 resize 方法。然后，你需要从图像的中心裁剪出 224x224 的部分。 图像的颜色通道通常编码为整数 0-255，但是该模型要求值为浮点数 0-1。你需要变换值。使用 Numpy 数组最简单，你可以从 PIL 图像中获取，例如 np_image = np.array(pil_image)。 和之前一样，网络要求图像按照特定的方式标准化。均值应标准化为 [0.485, 0.456, 0.406]，标准差应标准化为 [0.229, 0.224, 0.225]。你需要用每个颜色通道减去均值，然后除以标准差。 最后，PyTorch 要求颜色通道为第一个维度，但是在 PIL 图像和 Numpy 数组中是第三个维度。你可以使用 ndarray.transpose对维度重新排序。颜色通道必须是第一个维度，并保持另外两个维度的顺序。 def process_image(image): ''' Scales, crops, and normalizes a PIL image for a PyTorch model, returns an Numpy array ''' valid_transforms = transforms.Compose([ transforms.Resize(255), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) image_datasets_valid = datasets.ImageFolder(data_dir + valid_dir, transform=valid_transforms) valid_dataloaders = torch.utils.data.DataLoader(image_datasets_valid, batch_size=32) # TODO: Process a PIL image for use in a PyTorch model 要检查你的项目，可以使用以下函数来转换 PyTorch 张量并将其显示在 notebook 中。如果 process_image 函数可行，用该函数运行输出应该会返回原始图像（但是剪裁掉的部分除外）。 def imshow(image, ax=None, title=None): \"\"\"Imshow for Tensor.\"\"\" if ax is None: fig, ax = plt.subplots() # PyTorch tensors assume the color channel is the first dimension # but matplotlib assumes is the third dimension image = image.numpy().transpose((1, 2, 0)) # Undo preprocessing mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) image = std * image + mean # Image needs to be clipped between 0 and 1 or it looks like noise when displayed image = np.clip(image, 0, 1) ax.imshow(image) return ax 类别预测可以获得格式正确的图像后 要获得前 $K$ 个值，在张量中使用 x.topk(k)。该函数会返回前 k 个概率和对应的类别索引。你需要使用 class_to_idx（希望你将其添加到了模型中）将这些索引转换为实际类别标签，或者从用来加载数据的 ImageFolder进行转换。确保颠倒字典 同样，此方法应该接受图像路径和模型检查点，并返回概率和类别。 probs, classes = predict(image_path, model)print(probs)print(classes)&gt; [ 0.01558163 0.01541934 0.01452626 0.01443549 0.01407339]&gt; ['70', '3', '45', '62', '55'] 检查运行状况你已经可以使用训练的模型做出预测，现在检查模型的性能如何。即使测试准确率很高，始终有必要检查是否存在明显的错误。使用 matplotlib 将前 5 个类别的概率以及输入图像绘制为条形图，应该如下所示： 你可以使用 cat_to_name.json 文件（应该之前已经在 notebook 中加载该文件）将类别整数编码转换为实际花卉名称。要将 PyTorch 张量显示为图像，请使用定义如下的 imshow 函数。 # TODO: Display an image along with the top 5 classes","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://jinyaxuan.github.io/categories/深度学习/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://jinyaxuan.github.io/tags/Deep-Learning/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://jinyaxuan.github.io/categories/深度学习/"}]},{"title":"强化学习(二) 蒙特卡洛","slug":"蒙特卡洛","date":"2019-02-16T15:21:21.000Z","updated":"2019-02-16T15:26:44.506Z","comments":false,"path":"2019/02/16/蒙特卡洛/","link":"","permalink":"http://jinyaxuan.github.io/2019/02/16/蒙特卡洛/","excerpt":"","text":"迷你项目：蒙特卡洛方法在此 notebook 中，你将自己编写很多蒙特卡洛 (MC) 算法的实现。 虽然我们提供了一些起始代码，但是你可以删掉这些提示并从头编写代码。 第 0 部分：探索 BlackjackEnv请使用以下代码单元格创建 Blackjack 环境的实例。 import gymenv = gym.make('Blackjack-v0') 每个状态都是包含以下三个元素的 3 元组： 玩家的当前点数之和 $\\in {0, 1, \\ldots, 31}$， 庄家朝上的牌点数之和 $\\in {1, \\ldots, 10}$，及 玩家是否有能使用的王牌（no $=0$、yes $=1$）。 智能体可以执行两个潜在动作： STICK = 0HIT = 1 通过运行以下代码单元格进行验证。 print(env.observation_space)print(env.action_space) Tuple(Discrete(32), Discrete(11), Discrete(2)) Discrete(2) 执行以下代码单元格以按照随机策略玩二十一点。 （代码当前会玩三次二十一点——你可以随意修改该数字，或者多次运行该单元格。该单元格旨在让你体验当智能体与环境互动时返回的输出结果。） for i_episode in range(5): state = env.reset() while True: print(state) action = env.action_space.sample() state, reward, done, info = env.step(action) if done: print('End game! Reward: ', reward) print('You won :)\\n') if reward &gt; 0 else print('You lost :(\\n') break (12, 8, False) End game! Reward: -1.0 You lost :( (19, 10, True) (19, 10, False) (21, 10, False) End game! Reward: 1.0 You won :) (17, 9, False) End game! Reward: -1 You lost :( (10, 7, False) (12, 7, False) (13, 7, False) (15, 7, False) End game! Reward: -1 You lost :( (21, 10, True) (21, 10, False) End game! Reward: -1 You lost :( 第 1 部分：MC 预测 - 状态值在此部分，你将自己编写 MC 预测的实现（用于估算状态值函数）。 我们首先将研究以下策略：如果点数之和超过 18，玩家将始终停止出牌。函数 generate_episode_from_limit 会根据该策略抽取一个阶段。 该函数会接收以下输入： bj_env：这是 OpenAI Gym 的 Blackjack 环境的实例。 它会返回以下输出： episode：这是一个（状态、动作、奖励）元组列表，对应的是 $(S0, A_0, R_1, \\ldots, S{T-1}, A{T-1}, R{T})$， 其中 $T$ 是最终时间步。具体而言，episode[i] 返回 $(Si, A_i, R{i+1})$， episode[i][0]、episode[i][1]和 episode[i][2] 分别返回 $Si$, $A_i$和 $R{i+1}$。 def generate_episode_from_limit(bj_env): episode = [] state = bj_env.reset() while True: action = 0 if state[0] &gt; 18 else 1 next_state, reward, done, info = bj_env.step(action) episode.append((state, action, reward)) state = next_state if done: break return episode 执行以下代码单元格以按照该策略玩二十一点。 （代码当前会玩三次二十一点——你可以随意修改该数字，或者多次运行该单元格。该单元格旨在让你熟悉 generate_episode_from_limit 函数的输出结果。） for i in range(3): print(generate_episode_from_limit(env)) [((21, 6, True), 0, 1.0)] [((20, 10, False), 0, 1.0)] [((12, 10, False), 1, -1)] 现在你已经准备好自己编写 MC 预测的实现了。你可以选择实现首次经历或所有经历 MC 预测；对于 Blackjack 环境，这两种技巧是对等的。 你的算法将有四个参数： env：这是 OpenAI Gym 环境的实例。 num_episodes：这是通过智能体-环境互动生成的阶段次数。 generate_episode：这是返回互动阶段的函数。 gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。 该算法会返回以下输出结果： V：这是一个字典，其中 V[s] 是状态 s 的估算值。例如，如果代码返回以下输出结果：{(4, 7, False): -0.38775510204081631, (18, 6, False): -0.58434296365330851, (13, 2, False): -0.43409090909090908, (6, 7, False): -0.3783783783783784, …则状态 (4, 7, False) 的值估算为 -0.38775510204081631。 如果你不知道如何在 Python 中使用 defaultdict，建议查看此源代码。 from collections import defaultdictimport numpy as npimport sysdef mc_prediction_v(env, num_episodes, generate_episode, gamma=1.0): # initialize empty dictionary of lists returns = defaultdict(list) # loop over episodes for i_episode in range(1, num_episodes+1): # monitor progress if i_episode % 1000 == 0: print(\"\\rEpisode &#123;&#125;/&#123;&#125;.\".format(i_episode, num_episodes), end=\"\") sys.stdout.flush() ## TODO: complete the function episode = generate_episode(env) states, action, rewards = zip(*episode) discounts = np.array([gamma**i for i in range(len(rewards)+1)]) for i, state in enumerate(states): returns[state].append(sum(rewards[i:]*discounts[:-(1+i)])) V = &#123;k: np.mean(v) for k, v in returns.items()&#125; return V 使用以下单元格计算并绘制状态值函数估算值。 (用于绘制值函数的代码来自此源代码，并且稍作了修改。） 要检查你的实现是否正确，应将以下图与解决方案 notebook Monte_Carlo_Solution.ipynb 中的对应图进行比较。 from plot_utils import plot_blackjack_values# obtain the value functionV = mc_prediction_v(env, 500000, generate_episode_from_limit)# plot the value functionplot_blackjack_values(V) Episode 500000/500000. &lt;matplotlib.figure.Figure at 0x7f88aaf03e80&gt; 第 2 部分：MC 预测 - 动作值在此部分，你将自己编写 MC 预测的实现（用于估算动作值函数）。 我们首先将研究以下策略：如果点数之和超过 18，玩家将几乎始终停止出牌。具体而言，如果点数之和大于 18，她选择动作 STICK 的概率是 80%；如果点数之和不大于 18，她选择动作 HIT 的概率是 80%。函数 generate_episode_from_limit_stochastic 会根据该策略抽取一个阶段。 该函数会接收以下输入： bj_env：这是 OpenAI Gym 的 Blackjack 环境的实例。 该算法会返回以下输出结果： episode: 这是一个（状态、动作、奖励）元组列表，对应的是 $(S0, A_0, R_1, \\ldots, S{T-1}, A{T-1}, R{T})$， 其中 $T$ 是最终时间步。具体而言，episode[i] 返回 $(Si, A_i, R{i+1})$， episode[i][0]、episode[i][1]和 episode[i][2] 分别返回 $Si$, $A_i$和 $R{i+1}$。 def generate_episode_from_limit_stochastic(bj_env): episode = [] state = bj_env.reset() while True: probs = [0.8, 0.2] if state[0] &gt; 18 else [0.2, 0.8] action = np.random.choice(np.arange(2), p=probs) next_state, reward, done, info = bj_env.step(action) episode.append((state, action, reward)) state = next_state if done: break return episode 现在你已经准备好自己编写 MC 预测的实现了。你可以选择实现首次经历或所有经历 MC 预测；对于 Blackjack 环境，这两种技巧是对等的。 你的算法将有四个参数： env: 这是 OpenAI Gym 环境的实例。 num_episodes：这是通过智能体-环境互动生成的阶段次数。 generate_episode：这是返回互动阶段的函数。 gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。 该算法会返回以下输出结果： Q：这是一个字典（一维数组），其中 Q[s][a] 是状态 s 和动作 a 对应的估算动作值。 def mc_prediction_q(env, num_episodes, generate_episode, gamma=1.0): # initialize empty dictionaries of arrays returns_sum = defaultdict(lambda: np.zeros(env.action_space.n)) N = defaultdict(lambda: np.zeros(env.action_space.n)) Q = defaultdict(lambda: np.zeros(env.action_space.n)) # loop over episodes for i_episode in range(1, num_episodes+1): # monitor progress if i_episode % 1000 == 0: print(\"\\rEpisode &#123;&#125;/&#123;&#125;.\".format(i_episode, num_episodes), end=\"\") sys.stdout.flush() ## TODO: complete the function episode = generate_episode(env) states, actions, rewards = zip(*episode) discounts = np.array([gamma**i for i in range(len(rewards)+1)]) for i, state in enumerate(states): returns_sum[state][actions[i]] += sum(rewards[i:]*discounts[:-(1+i)]) N[state][actions[i]] += 1 Q[state][actions[i]] = returns_sum[state][actions[i]] / N[state][actions[i]] return Q 请使用以下单元格获取动作值函数估值 $Q$。我们还绘制了相应的状态值函数。 要检查你的实现是否正确，应将以下图与解决方案 notebook Monte_Carlo_Solution.ipynb 中的对应图进行比较。 # obtain the action-value functionQ = mc_prediction_q(env, 500000, generate_episode_from_limit_stochastic)# obtain the state-value functionV_to_plot = dict((k,(k[0]&gt;18)*(np.dot([0.8, 0.2],v)) + (k[0]&lt;=18)*(np.dot([0.2, 0.8],v))) \\ for k, v in Q.items())# plot the state-value functionplot_blackjack_values(V_to_plot) Episode 500000/500000. 第 3 部分：MC 控制 - GLIE在此部分，你将自己编写常量-$\\alpha$ MC 控制的实现。 你的算法将有四个参数： env: 这是 OpenAI Gym 环境的实例。 num_episodes：这是通过智能体-环境互动生成的阶段次数。 generate_episode：这是返回互动阶段的函数。 gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。 该算法会返回以下输出结果： Q：这是一个字典（一维数组），其中 Q[s][a] 是状态 s 和动作 a 对应的估算动作值。 policy：这是一个字典，其中 policy[s] 会返回智能体在观察状态 s 之后选择的动作。 （你可以随意定义其他函数，以帮助你整理代码。） def generate_episode_from_Q(env, Q, epsilon, nA): episode = [] state = env.reset() while True: action = np.random.choice(np.arange(nA), p=get_probs(Q[state], epsilon, nA)) \\ if state in Q else env.action_space.sample() next_state, reward, done, info = env.step(action) episode.append((state, action, reward)) state = next_state if done: break return episodedef get_probs(Q_s, epsilon, nA): \"\"\" obtains the action probabilities corresponding to epsilon-greedy policy \"\"\" policy_s = np.ones(nA) * epsilon / nA best_a = np.argmax(Q_s) policy_s[best_a] = 1 - epsilon + (epsilon / nA) return policy_sdef update_Q_GLIE(env, episode, Q, N, gamma): \"\"\" updates the action-value function estimate using the most recent episode \"\"\" states, actions, rewards = zip(*episode) # prepare for discounting discounts = np.array([gamma**i for i in range(len(rewards)+1)]) for i, state in enumerate(states): old_Q = Q[state][actions[i]] old_N = N[state][actions[i]] Q[state][actions[i]] = old_Q + (sum(rewards[i:]*discounts[:-(1+i)]) - old_Q)/(old_N+1) N[state][actions[i]] += 1 return Q, N def mc_control_GLIE(env, num_episodes, gamma=1.0): nA = env.action_space.n # initialize empty dictionaries of arrays Q = defaultdict(lambda: np.zeros(nA)) N = defaultdict(lambda: np.zeros(nA)) # loop over episodes for i_episode in range(1, num_episodes+1): # monitor progress if i_episode % 1000 == 0: print(\"\\rEpisode &#123;&#125;/&#123;&#125;.\".format(i_episode, num_episodes), end=\"\") sys.stdout.flush() ## TODO: complete the function epsilon = 1.0/((i_episode/8000)+1) episode = generate_episode_from_Q(env, Q, epsilon, nA) Q, N = update_Q_GLIE(env, episode, Q, N, gamma) # determine the policy corresponding to the final action-value function estimate policy = dict((k,np.argmax(v)) for k, v in Q.items()) return policy, Q 通过以下单元格获取估算的最优策略和动作值函数。 # obtain the estimated optimal policy and action-value functionpolicy_glie, Q_glie = mc_control_GLIE(env, 500000) Episode 500000/500000. 接着，我们将绘制相应的状态值函数。 # obtain the state-value functionV_glie = dict((k,np.max(v)) for k, v in Q_glie.items())# plot the state-value functionplot_blackjack_values(V_glie) 最后，我们将可视化估算为最优策略的策略。 from plot_utils import plot_policy# plot the policyplot_policy(policy_glie) 真最优策略 $\\pi_*$ 可以在该教科书的第 82 页找到（下文也提供了）。请将你的最终估算值与最优策略进行比较——它们能够有多接近？如果你对算法的效果不满意，请花时间调整 $\\epsilon$ 的衰减率和/或使该算法运行更多个阶段，以获得更好的结果。 第 4 部分：MC 控制 - 常量-$\\alpha$在此部分，你将自己编写常量-$\\alpha$ MC 控制的实现。 你的算法将有三个参数： env: 这是 OpenAI Gym 环境的实例。 num_episodes：这是通过智能体-环境互动生成的阶段次数。 generate_episode：这是返回互动阶段的函数。 alpha：这是更新步骤的步长参数。 gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。 该算法会返回以下输出结果： Q：这是一个字典（一维数组），其中 Q[s][a] 是状态 s 和动作 a 对应的估算动作值。 policy：这是一个字典，其中 policy[s] 会返回智能体在观察状态 s 之后选择的动作。 （你可以随意定义其他函数，以帮助你整理代码。） def mc_control_alpha(env, num_episodes, alpha, gamma=1.0): nA = env.action_space.n # initialize empty dictionary of arrays Q = defaultdict(lambda: np.zeros(nA)) # loop over episodes for i_episode in range(1, num_episodes+1): # monitor progress if i_episode % 1000 == 0: print(\"\\rEpisode &#123;&#125;/&#123;&#125;.\".format(i_episode, num_episodes), end=\"\") sys.stdout.flush() ## TODO: complete the function epsilon = 1.0/((i_episode/8000)+1) # generate an episode by following epsilon-greedy policy episode = generate_episode_from_Q(env, Q, epsilon, nA) states, actions, rewards = zip(*episode) # prepare for discounting discounts = np.array([gamma**i for i in range(len(rewards)+1)]) for i, state in enumerate(states): old_Q = Q[state][actions[i]] Q[state][actions[i]] = old_Q + alpha*(sum(rewards[i:]*discounts[:-(1+i)]) - old_Q) policy = dict((k,np.argmax(v)) for k, v in Q.items()) return policy, Q 通过以下单元格获得估算的最优策略和动作值函数。 # obtain the estimated optimal policy and action-value functionpolicy_alpha, Q_alpha = mc_control_alpha(env, 500000, 0.008) Episode 500000/500000. 接着，我们将绘制相应的状态值函数。 # obtain the state-value functionV_alpha = dict((k,np.max(v)) for k, v in Q_alpha.items())# plot the state-value functionplot_blackjack_values(V_alpha) 最后，我们将可视化估算为最优策略的策略。 # plot the policyplot_policy(policy_alpha) 真最优策略 $\\pi_*$ 可以在该教科书的第 82 页找到（下文也提供了）。请将你的最终估算值与最优策略进行比较——它们能够有多接近？如果你对算法的效果不满意，请花时间调整 $\\epsilon$ 的衰减率和/或使该算法运行更多个阶段，以获得更好的结果。","categories":[{"name":"强化学习","slug":"强化学习","permalink":"http://jinyaxuan.github.io/categories/强化学习/"}],"tags":[{"name":"Reinforcement learning","slug":"Reinforcement-learning","permalink":"http://jinyaxuan.github.io/tags/Reinforcement-learning/"}],"keywords":[{"name":"强化学习","slug":"强化学习","permalink":"http://jinyaxuan.github.io/categories/强化学习/"}]},{"title":"强化学习(一) 动态规划","slug":"动态规划","date":"2019-02-16T11:19:24.000Z","updated":"2019-02-16T15:19:10.252Z","comments":false,"path":"2019/02/16/动态规划/","link":"","permalink":"http://jinyaxuan.github.io/2019/02/16/动态规划/","excerpt":"","text":"迷你项目：动态规划在此 notebook 中，你将自己编写很多经典动态规划算法的实现。 虽然我们提供了一些起始代码，但是你可以删掉这些提示并从头编写代码。 第 0 部分：探索 FrozenLakeEnv请使用以下代码单元格创建 FrozenLake 环境的实例。 from frozenlake import FrozenLakeEnvenv = FrozenLakeEnv() 智能体将会在 $4 \\times 4$ 网格世界中移动，状态编号如下所示： [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15]] 智能体可以执行 4 个潜在动作： LEFT = 0DOWN = 1RIGHT = 2UP = 3 因此，$\\mathcal{S}^+ = {0, 1, \\ldots, 15}$ 以及 $\\mathcal{A} = {0, 1, 2, 3}$。请通过运行以下代码单元格验证这一点。 # print the state space and action spaceprint(env.observation_space)print(env.action_space)# print the total number of states and actionsprint(env.nS)print(env.nA) Discrete(16) Discrete(4) 16 4 动态规划假设智能体完全了解 MDP。我们已经修改了 frozenlake.py 文件以使智能体能够访问一步动态特性。 请执行以下代码单元格以返回特定状态和动作对应的一步动态特性。具体而言，当智能体在网格世界中以状态 1 向左移动时，env.P[1][0] 会返回每个潜在奖励的概率和下一个状态。 env.P[0][0] [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)] 每个条目的格式如下所示 prob, next_state, reward, done 其中： prob 详细说明了相应的 (next_state, reward) 对的条件概率，以及 如果 next_state 是终止状态，则 done 是 True ，否则是 False。 因此，我们可以按照以下方式解析 env.P[1][0]： \\mathbb{P}(S_{t+1}=s',R_{t+1}=r|S_t=1,A_t=0) = \\begin{cases} \\frac{1}{3} \\text{ if } s'=1, r=0\\\\ \\frac{1}{3} \\text{ if } s'=0, r=0\\\\ \\frac{1}{3} \\text{ if } s'=5, r=0\\\\ 0 \\text{ else} \\end{cases}你可以随意更改上述代码单元格，以探索在其他（状态、动作）对下环境的行为是怎样的。 第 1 部分：迭代策略评估在此部分，你将自己编写迭代策略评估的实现。 你的算法应该有四个输入参数： env：这是 OpenAI Gym 环境的实例，其中 env.P 会返回一步动态特性。 policy：这是一个二维 numpy 数组，其中 policy.shape[0] 等于状态数量 (env.nS) ， policy.shape[1] 等于动作数量 (env.nA) 。policy[s][a] 返回智能体在状态 s 时根据该策略选择动作 a 的概率。 gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。 theta：这是一个非常小的正数，用于判断估算值是否足够地收敛于真值函数 (默认值为：1e-8）。 该算法会返回以下输出结果： V：这是一个一维numpy数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 在输入策略下的估算值。 请完成以下代码单元格中的函数。 import numpy as npdef policy_evaluation(env, policy, gamma=1, theta=1e-8): V = np.zeros(env.nS) ## TODO: complete the function while True: delta = 0 for s in range(env.nS): Vs = 0 for a, action_prob in enumerate(policy[s]): for prob, next_state, reward, done in env.P[s][a]: Vs += action_prob * prob * (reward + gamma * V[next_state]) delta = max(delta, np.abs(V[s]-Vs)) V[s] = Vs if delta &lt; theta: break return V 我们将评估等概率随机策略 $\\pi$，其中对于所有 $s\\in\\mathcal{S}$ 和 $a\\in\\mathcal{A}(s)$ ，$\\pi(a|s) = \\frac{1}{|\\mathcal{A}(s)|}$。 请使用以下代码单元格在变量 random_policy中指定该策略。 random_policy = np.ones([env.nS, env.nA]) / env.nA 运行下个代码单元格以评估等概率随机策略并可视化输出结果。状态值函数已调整形状，以匹配网格世界的形状。 from plot_utils import plot_values# evaluate the policy V = policy_evaluation(env, random_policy)plot_values(V) 运行以下代码单元格以测试你的函数。如果代码单元格返回 PASSED，则表明你正确地实现了该函数！ 注意：为了确保结果准确，确保你的 policy_evaluation 函数满足上文列出的要求（具有四个输入、一个输出，并且没有更改输入参数的默认值）。 import check_testcheck_test.run_check('policy_evaluation_check', policy_evaluation) PASSED 第 2 部分：通过 $v\\pi$ 获取 $q\\pi$在此部分，你将编写一个函数，该函数的输入是状态值函数估值以及一些状态 $s\\in\\mathcal{S}$。它会返回输入状态 $s\\in\\mathcal{S}$ 对应的动作值函数中的行。即你的函数应同时接受输入 $v\\pi$ 和 $s$，并针对所有 $a\\in\\mathcal{A}(s)$ 返回 $q\\pi(s,a)$。 你的算法应该有四个输入参数： env：这是 OpenAI Gym 环境的实例，其中 env.P 会返回一步动态特性。 V：这是一个一维 numpy 数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 的估值。 s：这是环境中的状态对应的整数。它应该是在 0 到 (env.nS)-1（含）之间的值。 gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。 该算法会返回以下输出结果： q：这是一个一维 numpy 数组，其中 q.shape[0] 等于动作数量 (env.nA)。q[a] 包含状态 s 和动作 a 的（估算）值。 请完成以下代码单元格中的函数。 def q_from_v(env, V, s, gamma=1): q = np.zeros(env.nA) ## TODO: complete the function for a in range(env.nA): for prob, next_state, reward, done in env.P[s][a]: q[a] += prob * (reward + gamma * V[next_state]) return q 请运行以下代码单元格以输出上述状态值函数对应的动作值函数。 Q = np.zeros([env.nS, env.nA])for s in range(env.nS): Q[s] = q_from_v(env, V, s)print(\"Action-Value Function:\")print(Q) Action-Value Function: [[ 0.0147094 0.01393978 0.01393978 0.01317015] [ 0.00852356 0.01163091 0.0108613 0.01550788] [ 0.02444514 0.02095298 0.02406033 0.01435346] [ 0.01047649 0.01047649 0.00698432 0.01396865] [ 0.02166487 0.01701828 0.01624865 0.01006281] [ 0. 0. 0. 0. ] [ 0.05433538 0.04735105 0.05433538 0.00698432] [ 0. 0. 0. 0. ] [ 0.01701828 0.04099204 0.03480619 0.04640826] [ 0.07020885 0.11755991 0.10595784 0.05895312] [ 0.18940421 0.17582037 0.16001424 0.04297382] [ 0. 0. 0. 0. ] [ 0. 0. 0. 0. ] [ 0.08799677 0.20503718 0.23442716 0.17582037] [ 0.25238823 0.53837051 0.52711478 0.43929118] [ 0. 0. 0. 0. ]] 运行以下代码单元格以测试你的函数。如果代码单元格返回 PASSED，则表明你正确地实现了该函数！ 注意：为了确保结果准确，确保 q_from_v 函数满足上文列出的要求（具有四个输入、一个输出，并且没有更改输入参数的默认值）。 check_test.run_check('q_from_v_check', q_from_v) PASSED 第 3 部分：策略改进在此部分，你将自己编写策略改进实现。 你的算法应该有三个输入参数： env：这是 OpenAI Gym 环境的实例，其中 env.P 会返回一步动态特性。 V：这是一个一维 numpy 数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 的估值。 gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。 该算法会返回以下输出结果： policy：这是一个二维 numpy 数组，其中 policy.shape[0] 等于状态数量 (env.nS) ， policy.shape[1] 等于动作数量 (env.nA) 。policy[s][a] 返回智能体在状态 s 时根据该策略选择动作 a 的概率。 请完成以下代码单元格中的函数。建议你使用你在上文实现的 q_from_v 函数。 def policy_improvement(env, V, gamma=1): policy = np.zeros([env.nS, env.nA]) / env.nA ## TODO: complete the function for s in range(env.nS): q = q_from_v(env, V, s, gamma) # best_a = np.argwhere(q==np.max(q)).flatten()# policy[s] = np.sum([np.eye(env.nA)[i] for i in best_a], axis=0)/len(best_a) policy[s][np.argmax(q)] = 1 return policy 运行以下代码单元格以测试你的函数。如果代码单元格返回 PASSED，则表明你正确地实现了该函数！ 注意：为了确保结果准确，确保 policy_improvement 函数满足上文列出的要求（具有三个输入、一个输出，并且没有更改输入参数的默认值）。 在继续转到该 notebook 的下个部分之前，强烈建议你参阅 Dynamic_Programming_Solution.ipynb 中的解决方案。该函数有很多正确的实现方式！ check_test.run_check('policy_improvement_check', policy_improvement) PASSED 第 4 部分：策略迭代在此部分，你将自己编写策略迭代的实现。该算法会返回最优策略，以及相应的状态值函数。 你的算法应该有三个输入参数： env：这是 OpenAI Gym 环境的实例，其中 env.P 会返回一步动态特性。 gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。 theta：这是一个非常小的正数，用于判断策略评估步骤是否足够地收敛于真值函数 (默认值为：1e-8）。 该算法会返回以下输出结果： policy：这是一个二维 numpy 数组，其中 policy.shape[0] 等于状态数量 (env.nS) ， policy.shape[1] 等于动作数量 (env.nA) 。policy[s][a] 返回智能体在状态 s 时根据该策略选择动作 a 的概率。 V：这是一个一维 numpy 数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 的估值。 请完成以下代码单元格中的函数。强烈建议你使用你在上文实现的 policy_evaluation 和 policy_improvement 函数。 import copydef policy_iteration(env, gamma=1, theta=1e-8): policy = np.ones([env.nS, env.nA]) / env.nA while True: V = policy_evaluation(env, policy, gamma, theta) new_policy = policy_improvement(env, V) ## TODO: complete the function if np.max(abs(policy_evaluation(env, policy) - policy_evaluation(env, new_policy))) &lt; theta*1e2: break policy = copy.copy(new_policy) return policy, V 运行下个代码单元格以解决该 MDP 并可视化输出结果。最优状态值函数已调整形状，以匹配网格世界的形状。 将该最优状态值函数与此 notebook 第 1 部分的状态值函数进行比较。最优状态值函数一直都大于或等于等概率随机策略的状态值函数吗？ # obtain the optimal policy and optimal state-value functionpolicy_pi, V_pi = policy_iteration(env)# print the optimal policyprint(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")print(policy_pi,\"\\n\")plot_values(V_pi) Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3): [[ 1. 0. 0. 0.] [ 0. 0. 0. 1.] [ 0. 0. 0. 1.] [ 0. 0. 0. 1.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 0. 0. 0. 1.] [ 0. 1. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 0. 0. 1. 0.] [ 0. 1. 0. 0.] [ 1. 0. 0. 0.]] 运行以下代码单元格以测试你的函数。如果代码单元格返回 PASSED，则表明你正确地实现了该函数！ 注意：为了确保结果准确，确保 policy_iteratio 函数满足上文列出的要求（具有三个输入、两个输出，并且没有更改输入参数的默认值）。 check_test.run_check('policy_iteration_check', policy_iteration) PASSED 第 5 部分：截断策略迭代在此部分，你将自己编写截断策略迭代的实现。 首先，你将实现截断策略评估。你的算法应该有五个输入参数： env：这是 OpenAI Gym 环境的实例，其中 env.P 会返回一步动态特性。 policy：这是一个二维 numpy 数组，其中 policy.shape[0] 等于状态数量 (env.nS) ， policy.shape[1] 等于动作数量 (env.nA) 。policy[s][a] 返回智能体在状态 s 时根据该策略选择动作 a 的概率。 V：这是一个一维 numpy 数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 的估值。 max_it：这是一个正整数，对应的是经历状态空间的次数（默认值为：1）。 gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。 该算法会返回以下输出结果： V：这是一个一维 numpy 数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 的估值。 请完成以下代码单元格中的函数。 def truncated_policy_evaluation(env, policy, V, max_it=1, gamma=1): ## TODO: complete the function counter = 0 while counter &lt; max_it: for s in range(env.nS): v = 0 q = q_from_v(env, V, s, gamma) for a, action_prob in enumerate(policy[s]): v += action_prob * q[a] V[s] = v counter += 1 return V 接着，你将实现截断策略迭代。你的算法应该接受四个输入参数： env：这是 OpenAI Gym 环境的实例，其中 env.P 会返回一步动态特性。 max_it：这是一个正整数，对应的是经历状态空间的次数（默认值为：1）。 gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。 theta：这是一个非常小的正整数，用作停止条件（默认值为：1e-8）。 该算法会返回以下输出结果： policy：这是一个二维 numpy 数组，其中 policy.shape[0] 等于状态数量 (env.nS) ， policy.shape[1] 等于动作数量 (env.nA) 。policy[s][a] 返回智能体在状态 s 时根据该策略选择动作 a 的概率。 V：这是一个一维 numpy 数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 的估值。 请完成以下代码单元格中的函数。 def truncated_policy_iteration(env, max_it=1, gamma=1, theta=1e-8): V = np.zeros(env.nS) policy = np.zeros([env.nS, env.nA]) / env.nA ## TODO: complete the function while True: policy = policy_improvement(env, V) old_V = copy.copy(V) V = policy_evaluation(env, policy, gamma=1, theta=1e-8) if max(abs(V - old_V)) &lt; theta: break return policy, V 运行下个代码单元格以解决该 MDP 并可视化输出结果。状态值函数已调整形状，以匹配网格世界的形状。 请实验不同的 max_it 参数值。始终都能获得最优状态值函数吗？ policy_tpi, V_tpi = truncated_policy_iteration(env, max_it=2)# print the optimal policyprint(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")print(policy_tpi,\"\\n\")# plot the optimal state-value functionplot_values(V_tpi) Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3): [[ 1. 0. 0. 0.] [ 0. 0. 0. 1.] [ 0. 0. 0. 1.] [ 0. 0. 0. 1.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 0. 0. 0. 1.] [ 0. 1. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 0. 0. 1. 0.] [ 0. 1. 0. 0.] [ 1. 0. 0. 0.]] 运行以下代码单元格以测试你的函数。如果代码单元格返回 PASSED，则表明你正确地实现了该函数！ 注意：为了确保结果准确，确保 truncated_policy_iteration 函数满足上文列出的要求（具有四个输入、两个输出，并且没有更改输入参数的默认值）。 check_test.run_check('truncated_policy_iteration_check', truncated_policy_iteration) PASSED 第 6 部分：值迭代在此部分，你将自己编写值迭代的实现。 你的算法应该接受三个输入参数： env：这是 OpenAI Gym 环境的实例，其中 env.P 会返回一步动态特性。 gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。 theta：这是一个非常小的正整数，用作停止条件（默认值为：1e-8）。 该算法会返回以下输出结果： policy：这是一个二维 numpy 数组，其中 policy.shape[0] 等于状态数量 (env.nS) ， policy.shape[1] 等于动作数量 (env.nA) 。policy[s][a] 返回智能体在状态 s 时根据该策略选择动作 a 的概率。 V：这是一个一维 numpy 数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 的估值。 def value_iteration(env, gamma=1, theta=1e-8): V = np.zeros(env.nS) ## TODO: complete the function while True: delta = 0 for s in range(env.nS): v = copy.copy(V[s]) V[s] = max(q_from_v(env, V, s, gamma)) delta = max(delta, abs(v-V[s])) if delta &lt; theta: break policy = policy_improvement(env, V, gamma) return policy, V 运行下个代码单元格以解决该 MDP 并可视化输出结果。状态值函数已调整形状，以匹配网格世界的形状。 policy_vi, V_vi = value_iteration(env)# print the optimal policyprint(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")print(policy_vi,\"\\n\")# plot the optimal state-value functionplot_values(V_vi) Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3): [[ 1. 0. 0. 0.] [ 0. 0. 0. 1.] [ 0. 0. 0. 1.] [ 0. 0. 0. 1.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 0. 0. 0. 1.] [ 0. 1. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 0. 0. 1. 0.] [ 0. 1. 0. 0.] [ 1. 0. 0. 0.]] 运行以下代码单元格以测试你的函数。如果代码单元格返回 PASSED，则表明你正确地实现了该函数！ 注意：为了确保结果准确，确保 truncated_policy_iteration 函数满足上文列出的要求（具有三个输入、两个输出，并且没有更改输入参数的默认值）。 check_test.run_check('value_iteration_check', value_iteration) PASSED","categories":[{"name":"强化学习","slug":"强化学习","permalink":"http://jinyaxuan.github.io/categories/强化学习/"}],"tags":[{"name":"Reinforcement learning","slug":"Reinforcement-learning","permalink":"http://jinyaxuan.github.io/tags/Reinforcement-learning/"}],"keywords":[{"name":"强化学习","slug":"强化学习","permalink":"http://jinyaxuan.github.io/categories/强化学习/"}]},{"title":"Customer Setments","slug":"Customer-Setments","date":"2019-02-16T11:16:05.000Z","updated":"2019-02-16T11:16:49.399Z","comments":false,"path":"2019/02/16/Customer-Setments/","link":"","permalink":"http://jinyaxuan.github.io/2019/02/16/Customer-Setments/","excerpt":"","text":"机器学习纳米学位非监督学习项目 3: 创建用户分类欢迎来到机器学习工程师纳米学位的第三个项目！在这个 notebook 文件中，有些模板代码已经提供给你，但你还需要实现更多的功能来完成这个项目。除非有明确要求，你无须修改任何已给出的代码。以‘练习’开始的标题表示接下来的代码部分中有你必须要实现的功能。每一部分都会有详细的指导，需要实现的部分也会在注释中以 ‘TODO’ 标出。请仔细阅读所有的提示！ 除了实现代码外，你还必须回答一些与项目和你的实现有关的问题。每一个需要你回答的问题都会以‘问题 X’为标题。请仔细阅读每个问题，并且在问题后的‘回答’文字框中写出完整的答案。我们将根据你对问题的回答和撰写代码所实现的功能来对你提交的项目进行评分。 提示：Code 和 Markdown 区域可通过 Shift + Enter 快捷键运行。此外，Markdown 可以通过双击进入编辑模式。 开始在这个项目中，你将分析一个数据集的内在结构，这个数据集包含很多客户真对不同类型产品的年度采购额（用金额表示）。这个项目的任务之一是如何最好地描述一个批发商不同种类顾客之间的差异。这样做将能够使得批发商能够更好的组织他们的物流服务以满足每个客户的需求。 这个项目的数据集能够在UCI机器学习信息库中找到.因为这个项目的目的，分析将不会包括 ‘Channel’ 和 ‘Region’ 这两个特征——重点集中在6个记录的客户购买的产品类别上。 运行下面的的代码单元以载入整个客户数据集和一些这个项目需要的 Python 库。如果你的数据集载入成功，你将看到后面输出数据集的大小。 # 检查你的Python版本from sys import version_infoif version_info.major != 3: raise Exception('请使用Python 3.x 来完成此项目') import seaborn as sns # 引入这个项目需要的库import numpy as npimport pandas as pdimport visuals as vsfrom IPython.display import display # 使得我们可以对DataFrame使用display()函数# 设置以内联的形式显示matplotlib绘制的图片（在notebook中显示更美观）%matplotlib inline# 高分辨率显示%config InlineBackend.figure_format='retina'# 载入整个客户数据集try: data = pd.read_csv(\"customers.csv\") data.drop(['Region', 'Channel'], axis = 1, inplace = True) print(\"Wholesale customers dataset has &#123;&#125; samples with &#123;&#125; features each.\".format(*data.shape))except: print(\"Dataset could not be loaded. Is the dataset missing?\") Wholesale customers dataset has 440 samples with 6 features each. 分析数据在这部分，你将开始分析数据，通过可视化和代码来理解每一个特征和其他特征的联系。你会看到关于数据集的统计描述，考虑每一个属性的相关性，然后从数据集中选择若干个样本数据点，你将在整个项目中一直跟踪研究这几个数据点。 运行下面的代码单元给出数据集的一个统计描述。注意这个数据集包含了6个重要的产品类型：‘Fresh’, ‘Milk’, ‘Grocery’, ‘Frozen’, ‘Detergents_Paper’和 ‘Delicatessen’。想一下这里每一个类型代表你会购买什么样的产品。 # 显示数据集的一个描述display(data.describe()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen count 440.000000 440.000000 440.000000 440.000000 440.000000 440.000000 mean 12000.297727 5796.265909 7951.277273 3071.931818 2881.493182 1524.870455 std 12647.328865 7380.377175 9503.162829 4854.673333 4767.854448 2820.105937 min 3.000000 55.000000 3.000000 25.000000 3.000000 3.000000 25% 3127.750000 1533.000000 2153.000000 742.250000 256.750000 408.250000 50% 8504.000000 3627.000000 4755.500000 1526.000000 816.500000 965.500000 75% 16933.750000 7190.250000 10655.750000 3554.250000 3922.000000 1820.250000 max 112151.000000 73498.000000 92780.000000 60869.000000 40827.000000 47943.000000 练习: 选择样本为了对客户有一个更好的了解，并且了解代表他们的数据将会在这个分析过程中如何变换。最好是选择几个样本数据点，并且更为详细地分析它们。在下面的代码单元中，选择三个索引加入到索引列表indices中，这三个索引代表你要追踪的客户。我们建议你不断尝试，直到找到三个明显不同的客户。 # TODO：从数据集中选择三个你希望抽样的数据点的索引indices = [1, 14, 168]# 为选择的样本建立一个DataFramesamples = pd.DataFrame(data.loc[indices], columns = data.keys()).reset_index(drop = True)print(\"Chosen samples of wholesale customers dataset:\")display(samples) Chosen samples of wholesale customers dataset: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 0 7057 9810 9568 1762 3293 1776 1 24653 9465 12091 294 5058 2168 2 5809 735 803 1393 79 429 问题 1在你看来你选择的这三个样本点分别代表什么类型的企业（客户）？对每一个你选择的样本客户，通过它在每一种产品类型上的花费与数据集的统计描述进行比较，给出你做上述判断的理由。 提示： 企业的类型包括超市、咖啡馆、零售商以及其他。注意不要使用具体企业的名字，比如说在描述一个餐饮业客户时，你不能使用麦当劳。 回答:第一个可能是咖啡厅 Milk 和 Grocery的购买需求高于平均值 第二个可能是餐厅 Fresh Milk Grocery Detergents_Paper Delicatessen都高于平均值 第三个可能是生鲜超市 Fresh 和 Frozen的需求要大些 练习: 特征相关性一个有趣的想法是，考虑这六个类别中的一个（或者多个）产品类别，是否对于理解客户的购买行为具有实际的相关性。也就是说，当用户购买了一定数量的某一类产品，我们是否能够确定他们必然会成比例地购买另一种类的产品。有一个简单的方法可以检测相关性：我们用移除了某一个特征之后的数据集来构建一个监督学习（回归）模型，然后用这个模型去预测那个被移除的特征，再对这个预测结果进行评分，看看预测结果如何。 在下面的代码单元中，你需要实现以下的功能： 使用 DataFrame.drop 函数移除数据集中你选择的不需要的特征，并将移除后的结果赋值给 new_data 。 使用 sklearn.model_selection.train_test_split 将数据集分割成训练集和测试集。 使用移除的特征作为你的目标标签。设置 test_size 为 0.25 并设置一个 random_state 。 导入一个 DecisionTreeRegressor （决策树回归器），设置一个 random_state，然后用训练集训练它。 使用回归器的 score 函数输出模型在测试集上的预测得分。 # TODO：为DataFrame创建一个副本，用'drop'函数丢弃一个特征# TODO： new_data = data.drop('Detergents_Paper', axis=1)# TODO：使用给定的特征作为目标，将数据分割成训练集和测试集from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(new_data, data['Detergents_Paper'], test_size=0.25, random_state=40)# TODO：创建一个DecisionTreeRegressor（决策树回归器）并在训练集上训练它from sklearn.tree import DecisionTreeRegressorregressor = DecisionTreeRegressor(random_state=0)regressor.fit(X_train, y_train)# TODO：输出在测试集上的预测得分score = regressor.score(X_test, y_test)print(score) 0.7879942418323117 问题 2你尝试预测哪一个特征？预测的得分是多少？这个特征对于区分用户的消费习惯来说必要吗？为什么？提示： 决定系数（coefficient of determination），$R^2$ 结果在0到1之间，1表示完美拟合，一个负的 $R^2$ 表示模型不能够拟合数据。 回答:尝试预测Detergents_Paper 得分为0.787 不必要 该特征与其他特征有一定相关性 决定系数较高 可通过其他特征预测 可视化特征分布为了能够对这个数据集有一个更好的理解，我们可以对数据集中的每一个产品特征构建一个散布矩阵（scatter matrix）。如果你发现你在上面尝试预测的特征对于区分一个特定的用户来说是必须的，那么这个特征和其它的特征可能不会在下面的散射矩阵中显示任何关系。相反的，如果你认为这个特征对于识别一个特定的客户是没有作用的，那么通过散布矩阵可以看出在这个数据特征和其它特征中有关联性。运行下面的代码以创建一个散布矩阵。 # 对于数据中的每一对特征构造一个散布矩阵pd.scatter_matrix(data, alpha = 0.3, figsize = (14,8), diagonal = 'kde'); C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: pandas.scatter_matrix is deprecated, use pandas.plotting.scatter_matrix instead 问题 3这里是否存在一些特征他们彼此之间存在一定程度相关性？如果有请列出。这个结果是验证了还是否认了你尝试预测的那个特征的相关性？这些特征的数据是怎么分布的？ 提示： 这些数据是正态分布（normally distributed）的吗？大多数的数据点分布在哪？ 回答:特征之间存在一定相关性 在矩阵对角线出现正偏态分布 数据预处理在这个部分，你将通过在数据上做一个合适的缩放，并检测异常点（你可以选择性移除）将数据预处理成一个更好的代表客户的形式。预处理数据是保证你在分析中能够得到显著且有意义的结果的重要环节。 练习: 特征缩放如果数据不是正态分布的，尤其是数据的平均数和中位数相差很大的时候（表示数据非常歪斜）。这时候通常用一个非线性的缩放是很合适的，（英文原文） — 尤其是对于金融数据。一种实现这个缩放的方法是使用 Box-Cox 变换，这个方法能够计算出能够最佳减小数据倾斜的指数变换方法。一个比较简单的并且在大多数情况下都适用的方法是使用自然对数。 在下面的代码单元中，你将需要实现以下功能： 使用 np.log 函数在数据 data 上做一个对数缩放，然后将它的副本（不改变原始data的值）赋值给 log_data。 使用 np.log 函数在样本数据 samples 上做一个对数缩放，然后将它的副本赋值给 log_samples。 # TODO：使用自然对数缩放数据log_data = np.log(data)# TODO：使用自然对数缩放样本数据log_samples = np.log(samples)# 为每一对新产生的特征制作一个散射矩阵pd.scatter_matrix(log_data, alpha = 0.3, figsize = (14,8), diagonal = 'kde'); C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: pandas.scatter_matrix is deprecated, use pandas.plotting.scatter_matrix instead 观察在使用了一个自然对数的缩放之后，数据的各个特征会显得更加的正态分布。对于任意的你以前发现有相关关系的特征对，观察他们的相关关系是否还是存在的（并且尝试观察，他们的相关关系相比原来是变强了还是变弱了）。 运行下面的代码以观察样本数据在进行了自然对数转换之后如何改变了。 # 展示经过对数变换后的样本数据display(log_samples) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 0 8.861775 9.191158 9.166179 7.474205 8.099554 7.482119 1 10.112654 9.155356 9.400217 5.683580 8.528726 7.681560 2 8.667164 6.599870 6.688355 7.239215 4.369448 6.061457 练习: 异常值检测对于任何的分析，在数据预处理的过程中检测数据中的异常值都是非常重要的一步。异常值的出现会使得把这些值考虑进去后结果出现倾斜。这里有很多关于怎样定义什么是数据集中的异常值的经验法则。这里我们将使用 Tukey 的定义异常值的方法：一个异常阶（outlier step）被定义成1.5倍的四分位距（interquartile range，IQR）。一个数据点如果某个特征包含在该特征的 IQR 之外的特征，那么该数据点被认定为异常点。 在下面的代码单元中，你需要完成下面的功能： 将指定特征的 25th 分位点的值分配给 Q1 。使用 np.percentile 来完成这个功能。 将指定特征的 75th 分位点的值分配给 Q3 。同样的，使用 np.percentile 来完成这个功能。 将指定特征的异常阶的计算结果赋值给 step。 选择性地通过将索引添加到 outliers 列表中，以移除异常值。 注意： 如果你选择移除异常值，请保证你选择的样本点不在这些移除的点当中！一旦你完成了这些功能，数据集将存储在 good_data 中。 # 对于每一个特征，找到值异常高或者是异常低的数据点for feature in log_data.keys(): # TODO: 计算给定特征的Q1（数据的25th分位点） Q1 = np.percentile(log_data[feature], 25) # TODO: 计算给定特征的Q3（数据的75th分位点） Q3 = np.percentile(log_data[feature], 75) # TODO: 使用四分位范围计算异常阶（1.5倍的四分位距） step = (Q3 - Q1) * 1.5 # 显示异常点 print(\"Data points considered outliers for the feature '&#123;&#125;':\".format(feature)) display(log_data[~((log_data[feature] &gt;= Q1 - step) &amp; (log_data[feature] &lt;= Q3 + step))]) # TODO(可选): 选择你希望移除的数据点的索引outliers = [65,66,75,128,154]# 以下代码会移除outliers中索引的数据点, 并储存在good_data中good_data = log_data.drop(log_data.index[outliers]).reset_index(drop = True) Data points considered outliers for the feature &#39;Fresh&#39;: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 65 4.442651 9.950323 10.732651 3.583519 10.095388 7.260523 66 2.197225 7.335634 8.911530 5.164786 8.151333 3.295837 81 5.389072 9.163249 9.575192 5.645447 8.964184 5.049856 95 1.098612 7.979339 8.740657 6.086775 5.407172 6.563856 96 3.135494 7.869402 9.001839 4.976734 8.262043 5.379897 128 4.941642 9.087834 8.248791 4.955827 6.967909 1.098612 171 5.298317 10.160530 9.894245 6.478510 9.079434 8.740337 193 5.192957 8.156223 9.917982 6.865891 8.633731 6.501290 218 2.890372 8.923191 9.629380 7.158514 8.475746 8.759669 304 5.081404 8.917311 10.117510 6.424869 9.374413 7.787382 305 5.493061 9.468001 9.088399 6.683361 8.271037 5.351858 338 1.098612 5.808142 8.856661 9.655090 2.708050 6.309918 353 4.762174 8.742574 9.961898 5.429346 9.069007 7.013016 355 5.247024 6.588926 7.606885 5.501258 5.214936 4.844187 357 3.610918 7.150701 10.011086 4.919981 8.816853 4.700480 412 4.574711 8.190077 9.425452 4.584967 7.996317 4.127134 Data points considered outliers for the feature &#39;Milk&#39;: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 86 10.039983 11.205013 10.377047 6.894670 9.906981 6.805723 98 6.220590 4.718499 6.656727 6.796824 4.025352 4.882802 154 6.432940 4.007333 4.919981 4.317488 1.945910 2.079442 356 10.029503 4.897840 5.384495 8.057377 2.197225 6.306275 Data points considered outliers for the feature &#39;Grocery&#39;: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 75 9.923192 7.036148 1.098612 8.390949 1.098612 6.882437 154 6.432940 4.007333 4.919981 4.317488 1.945910 2.079442 Data points considered outliers for the feature &#39;Frozen&#39;: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 38 8.431853 9.663261 9.723703 3.496508 8.847360 6.070738 57 8.597297 9.203618 9.257892 3.637586 8.932213 7.156177 65 4.442651 9.950323 10.732651 3.583519 10.095388 7.260523 145 10.000569 9.034080 10.457143 3.737670 9.440738 8.396155 175 7.759187 8.967632 9.382106 3.951244 8.341887 7.436617 264 6.978214 9.177714 9.645041 4.110874 8.696176 7.142827 325 10.395650 9.728181 9.519735 11.016479 7.148346 8.632128 420 8.402007 8.569026 9.490015 3.218876 8.827321 7.239215 429 9.060331 7.467371 8.183118 3.850148 4.430817 7.824446 439 7.932721 7.437206 7.828038 4.174387 6.167516 3.951244 Data points considered outliers for the feature &#39;Detergents_Paper&#39;: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 75 9.923192 7.036148 1.098612 8.390949 1.098612 6.882437 161 9.428190 6.291569 5.645447 6.995766 1.098612 7.711101 Data points considered outliers for the feature &#39;Delicatessen&#39;: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 66 2.197225 7.335634 8.911530 5.164786 8.151333 3.295837 109 7.248504 9.724899 10.274568 6.511745 6.728629 1.098612 128 4.941642 9.087834 8.248791 4.955827 6.967909 1.098612 137 8.034955 8.997147 9.021840 6.493754 6.580639 3.583519 142 10.519646 8.875147 9.018332 8.004700 2.995732 1.098612 154 6.432940 4.007333 4.919981 4.317488 1.945910 2.079442 183 10.514529 10.690808 9.911952 10.505999 5.476464 10.777768 184 5.789960 6.822197 8.457443 4.304065 5.811141 2.397895 187 7.798933 8.987447 9.192075 8.743372 8.148735 1.098612 203 6.368187 6.529419 7.703459 6.150603 6.860664 2.890372 233 6.871091 8.513988 8.106515 6.842683 6.013715 1.945910 285 10.602965 6.461468 8.188689 6.948897 6.077642 2.890372 289 10.663966 5.655992 6.154858 7.235619 3.465736 3.091042 343 7.431892 8.848509 10.177932 7.283448 9.646593 3.610918 sns.boxplot(data=data, x='Fresh', orient='v') &lt;matplotlib.axes._subplots.AxesSubplot at 0x256682c49b0&gt; 问题 4请列出所有在多于一个特征下被看作是异常的数据点。这些点应该被从数据集中移除吗？为什么？把你认为需要移除的数据点全部加入到到 outliers 变量中。 回答:不应该 共有48个异常值 对于本样本数据一共有400左右样本 占比过大 如果全部移除会对结果有较大影响 会造成数据损失 因此 判断有两个以上异常值的数据再进行移除 所以移除65,66,75,128,154 这5个样本 特征转换在这个部分中你将使用主成分分析（PCA）来分析批发商客户数据的内在结构。由于使用PCA在一个数据集上会计算出最大化方差的维度，我们将找出哪一个特征组合能够最好的描绘客户。 练习: 主成分分析（PCA）既然数据被缩放到一个更加正态分布的范围中并且我们也移除了需要移除的异常点，我们现在就能够在 good_data 上使用PCA算法以发现数据的哪一个维度能够最大化特征的方差。除了找到这些维度，PCA 也将报告每一个维度的解释方差比（explained variance ratio）—这个数据有多少方差能够用这个单独的维度来解释。注意 PCA 的一个组成部分（维度）能够被看做这个空间中的一个新的“特征”，但是它是原来数据中的特征构成的。 在下面的代码单元中，你将要实现下面的功能： 导入 sklearn.decomposition.PCA 并且将 good_data 用 PCA 并且使用6个维度进行拟合后的结果保存到 pca 中。 使用 pca.transform 将 log_samples 进行转换，并将结果存储到 pca_samples 中。 # TODO：通过在good data上进行PCA，将其转换成6个维度from sklearn.decomposition import PCApca = PCA(n_components=6)pca.fit(good_data)# TODO：使用上面的PCA拟合将变换施加在log_samples上pca_samples = pca.transform(good_data)# 生成PCA的结果图pca_results = vs.pca_results(good_data, pca) 问题 5数据的第一个和第二个主成分总共表示了多少的方差？ 前四个主成分呢？使用上面提供的可视化图像，从用户花费的角度来讨论前四个主要成分中每个主成分代表的消费行为并给出你做出判断的理由。 提示： 对每个主成分中的特征分析权重的正负和大小。 结合每个主成分权重的正负讨论消费行为。 某一特定维度上的正向增长对应正权特征的增长和负权特征的减少。增长和减少的速率和每个特征的权重相关。参考资料：Interpretation of the Principal Components 回答:0.72 0.93 第一个 咖啡厅 最大的负权特征是洗涤类商品 第二个 超市 生鲜冻品和鱼的权值最大 第三个 鱼类零售店 鱼的权值最大 第四个 熟食店 熟食的权值占比最大 观察运行下面的代码，查看经过对数转换的样本数据在进行一个6个维度的主成分分析（PCA）之后会如何改变。观察样本数据的前四个维度的数值。考虑这和你初始对样本点的解释是否一致。 # 展示经过PCA转换的sample log-datadisplay(pd.DataFrame(np.round(pca_samples, 4), columns = pca_results.index.values)) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Dimension 1 Dimension 2 Dimension 3 Dimension 4 Dimension 5 Dimension 6 0 -1.7580 0.0097 -0.9590 -1.6824 0.2680 -0.3891 1 -1.7887 -0.8123 0.2315 -0.0036 0.1194 -0.2106 2 -1.8834 -1.5991 1.3204 -0.5432 -0.3934 -0.3117 3 1.1553 -1.4052 0.5422 0.4127 -0.6865 0.6409 4 -0.7848 -2.3943 0.4798 -0.3483 -0.3191 0.0613 5 -1.0850 -0.3243 -0.2635 -0.8812 0.1862 -0.5347 6 -1.1286 0.2629 -1.3162 -0.5369 -0.4836 0.1097 7 -1.5672 -0.9010 0.3684 -0.2682 -0.4571 0.1526 8 -0.8636 0.6650 -0.5376 -0.7922 -0.1551 0.0344 9 -2.8734 -0.6774 0.1330 -0.1802 -0.0250 0.1224 10 -2.0887 -0.7006 0.8537 1.0105 -0.5587 0.2495 11 1.0120 -0.0103 -0.7516 -0.0545 -0.4333 0.6602 12 -2.2406 -1.2419 -1.0729 -1.9589 0.2160 -0.1782 13 -1.8891 -1.3001 -1.1945 0.9689 -0.2426 0.2970 14 -2.3388 -0.9013 -1.1515 -1.6713 -0.0485 -0.0739 15 0.4258 0.8803 -1.2189 -0.7945 -0.7319 0.3868 16 -2.7939 2.0377 0.3420 -1.2847 0.1457 -0.1353 17 0.2575 -0.5179 1.1702 -1.5806 0.4159 -0.5328 18 -1.3906 -1.8004 0.0301 -0.3807 -0.2116 0.1467 19 -0.9992 0.4720 -0.9332 -0.1723 -0.4229 0.5269 20 -0.8375 -1.0765 -0.3684 -0.8111 -0.5111 -0.3040 21 1.7467 0.1939 0.2753 0.6012 -0.7470 0.1974 22 -0.1419 -2.7722 0.3293 0.3928 -1.3904 0.2012 23 -2.8096 -3.6459 1.0567 -0.5186 0.6999 -0.1811 24 -2.0709 -2.4853 0.2692 -0.4013 -0.1917 0.1027 25 -1.2292 1.5540 -3.2462 0.0043 0.1124 -0.0697 26 1.9083 -0.3765 0.1924 0.1502 -0.3852 0.5367 27 2.4162 0.6069 -0.7652 -1.3209 0.1614 0.8089 28 -3.5695 -0.9977 0.9477 -0.5400 0.2579 0.0323 29 0.5684 -1.0850 -1.4044 -0.5784 -0.6738 -0.2157 ... ... ... ... ... ... ... 405 -0.4777 -0.3472 0.3116 -0.3929 -0.9402 0.1142 406 0.7424 0.0059 2.1018 -0.9815 0.2461 -0.0371 407 -2.1528 5.3859 0.0930 0.4023 0.3577 0.3111 408 -0.0741 -1.6911 1.6461 1.4172 0.0587 0.1461 409 0.5554 -0.0029 -0.2440 1.6316 -0.4586 -0.0161 410 -1.5972 -0.8047 0.1483 -0.0839 -0.3191 -0.0512 411 -2.5495 0.1090 -0.1921 -0.0073 -0.0074 -0.3327 412 -1.9218 0.5424 -0.4014 -0.8837 -0.1168 0.1580 413 -3.2940 2.4621 0.3317 -0.9146 0.1175 0.1444 414 -0.3359 -0.0856 -0.1970 -1.0176 -0.6091 -0.7771 415 -3.0266 1.8034 -1.1358 -2.9577 -0.4264 0.1528 416 -1.4571 -1.0316 -0.5676 -0.6119 -0.4132 0.1330 417 0.4422 -0.7142 -0.9356 -0.9923 -0.7925 0.4116 418 -0.4191 -0.4595 -1.0590 -0.2384 -0.2853 -0.1660 419 -1.0698 0.0957 -1.8679 0.3247 -0.2282 0.6291 420 2.3699 -1.7726 1.3282 0.7617 0.4672 0.1593 421 -2.0740 -1.5983 -0.0683 0.4013 -0.0483 0.0983 422 0.4545 -2.6564 0.0980 1.1628 1.4384 -0.5162 423 -0.1223 0.6924 0.0667 0.9488 0.6363 -0.2967 424 1.4269 1.2099 -0.1030 -3.9222 0.6257 0.5211 425 -0.0856 0.4483 1.0450 -1.3292 1.1732 1.1231 426 -0.2111 -1.6998 0.8104 1.4239 -0.0610 -0.2023 427 0.1304 0.5643 -1.9278 -1.1452 -0.7829 0.4971 428 0.9382 0.6387 1.3840 -0.3231 -0.0505 -0.7708 429 -1.0055 -0.3825 -1.0855 -0.6019 -0.2346 0.1880 430 0.6448 -2.8583 0.6377 0.5879 1.9515 0.7170 431 3.1848 -1.9448 0.2677 -0.6799 -0.2663 -0.5194 432 -3.7425 -0.8561 -0.9885 -0.8879 0.0503 0.2058 433 1.6691 -0.3980 0.5161 -1.3189 0.0913 0.0056 434 0.7390 3.6914 -2.0335 -0.9927 0.3109 -0.1734 435 rows × 6 columns 练习：降维当使用主成分分析的时候，一个主要的目的是减少数据的维度，这实际上降低了问题的复杂度。当然降维也是需要一定代价的：更少的维度能够表示的数据中的总方差更少。因为这个，累计解释方差比（cumulative explained variance ratio）对于我们确定这个问题需要多少维度非常重要。另外，如果大部分的方差都能够通过两个或者是三个维度进行表示的话，降维之后的数据能够被可视化。 在下面的代码单元中，你将实现下面的功能： 将 good_data 用两个维度的PCA进行拟合，并将结果存储到 pca 中去。 使用 pca.transform 将 good_data 进行转换，并将结果存储在 reduced_data 中。 使用 pca.transform 将 log_samples 进行转换，并将结果存储在 pca_samples 中。 # TODO：通过在good data上进行PCA，将其转换成两个维度pca = PCA(n_components=2)pca.fit(good_data)# TODO：使用上面训练的PCA将good data进行转换reduced_data = pca.transform(good_data)# TODO：使用上面训练的PCA将log_samples进行转换pca_samples = pca.transform(log_samples)# 为降维后的数据创建一个DataFramereduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2']) 观察运行以下代码观察当仅仅使用两个维度进行 PCA 转换后，这个对数样本数据将怎样变化。观察这里的结果与一个使用六个维度的 PCA 转换相比较时，前两维的数值是保持不变的。 # 展示经过两个维度的PCA转换之后的样本log-datadisplay(pd.DataFrame(np.round(pca_samples, 4), columns = ['Dimension 1', 'Dimension 2'])) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Dimension 1 Dimension 2 0 -1.7887 -0.8123 1 -2.3388 -0.9013 2 3.2785 0.9078 可视化一个双标图（Biplot）双标图是一个散点图，每个数据点的位置由它所在主成分的分数确定。坐标系是主成分（这里是 Dimension 1 和 Dimension 2）。此外，双标图还展示出初始特征在主成分上的投影。一个双标图可以帮助我们理解降维后的数据，发现主成分和初始特征之间的关系。 运行下面的代码来创建一个降维后数据的双标图。 # 可视化双标图vs.biplot(good_data, reduced_data, pca) &lt;matplotlib.axes._subplots.AxesSubplot at 0x256683adc88&gt; 观察一旦我们有了原始特征的投影（红色箭头），就能更加容易的理解散点图每个数据点的相对位置。 在这个双标图中，哪些初始特征与第一个主成分有强关联？哪些初始特征与第二个主成分相关联？你观察到的是否与之前得到的 pca_results 图相符？ 聚类在这个部分，你讲选择使用 K-Means 聚类算法或者是高斯混合模型聚类算法以发现数据中隐藏的客户分类。然后，你将从簇中恢复一些特定的关键数据点，通过将它们转换回原始的维度和规模，从而理解他们的含义。 问题 6使用 K-Means 聚类算法的优点是什么？使用高斯混合模型聚类算法的优点是什么？基于你现在对客户数据的观察结果，你选用了这两个算法中的哪一个，为什么？ 回答:K-Means优点是：计算速度快、时间短，易解释，聚类效果还不错；但缺点主要是需要提前确定K值，对异常值极度敏感。 高斯混合模型聚类算法的优点是聚类输出的信息量更大，理论上可以拟合任何连续的概率密度函数。 我会选高斯混合模型，因为两种算法聚类得分差异很小，且GMM能输出数据点属于某一类别的概率，因此输出的信息丰富程度大大高于K-means算法 练习: 创建聚类针对不同情况，有些问题你需要的聚类数目可能是已知的。但是在聚类数目不作为一个先验知道的情况下，我们并不能够保证某个聚类的数目对这个数据是最优的，因为我们对于数据的结构（如果存在的话）是不清楚的。但是，我们可以通过计算每一个簇中点的轮廓系数来衡量聚类的质量。数据点的轮廓系数衡量了它与分配给他的簇的相似度，这个值范围在-1（不相似）到1（相似）。平均轮廓系数为我们提供了一种简单地度量聚类质量的方法。 在接下来的代码单元中，你将实现下列功能： 在 reduced_data 上使用一个聚类算法，并将结果赋值到 clusterer，需要设置 random_state 使得结果可以复现。 使用 clusterer.predict 预测 reduced_data 中的每一个点的簇，并将结果赋值到 preds。 使用算法的某个属性值找到聚类中心，并将它们赋值到 centers。 预测 pca_samples 中的每一个样本点的类别并将结果赋值到 sample_preds。 导入 sklearn.metrics.silhouette_score 包并计算 reduced_data 相对于 preds 的轮廓系数。 将轮廓系数赋值给 score 并输出结果。 # TODO：在降维后的数据上使用你选择的聚类算法from sklearn.mixture import GaussianMixtureclusterer = GaussianMixture(n_components=2, random_state=40)clusterer.fit(reduced_data)# TODO：预测每一个点的簇preds = clusterer.predict(reduced_data)# TODO：找到聚类中心centers = clusterer.means_# TODO：预测在每一个转换后的样本点的类sample_preds = clusterer.predict(pca_samples)from sklearn.metrics import silhouette_score# TODO：计算选择的类别的平均轮廓系数（mean silhouette coefficient）score = silhouette_score(reduced_data, preds)print(score) 0.4219168464626149 问题 7汇报你尝试的不同的聚类数对应的轮廓系数。在这些当中哪一个聚类的数目能够得到最佳的轮廓系数？ 回答:聚类为2时 0.421 聚类为3时 0.375 聚类为4时0.248 所以聚类为2时效果最佳 聚类可视化一旦你选好了通过上面的评价函数得到的算法的最佳聚类数目，你就能够通过使用下面的代码块可视化来得到的结果。作为实验，你可以试着调整你的聚类算法的聚类的数量来看一下不同的可视化结果。但是你提供的最终的可视化图像必须和你选择的最优聚类数目一致。 # 从已有的实现中展示聚类的结果vs.cluster_results(reduced_data, preds, centers, pca_samples) 练习: 数据恢复上面的可视化图像中提供的每一个聚类都有一个中心点。这些中心（或者叫平均点）并不是数据中真实存在的点，但是是所有预测在这个簇中的数据点的平均。对于创建客户分类的问题，一个簇的中心对应于那个分类的平均用户。因为这个数据现在进行了降维并缩放到一定的范围，我们可以通过施加一个反向的转换恢复这个点所代表的用户的花费。 在下面的代码单元中，你将实现下列的功能： 使用 pca.inverse_transform 将 centers 反向转换，并将结果存储在 log_centers 中。 使用 np.log 的反函数 np.exp 反向转换 log_centers 并将结果存储到 true_centers 中。 # TODO：反向转换中心点log_centers = pca.inverse_transform(centers)# TODO：对中心点做指数转换true_centers = np.exp(log_centers)# 显示真实的中心点segments = ['Segment &#123;&#125;'.format(i) for i in range(0,len(centers))]true_centers = pd.DataFrame(np.round(true_centers), columns = data.keys())true_centers.index = segmentsdisplay(true_centers) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen Segment 0 3552.0 7837.0 12219.0 870.0 4696.0 962.0 Segment 1 8953.0 2114.0 2765.0 2075.0 353.0 732.0 问题 8考虑上面的代表性数据点在每一个产品类型的花费总数，你认为这些客户分类代表了哪类客户？为什么？需要参考在项目最开始得到的统计值来给出理由。 提示： 一个被分到&#39;Cluster X&#39;的客户最好被用 &#39;Segment X&#39;中的特征集来标识的企业类型表示。 回答:Segment 0 代表餐厅 食品类出售比重较大 Segment 1 代表超市 基本符合超市出售商品特征 问题 9对于每一个样本点问题 8 中的哪一个分类能够最好的表示它？你之前对样本的预测和现在的结果相符吗？ 运行下面的代码单元以找到每一个样本点被预测到哪一个簇中去。 # 显示预测结果for i, pred in enumerate(sample_preds): print(\"Sample point\", i, \"predicted to be in Cluster\", pred) Sample point 0 predicted to be in Cluster 0 Sample point 1 predicted to be in Cluster 0 Sample point 2 predicted to be in Cluster 1 回答:cluster0 结果不太相符 原数据分类比较细致 这里我们只是使用了2个簇 结论在最后一部分中，你要学习如何使用已经被分类的数据。首先，你要考虑不同组的客户客户分类，针对不同的派送策略受到的影响会有什么不同。其次，你要考虑到，每一个客户都被打上了标签（客户属于哪一个分类）可以给客户数据提供一个多一个特征。最后，你会把客户分类与一个数据中的隐藏变量做比较，看一下这个分类是否辨识了特定的关系。 问题 10在对他们的服务或者是产品做细微的改变的时候，公司经常会使用 A/B tests 以确定这些改变会对客户产生积极作用还是消极作用。这个批发商希望考虑将他的派送服务从每周5天变为每周3天，但是他只会对他客户当中对此有积极反馈的客户采用。这个批发商应该如何利用客户分类来知道哪些客户对它的这个派送策略的改变有积极的反馈，如果有的话？你需要给出在这个情形下A/B 测试具体的实现方法，以及最终得出结论的依据是什么？ 提示： 我们能假设这个改变对所有的客户影响都一致吗？我们怎样才能够确定它对于哪个类型的客户影响最大？ 回答：对两组客户分别为A1;A2,B1;B2 A1参照组 A2实验组 分别应用每周5天和每周3天的服务 分别记录每组的周均销售额 如果A2组的销售额较高 则说明该组对该类客户组有积极作用 然后对A1 A2组 使用新的策略否则保持不变 同理B1,B2采取同样的方式 问题 11通过聚类技术，我们能够将原有的没有标记的数据集中的附加结构分析出来。因为每一个客户都有一个最佳的划分（取决于你选择使用的聚类算法），我们可以把用户分类作为数据的一个工程特征。假设批发商最近迎来十位新顾客，并且他已经为每位顾客每个产品类别年度采购额进行了预估。进行了这些估算之后，批发商该如何运用它的预估和非监督学习的结果来对这十个新的客户进行更好的预测？ 提示：在下面的代码单元中，我们提供了一个已经做好聚类的数据（聚类结果为数据中的cluster属性），我们将在这个数据集上做一个小实验。尝试运行下面的代码看看我们尝试预测‘Region’的时候，如果存在聚类特征’cluster’与不存在相比对最终的得分会有什么影响？这对你有什么启发？ from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import train_test_split# 读取包含聚类结果的数据cluster_data = pd.read_csv(\"cluster.csv\")y = cluster_data['Region']X = cluster_data.drop(['Region'], axis = 1)# 划分训练集测试集X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24)clf = RandomForestClassifier(random_state=24)clf.fit(X_train, y_train)score_with_cluster = clf.score(X_test, y_test)# 移除cluster特征X_train = X_train.copy()X_train.drop(['cluster'], axis=1, inplace=True)X_test = X_test.copy()X_test.drop(['cluster'], axis=1, inplace=True)clf.fit(X_train, y_train)score_no_cluster = clf.score(X_test, y_test)print(\"不使用cluster特征的得分: %.4f\"%score_no_cluster)print(\"使用cluster特征的得分: %.4f\"%score_with_cluster) 不使用cluster特征的得分: 0.6437 使用cluster特征的得分: 0.6667 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release. from numpy.core.umath_tests import inner1d 回答：cluster特征对用户的影响较小 但使用caluster有益于精度提高 可视化内在的分布在这个项目的开始，我们讨论了从数据集中移除 &#39;Channel&#39; 和 &#39;Region&#39; 特征，这样在分析过程中我们就会着重分析用户产品类别。通过重新引入 Channel 这个特征到数据集中，并施加和原来数据集同样的 PCA 变换的时候我们将能够发现数据集产生一个有趣的结构。 运行下面的代码单元以查看哪一个数据点在降维的空间中被标记为 &#39;HoReCa&#39; (旅馆/餐馆/咖啡厅)或者 &#39;Retail&#39;。另外，你将发现样本点在图中被圈了出来，用以显示他们的标签。 # 根据‘Channel‘数据显示聚类的结果vs.channel_results(reduced_data, outliers, pca_samples) 问题 12你选择的聚类算法和聚类点的数目，与内在的旅馆/餐馆/咖啡店和零售商的分布相比，有足够好吗？根据这个分布有没有哪个簇能够刚好划分成’零售商’或者是’旅馆/饭店/咖啡馆’？你觉得这个分类和前面你对于用户分类的定义是一致的吗？ 回答: 基本一致 零售类也属于超市的范围 所有都一样因为特征不多 很多分类比较模糊 注意: 当你写完了所有的代码，并且回答了所有的问题。你就可以把你的 iPython Notebook 导出成 HTML 文件。你可以在菜单栏，这样导出File -&gt; Download as -&gt; HTML (.html)把这个 HTML 和这个 iPython notebook 一起做为你的作业提交。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://jinyaxuan.github.io/categories/机器学习/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://jinyaxuan.github.io/tags/Machine-Learning/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://jinyaxuan.github.io/categories/机器学习/"}]},{"title":"finding donors","slug":"finding-donors","date":"2019-02-16T11:13:44.000Z","updated":"2019-02-16T11:14:39.171Z","comments":false,"path":"2019/02/16/finding-donors/","link":"","permalink":"http://jinyaxuan.github.io/2019/02/16/finding-donors/","excerpt":"","text":"机器学习纳米学位监督学习项目2: 为CharityML寻找捐献者欢迎来到机器学习工程师纳米学位的第二个项目！在此文件中，有些示例代码已经提供给你，但你还需要实现更多的功能让项目成功运行。除非有明确要求，你无须修改任何已给出的代码。以‘练习’开始的标题表示接下来的代码部分中有你必须要实现的功能。每一部分都会有详细的指导，需要实现的部分也会在注释中以’TODO’标出。请仔细阅读所有的提示！ 除了实现代码外，你还必须回答一些与项目和你的实现有关的问题。每一个需要你回答的问题都会以‘问题 X’为标题。请仔细阅读每个问题，并且在问题后的‘回答’文字框中写出完整的答案。我们将根据你对问题的回答和撰写代码所实现的功能来对你提交的项目进行评分。 提示：Code 和 Markdown 区域可通过Shift + Enter快捷键运行。此外，Markdown可以通过双击进入编辑模式。 开始在这个项目中，你将使用1994年美国人口普查收集的数据，选用几个监督学习算法以准确地建模被调查者的收入。然后，你将根据初步结果从中选择出最佳的候选算法，并进一步优化该算法以最好地建模这些数据。你的目标是建立一个能够准确地预测被调查者年收入是否超过50000美元的模型。这种类型的任务会出现在那些依赖于捐款而存在的非营利性组织。了解人群的收入情况可以帮助一个非营利性的机构更好地了解他们要多大的捐赠，或是否他们应该接触这些人。虽然我们很难直接从公开的资源中推断出一个人的一般收入阶层，但是我们可以（也正是我们将要做的）从其他的一些公开的可获得的资源中获得一些特征从而推断出该值。 这个项目的数据集来自UCI机器学习知识库。这个数据集是由Ron Kohavi和Barry Becker在发表文章“Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid”之后捐赠的，你可以在Ron Kohavi提供的在线版本中找到这个文章。我们在这里探索的数据集相比于原有的数据集有一些小小的改变，比如说移除了特征&#39;fnlwgt&#39; 以及一些遗失的或者是格式不正确的记录。 探索数据运行下面的代码单元以载入需要的Python库并导入人口普查数据。注意数据集的最后一列&#39;income&#39;将是我们需要预测的列（表示被调查者的年收入会大于或者是最多50,000美元），人口普查数据中的每一列都将是关于被调查者的特征。 # 为这个项目导入需要的库import numpy as npimport pandas as pdfrom time import timefrom IPython.display import display # 允许为DataFrame使用display()# 导入附加的可视化代码visuals.pyimport visuals as vs# 为notebook提供更加漂亮的可视化%matplotlib inline# 导入人口普查数据data = pd.read_csv(\"census.csv\")# 成功 - 显示第一条记录display(data.head(n=1)) age workclass education_level education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country income 0 39 State-gov Bachelors 13.0 Never-married Adm-clerical Not-in-family White Male 2174.0 0.0 40.0 United-States &lt;=50K 练习：数据探索首先我们对数据集进行一个粗略的探索，我们将看看每一个类别里会有多少被调查者？并且告诉我们这些里面多大比例是年收入大于50,000美元的。在下面的代码单元中，你将需要计算以下量： 总的记录数量，&#39;n_records&#39; 年收入大于50,000美元的人数，&#39;n_greater_50k&#39;. 年收入最多为50,000美元的人数 &#39;n_at_most_50k&#39;. 年收入大于50,000美元的人所占的比例， &#39;greater_percent&#39;. 提示： 您可能需要查看上面的生成的表，以了解&#39;income&#39;条目的格式是什么样的。 # TODO：总的记录数n_records = data.shape[0]# TODO：被调查者的收入大于$50,000的人数n_greater_50k = len(data[data.income == '&gt;50K'])# TODO：被调查者的收入最多为$50,000的人数n_at_most_50k = len(data[data.income == '&lt;=50K'])# TODO：被调查者收入大于$50,000所占的比例greater_percent = float(n_greater_50k) / n_records * 100# 打印结果print (\"Total number of records: &#123;&#125;\".format(n_records))print (\"Individuals making more than $50,000: &#123;&#125;\".format(n_greater_50k))print (\"Individuals making at most $50,000: &#123;&#125;\".format(n_at_most_50k))print (\"Percentage of individuals making more than $50,000: &#123;:.2f&#125;%\".format(greater_percent)) Total number of records: 45222 Individuals making more than $50,000: 11208 Individuals making at most $50,000: 34014 Percentage of individuals making more than $50,000: 24.78% 准备数据在数据能够被作为输入提供给机器学习算法之前，它经常需要被清洗，格式化，和重新组织 - 这通常被叫做预处理。幸运的是，对于这个数据集，没有我们必须处理的无效或丢失的条目，然而，由于某一些特征存在的特性我们必须进行一定的调整。这个预处理都可以极大地帮助我们提升几乎所有的学习算法的结果和预测能力。 获得特征和标签income 列是我们需要的标签，记录一个人的年收入是否高于50K。 因此我们应该把他从数据中剥离出来，单独存放。 # 将数据切分成特征和对应的标签income_raw = data['income']features_raw = data.drop('income', axis = 1) 转换倾斜的连续特征一个数据集有时可能包含至少一个靠近某个数字的特征，但有时也会有一些相对来说存在极大值或者极小值的不平凡分布的的特征。算法对这种分布的数据会十分敏感，并且如果这种数据没有能够很好地规一化处理会使得算法表现不佳。在人口普查数据集的两个特征符合这个描述：’capital-gain&#39;和&#39;capital-loss&#39;。 运行下面的代码单元以创建一个关于这两个特征的条形图。请注意当前的值的范围和它们是如何分布的。 # 可视化 'capital-gain'和'capital-loss' 两个特征vs.distribution(features_raw) 对于高度倾斜分布的特征如&#39;capital-gain&#39;和&#39;capital-loss&#39;，常见的做法是对数据施加一个对数转换，将数据转换成对数，这样非常大和非常小的值不会对学习算法产生负面的影响。并且使用对数变换显著降低了由于异常值所造成的数据范围异常。但是在应用这个变换时必须小心：因为0的对数是没有定义的，所以我们必须先将数据处理成一个比0稍微大一点的数以成功完成对数转换。 运行下面的代码单元来执行数据的转换和可视化结果。再次，注意值的范围和它们是如何分布的。 # 对于倾斜的数据使用Log转换skewed = ['capital-gain', 'capital-loss']features_raw[skewed] = data[skewed].apply(lambda x: np.log(x + 1))# 可视化对数转换后 'capital-gain'和'capital-loss' 两个特征vs.distribution(features_raw, transformed = True) 规一化数字特征除了对于高度倾斜的特征施加转换，对数值特征施加一些形式的缩放通常会是一个好的习惯。在数据上面施加一个缩放并不会改变数据分布的形式（比如上面说的’capital-gain’ or ‘capital-loss’）；但是，规一化保证了每一个特征在使用监督学习器的时候能够被平等的对待。注意一旦使用了缩放，观察数据的原始形式不再具有它本来的意义了，就像下面的例子展示的。 运行下面的代码单元来规一化每一个数字特征。我们将使用sklearn.preprocessing.MinMaxScaler来完成这个任务。 from sklearn.preprocessing import MinMaxScaler# 初始化一个 scaler，并将它施加到特征上scaler = MinMaxScaler()numerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']features_raw[numerical] = scaler.fit_transform(data[numerical])# 显示一个经过缩放的样例记录display(features_raw.head(n = 5)) age workclass education_level education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country 0 0.301370 State-gov Bachelors 0.800000 Never-married Adm-clerical Not-in-family White Male 0.02174 0.0 0.397959 United-States 1 0.452055 Self-emp-not-inc Bachelors 0.800000 Married-civ-spouse Exec-managerial Husband White Male 0.00000 0.0 0.122449 United-States 2 0.287671 Private HS-grad 0.533333 Divorced Handlers-cleaners Not-in-family White Male 0.00000 0.0 0.397959 United-States 3 0.493151 Private 11th 0.400000 Married-civ-spouse Handlers-cleaners Husband Black Male 0.00000 0.0 0.397959 United-States 4 0.150685 Private Bachelors 0.800000 Married-civ-spouse Prof-specialty Wife Black Female 0.00000 0.0 0.397959 Cuba 练习：数据预处理从上面的数据探索中的表中，我们可以看到有几个属性的每一条记录都是非数字的。通常情况下，学习算法期望输入是数字的，这要求非数字的特征（称为类别变量）被转换。转换类别变量的一种流行的方法是使用独热编码方案。独热编码为每一个非数字特征的每一个可能的类别创建一个“虚拟”变量。例如，假设someFeature有三个可能的取值A，B或者C，。我们将把这个特征编码成someFeature_A, someFeature_B和someFeature_C. 特征X 特征X_A 特征X_B 特征X_C B 0 1 0 C ——&gt; 独热编码 ——&gt; 0 0 1 A 1 0 0 此外，对于非数字的特征，我们需要将非数字的标签&#39;income&#39;转换成数值以保证学习算法能够正常工作。因为这个标签只有两种可能的类别（”&lt;=50K”和”&gt;50K”），我们不必要使用独热编码，可以直接将他们编码分别成两个类0和1，在下面的代码单元中你将实现以下功能： 使用pandas.get_dummies()对&#39;features_raw&#39;数据来施加一个独热编码。 将目标标签&#39;income_raw&#39;转换成数字项。 将”&lt;=50K”转换成0；将”&gt;50K”转换成1。 # TODO：使用pandas.get_dummies()对'features_raw'数据进行独热编码features = pd.get_dummies(features_raw)# TODO：将'income_raw'编码成数字值from sklearn import preprocessingincome = pd.Series(preprocessing.LabelEncoder().fit_transform(income_raw))# 打印经过独热编码之后的特征数量encoded = list(features.columns)print (\"&#123;&#125; total features after one-hot encoding.\".format(len(encoded)))# 移除下面一行的注释以观察编码的特征名字print (encoded) 103 total features after one-hot encoding. [&#39;age&#39;, &#39;education-num&#39;, &#39;capital-gain&#39;, &#39;capital-loss&#39;, &#39;hours-per-week&#39;, &#39;workclass_ Federal-gov&#39;, &#39;workclass_ Local-gov&#39;, &#39;workclass_ Private&#39;, &#39;workclass_ Self-emp-inc&#39;, &#39;workclass_ Self-emp-not-inc&#39;, &#39;workclass_ State-gov&#39;, &#39;workclass_ Without-pay&#39;, &#39;education_level_ 10th&#39;, &#39;education_level_ 11th&#39;, &#39;education_level_ 12th&#39;, &#39;education_level_ 1st-4th&#39;, &#39;education_level_ 5th-6th&#39;, &#39;education_level_ 7th-8th&#39;, &#39;education_level_ 9th&#39;, &#39;education_level_ Assoc-acdm&#39;, &#39;education_level_ Assoc-voc&#39;, &#39;education_level_ Bachelors&#39;, &#39;education_level_ Doctorate&#39;, &#39;education_level_ HS-grad&#39;, &#39;education_level_ Masters&#39;, &#39;education_level_ Preschool&#39;, &#39;education_level_ Prof-school&#39;, &#39;education_level_ Some-college&#39;, &#39;marital-status_ Divorced&#39;, &#39;marital-status_ Married-AF-spouse&#39;, &#39;marital-status_ Married-civ-spouse&#39;, &#39;marital-status_ Married-spouse-absent&#39;, &#39;marital-status_ Never-married&#39;, &#39;marital-status_ Separated&#39;, &#39;marital-status_ Widowed&#39;, &#39;occupation_ Adm-clerical&#39;, &#39;occupation_ Armed-Forces&#39;, &#39;occupation_ Craft-repair&#39;, &#39;occupation_ Exec-managerial&#39;, &#39;occupation_ Farming-fishing&#39;, &#39;occupation_ Handlers-cleaners&#39;, &#39;occupation_ Machine-op-inspct&#39;, &#39;occupation_ Other-service&#39;, &#39;occupation_ Priv-house-serv&#39;, &#39;occupation_ Prof-specialty&#39;, &#39;occupation_ Protective-serv&#39;, &#39;occupation_ Sales&#39;, &#39;occupation_ Tech-support&#39;, &#39;occupation_ Transport-moving&#39;, &#39;relationship_ Husband&#39;, &#39;relationship_ Not-in-family&#39;, &#39;relationship_ Other-relative&#39;, &#39;relationship_ Own-child&#39;, &#39;relationship_ Unmarried&#39;, &#39;relationship_ Wife&#39;, &#39;race_ Amer-Indian-Eskimo&#39;, &#39;race_ Asian-Pac-Islander&#39;, &#39;race_ Black&#39;, &#39;race_ Other&#39;, &#39;race_ White&#39;, &#39;sex_ Female&#39;, &#39;sex_ Male&#39;, &#39;native-country_ Cambodia&#39;, &#39;native-country_ Canada&#39;, &#39;native-country_ China&#39;, &#39;native-country_ Columbia&#39;, &#39;native-country_ Cuba&#39;, &#39;native-country_ Dominican-Republic&#39;, &#39;native-country_ Ecuador&#39;, &#39;native-country_ El-Salvador&#39;, &#39;native-country_ England&#39;, &#39;native-country_ France&#39;, &#39;native-country_ Germany&#39;, &#39;native-country_ Greece&#39;, &#39;native-country_ Guatemala&#39;, &#39;native-country_ Haiti&#39;, &#39;native-country_ Holand-Netherlands&#39;, &#39;native-country_ Honduras&#39;, &#39;native-country_ Hong&#39;, &#39;native-country_ Hungary&#39;, &#39;native-country_ India&#39;, &#39;native-country_ Iran&#39;, &#39;native-country_ Ireland&#39;, &#39;native-country_ Italy&#39;, &#39;native-country_ Jamaica&#39;, &#39;native-country_ Japan&#39;, &#39;native-country_ Laos&#39;, &#39;native-country_ Mexico&#39;, &#39;native-country_ Nicaragua&#39;, &#39;native-country_ Outlying-US(Guam-USVI-etc)&#39;, &#39;native-country_ Peru&#39;, &#39;native-country_ Philippines&#39;, &#39;native-country_ Poland&#39;, &#39;native-country_ Portugal&#39;, &#39;native-country_ Puerto-Rico&#39;, &#39;native-country_ Scotland&#39;, &#39;native-country_ South&#39;, &#39;native-country_ Taiwan&#39;, &#39;native-country_ Thailand&#39;, &#39;native-country_ Trinadad&amp;Tobago&#39;, &#39;native-country_ United-States&#39;, &#39;native-country_ Vietnam&#39;, &#39;native-country_ Yugoslavia&#39;] 混洗和切分数据现在所有的 类别变量 已被转换成数值特征，而且所有的数值特征已被规一化。和我们一般情况下做的一样，我们现在将数据（包括特征和它们的标签）切分成训练和测试集。其中80%的数据将用于训练和20%的数据用于测试。然后再进一步把训练数据分为训练集和验证集，用来选择和优化模型。 运行下面的代码单元来完成切分。 # 导入 train_test_splitfrom sklearn.model_selection import train_test_split# 将'features'和'income'数据切分成训练集和测试集X_train, X_test, y_train, y_test = train_test_split(features, income, test_size = 0.2, random_state = 0, stratify = income)# 将'X_train'和'y_train'进一步切分为训练集和验证集X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0, stratify = y_train)# 显示切分的结果print (\"Training set has &#123;&#125; samples.\".format(X_train.shape[0]))print (\"Validation set has &#123;&#125; samples.\".format(X_val.shape[0]))print (\"Testing set has &#123;&#125; samples.\".format(X_test.shape[0])) Training set has 28941 samples. Validation set has 7236 samples. Testing set has 9045 samples. 评价模型性能在这一部分中，我们将尝试四种不同的算法，并确定哪一个能够最好地建模数据。四种算法包含一个天真的预测器 和三个你选择的监督学习器。 评价方法和朴素的预测器CharityML通过他们的研究人员知道被调查者的年收入大于$50,000最有可能向他们捐款。因为这个原因CharityML对于准确预测谁能够获得$50,000以上收入尤其有兴趣。这样看起来使用准确率作为评价模型的标准是合适的。另外，把没有收入大于$50,000的人识别成年收入大于$50,000对于CharityML来说是有害的，因为他想要找到的是有意愿捐款的用户。这样，我们期望的模型具有准确预测那些能够年收入大于$50,000的能力比模型去查全这些被调查者更重要。我们能够使用F-beta score作为评价指标，这样能够同时考虑查准率和查全率： F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{precision \\cdot recall}{\\left( \\beta^2 \\cdot precision \\right) + recall}尤其是，当 $\\beta = 0.5$ 的时候更多的强调查准率，这叫做F$_{0.5}$ score （或者为了简单叫做F-score）。 问题 1 - 天真的预测器的性能通过查看收入超过和不超过 $50,000 的人数，我们能发现多数被调查者年收入没有超过 $50,000。如果我们简单地预测说“这个人的收入没有超过 $50,000”，我们就可以得到一个 准确率超过 50% 的预测。这样我们甚至不用看数据就能做到一个准确率超过 50%。这样一个预测被称作是天真的。通常对数据使用一个天真的预测器是十分重要的，这样能够帮助建立一个模型表现是否好的基准。 使用下面的代码单元计算天真的预测器的相关性能。将你的计算结果赋值给&#39;accuracy&#39;, ‘precision’, ‘recall’ 和 &#39;fscore&#39;，这些值会在后面被使用，请注意这里不能使用scikit-learn，你需要根据公式自己实现相关计算。 如果我们选择一个无论什么情况都预测被调查者年收入大于 $50,000 的模型，那么这个模型在验证集上的准确率，查准率，查全率和 F-score是多少？ #不能使用scikit-learn，你需要根据公式自己实现相关计算。TP = float(len(y_val[y_val == 1]))FP = float(len(y_val[y_val == 0]))FN = 0#TODO： 计算准确率accuracy = float(TP)/len(y_val)# TODO： 计算查准率 Precisionprecision = TP/(TP+FP)# TODO： 计算查全率 Recallrecall = TP/(TP+FN)# TODO： 使用上面的公式，设置beta=0.5，计算F-scorefscore = (1+0.5**2)*((precision*recall)) / (0.5**2*precision+recall)# 打印结果print (\"Naive Predictor on validation data: \\n \\ Accuracy score: &#123;:.4f&#125; \\n \\ Precision: &#123;:.4f&#125; \\n \\ Recall: &#123;:.4f&#125; \\n \\ F-score: &#123;:.4f&#125;\".format(accuracy, precision, recall, fscore)) Naive Predictor on validation data: Accuracy score: 0.2478 Precision: 0.2478 Recall: 1.0000 F-score: 0.2917 监督学习模型问题 2 - 模型应用你能够在 scikit-learn 中选择以下监督学习模型 高斯朴素贝叶斯 (GaussianNB) 决策树 (DecisionTree) 集成方法 (Bagging, AdaBoost, Random Forest, Gradient Boosting) K近邻 (K Nearest Neighbors) 随机梯度下降分类器 (SGDC) 支撑向量机 (SVM) Logistic回归（LogisticRegression） 从上面的监督学习模型中选择三个适合我们这个问题的模型，并回答相应问题。 模型1模型名称 回答：SVM 描述一个该模型在真实世界的一个应用场景。（你需要为此做点研究，并给出你的引用出处） 回答：人类行为认知的辨别，根据图像判断人物在做什么 这个模型的优势是什么？他什么情况下表现最好？ 回答：丰富的核函数可以灵活解决回归问题和分类问题，当特征大于样本数且样本总数较小的时候表现优异 这个模型的缺点是什么？什么条件下它表现很差？ 回答：当需要计算大量样本时丰富的核函数因为没有通用标准计算量大 且表现平平 根据我们当前数据集的特点，为什么这个模型适合这个问题。 回答：当前数据集含有大量特征 且样本总数适中 而且svm丰富的核函数可以很好的满足该项目需求 模型2模型名称 回答：Random Forest 描述一个该模型在真实世界的一个应用场景。（你需要为此做点研究，并给出你的引用出处） 回答：随机森林分类器对土地覆盖进行分类 这个模型的优势是什么？他什么情况下表现最好？ 回答：简单直观 便于理解 提前归一化 以及处理缺失值 可以解决任何类型的数据集 且不需要对数据预处理 通过网格搜索选择超参数 高泛化能力 当特征和样本数量比列保持平衡时表现优异 这个模型的缺点是什么？什么条件下它表现很差？ 回答：容易过拟合 当某叶子结点发生变化时整体结构也发生变化 且当特征的样本比例不平衡的时候容易出现偏向 根据我们当前数据集的特点，为什么这个模型适合这个问题。 回答：特征唯独高 评估各个特征重要性 小范围噪声不会过拟合 模型3模型名称 回答：K近邻 (K Nearest Neighbors) 描述一个该模型在真实世界的一个应用场景。（你需要为此做点研究，并给出你的引用出处） 回答：使用k近邻法估计和绘制森林林分密度，体积和覆盖类型 这个模型的优势是什么？他什么情况下表现最好？ 回答：思想简单 容易理解 聚类效果较优 这个模型的缺点是什么？什么条件下它表现很差？ 回答：对异常值敏感 提前判断K值 局部最优 复杂度高不易控制 迭代次数较多 根据我们当前数据集的特点，为什么这个模型适合这个问题。 回答：该项目数据适中且属于二分类为题 练习 - 创建一个训练和预测的流水线为了正确评估你选择的每一个模型的性能，创建一个能够帮助你快速有效地使用不同大小的训练集并在验证集上做预测的训练和验证的流水线是十分重要的。你在这里实现的功能将会在接下来的部分中被用到。在下面的代码单元中，你将实现以下功能： 从sklearn.metrics中导入fbeta_score和accuracy_score。 用训练集拟合学习器，并记录训练时间。 对训练集的前300个数据点和验证集进行预测并记录预测时间。 计算预测训练集的前300个数据点的准确率和F-score。 计算预测验证集的准确率和F-score。 # TODO：从sklearn中导入两个评价指标 - fbeta_score和accuracy_scorefrom sklearn.metrics import fbeta_score, accuracy_scoredef train_predict(learner, sample_size, X_train, y_train, X_val, y_val): ''' inputs: - learner: the learning algorithm to be trained and predicted on - sample_size: the size of samples (number) to be drawn from training set - X_train: features training set - y_train: income training set - X_val: features validation set - y_val: income validation set ''' results = &#123;&#125; # TODO：使用sample_size大小的训练数据来拟合学习器 # TODO: Fit the learner to the training data using slicing with 'sample_size' start = time() # 获得程序开始时间 learner = learner.fit(X_train[:sample_size], y_train[:sample_size]) end = time() # 获得程序结束时间 # TODO：计算训练时间 results['train_time'] = end - start # TODO: 得到在验证集上的预测值 # 然后得到对前300个训练数据的预测结果 start = time() # 获得程序开始时间 predictions_val = learner.predict(X_val) predictions_train = learner.predict(X_train[:300]) end = time() # 获得程序结束时间 # TODO：计算预测用时 results['pred_time'] = end - start # TODO：计算在最前面的300个训练数据的准确率 results['acc_train'] = accuracy_score(y_test[:300], predictions_train) # TODO：计算在验证上的准确率 results['acc_val'] = accuracy_score(y_val, predictions_val) # TODO：计算在最前面300个训练数据上的F-score results['f_train'] = fbeta_score(y_train[:300], predictions_train, beta=0.5) # TODO：计算验证集上的F-score results['f_val'] = fbeta_score(y_val, predictions_val, beta=0.5) # 成功 print (\"&#123;&#125; trained on &#123;&#125; samples.\".format(learner.__class__.__name__, sample_size)) # 返回结果 return results 练习：初始模型的评估在下面的代码单元中，您将需要实现以下功能： 导入你在前面讨论的三个监督学习模型。 初始化三个模型并存储在&#39;clf_A&#39;，&#39;clf_B&#39;和&#39;clf_C&#39;中。 使用模型的默认参数值，在接下来的部分中你将需要对某一个模型的参数进行调整。 设置random_state (如果有这个参数)。 计算1%， 10%， 100%的训练数据分别对应多少个数据点，并将这些值存储在&#39;samples_1&#39;, &#39;samples_10&#39;, &#39;samples_100&#39;中 注意：取决于你选择的算法，下面实现的代码可能需要一些时间来运行！ # TODO：从sklearn中导入三个监督学习模型from sklearn.neighbors import KNeighborsClassifierfrom sklearn.svm import SVCfrom sklearn.ensemble import RandomForestClassifier# TODO：初始化三个模型clf_A = KNeighborsClassifier(n_neighbors=2)clf_B = SVC(kernel='linear')clf_C = RandomForestClassifier(random_state=0)# TODO：计算1%， 10%， 100%的训练数据分别对应多少点samples_1 = int(len(X_train) * 0.01)samples_10 = int(len(X_train) * 0.10)samples_100 = len(X_train)# 收集学习器的结果results = &#123;&#125;for clf in [clf_A, clf_B, clf_C]: clf_name = clf.__class__.__name__ results[clf_name] = &#123;&#125; for i, samples in enumerate([samples_1, samples_10, samples_100]): results[clf_name][i] = train_predict(clf, samples, X_train, y_train, X_val, y_val)# 对选择的三个模型得到的评价结果进行可视化vs.evaluate(results, accuracy, fscore) KNeighborsClassifier trained on 289 samples. KNeighborsClassifier trained on 2894 samples. KNeighborsClassifier trained on 28941 samples. SVC trained on 289 samples. SVC trained on 2894 samples. SVC trained on 28941 samples. RandomForestClassifier trained on 289 samples. RandomForestClassifier trained on 2894 samples. RandomForestClassifier trained on 28941 samples. 提高效果在这最后一节中，您将从三个有监督的学习模型中选择 最好的 模型来使用学生数据。你将在整个训练集（X_train和y_train）上使用网格搜索优化至少调节一个参数以获得一个比没有调节之前更好的 F-score。 问题 3 - 选择最佳的模型基于你前面做的评价，用一到两段话向 CharityML 解释这三个模型中哪一个对于判断被调查者的年收入大于 $50,000 是最合适的。提示：你的答案应该包括评价指标，预测/训练时间，以及该算法是否适合这里的数据。 回答：从F-score来看，RandomForest在所有训练集上和1%、10%的验证集表现最好，SVM在100%的验证集表现最好从准确度上来看，RandomForest在所有训练集上和1%、10%的验证集表现最好，SVM在100%的验证集表现最好从训练时间上来看，SVM明显多与其他两种算法从预测时间上来看，KNeighbors最多，SVM次之，RandomForest最少KNeighbors由于初始点选择的问题可能会导致分类效果不固定综上所述RandomForest综合表现较好，时间短分类准，我认为最合适，有调优的空间。如果不考虑时间SVM也有可能调出更优化的结果。 问题 4 - 用通俗的话解释模型用一到两段话，向 CharityML 用外行也听得懂的话来解释最终模型是如何工作的。你需要解释所选模型的主要特点。例如，这个模型是怎样被训练的，它又是如何做出预测的。避免使用高级的数学或技术术语，不要使用公式或特定的算法名词。 回答： 训练 根据所有数据依次找到能最大区分当前数据的一个特征，进行数据分割，然后对分割的数据接着重复上述步骤，直到所有的特征都判断完，这样一系列的判断对应的结果为数据的类别，这样的判断构成的就是决策树 再多次执行上一步，每次执行的时候对样本进行随机有放回的抽样，构成多个不一样的决策树，这些决策树合并起来就是随机森林 预测 对于新来的样本，每个决策树做一个分类结果进行相等权重投票，然后以多数者投票的结果作为该样本的分类结果 练习：模型调优调节选择的模型的参数。使用网格搜索（GridSearchCV）来至少调整模型的重要参数（至少调整一个），这个参数至少需尝试3个不同的值。你要使用整个训练集来完成这个过程。在接下来的代码单元中，你需要实现以下功能： 导入sklearn.model_selection.GridSearchCV 和 sklearn.metrics.make_scorer. 初始化你选择的分类器，并将其存储在clf中。 设置random_state (如果有这个参数)。 创建一个对于这个模型你希望调整参数的字典。 例如: parameters = {‘parameter’ : [list of values]}。 注意： 如果你的学习器有 max_features 参数，请不要调节它！ 使用make_scorer来创建一个fbeta_score评分对象（设置$\\beta = 0.5$）。 在分类器clf上用’scorer’作为评价函数运行网格搜索，并将结果存储在grid_obj中。 用训练集（X_train, y_train）训练grid search object,并将结果存储在grid_fit中。 注意： 取决于你选择的参数列表，下面实现的代码可能需要花一些时间运行！ # TODO：导入'GridSearchCV', 'make_scorer'和其他一些需要的库from sklearn.model_selection import GridSearchCVfrom sklearn.metrics import make_scorer# TODO：初始化分类器clf = RandomForestClassifier(random_state=0)# TODO：创建你希望调节的参数列表parameters = &#123;'n_estimators':[10,50,100,150]&#125;# TODO：创建一个fbeta_score打分对象scorer = make_scorer(fbeta_score, beta=0.5)# TODO：在分类器上使用网格搜索，使用'scorer'作为评价函数grid_obj = GridSearchCV(clf, parameters, scorer)# TODO：用训练数据拟合网格搜索对象并找到最佳参数grid_obj.fit(X_train, y_train)# 得到estimatorbest_clf = grid_obj.best_estimator_# 使用没有调优的模型做预测predictions = (clf.fit(X_train, y_train)).predict(X_val)best_predictions = best_clf.predict(X_val)# 汇报调优后的模型print (\"best_clf\\n------\")print (best_clf)# 汇报调参前和调参后的分数print (\"\\nUnoptimized model\\n------\")print (\"Accuracy score on validation data: &#123;:.4f&#125;\".format(accuracy_score(y_val, predictions)))print (\"F-score on validation data: &#123;:.4f&#125;\".format(fbeta_score(y_val, predictions, beta = 0.5)))print (\"\\nOptimized Model\\n------\")print (\"Final accuracy score on the validation data: &#123;:.4f&#125;\".format(accuracy_score(y_val, best_predictions)))print (\"Final F-score on the validation data: &#123;:.4f&#125;\".format(fbeta_score(y_val, best_predictions, beta = 0.5))) best_clf ------ RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1, oob_score=False, random_state=0, verbose=0, warm_start=False) Unoptimized model ------ Accuracy score on validation data: 0.8389 F-score on validation data: 0.6812 Optimized Model ------ Final accuracy score on the validation data: 0.8456 Final F-score on the validation data: 0.6940 问题 5 - 最终模型评估你的最优模型在测试数据上的准确率和 F-score 是多少？这些分数比没有优化的模型好还是差？注意：请在下面的表格中填写你的结果，然后在答案框中提供讨论。 结果: 评价指标 未优化的模型 优化的模型 准确率 0.8389 0.8456 F-score 0.6812 0.6940 回答：相比较没有优化的模型有少许提升 特征的重要性在数据上（比如我们这里使用的人口普查的数据）使用监督学习算法的一个重要的任务是决定哪些特征能够提供最强的预测能力。专注于少量的有效特征和标签之间的关系，我们能够更加简单地理解这些现象，这在很多情况下都是十分有用的。在这个项目的情境下这表示我们希望选择一小部分特征，这些特征能够在预测被调查者是否年收入大于$50,000这个问题上有很强的预测能力。 选择一个有 &#39;feature_importance_&#39; 属性的scikit学习分类器（例如 AdaBoost，随机森林）。&#39;feature_importance_&#39; 属性是对特征的重要性排序的函数。在下一个代码单元中用这个分类器拟合训练集数据并使用这个属性来决定人口普查数据中最重要的5个特征。 问题 6 - 观察特征相关性当探索数据的时候，它显示在这个人口普查数据集中每一条记录我们有十三个可用的特征。在这十三个记录中，你认为哪五个特征对于预测是最重要的，选择每个特征的理由是什么？你会怎样对他们排序？ 回答： 特征1:age 年龄的增长事业和收入也会增长 特征2:hours-per-week 理论上工作时间越多收入越高 特征3:captial-gain 拥有其他资本收益代表经济条件更好 特征4:martial-status 收入大雨50K说明拥有更好的承担能力 特征5:education-num 教育程度越高 越有可能高收入 练习 - 提取特征重要性选择一个scikit-learn中有feature_importance_属性的监督学习分类器，这个属性是一个在做预测的时候根据所选择的算法来对特征重要性进行排序的功能。 在下面的代码单元中，你将要实现以下功能： 如果这个模型和你前面使用的三个模型不一样的话从sklearn中导入一个监督学习模型。 在整个训练集上训练一个监督学习模型。 使用模型中的 &#39;feature_importances_&#39;提取特征的重要性。 # TODO：导入一个有'feature_importances_'的监督学习模型# TODO：在训练集上训练一个监督学习模型model = best_clf# TODO： 提取特征重要性importances = model.feature_importances_# 绘图best_clfvs.feature_plot(importances, X_train, y_train) 问题 7 - 提取特征重要性观察上面创建的展示五个用于预测被调查者年收入是否大于$50,000最相关的特征的可视化图像。 这五个特征的权重加起来是否超过了0.5?这五个特征和你在问题 6中讨论的特征比较怎么样？如果说你的答案和这里的相近，那么这个可视化怎样佐证了你的想法？如果你的选择不相近，那么为什么你觉得这些特征更加相关？ 回答：0.24+0.12+0.10+0.07+0.06&gt;0.5 基本一致。权重越高说明重要性越高 特征选择如果我们只是用可用特征的一个子集的话模型表现会怎么样？通过使用更少的特征来训练，在评价指标的角度来看我们的期望是训练和预测的时间会更少。从上面的可视化来看，我们可以看到前五个最重要的特征贡献了数据中所有特征中超过一半的重要性。这提示我们可以尝试去减小特征空间，简化模型需要学习的信息。下面代码单元将使用你前面发现的优化模型，并只使用五个最重要的特征在相同的训练集上训练模型。 # 导入克隆模型的功能from sklearn.base import clone# 减小特征空间X_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::-1])[:5]]]X_val_reduced = X_val[X_val.columns.values[(np.argsort(importances)[::-1])[:5]]]# 在前面的网格搜索的基础上训练一个“最好的”模型clf_on_reduced = (clone(best_clf)).fit(X_train_reduced, y_train)# 做一个新的预测reduced_predictions = clf_on_reduced.predict(X_val_reduced)# 对于每一个版本的数据汇报最终模型的分数print (\"Final Model trained on full data\\n------\")print (\"Accuracy on validation data: &#123;:.4f&#125;\".format(accuracy_score(y_val, best_predictions)))print (\"F-score on validation data: &#123;:.4f&#125;\".format(fbeta_score(y_val, best_predictions, beta = 0.5)))print (\"\\nFinal Model trained on reduced data\\n------\")print (\"Accuracy on validation data: &#123;:.4f&#125;\".format(accuracy_score(y_val, reduced_predictions)))print (\"F-score on validation data: &#123;:.4f&#125;\".format(fbeta_score(y_val, reduced_predictions, beta = 0.5))) Final Model trained on full data ------ Accuracy on validation data: 0.8456 F-score on validation data: 0.6940 Final Model trained on reduced data ------ Accuracy on validation data: 0.8333 F-score on validation data: 0.6678 问题 8 - 特征选择的影响最终模型在只是用五个特征的数据上和使用所有的特征数据上的 F-score 和准确率相比怎么样？如果训练时间是一个要考虑的因素，你会考虑使用部分特征的数据作为你的训练集吗？ 回答： 评价指标 使用所有特征 使用部分特征 准确率 0.8456 0.8333 F-score 0.6940 0.6678 使用部分特征和所有特征相比准确率和F1值出现了下降现象 如果考虑时间问题我会考虑使用部分特征 会缩短大量训练时间 问题 9 - 在测试集上测试你的模型终于到了测试的时候，记住，测试集只能用一次。 使用你最有信心的模型，在测试集上测试，计算出准确率和 F-score。简述你选择这个模型的原因，并分析测试结果 #TODO test your model on testing data and report accuracy and F scoreres = best_clf.predict(X_test)from sklearn.metrics import fbeta_score,accuracy_scoreprint('accuracy score:&#123;&#125;'.format(accuracy_score(y_test, res)))print('fbeta score:&#123;&#125;'.format(fbeta_score(y_test, res, beta=0.5))) accuracy score:0.8383637368711996 fbeta score:0.6780398221534892 注意： 当你写完了所有的代码，并且回答了所有的问题。你就可以把你的 iPython Notebook 导出成 HTML 文件。你可以在菜单栏，这样导出File -&gt; Download as -&gt; HTML (.html)把这个 HTML 和这个 iPython notebook 一起做为你的作业提交。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://jinyaxuan.github.io/categories/机器学习/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://jinyaxuan.github.io/tags/Machine-Learning/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://jinyaxuan.github.io/categories/机器学习/"}]},{"title":"dog_app by CNN","slug":"dog-app-for-CNN","date":"2019-02-15T13:34:56.000Z","updated":"2019-02-16T11:07:20.711Z","comments":false,"path":"2019/02/15/dog-app-for-CNN/","link":"","permalink":"http://jinyaxuan.github.io/2019/02/15/dog-app-for-CNN/","excerpt":"","text":"卷积神经网络（Convolutional Neural Network, CNN）项目：实现一个狗品种识别算法App在这个notebook文件中，有些模板代码已经提供给你，但你还需要实现更多的功能来完成这个项目。除非有明确要求，你无须修改任何已给出的代码。以‘(练习)’开始的标题表示接下来的代码部分中有你需要实现的功能。这些部分都配有详细的指导，需要实现的部分也会在注释中以’TODO’标出。请仔细阅读所有的提示。 除了实现代码外，你还需要回答一些与项目及代码相关的问题。每个需要回答的问题都会以 ‘问题 X’ 标记。请仔细阅读每个问题，并且在问题后的 ‘回答’ 部分写出完整的答案。我们将根据 你对问题的回答 和 撰写代码实现的功能 来对你提交的项目进行评分。 提示：Code 和 Markdown 区域可通过 Shift + Enter 快捷键运行。此外，Markdown可以通过双击进入编辑模式。 项目中显示为选做的部分可以帮助你的项目脱颖而出，而不是仅仅达到通过的最低要求。如果你决定追求更高的挑战，请在此 notebook 中完成选做部分的代码。 让我们开始吧在这个notebook中，你将迈出第一步，来开发可以作为移动端或 Web应用程序一部分的算法。在这个项目的最后，你的程序将能够把用户提供的任何一个图像作为输入。如果可以从图像中检测到一只狗，它会输出对狗品种的预测。如果图像中是一个人脸，它会预测一个与其最相似的狗的种类。下面这张图展示了完成项目后可能的输出结果。（……实际上我们希望每个学生的输出结果不相同！） 在现实世界中，你需要拼凑一系列的模型来完成不同的任务；举个例子，用来预测狗种类的算法会与预测人类的算法不同。在做项目的过程中，你可能会遇到不少失败的预测，因为并不存在完美的算法和模型。你最终提交的不完美的解决方案也一定会给你带来一个有趣的学习经验！ 项目内容我们将这个notebook分为不同的步骤，你可以使用下面的链接来浏览此notebook。 Step 0: 导入数据集 Step 1: 检测人脸 Step 2: 检测狗狗 Step 3: 从头创建一个CNN来分类狗品种 Step 4: 使用一个CNN来区分狗的品种(使用迁移学习) Step 5: 建立一个CNN来分类狗的品种（使用迁移学习） Step 6: 完成你的算法 Step 7: 测试你的算法 在该项目中包含了如下的问题： 问题 1 问题 2 问题 3 问题 4 问题 5 问题 6 问题 7 问题 8 问题 9 问题 10 问题 11 步骤 0: 导入数据集导入狗数据集在下方的代码单元（cell）中，我们导入了一个狗图像的数据集。我们使用 scikit-learn 库中的 load_files 函数来获取一些变量： train_files, valid_files, test_files - 包含图像的文件路径的numpy数组 train_targets, valid_targets, test_targets - 包含独热编码分类标签的numpy数组 dog_names - 由字符串构成的与标签相对应的狗的种类 from sklearn.datasets import load_files from keras.utils import np_utilsimport numpy as npfrom glob import glob# 定义函数来加载train，test和validation数据集def load_dataset(path): data = load_files(path) dog_files = np.array(data['filenames']) dog_targets = np_utils.to_categorical(np.array(data['target']), 133) return dog_files, dog_targets# 加载train，test和validation数据集train_files, train_targets = load_dataset('dogImages/dogImages/train')valid_files, valid_targets = load_dataset('dogImages/dogImages/valid')test_files, test_targets = load_dataset('dogImages/dogImages/test')# 加载狗品种列表dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/dogImages/train/*/\"))]# 打印数据统计描述print('There are %d total dog categories.' % len(dog_names))print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))print('There are %d training dog images.' % len(train_files))print('There are %d validation dog images.' % len(valid_files))print('There are %d test dog images.'% len(test_files)) Using TensorFlow backend. There are 133 total dog categories. There are 8351 total dog images. There are 6680 training dog images. There are 835 validation dog images. There are 836 test dog images. 导入人脸数据集在下方的代码单元中，我们导入人脸图像数据集，文件所在路径存储在名为 human_files 的 numpy 数组。 import randomrandom.seed(8675309)# 加载打乱后的人脸数据集的文件名human_files = np.array(glob(\"lfw/lfw/*/*\"))random.shuffle(human_files)# 打印数据集的数据量print('There are %d total human images.' % len(human_files)) There are 13233 total human images. 步骤1：检测人脸我们将使用 OpenCV 中的 Haar feature-based cascade classifiers 来检测图像中的人脸。OpenCV 提供了很多预训练的人脸检测模型，它们以XML文件保存在 github。我们已经下载了其中一个检测模型，并且把它存储在 haarcascades 的目录中。 在如下代码单元中，我们将演示如何使用这个检测模型在样本图像中找到人脸。 import cv2 import matplotlib.pyplot as plt %matplotlib inline # 提取预训练的人脸检测模型face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')# 加载彩色（通道顺序为BGR）图像img = cv2.imread(human_files[3])# 将BGR图像进行灰度处理gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)# 在图像中找出脸faces = face_cascade.detectMultiScale(gray)# 打印图像中检测到的脸的个数print('Number of faces detected:', len(faces))# 获取每一个所检测到的脸的识别框for (x,y,w,h) in faces: # 在人脸图像中绘制出识别框 cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) # 将BGR图像转变为RGB图像以打印cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)# 展示含有识别框的图像plt.imshow(cv_rgb)plt.show() Number of faces detected: 1 在使用任何一个检测模型之前，将图像转换为灰度图是常用过程。detectMultiScale 函数使用储存在 face_cascade 中的的数据，对输入的灰度图像进行分类。 在上方的代码中，faces 以 numpy 数组的形式，保存了识别到的面部信息。它其中每一行表示一个被检测到的脸，该数据包括如下四个信息：前两个元素 x、y 代表识别框左上角的 x 和 y 坐标（参照上图，注意 y 坐标的方向和我们默认的方向不同）；后两个元素代表识别框在 x 和 y 轴两个方向延伸的长度 w 和 d。 写一个人脸识别器我们可以将这个程序封装为一个函数。该函数的输入为人脸图像的路径，当图像中包含人脸时，该函数返回 True，反之返回 False。该函数定义如下所示。 # 如果img_path路径表示的图像检测到了脸，返回\"True\" def face_detector(img_path): img = cv2.imread(img_path) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) faces = face_cascade.detectMultiScale(gray) return len(faces) &gt; 0 【练习】 评估人脸检测模型 问题 1:在下方的代码块中，使用 face_detector 函数，计算： human_files 的前100张图像中，能够检测到人脸的图像占比多少？ dog_files 的前100张图像中，能够检测到人脸的图像占比多少？ 理想情况下，人图像中检测到人脸的概率应当为100%，而狗图像中检测到人脸的概率应该为0%。你会发现我们的算法并非完美，但结果仍然是可以接受的。我们从每个数据集中提取前100个图像的文件路径，并将它们存储在human_files_short和dog_files_short中。 human_files_short = human_files[:100]dog_files_short = train_files[:100]## 请不要修改上方代码def detect(detector, files): return np.mean(list(map(detector, files)))## TODO: 基于human_files_short和dog_files_short## 中的图像测试face_detector的表现print('humna: &#123;:.2%&#125;'.format(detect(face_detector,human_files_short)))print('dog: &#123;:.2%&#125;'.format(detect(face_detector, dog_files_short))) humna: 99.00% dog: 12.00% 问题 2:就算法而言，该算法成功与否的关键在于，用户能否提供含有清晰面部特征的人脸图像。那么你认为，这样的要求在实际使用中对用户合理吗？如果你觉得不合理，你能否想到一个方法，即使图像中并没有清晰的面部特征，也能够检测到人脸？ 回答: 不合理 轮廓检测 压缩图像特征 通过正图像和负图像训练分类器 给予权重 确定位置 最后保留正图像 选做：我们建议在你的算法中使用opencv的人脸检测模型去检测人类图像，不过你可以自由地探索其他的方法，尤其是尝试使用深度学习来解决它:)。请用下方的代码单元来设计和测试你的面部监测算法。如果你决定完成这个选做任务，你需要报告算法在每一个数据集上的表现。 ## (选做) TODO: 报告另一个面部检测算法在LFW数据集上的表现### 你可以随意使用所需的代码单元数import cv2 import matplotlib.pyplot as plt %matplotlib inline # 提取预训练的人脸检测模型face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt2.xml')# 加载彩色（通道顺序为BGR）图像img = cv2.imread(human_files[4])# 将BGR图像进行灰度处理gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)# 在图像中找出脸faces = face_cascade.detectMultiScale(gray)# 打印图像中检测到的脸的个数print('Number of faces detected:', len(faces))# 获取每一个所检测到的脸的识别框for (x,y,w,h) in faces: # 在人脸图像中绘制出识别框 cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) # 将BGR图像转变为RGB图像以打印cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)# 展示含有识别框的图像plt.imshow(cv_rgb)plt.show() Number of faces detected: 1 步骤 2: 检测狗狗在这个部分中，我们使用预训练的 ResNet-50 模型去检测图像中的狗。下方的第一行代码就是下载了 ResNet-50 模型的网络结构参数，以及基于 ImageNet 数据集的预训练权重。 ImageNet 这目前一个非常流行的数据集，常被用来测试图像分类等计算机视觉任务相关的算法。它包含超过一千万个 URL，每一个都链接到 1000 categories 中所对应的一个物体的图像。任给输入一个图像，该 ResNet-50 模型会返回一个对图像中物体的预测结果。 from keras.applications.resnet50 import ResNet50# 定义ResNet50模型ResNet50_model = ResNet50(weights='imagenet') 数据预处理 在使用 TensorFlow 作为后端的时候，在 Keras 中，CNN 的输入是一个4维数组（也被称作4维张量），它的各维度尺寸为 (nb_samples, rows, columns, channels)。其中 nb_samples 表示图像（或者样本）的总数，rows, columns, 和 channels 分别表示图像的行数、列数和通道数。 下方的 path_to_tensor 函数实现如下将彩色图像的字符串型的文件路径作为输入，返回一个4维张量，作为 Keras CNN 输入。因为我们的输入图像是彩色图像，因此它们具有三个通道（ channels 为 3）。 该函数首先读取一张图像，然后将其缩放为 224×224 的图像。 随后，该图像被调整为具有4个维度的张量。 对于任一输入图像，最后返回的张量的维度是：(1, 224, 224, 3)。 paths_to_tensor 函数将图像路径的字符串组成的 numpy 数组作为输入，并返回一个4维张量，各维度尺寸为 (nb_samples, 224, 224, 3)。 在这里，nb_samples是提供的图像路径的数据中的样本数量或图像数量。你也可以将 nb_samples 理解为数据集中3维张量的个数（每个3维张量表示一个不同的图像。 from keras.preprocessing import image from tqdm import tqdmdef path_to_tensor(img_path): # 用PIL加载RGB图像为PIL.Image.Image类型 img = image.load_img(img_path, target_size=(224, 224)) # 将PIL.Image.Image类型转化为格式为(224, 224, 3)的3维张量 x = image.img_to_array(img) # 将3维张量转化为格式为(1, 224, 224, 3)的4维张量并返回 return np.expand_dims(x, axis=0)def paths_to_tensor(img_paths): list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)] return np.vstack(list_of_tensors) 基于 ResNet-50 架构进行预测对于通过上述步骤得到的四维张量，在把它们输入到 ResNet-50 网络、或 Keras 中其他类似的预训练模型之前，还需要进行一些额外的处理： 首先，这些图像的通道顺序为 RGB，我们需要重排他们的通道顺序为 BGR。 其次，预训练模型的输入都进行了额外的归一化过程。因此我们在这里也要对这些张量进行归一化，即对所有图像所有像素都减去像素均值 [103.939, 116.779, 123.68]（以 RGB 模式表示，根据所有的 ImageNet 图像算出）。 导入的 preprocess_input 函数实现了这些功能。如果你对此很感兴趣，可以在 这里 查看 preprocess_input的代码。 在实现了图像处理的部分之后，我们就可以使用模型来进行预测。这一步通过 predict 方法来实现，它返回一个向量，向量的第 i 个元素表示该图像属于第 i 个 ImageNet 类别的概率。这通过如下的 ResNet50_predict_labels 函数实现。 通过对预测出的向量取用 argmax 函数（找到有最大概率值的下标序号），我们可以得到一个整数，即模型预测到的物体的类别。进而根据这个 清单，我们能够知道这具体是哪个品种的狗狗。 from keras.applications.resnet50 import preprocess_input, decode_predictionsdef ResNet50_predict_labels(img_path): # 返回img_path路径的图像的预测向量 img = preprocess_input(path_to_tensor(img_path)) return np.argmax(ResNet50_model.predict(img)) 完成狗检测模型在研究该 清单 的时候，你会注意到，狗类别对应的序号为151-268。因此，在检查预训练模型判断图像是否包含狗的时候，我们只需要检查如上的 ResNet50_predict_labels 函数是否返回一个介于151和268之间（包含区间端点）的值。 我们通过这些想法来完成下方的 dog_detector 函数，如果从图像中检测到狗就返回 True，否则返回 False。 def dog_detector(img_path): prediction = ResNet50_predict_labels(img_path) return ((prediction &lt;= 268) &amp; (prediction &gt;= 151)) 【作业】评估狗狗检测模型 问题 3:在下方的代码块中，使用 dog_detector 函数，计算： human_files_short中图像检测到狗狗的百分比？ dog_files_short中图像检测到狗狗的百分比？ ### TODO: 测试dog_detector函数在human_files_short和dog_files_short的表现print('humna: &#123;:.2%&#125;'.format(detect(dog_detector,human_files_short)))print('dog: &#123;:.2%&#125;'.format(detect(dog_detector, dog_files_short))) humna: 2.00% dog: 100.00% 步骤 3: 从头开始创建一个CNN来分类狗品种现在我们已经实现了一个函数，能够在图像中识别人类及狗狗。但我们需要更进一步的方法，来对狗的类别进行识别。在这一步中，你需要实现一个卷积神经网络来对狗的品种进行分类。你需要从头实现你的卷积神经网络（在这一阶段，你还不能使用迁移学习），并且你需要达到超过1%的测试集准确率。在本项目的步骤五种，你还有机会使用迁移学习来实现一个准确率大大提高的模型。 在添加卷积层的时候，注意不要加上太多的（可训练的）层。更多的参数意味着更长的训练时间，也就是说你更可能需要一个 GPU 来加速训练过程。万幸的是，Keras 提供了能够轻松预测每次迭代（epoch）花费时间所需的函数。你可以据此推断你算法所需的训练时间。 值得注意的是，对狗的图像进行分类是一项极具挑战性的任务。因为即便是一个正常人，也很难区分布列塔尼犬和威尔士史宾格犬。 布列塔尼犬（Brittany） 威尔士史宾格犬（Welsh Springer Spaniel） 不难发现其他的狗品种会有很小的类间差别（比如金毛寻回犬和美国水猎犬）。 金毛寻回犬（Curly-Coated Retriever） 美国水猎犬（American Water Spaniel） 同样，拉布拉多犬（labradors）有黄色、棕色和黑色这三种。那么你设计的基于视觉的算法将不得不克服这种较高的类间差别，以达到能够将这些不同颜色的同类狗分到同一个品种中。 黄色拉布拉多犬（Yellow Labrador） 棕色拉布拉多犬（Chocolate Labrador） 黑色拉布拉多犬（Black Labrador） 我们也提到了随机分类将得到一个非常低的结果：不考虑品种略有失衡的影响，随机猜测到正确品种的概率是1/133，相对应的准确率是低于1%的。 请记住，在深度学习领域，实践远远高于理论。大量尝试不同的框架吧，相信你的直觉！当然，玩得开心！ 数据预处理通过对每张图像的像素值除以255，我们对图像实现了归一化处理。 from PIL import ImageFile ImageFile.LOAD_TRUNCATED_IMAGES = True # Keras中的数据预处理过程train_tensors = paths_to_tensor(train_files).astype('float32')/255valid_tensors = paths_to_tensor(valid_files).astype('float32')/255test_tensors = paths_to_tensor(test_files).astype('float32')/255 100%|██████████| 6680/6680 [00:56&lt;00:00, 117.40it/s] 100%|██████████| 835/835 [00:32&lt;00:00, 25.94it/s] 100%|██████████| 836/836 [00:15&lt;00:00, 54.77it/s] 【练习】模型架构创建一个卷积神经网络来对狗品种进行分类。在你代码块的最后，执行 model.summary() 来输出你模型的总结信息。 我们已经帮你导入了一些所需的 Python 库，如有需要你可以自行导入。如果你在过程中遇到了困难，如下是给你的一点小提示——该模型能够在5个 epoch 内取得超过1%的测试准确率，并且能在CPU上很快地训练。 问题 4:在下方的代码块中尝试使用 Keras 搭建卷积网络的架构，并回答相关的问题。 你可以尝试自己搭建一个卷积网络的模型，那么你需要回答你搭建卷积网络的具体步骤（用了哪些层）以及为什么这样搭建。 你也可以根据上图提示的步骤搭建卷积网络，那么请说明为何如上的架构能够在该问题上取得很好的表现。 回答: 建立一个卷基层 建立一个池化层 再建立一个卷基层 再建立一个池化层 再建立一个卷基层 再建立一个池化层 添加一个全局池化层 再添加一个全连接层 可以对图像进行更细致的处理 但最终结果不会因为层数越深处理越好 from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Flatten, BatchNormalization, Activationfrom keras.layers import Dropout, Flatten, Densefrom keras.models import Sequentialmodel = Sequential()### TODO: 定义你的网络架构model.add(Conv2D(filters=3, kernel_size=2, padding='same',activation='relu',input_shape=(224,224,3)))model.add(MaxPooling2D(pool_size=2))model.add(Dropout(0.2))model.add(Conv2D(filters=6, kernel_size=2, padding='same',activation='relu'))model.add(MaxPooling2D(pool_size=2))model.add(Dropout(0.2))model.add(Conv2D(16, (3, 3), strides=(1, 1), padding='valid'))model.add(MaxPooling2D((2, 2)))model.add(BatchNormalization())model.add(Activation('relu'))model.add(Dropout(0.2))model.add(GlobalAveragePooling2D())model.add(Dense(133 ,activation='softmax'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 224, 224, 3) 39 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 112, 112, 3) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 112, 112, 3) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 112, 112, 6) 78 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 56, 56, 6) 0 _________________________________________________________________ dropout_2 (Dropout) (None, 56, 56, 6) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 54, 54, 16) 880 _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 27, 27, 16) 0 _________________________________________________________________ batch_normalization_1 (Batch (None, 27, 27, 16) 64 _________________________________________________________________ activation_50 (Activation) (None, 27, 27, 16) 0 _________________________________________________________________ dropout_3 (Dropout) (None, 27, 27, 16) 0 _________________________________________________________________ global_average_pooling2d_1 ( (None, 16) 0 _________________________________________________________________ dense_1 (Dense) (None, 133) 2261 ================================================================= Total params: 3,322 Trainable params: 3,290 Non-trainable params: 32 _________________________________________________________________ ## 编译模型model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) 【练习】训练模型 问题 5:在下方代码单元训练模型。使用模型检查点（model checkpointing）来储存具有最低验证集 loss 的模型。 可选题：你也可以对训练集进行 数据增强，来优化模型的表现。 from keras.callbacks import ModelCheckpoint ### TODO: 设置训练模型的epochs的数量epochs = 35### 不要修改下方代码checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', verbose=1, save_best_only=True)model.fit(train_tensors, train_targets, validation_data=(valid_tensors, valid_targets), epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1) Train on 6680 samples, validate on 835 samples Epoch 1/35 6680/6680 [==============================] - 77s 12ms/step - loss: 4.8886 - acc: 0.0073 - val_loss: 4.8812 - val_acc: 0.0108 Epoch 00001: val_loss improved from inf to 4.88124, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 2/35 6680/6680 [==============================] - 67s 10ms/step - loss: 4.8673 - acc: 0.0099 - val_loss: 4.8750 - val_acc: 0.0108 - ETA: 1:06 - loss: 4.8847 - acc: 0.0000e+00 Epoch 00002: val_loss improved from 4.88124 to 4.87504, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 3/35 6680/6680 [==============================] - 67s 10ms/step - loss: 4.8608 - acc: 0.0121 - val_loss: 4.8750 - val_acc: 0.0132 - ETA: 51s - loss: 4.8555 - acc: 0.0149 Epoch 00003: val_loss did not improve Epoch 4/35 6680/6680 [==============================] - 67s 10ms/step - loss: 4.8569 - acc: 0.0123 - val_loss: 4.8735 - val_acc: 0.0156 Epoch 00004: val_loss improved from 4.87504 to 4.87351, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 5/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.8508 - acc: 0.0144 - val_loss: 4.8756 - val_acc: 0.0120 Epoch 00005: val_loss did not improve Epoch 6/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.8452 - acc: 0.0138 - val_loss: 4.8623 - val_acc: 0.0156 Epoch 00006: val_loss improved from 4.87351 to 4.86231, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 7/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.8388 - acc: 0.0163 - val_loss: 4.8579 - val_acc: 0.0144 Epoch 00007: val_loss improved from 4.86231 to 4.85791, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 8/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.8278 - acc: 0.0196 - val_loss: 4.8477 - val_acc: 0.0240 - ETA: 55s - loss: 4.8244 - acc: 0.0224 - ETA: 41s - loss: 4.8203 - acc: 0.0230 Epoch 00008: val_loss improved from 4.85791 to 4.84765, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 9/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.8167 - acc: 0.0220 - val_loss: 4.8453 - val_acc: 0.0204 - ETA: 31s - loss: 4.8156 - acc: 0.0231 Epoch 00009: val_loss improved from 4.84765 to 4.84525, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 10/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.8049 - acc: 0.0228 - val_loss: 4.8268 - val_acc: 0.0275 - ETA: 1:02 - loss: 4.7917 - acc: 0.0214 - ETA: 49s - loss: 4.8110 - acc: 0.0207 - ETA: 5s - loss: 4.8058 - acc: 0.0220 Epoch 00010: val_loss improved from 4.84525 to 4.82678, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 11/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.7932 - acc: 0.0216 - val_loss: 4.8224 - val_acc: 0.0287 - ETA: 2s - loss: 4.7937 - acc: 0.0218 Epoch 00011: val_loss improved from 4.82678 to 4.82244, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 12/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.7788 - acc: 0.0234 - val_loss: 4.8113 - val_acc: 0.0216 Epoch 00012: val_loss improved from 4.82244 to 4.81131, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 13/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.7625 - acc: 0.0238 - val_loss: 4.8150 - val_acc: 0.0263 - ETA: 37s - loss: 4.7487 - acc: 0.0236 Epoch 00013: val_loss did not improve Epoch 14/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.7494 - acc: 0.0256 - val_loss: 4.8236 - val_acc: 0.0204 - ETA: 30s - loss: 4.7515 - acc: 0.0266 Epoch 00014: val_loss did not improve Epoch 15/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.7343 - acc: 0.0244 - val_loss: 4.8024 - val_acc: 0.0180 - ETA: 36s - loss: 4.7482 - acc: 0.0243 - ETA: 32s - loss: 4.7423 - acc: 0.0234 - ETA: 15s - loss: 4.7333 - acc: 0.0235 Epoch 00015: val_loss improved from 4.81131 to 4.80236, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 16/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.7182 - acc: 0.0295 - val_loss: 4.7744 - val_acc: 0.0263 - ETA: 12s - loss: 4.7098 - acc: 0.0305 Epoch 00016: val_loss improved from 4.80236 to 4.77445, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 17/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.7020 - acc: 0.0299 - val_loss: 4.7623 - val_acc: 0.0311 - ETA: 1:00 - loss: 4.6920 - acc: 0.0222 - ETA: 35s - loss: 4.7126 - acc: 0.0307 - ETA: 30s - loss: 4.7073 - acc: 0.0305 Epoch 00017: val_loss improved from 4.77445 to 4.76233, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 18/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.6870 - acc: 0.0331 - val_loss: 4.7441 - val_acc: 0.0323 - ETA: 59s - loss: 4.6866 - acc: 0.0357 - ETA: 55s - loss: 4.6717 - acc: 0.0327 - ETA: 42s - loss: 4.6893 - acc: 0.0343 - ETA: 17s - loss: 4.6844 - acc: 0.0329 Epoch 00018: val_loss improved from 4.76233 to 4.74409, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 19/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.6697 - acc: 0.0332 - val_loss: 4.7471 - val_acc: 0.0299 - ETA: 57s - loss: 4.6509 - acc: 0.0463 - ETA: 5s - loss: 4.6693 - acc: 0.0331 Epoch 00019: val_loss did not improve Epoch 20/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.6548 - acc: 0.0349 - val_loss: 4.7546 - val_acc: 0.0299 - ETA: 9s - loss: 4.6577 - acc: 0.0352 - ETA: 0s - loss: 4.6559 - acc: 0.0350 Epoch 00020: val_loss did not improve Epoch 21/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.6410 - acc: 0.0365 - val_loss: 4.7458 - val_acc: 0.0323 Epoch 00021: val_loss did not improve Epoch 22/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.6292 - acc: 0.0380 - val_loss: 4.7081 - val_acc: 0.0371 - ETA: 29s - loss: 4.6224 - acc: 0.0403 Epoch 00022: val_loss improved from 4.74409 to 4.70811, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 23/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.6140 - acc: 0.0362 - val_loss: 4.7357 - val_acc: 0.0180 - ETA: 32s - loss: 4.6099 - acc: 0.0361 Epoch 00023: val_loss did not improve Epoch 24/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.6050 - acc: 0.0365 - val_loss: 4.6911 - val_acc: 0.0347 Epoch 00024: val_loss improved from 4.70811 to 4.69114, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 25/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.5925 - acc: 0.0379 - val_loss: 4.6984 - val_acc: 0.0287 Epoch 00025: val_loss did not improve Epoch 26/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.5854 - acc: 0.0391 - val_loss: 4.6992 - val_acc: 0.0323 - ETA: 1:03 - loss: 4.6114 - acc: 0.0250 Epoch 00026: val_loss did not improve Epoch 27/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.5708 - acc: 0.0397 - val_loss: 4.7011 - val_acc: 0.0311 - ETA: 55s - loss: 4.5531 - acc: 0.0390 - ETA: 37s - loss: 4.5666 - acc: 0.0388 - ETA: 25s - loss: 4.5774 - acc: 0.0397 Epoch 00027: val_loss did not improve Epoch 28/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.5707 - acc: 0.0400 - val_loss: 4.6913 - val_acc: 0.0287 - ETA: 22s - loss: 4.5671 - acc: 0.0396 - ETA: 10s - loss: 4.5716 - acc: 0.0389 - ETA: 4s - loss: 4.5709 - acc: 0.0400 Epoch 00028: val_loss did not improve Epoch 29/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.5552 - acc: 0.0428 - val_loss: 4.6915 - val_acc: 0.0311 Epoch 00029: val_loss did not improve Epoch 30/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.5480 - acc: 0.0394 - val_loss: 4.6864 - val_acc: 0.0359 - ETA: 45s - loss: 4.5450 - acc: 0.0424 Epoch 00030: val_loss improved from 4.69114 to 4.68639, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 31/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.5408 - acc: 0.0431 - val_loss: 4.6938 - val_acc: 0.0311 - ETA: 30s - loss: 4.5420 - acc: 0.0447 - ETA: 1s - loss: 4.5420 - acc: 0.0434 Epoch 00031: val_loss did not improve Epoch 32/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.5377 - acc: 0.0424 - val_loss: 4.6689 - val_acc: 0.0311 Epoch 00032: val_loss improved from 4.68639 to 4.66890, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 33/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.5289 - acc: 0.0434 - val_loss: 4.6723 - val_acc: 0.0323 - ETA: 46s - loss: 4.5195 - acc: 0.0368 Epoch 00033: val_loss did not improve Epoch 34/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.5216 - acc: 0.0436 - val_loss: 4.7055 - val_acc: 0.0263 - ETA: 55s - loss: 4.5110 - acc: 0.0490 - ETA: 26s - loss: 4.5125 - acc: 0.0468 Epoch 00034: val_loss did not improve Epoch 35/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.5189 - acc: 0.0443 - val_loss: 4.6770 - val_acc: 0.0287 Epoch 00035: val_loss did not improve &lt;keras.callbacks.History at 0x7f9b4cfe3da0&gt; ## 加载具有最好验证loss的模型model.load_weights('saved_models/weights.best.from_scratch.hdf5') 测试模型在狗图像的测试数据集上试用你的模型。确保测试准确率大于1%。 # 获取测试数据集中每一个图像所预测的狗品种的indexdog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]# 报告测试准确率test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)print('Test accuracy: %.4f%%' % test_accuracy) Test accuracy: 3.1100% 步骤 4: 使用一个CNN来区分狗的品种使用 迁移学习（Transfer Learning）的方法，能帮助我们在不损失准确率的情况下大大减少训练时间。在以下步骤中，你可以尝试使用迁移学习来训练你自己的CNN。 得到从图像中提取的特征向量（Bottleneck Features）bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')train_VGG16 = bottleneck_features['train']valid_VGG16 = bottleneck_features['valid']test_VGG16 = bottleneck_features['test'] 模型架构该模型使用预训练的 VGG-16 模型作为固定的图像特征提取器，其中 VGG-16 最后一层卷积层的输出被直接输入到我们的模型。我们只需要添加一个全局平均池化层以及一个全连接层，其中全连接层使用 softmax 激活函数，对每一个狗的种类都包含一个节点。 VGG16_model = Sequential()VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))VGG16_model.add(Dense(133, activation='softmax'))VGG16_model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= global_average_pooling2d_2 ( (None, 512) 0 _________________________________________________________________ dense_2 (Dense) (None, 133) 68229 ================================================================= Total params: 68,229 Trainable params: 68,229 Non-trainable params: 0 _________________________________________________________________ ## 编译模型VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) ## 训练模型checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', verbose=1, save_best_only=True)VGG16_model.fit(train_VGG16, train_targets, validation_data=(valid_VGG16, valid_targets), epochs=20, batch_size=20, callbacks=[checkpointer], verbose=10) Train on 6680 samples, validate on 835 samples Epoch 1/20 Epoch 00001: val_loss improved from inf to 10.56545, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 2/20 Epoch 00002: val_loss improved from 10.56545 to 9.88948, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 3/20 Epoch 00003: val_loss improved from 9.88948 to 9.41489, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 4/20 Epoch 00004: val_loss improved from 9.41489 to 9.28659, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 5/20 Epoch 00005: val_loss improved from 9.28659 to 9.20906, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 6/20 Epoch 00006: val_loss improved from 9.20906 to 9.11723, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 7/20 Epoch 00007: val_loss improved from 9.11723 to 9.03948, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 8/20 Epoch 00008: val_loss improved from 9.03948 to 8.85330, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 9/20 Epoch 00009: val_loss did not improve Epoch 10/20 Epoch 00010: val_loss improved from 8.85330 to 8.65410, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 11/20 Epoch 00011: val_loss improved from 8.65410 to 8.40498, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 12/20 Epoch 00012: val_loss improved from 8.40498 to 8.39156, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 13/20 Epoch 00013: val_loss improved from 8.39156 to 8.36496, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 14/20 Epoch 00014: val_loss improved from 8.36496 to 8.30643, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 15/20 Epoch 00015: val_loss improved from 8.30643 to 8.16639, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 16/20 Epoch 00016: val_loss improved from 8.16639 to 8.06120, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 17/20 Epoch 00017: val_loss improved from 8.06120 to 7.94562, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 18/20 Epoch 00018: val_loss improved from 7.94562 to 7.90346, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 19/20 Epoch 00019: val_loss did not improve Epoch 20/20 Epoch 00020: val_loss improved from 7.90346 to 7.84868, saving model to saved_models/weights.best.VGG16.hdf5 &lt;keras.callbacks.History at 0x7f9b4c84af28&gt; ## 加载具有最好验证loss的模型VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5') 测试模型现在，我们可以测试此CNN在狗图像测试数据集中识别品种的效果如何。我们在下方打印出测试准确率。 # 获取测试数据集中每一个图像所预测的狗品种的indexVGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]# 报告测试准确率test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)print('Test accuracy: %.4f%%' % test_accuracy) Test accuracy: 42.8230% 使用模型预测狗的品种from extract_bottleneck_features import *def VGG16_predict_breed(img_path): # 提取bottleneck特征 bottleneck_feature = extract_VGG16(path_to_tensor(img_path)) # 获取预测向量 predicted_vector = VGG16_model.predict(bottleneck_feature) # 返回此模型预测的狗的品种 return dog_names[np.argmax(predicted_vector)] 步骤 5: 建立一个CNN来分类狗的品种（使用迁移学习）现在你将使用迁移学习来建立一个CNN，从而可以从图像中识别狗的品种。你的 CNN 在测试集上的准确率必须至少达到60%。 在步骤4中，我们使用了迁移学习来创建一个使用基于 VGG-16 提取的特征向量来搭建一个 CNN。在本部分内容中，你必须使用另一个预训练模型来搭建一个 CNN。为了让这个任务更易实现，我们已经预先对目前 keras 中可用的几种网络进行了预训练： VGG-19 bottleneck features ResNet-50 bottleneck features Inception bottleneck features Xception bottleneck features 这些文件被命名为为： Dog{network}Data.npz 其中 {network} 可以是 VGG19、Resnet50、InceptionV3 或 Xception 中的一个。选择上方网络架构中的一个，下载相对应的bottleneck特征，并将所下载的文件保存在目录 bottleneck_features/ 中。 【练习】获取模型的特征向量在下方代码块中，通过运行下方代码提取训练、测试与验证集相对应的bottleneck特征。 bottleneck_features = np.load(&#39;bottleneck_features/Dog{network}Data.npz&#39;) train_{network} = bottleneck_features[&#39;train&#39;] valid_{network} = bottleneck_features[&#39;valid&#39;] test_{network} = bottleneck_features[&#39;test&#39;] ### TODO: 从另一个预训练的CNN获取bottleneck特征bottleneck_features = np.load('bottleneck_features/DogResnet50Data.npz')train_Resnet = bottleneck_features['train']valid_Resnet = bottleneck_features['valid']test_Resnet = bottleneck_features['test'] 【练习】模型架构建立一个CNN来分类狗品种。在你的代码单元块的最后，通过运行如下代码输出网络的结构： &lt;your model&#39;s name&gt;.summary() 问题 6:在下方的代码块中尝试使用 Keras 搭建最终的网络架构，并回答你实现最终 CNN 架构的步骤与每一步的作用，并描述你在迁移学习过程中，使用该网络架构的原因。 回答:Resnet50网络相较于其他net 速度更快 准确率更高 且实验证明 经过多种类训练过的网络比训练单一种类识别的网络效果更优 所以选择resent50网络来进行迁移学习 ### TODO: 定义你的框架model = Sequential()model.add(GlobalAveragePooling2D(input_shape=train_Resnet.shape[1:]))model.add(Dense(133, activation='softmax'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= global_average_pooling2d_3 ( (None, 2048) 0 _________________________________________________________________ dense_3 (Dense) (None, 133) 272517 ================================================================= Total params: 272,517 Trainable params: 272,517 Non-trainable params: 0 _________________________________________________________________ ### TODO: 编译模型model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) 【练习】训练模型 问题 7:在下方代码单元中训练你的模型。使用模型检查点（model checkpointing）来储存具有最低验证集 loss 的模型。 当然，你也可以对训练集进行 数据增强 以优化模型的表现，不过这不是必须的步骤。 ### TODO: 训练模型checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Resnet50.hdf5', verbose=10, save_best_only=True)model.fit(train_Resnet, train_targets, validation_data=(valid_Resnet, valid_targets), epochs=20, batch_size=20, callbacks=[checkpointer], verbose=10) Train on 6680 samples, validate on 835 samples Epoch 1/20 Epoch 00001: val_loss improved from inf to 0.83976, saving model to saved_models/weights.best.Resnet50.hdf5 Epoch 2/20 Epoch 00002: val_loss improved from 0.83976 to 0.73381, saving model to saved_models/weights.best.Resnet50.hdf5 Epoch 3/20 Epoch 00003: val_loss improved from 0.73381 to 0.67062, saving model to saved_models/weights.best.Resnet50.hdf5 Epoch 4/20 Epoch 00004: val_loss improved from 0.67062 to 0.64336, saving model to saved_models/weights.best.Resnet50.hdf5 Epoch 5/20 Epoch 00005: val_loss did not improve Epoch 6/20 Epoch 00006: val_loss improved from 0.64336 to 0.55801, saving model to saved_models/weights.best.Resnet50.hdf5 Epoch 7/20 Epoch 00007: val_loss did not improve Epoch 8/20 Epoch 00008: val_loss did not improve Epoch 9/20 Epoch 00009: val_loss did not improve Epoch 10/20 Epoch 00010: val_loss did not improve Epoch 11/20 Epoch 00011: val_loss did not improve Epoch 12/20 Epoch 00012: val_loss did not improve Epoch 13/20 Epoch 00013: val_loss did not improve Epoch 14/20 Epoch 00014: val_loss did not improve Epoch 15/20 Epoch 00015: val_loss did not improve Epoch 16/20 Epoch 00016: val_loss did not improve Epoch 17/20 Epoch 00017: val_loss did not improve Epoch 18/20 Epoch 00018: val_loss did not improve Epoch 19/20 Epoch 00019: val_loss did not improve Epoch 20/20 Epoch 00020: val_loss did not improve &lt;keras.callbacks.History at 0x7f9b4c0d0c88&gt; ### TODO: 加载具有最佳验证loss的模型权重model.load_weights('saved_models/weights.best.Resnet50.hdf5') 【练习】测试模型 问题 8:在狗图像的测试数据集上试用你的模型。确保测试准确率大于60%。 ### TODO: 在测试集上计算分类准确率model_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet]# 报告测试准确率test_accuracy = 100*np.sum(np.array(model_predictions)==np.argmax(test_targets, axis=1))/len(model_predictions)print('Test accuracy: %.4f%%' % test_accuracy)# Fit the modelhistory = model.fit(train_Resnet, train_targets, validation_split=0.33, epochs=150, batch_size=10, verbose=0)# list all data in historyprint(history.history.keys())# summarize history for accuracyplt.plot(history.history['acc'])plt.plot(history.history['val_acc'])plt.title('model accuracy')plt.ylabel('accuracy')plt.xlabel('epoch')plt.legend(['train', 'test'], loc='upper left')plt.show()# summarize history for lossplt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.title('model loss')plt.ylabel('loss')plt.xlabel('epoch')plt.legend(['train', 'test'], loc='upper left')plt.show() Test accuracy: 82.7751% dict_keys([&#39;val_loss&#39;, &#39;val_acc&#39;, &#39;loss&#39;, &#39;acc&#39;]) 【练习】使用模型测试狗的品种实现一个函数，它的输入为图像路径，功能为预测对应图像的类别，输出为你模型预测出的狗类别（Affenpinscher, Afghan_hound 等）。 与步骤5中的模拟函数类似，你的函数应当包含如下三个步骤： 根据选定的模型载入图像特征（bottleneck features） 将图像特征输输入到你的模型中，并返回预测向量。注意，在该向量上使用 argmax 函数可以返回狗种类的序号。 使用在步骤0中定义的 dog_names 数组来返回对应的狗种类名称。 提取图像特征过程中使用到的函数可以在 extract_bottleneck_features.py 中找到。同时，他们应已在之前的代码块中被导入。根据你选定的 CNN 网络，你可以使用 extract_{network} 函数来获得对应的图像特征，其中 {network} 代表 VGG19, Resnet50, InceptionV3, 或 Xception 中的一个。 问题 9:### TODO: 写一个函数，该函数将图像的路径作为输入### 然后返回此模型所预测的狗的品种from extract_bottleneck_features import *def Resnet50_predict_breed(img_path): bottleneck_features = extract_Resnet50(path_to_tensor(img_path)) predicted_vector = model.predict(bottleneck_features) return dog_names[np.argmax(predicted_vector)] 步骤 6: 完成你的算法实现一个算法，它的输入为图像的路径，它能够区分图像是否包含一个人、狗或两者都不包含，然后： 如果从图像中检测到一只狗，返回被预测的品种。 如果从图像中检测到人，返回最相像的狗品种。 如果两者都不能在图像中检测到，输出错误提示。 我们非常欢迎你来自己编写检测图像中人类与狗的函数，你可以随意地使用上方完成的 face_detector 和 dog_detector 函数。你需要在步骤5使用你的CNN来预测狗品种。 下面提供了算法的示例输出，但你可以自由地设计自己的模型！ 问题 10:在下方代码块中完成你的代码。 ### TODO: 设计你的算法### 自由地使用所需的代码单元数吧def Predict(path): image = cv2.imread(path) cv = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) plt.imshow(cv) plt.show() if face_detector(path) &gt; 0: print('you look like a ...' + Resnet50_predict_breed(path)) elif dog_detector(path) == True: print('you are a &#123;&#125; dog'.format(Resnet50_predict_breed(path))) else: print(\"sorry\") 步骤 7: 测试你的算法在这个部分中，你将尝试一下你的新算法！算法认为你看起来像什么类型的狗？如果你有一只狗，它可以准确地预测你的狗的品种吗？如果你有一只猫，它会将你的猫误判为一只狗吗？ 问题 11:在下方编写代码，用至少6张现实中的图片来测试你的算法。你可以使用任意照片，不过请至少使用两张人类图片（要征得当事人同意哦）和两张狗的图片。同时请回答如下问题： 输出结果比你预想的要好吗 :) ？或者更糟 :( ？：输出结果比我预想的要好 提出至少三点改进你的模型的想法。添加dropout防止过拟合 减少或增加隐藏层层数 压缩input时图像的像素 ## TODO: 在你的电脑上，在步骤6中，至少在6张图片上运行你的算法。## 自由地使用所需的代码单元数吧import osjpg = os.listdir('images')for i in jpg[:6]: Predict('images/'+i) you are a train/130.Welsh_springer_spaniel dog you are a train/009.American_water_spaniel dog you look like a ...train/037.Brittany you are a train/055.Curly-coated_retriever dog you are a train/096.Labrador_retriever dog sorry 注意: 当你写完了所有的代码，并且回答了所有的问题。你就可以把你的 iPython Notebook 导出成 HTML 文件。你可以在菜单栏，这样导出File -&gt; Download as -&gt; HTML (.html)把这个 HTML 和这个 iPython notebook 一起做为你的作业提交。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://jinyaxuan.github.io/categories/深度学习/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://jinyaxuan.github.io/tags/Deep-Learning/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://jinyaxuan.github.io/categories/深度学习/"}]},{"title":"PredictYourCuisine","slug":"PredictYourCuisine","date":"2019-02-15T13:06:53.000Z","updated":"2019-02-16T11:18:20.791Z","comments":false,"path":"2019/02/15/PredictYourCuisine/","link":"","permalink":"http://jinyaxuan.github.io/2019/02/15/PredictYourCuisine/","excerpt":"","text":"机器学习工程师纳米学位项目 0: 预测你的下一道世界料理欢迎来到机器学习的预测烹饪菜系项目！在此文件中，有些示例代码已经提供给你，但你还需要实现更多的功能来让项目成功运行。除非有明确要求，你无须修改任何已给出的代码。以编程练习开始的标题表示接下来的内容中有需要你必须实现的功能。每一部分都会有详细的指导，需要实现的部分也会在注释中以TODO标出。请仔细阅读所有的提示！ 实验任务：给定佐料名称，预测菜品所属的菜系。 实验步骤：菜品数据载入；佐料名称预处理，并预览数据集结构；载入逻辑回归模型，并训练；结果测试并提交，查看实验分数。 提示：Code 和 Markdown 区域可通过 Shift + Enter 快捷键运行。此外，Markdown可以通过双击进入编辑模式。 第一步. 下载并导入数据在这个项目中，你将利用Yummly所提供的数据集来训练和测试一个模型，并对模型的性能和预测能力进行测试。通过该数据训练后的好的模型可以被用来对菜系进行预测。 此项目的数据集来自Kaggle What’s Cooking 竞赛。共 39774/9944 个训练和测试数据点，涵盖了中国菜、越南菜、法国菜等的信息。数据集包含以下特征： ‘id’：24717, 数据编号 ‘cuisine’：”indian”, 菜系 ‘ingredients’：[“tumeric”, “vegetable stock”, …] 此菜所包含的佐料 首先你需要前往此 菜系数据集 下载(选择 Download All )。如果不能正常下载，请参考教室中的下载教程。然后运行下面区域的代码以载入数据集，以及一些此项目所需的 Python 库。如果成功返回数据集的大小，表示数据集已载入成功。 1.1 配置环境首先按照本目录中README.md文件中的第一部分内容，配置实验开发环境和所需库函数。 1.2 加载数据其次，在下载完实验数据集后，我们将其解压至当前目录中(即：MLND-cn-trial\\目录下面)， 然后依次输入以下代码，加载本次实验的训练集和测试集。 ## 请不要修改下方代码# 导入依赖库import jsonimport codecsimport pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline# 加载数据集train_filename='all/train.json'train_content = pd.read_json(codecs.open(train_filename, mode='r', encoding='utf-8'))test_filename = 'all/test.json'test_content = pd.read_json(codecs.open(test_filename, mode='r', encoding='utf-8')) # 打印加载的数据集数量print(\"菜名数据集一共包含 &#123;&#125; 训练数据 和 &#123;&#125; 测试样例。\\n\".format(len(train_content), len(test_content)))if len(train_content)==39774 and len(test_content)==9944: print(\"数据成功载入！\")else: print(\"数据载入有问题，请检查文件路径！\") 菜名数据集一共包含 39774 训练数据 和 9944 测试样例。 数据成功载入！ 1.3 数据预览为了查看我们的数据集的分布和菜品总共的种类，我们打印出部分数据样例。 ## 请不要修改下方代码pd.set_option('display.max_colwidth',120) 编程练习你需要通过head()函数来预览训练集train_content数据。（输出前5条） ### TODO：打印train_content中前5个数据样例以预览数据train_content['cuisine'] 0 greek 1 southern_us 2 filipino 3 indian 4 indian 5 jamaican 6 spanish 7 italian 8 mexican 9 italian 10 italian 11 chinese 12 italian 13 mexican 14 italian 15 indian 16 british 17 italian 18 thai 19 vietnamese 20 thai 21 mexican 22 southern_us 23 chinese 24 italian 25 chinese 26 cajun_creole 27 italian 28 chinese 29 mexican ... 39744 greek 39745 spanish 39746 indian 39747 moroccan 39748 italian 39749 mexican 39750 mexican 39751 moroccan 39752 southern_us 39753 italian 39754 vietnamese 39755 indian 39756 mexican 39757 greek 39758 greek 39759 korean 39760 southern_us 39761 chinese 39762 indian 39763 italian 39764 mexican 39765 indian 39766 irish 39767 italian 39768 mexican 39769 irish 39770 italian 39771 irish 39772 chinese 39773 mexican Name: cuisine, Length: 39774, dtype: object ## 请不要修改下方代码## 查看总共菜品分类categories=np.unique(train_content['cuisine'])print(\"一共包含 &#123;&#125; 种菜品，分别是:\\n&#123;&#125;\".format(len(categories),categories)) 一共包含 20 种菜品，分别是: [&#39;brazilian&#39; &#39;british&#39; &#39;cajun_creole&#39; &#39;chinese&#39; &#39;filipino&#39; &#39;french&#39; &#39;greek&#39; &#39;indian&#39; &#39;irish&#39; &#39;italian&#39; &#39;jamaican&#39; &#39;japanese&#39; &#39;korean&#39; &#39;mexican&#39; &#39;moroccan&#39; &#39;russian&#39; &#39;southern_us&#39; &#39;spanish&#39; &#39;thai&#39; &#39;vietnamese&#39;] 第二步. 分析数据在项目的第二个部分，你会对菜肴数据进行初步的观察并给出你的分析。通过对数据的探索来熟悉数据可以让你更好地理解和解释你的结果。 由于这个项目的最终目标是建立一个预测世界菜系的模型，我们需要将数据集分为特征(Features)和目标变量(Target Variables)。 特征: &#39;ingredients&#39;，给我们提供了每个菜品所包含的佐料名称。 目标变量：&#39;cuisine&#39;，是我们希望预测的菜系分类。 他们分别被存在 train_ingredients 和 train_targets 两个变量名中。 编程练习：数据提取 将train_content中的ingredients赋值到train_integredients 将train_content中的cuisine赋值到train_targets ### TODO：将特征与目标变量分别赋值train_ingredients = train_content['ingredients']train_targets = train_content['cuisine']### TODO: 打印结果，检查是否正确赋值display(train_ingredients)display(train_targets) 0 [romaine lettuce, black olives, grape tomatoes, garlic, pepper, purple onion, seasoning, garbanzo beans, feta cheese... 1 [plain flour, ground pepper, salt, tomatoes, ground black pepper, thyme, eggs, green tomatoes, yellow corn meal, mil... 2 [eggs, pepper, salt, mayonaise, cooking oil, green chilies, grilled chicken breasts, garlic powder, yellow onion, so... 3 [water, vegetable oil, wheat, salt] 4 [black pepper, shallots, cornflour, cayenne pepper, onions, garlic paste, milk, butter, salt, lemon juice, water, ch... 5 [plain flour, sugar, butter, eggs, fresh ginger root, salt, ground cinnamon, milk, vanilla extract, ground ginger, p... 6 [olive oil, salt, medium shrimp, pepper, garlic, chopped cilantro, jalapeno chilies, flat leaf parsley, skirt steak,... 7 [sugar, pistachio nuts, white almond bark, flour, vanilla extract, olive oil, almond extract, eggs, baking powder, d... 8 [olive oil, purple onion, fresh pineapple, pork, poblano peppers, corn tortillas, cheddar cheese, ground black peppe... 9 [chopped tomatoes, fresh basil, garlic, extra-virgin olive oil, kosher salt, flat leaf parsley] 10 [pimentos, sweet pepper, dried oregano, olive oil, garlic, sharp cheddar cheese, pepper, swiss cheese, provolone che... 11 [low sodium soy sauce, fresh ginger, dry mustard, green beans, white pepper, sesame oil, scallions, canola oil, suga... 12 [Italian parsley leaves, walnuts, hot red pepper flakes, extra-virgin olive oil, fresh lemon juice, trout fillet, ga... 13 [ground cinnamon, fresh cilantro, chili powder, ground coriander, kosher salt, ground black pepper, garlic, plum tom... 14 [fresh parmesan cheese, butter, all-purpose flour, fat free less sodium chicken broth, chopped fresh chives, gruyere... 15 [tumeric, vegetable stock, tomatoes, garam masala, naan, red lentils, red chili peppers, onions, spinach, sweet pota... 16 [greek yogurt, lemon curd, confectioners sugar, raspberries] 17 [italian seasoning, broiler-fryer chicken, mayonaise, zesty italian dressing] 18 [sugar, hot chili, asian fish sauce, lime juice] 19 [soy sauce, vegetable oil, red bell pepper, chicken broth, yellow squash, garlic chili sauce, sliced green onions, b... 20 [pork loin, roasted peanuts, chopped cilantro fresh, hoisin sauce, creamy peanut butter, chopped fresh mint, thai ba... 21 [roma tomatoes, kosher salt, purple onion, jalapeno chilies, lime, chopped cilantro] 22 [low-fat mayonnaise, pepper, salt, baking potatoes, eggs, spicy brown mustard] 23 [sesame seeds, red pepper, yellow peppers, water, extra firm tofu, broccoli, soy sauce, orange bell pepper, arrowroo... 24 [marinara sauce, flat leaf parsley, olive oil, linguine, capers, crushed red pepper flakes, olives, lemon zest, garlic] 25 [sugar, lo mein noodles, salt, chicken broth, light soy sauce, flank steak, beansprouts, dried black mushrooms, pepp... 26 [herbs, lemon juice, fresh tomatoes, paprika, mango, stock, chile pepper, onions, red chili peppers, oil] 27 [ground black pepper, butter, sliced mushrooms, sherry, salt, grated parmesan cheese, heavy cream, spaghetti, chicke... 28 [green bell pepper, egg roll wrappers, sweet and sour sauce, corn starch, molasses, vegetable oil, oil, soy sauce, s... 29 [flour tortillas, cheese, breakfast sausages, large eggs] ... 39744 [extra-virgin olive oil, oregano, potatoes, garlic cloves, pepper, salt, yellow mustard, fresh lemon juice] 39745 [quinoa, extra-virgin olive oil, fresh thyme leaves, scallion greens] 39746 [clove, bay leaves, ginger, chopped cilantro, ground turmeric, white onion, cinnamon, cardamom pods, serrano chile, ... 39747 [water, sugar, grated lemon zest, butter, pitted date, blanched almonds] 39748 [sea salt, pizza doughs, all-purpose flour, cornmeal, extra-virgin olive oil, shredded mozzarella cheese, kosher sal... 39749 [kosher salt, minced onion, tortilla chips, sugar, tomato juice, cilantro leaves, avocado, lime juice, roma tomatoes... 39750 [ground black pepper, chicken breasts, salsa, cheddar cheese, pepper jack, heavy cream, red enchilada sauce, unsalte... 39751 [olive oil, cayenne pepper, chopped cilantro fresh, boneless chicken skinless thigh, fine sea salt, low salt chicken... 39752 [self rising flour, milk, white sugar, butter, peaches in light syrup] 39753 [rosemary sprigs, lemon zest, garlic cloves, ground black pepper, vegetable broth, fresh basil leaves, minced garlic... 39754 [jasmine rice, bay leaves, sticky rice, rotisserie chicken, chopped cilantro, large eggs, vegetable oil, yellow onio... 39755 [mint leaves, cilantro leaves, ghee, tomatoes, cinnamon, oil, basmati rice, garlic paste, salt, coconut milk, clove,... 39756 [vegetable oil, cinnamon sticks, water, all-purpose flour, piloncillo, salt, orange zest, baking powder, hot water] 39757 [red bell pepper, garlic cloves, extra-virgin olive oil, feta cheese crumbles] 39758 [milk, salt, ground cayenne pepper, ground lamb, ground cinnamon, ground black pepper, pomegranate, chopped fresh mi... 39759 [red chili peppers, sea salt, onions, water, chilli bean sauce, caster sugar, garlic, white vinegar, chili oil, cucu... 39760 [butter, large eggs, cornmeal, baking powder, boiling water, milk, salt] 39761 [honey, chicken breast halves, cilantro leaves, carrots, soy sauce, Sriracha, wonton wrappers, freshly ground pepper... 39762 [curry powder, salt, chicken, water, vegetable oil, basmati rice, eggs, finely chopped onion, lemon juice, pepper, m... 39763 [fettuccine pasta, low-fat cream cheese, garlic, nonfat evaporated milk, grated parmesan cheese, corn starch, nonfat... 39764 [chili powder, worcestershire sauce, celery, red kidney beans, lean ground beef, stewed tomatoes, dried parsley, pep... 39765 [coconut, unsweetened coconut milk, mint leaves, plain yogurt] 39766 [rutabaga, ham, thick-cut bacon, potatoes, fresh parsley, salt, onions, pepper, carrots, pork sausages] 39767 [low-fat sour cream, grated parmesan cheese, salt, dried oregano, low-fat cottage cheese, butter, onions, olive oil,... 39768 [shredded cheddar cheese, crushed cheese crackers, cheddar cheese soup, cream of chicken soup, hot sauce, diced gree... 39769 [light brown sugar, granulated sugar, butter, warm water, large eggs, all-purpose flour, whole wheat flour, cooking ... 39770 [KRAFT Zesty Italian Dressing, purple onion, broccoli florets, rotini, pitted black olives, Kraft Grated Parmesan Ch... 39771 [eggs, citrus fruit, raisins, sourdough starter, flour, hot tea, sugar, ground nutmeg, salt, ground cinnamon, milk, ... 39772 [boneless chicken skinless thigh, minced garlic, steamed white rice, baking powder, corn starch, dark soy sauce, kos... 39773 [green chile, jalapeno chilies, onions, ground black pepper, salt, chopped cilantro fresh, green bell pepper, garlic... Name: ingredients, Length: 39774, dtype: object 0 greek 1 southern_us 2 filipino 3 indian 4 indian 5 jamaican 6 spanish 7 italian 8 mexican 9 italian 10 italian 11 chinese 12 italian 13 mexican 14 italian 15 indian 16 british 17 italian 18 thai 19 vietnamese 20 thai 21 mexican 22 southern_us 23 chinese 24 italian 25 chinese 26 cajun_creole 27 italian 28 chinese 29 mexican ... 39744 greek 39745 spanish 39746 indian 39747 moroccan 39748 italian 39749 mexican 39750 mexican 39751 moroccan 39752 southern_us 39753 italian 39754 vietnamese 39755 indian 39756 mexican 39757 greek 39758 greek 39759 korean 39760 southern_us 39761 chinese 39762 indian 39763 italian 39764 mexican 39765 indian 39766 irish 39767 italian 39768 mexican 39769 irish 39770 italian 39771 irish 39772 chinese 39773 mexican Name: cuisine, Length: 39774, dtype: object 编程练习：基础统计运算你的第一个编程练习是计算有关菜系佐料的统计数据。我们已为你导入了 numpy，你需要使用这个库来执行必要的计算。这些统计数据对于分析模型的预测结果非常重要的。在下面的代码中，你要做的是： 使用最频繁的佐料前10分别有哪些？ 意大利菜中最常见的10个佐料有哪些？ ## TODO: 统计佐料出现次数，并赋值到sum_ingredients字典中sum_ingredients = &#123;&#125;for i in train_content['ingredients']: for a in i: if a not in sum_ingredients: sum_ingredients[a] = 1 elif a in sum_ingredients: sum_ingredients[a] += 1sum_ingredients {&#39;sugar&#39;: 760, &#39;pistachio nuts&#39;: 7, &#39;white almond bark&#39;: 1, &#39;flour&#39;: 142, &#39;vanilla extract&#39;: 219, &#39;olive oil&#39;: 3111, &#39;almond extract&#39;: 56, &#39;eggs&#39;: 627, &#39;baking powder&#39;: 186, &#39;dried cranberries&#39;: 8, &#39;chopped tomatoes&#39;: 37, &#39;fresh basil&#39;: 787, &#39;garlic&#39;: 1471, &#39;extra-virgin olive oil&#39;: 1362, &#39;kosher salt&#39;: 656, &#39;flat leaf parsley&#39;: 588, &#39;pimentos&#39;: 16, &#39;sweet pepper&#39;: 7, &#39;dried oregano&#39;: 626, &#39;sharp cheddar cheese&#39;: 9, &#39;pepper&#39;: 965, &#39;swiss cheese&#39;: 7, &#39;provolone cheese&#39;: 138, &#39;canola oil&#39;: 41, &#39;mushrooms&#39;: 184, &#39;black olives&#39;: 67, &#39;sausages&#39;: 58, &#39;Italian parsley leaves&#39;: 74, &#39;walnuts&#39;: 38, &#39;hot red pepper flakes&#39;: 76, &#39;fresh lemon juice&#39;: 471, &#39;trout fillet&#39;: 3, &#39;garlic cloves&#39;: 1619, &#39;chipotle chile&#39;: 2, &#39;fine sea salt&#39;: 77, &#39;fresh parmesan cheese&#39;: 251, &#39;butter&#39;: 1030, &#39;all-purpose flour&#39;: 918, &#39;fat free less sodium chicken broth&#39;: 158, &#39;chopped fresh chives&#39;: 68, &#39;gruyere cheese&#39;: 18, &#39;ground black pepper&#39;: 1444, &#39;bacon slices&#39;: 48, &#39;gnocchi&#39;: 41, &#39;fat free milk&#39;: 42, &#39;cooking spray&#39;: 491, &#39;salt&#39;: 3454, &#39;italian seasoning&#39;: 347, &#39;broiler-fryer chicken&#39;: 1, &#39;mayonaise&#39;: 63, &#39;zesty italian dressing&#39;: 11, &#39;marinara sauce&#39;: 222, &#39;linguine&#39;: 193, &#39;capers&#39;: 306, &#39;crushed red pepper flakes&#39;: 179, &#39;olives&#39;: 29, &#39;lemon zest&#39;: 98, &#39;sliced mushrooms&#39;: 131, &#39;sherry&#39;: 13, &#39;grated parmesan cheese&#39;: 1580, &#39;heavy cream&#39;: 300, &#39;spaghetti&#39;: 296, &#39;chicken broth&#39;: 245, &#39;cooked chicken&#39;: 33, &#39;yellow corn meal&#39;: 64, &#39;boiling water&#39;: 63, &#39;sea salt&#39;: 202, &#39;onions&#39;: 1240, &#39;crushed garlic&#39;: 20, &#39;green onions&#39;: 144, &#39;white sugar&#39;: 231, &#39;dried basil&#39;: 425, &#39;diced tomatoes&#39;: 429, &#39;bread slices&#39;: 15, &#39;great northern beans&#39;: 21, &#39;shrimp&#39;: 59, &#39;sage leaves&#39;: 89, &#39;Oscar Mayer Deli Fresh Smoked Ham&#39;: 1, &#39;hoagie rolls&#39;: 8, &#39;salami&#39;: 41, &#39;giardiniera&#39;: 5, &#39;mozzarella cheese&#39;: 396, &#39;pepperoni&#39;: 48, &#39;bay leaves&#39;: 107, &#39;crushed red pepper&#39;: 418, &#39;mussels&#39;: 38, &#39;basil&#39;: 174, &#39;black pepper&#39;: 636, &#39;dry white wine&#39;: 658, &#39;tomatoes&#39;: 601, &#39;finely chopped onion&#39;: 145, &#39;lemon&#39;: 236, &#39;pesto&#39;: 113, &#39;salmon fillets&#39;: 11, &#39;white wine&#39;: 176, &#39;pizza crust&#39;: 36, &#39;plum tomatoes&#39;: 340, &#39;part-skim mozzarella cheese&#39;: 201, &#39;crushed tomatoes&#39;: 241, &#39;fresh rosemary&#39;: 292, &#39;boneless pork loin&#39;: 9, &#39;pappardelle&#39;: 11, &#39;red pepper&#39;: 49, &#39;Italian bread&#39;: 89, &#39;balsamic vinegar&#39;: 348, &#39;sausage casings&#39;: 78, &#39;honey&#39;: 126, &#39;shredded mozzarella cheese&#39;: 413, &#39;roasted red peppers&#39;: 114, &#39;penne pasta&#39;: 146, &#39;spinach&#39;: 119, &#39;asiago&#39;: 72, &#39;whole wheat pasta&#39;: 13, &#39;sweet onion&#39;: 69, &#39;grape tomatoes&#39;: 98, &#39;chestnuts&#39;: 9, &#39;granulated sugar&#39;: 82, &#39;whole milk ricotta cheese&#39;: 47, &#39;coffee ice cream&#39;: 3, &#39;large eggs&#39;: 625, &#39;mascarpone&#39;: 124, &#39;rum&#39;: 12, &#39;powdered sugar&#39;: 69, &#39;semisweet chocolate&#39;: 46, &#39;chestnut flour&#39;: 1, &#39;starchy potatoes&#39;: 2, &#39;grated nutmeg&#39;: 64, &#39;blood orange&#39;: 5, &#39;freshly ground pepper&#39;: 316, &#39;fennel bulb&#39;: 103, &#39;low salt chicken broth&#39;: 138, &#39;dijon mustard&#39;: 99, &#39;corn starch&#39;: 83, &#39;white wine vinegar&#39;: 73, &#39;tomato sauce&#39;: 357, &#39;shredded carrots&#39;: 11, &#39;english muffins, split and toasted&#39;: 2, &#39;chopped onion&#39;: 327, &#39;vegetable oil cooking spray&#39;: 73, &#39;chopped green bell pepper&#39;: 39, &#39;cheddar cheese&#39;: 18, &#39;lasagna noodles&#39;: 196, &#39;ranch dressing&#39;: 5, &#39;evaporated milk&#39;: 13, &#39;fresh parsley&#39;: 631, &#39;fresh oregano&#39;: 209, &#39;cold water&#39;: 58, &#39;chocolate morsels&#39;: 4, &#39;cream sweeten whip&#39;: 3, &#39;instant espresso granules&#39;: 4, &#39;whipping cream&#39;: 162, &#39;kahlúa&#39;: 12, &#39;chocolate covered coffee beans&#39;: 1, &#39;unflavored gelatin&#39;: 48, &#39;pound cake&#39;: 7, &#39;pinenuts&#39;: 252, &#39;zucchini&#39;: 326, &#39;baby carrots&#39;: 15, &#39;fresh basil leaves&#39;: 352, &#39;asparagus spears&#39;: 24, &#39;white onion&#39;: 48, &#39;carrots&#39;: 379, &#39;frozen peas&#39;: 67, &#39;arborio rice&#39;: 261, &#39;yellow crookneck squash&#39;: 3, &#39;fresh leav spinach&#39;: 25, &#39;cheese tortellini&#39;: 53, &#39;cherry tomatoes&#39;: 170, &#39;navy beans&#39;: 8, &#39;pecorino romano cheese&#39;: 146, &#39;fresh fava bean&#39;: 12, &#39;italian sausage&#39;: 129, &#39;large garlic cloves&#39;: 293, &#39;pasta sheets&#39;: 9, &#39;water&#39;: 1052, &#39;Turkish bay leaves&#39;: 3, &#39;dried chickpeas&#39;: 1, &#39;celery ribs&#39;: 130, &#39;semolina&#39;: 14, &#39;warm water&#39;: 182, &#39;vine ripened tomatoes&#39;: 12, &#39;bittersweet chocolate&#39;: 42, &#39;fat free yogurt&#39;: 4, &#39;skim milk&#39;: 16, &#39;angel food cake&#39;: 4, &#39;unsweetened cocoa powder&#39;: 84, &#39;instant espresso&#39;: 7, &#39;garlic salt&#39;: 61, &#39;tomato paste&#39;: 376, &#39;veal cutlets&#39;: 18, &#39;broccoli rabe&#39;: 47, &#39;whole milk&#39;: 157, &#39;parmigiano reggiano cheese&#39;: 306, &#39;dry bread crumbs&#39;: 115, &#39;fontina&#39;: 27, &#39;unsalted butter&#39;: 564, &#39;pasta sauce&#39;: 246, &#39;olive oil flavored cooking spray&#39;: 29, &#39;frozen chopped spinach&#39;: 131, &#39;large egg whites&#39;: 138, &#39;part-skim ricotta cheese&#39;: 111, &#39;manicotti shells&#39;: 18, &#39;fettucine&#39;: 136, &#39;parmesan cheese&#39;: 474, &#39;large shrimp&#39;: 77, &#39;chicken bouillon&#39;: 10, &#39;cream of tartar&#39;: 9, &#39;orange liqueur&#39;: 15, &#39;heavy whipping cream&#39;: 73, &#39;semi-sweet chocolate morsels&#39;: 21, &#39;cake flour&#39;: 24, &#39;confectioners sugar&#39;: 94, &#39;ground cinnamon&#39;: 85, &#39;ricotta cheese&#39;: 333, &#39;cream cheese&#39;: 105, &#39;dried porcini mushrooms&#39;: 81, &#39;chopped fresh thyme&#39;: 151, &#39;beef rib short&#39;: 5, &#39;dry red wine&#39;: 131, &#39;hot water&#39;: 85, &#39;fat free less sodium beef broth&#39;: 7, &#39;cremini mushrooms&#39;: 67, &#39;pitted kalamata olives&#39;: 94, &#39;cauliflower&#39;: 34, &#39;whole peeled tomatoes&#39;: 60, &#39;swiss chard&#39;: 63, &#39;banana squash&#39;: 1, &#39;vegetable broth&#39;: 81, &#39;bay leaf&#39;: 130, &#39;cannellini beans&#39;: 161, &#39;boneless skinless chicken breast halves&#39;: 182, &#39;light sour cream&#39;: 5, &#39;fava beans&#39;: 13, &#39;finely chopped fresh parsley&#39;: 33, &#39;fresh tarragon&#39;: 33, &#39;grated lemon zest&#39;: 142, &#39;chopped almonds&#39;: 12, &#39;broccoli&#39;: 64, &#39;potatoes&#39;: 59, &#39;self rising flour&#39;: 1, &#39;eggplant&#39;: 204, &#39;chicken&#39;: 84, &#39;pitas&#39;: 5, &#39;goat cheese&#39;: 92, &#39;reduced fat milk&#39;: 29, &#39;smoked trout&#39;: 1, &#39;fresh dill&#39;: 16, &#39;asparagus&#39;: 123, &#39;oil&#39;: 92, &#39;ricotta salata&#39;: 22, &#39;pecorino cheese&#39;: 52, &#39;grana padano&#39;: 7, &#39;pasta&#39;: 255, &#39;cracked black pepper&#39;: 117, &#39;green bell pepper&#39;: 163, &#39;vidalia onion&#39;: 19, &#39;smoked gouda&#39;: 14, &#39;baby spinach leaves&#39;: 39, &#39;shredded sharp cheddar cheese&#39;: 17, &#39;small red potato&#39;: 13, &#39;Bertolli® Classico Olive Oil&#39;: 16, &#39;bacon, crisp-cooked and crumbled&#39;: 2, &#39;bertolli vineyard premium collect marinara with burgundi wine sauc&#39;: 5, &#39;bread crumb fresh&#39;: 74, &#39;( oz.) tomato sauce&#39;: 7, &#39;ground veal&#39;: 35, &#39;italian seasoning mix&#39;: 1, &#39;beef&#39;: 22, &#39;fat skimmed chicken broth&#39;: 28, &#39;solid pack pumpkin&#39;: 4, &#39;ground nutmeg&#39;: 127, &#39;white rice&#39;: 11, &#39;ground pepper&#39;: 77, &#39;fresh thyme leaves&#39;: 53, &#39;pistachios&#39;: 10, &#39;dried cherry&#39;: 6, &#39;prosciutto&#39;: 256, &#39;romano cheese&#39;: 89, &#39;parsley leaves&#39;: 23, &#39;shallots&#39;: 316, &#39;fresh raspberries&#39;: 14, &#39;dry vermouth&#39;: 18, &#39;canned low sodium chicken broth&#39;: 46, &#39;chicken livers&#39;: 14, &#39;raisins&#39;: 39, &#39;seasoned bread crumbs&#39;: 71, &#39;minced garlic&#39;: 334, &#39;half &amp; half&#39;: 89, &#39;ground beef&#39;: 237, &#39;jack cheese&#39;: 9, &#39;vegetables&#39;: 29, &#39;lemon juice&#39;: 216, &#39;ricotta&#39;: 95, &#39;yellow onion&#39;: 153, &#39;reduced sodium chicken broth&#39;: 36, &#39;chopped fresh mint&#39;: 54, &#39;purple onion&#39;: 350, &#39;low sodium chicken broth&#39;: 78, &#39;polenta&#39;: 132, &#39;bell pepper&#39;: 38, &#39;olive oil cooking spray&#39;: 13, &#39;bacon&#39;: 124, &#39;egg yolks&#39;: 82, &#39;duck breast halves&#39;: 1, &#39;fennel seeds&#39;: 114, &#39;roma tomatoes&#39;: 44, &#39;pesto sauce&#39;: 26, &#39;arugula&#39;: 125, &#39;summer squash&#39;: 9, &#39;red wine vinegar&#39;: 227, &#39;ciabatta&#39;: 21, &#39;juice&#39;: 72, &#39;fresh herbs&#39;: 14, &#39;grated lemon peel&#39;: 89, &#39;dough&#39;: 46, &#39;coarse sea salt&#39;: 17, &#39;rosemary leaves&#39;: 10, &#39;red pepper flakes&#39;: 206, &#39;brie cheese&#39;: 14, &#39;cheese ravioli&#39;: 18, &#39;Italian seasoned breadcrumbs&#39;: 23, &#39;cheese&#39;: 127, &#39;parsley&#39;: 95, &#39;fat-free cottage cheese&#39;: 11, &#39;oven-ready lasagna noodles&#39;: 43, &#39;margarine&#39;: 38, &#39;radicchio&#39;: 40, &#39;garbanzo beans&#39;: 18, &#39;orzo pasta&#39;: 16, &#39;rubbed sage&#39;: 17, &#39;dried rosemary&#39;: 96, &#39;canned beef broth&#39;: 4, &#39;kale leaves&#39;: 7, &#39;chicken noodle soup&#39;: 1, &#39;italian style rolls&#39;: 1, &#39;genoa salami&#39;: 22, &#39;oregano&#39;: 114, &#39;boiled ham&#39;: 1, &#39;capicola&#39;: 4, &#39;iceberg lettuce&#39;: 6, &#39;jalapeno chilies&#39;: 20, &#39;diced celery&#39;: 15, &#39;italian salad dressing mix&#39;: 7, &#39;chopped cilantro fresh&#39;: 25, &#39;cider vinegar&#39;: 14, &#39;red bell pepper&#39;: 354, &#39;sliced green onions&#39;: 40, &#39;barbecue sauce&#39;: 9, &#39;prepared pizza crust&#39;: 2, &#39;boneless skinless chicken breasts&#39;: 132, &#39;Sargento® Traditional Cut Shredded Mozzarella Cheese&#39;: 1, &#39;vegetable oil&#39;: 253, &#39;basil pesto sauce&#39;: 33, &#39;aioli&#39;: 3, &#39;cooked shrimp&#39;: 11, &#39;mozzarella balls&#39;: 4, &#39;sun-dried tomatoes&#39;: 118, &#39;milk&#39;: 277, &#39;condensed cream of mushroom soup&#39;: 16, &#39;fettuccine pasta&#39;: 24, &#39;frozen spinach&#39;: 24, &#39;lasagne&#39;: 6, &#39;passata&#39;: 3, &#39;fat free cream cheese&#39;: 8, &#39;french bread&#39;: 54, &#39;non-fat sour cream&#39;: 14, &#39;reduced fat swiss cheese&#39;: 3, &#39;fat-free mayonnaise&#39;: 8, &#39;roasted garlic&#39;: 7, &#39;seasoning&#39;: 12, &#39;kale&#39;: 58, &#39;sundried tomato paste&#39;: 4, &#39;dried thyme&#39;: 145, &#39;porcini&#39;: 7, &#39;fresh thyme&#39;: 62, &#39;sourdough loaf&#39;: 4, &#39;crust&#39;: 3, &#39;duck fat&#39;: 1, &#39;squabs&#39;: 1, &#39;confit&#39;: 2, &#39;aged gouda&#39;: 1, &#39;soppressata&#39;: 16, &#39;artichokes&#39;: 44, &#39;anchovy fillets&#39;: 124, &#39;lemon slices&#39;: 12, &#39;chicken cutlets&#39;: 23, &#39;saffron threads&#39;: 28, &#39;orzo&#39;: 51, &#39;veal chops&#39;: 4, &#39;oil cured olives&#39;: 3, &#39;pasta rotel&#39;: 2, &#39;pasta water&#39;: 10, &#39;cooking wine&#39;: 3, &#39;crusty bread&#39;: 10, &#39;fresh tomatoes&#39;: 32, &#39;uncooked rigatoni&#39;: 6, &#39;grating cheese&#39;: 6, &#39;vegetable stock&#39;: 29, &#39;freshly grated parmesan&#39;: 87, &#39;florets&#39;: 15, &#39;sliced almonds&#39;: 37, &#39;thyme sprigs&#39;: 45, &#39;egg whites&#39;: 95, &#39;greek yogurt&#39;: 6, &#39;penne&#39;: 99, &#39;fish fillets&#39;: 10, &#39;russet potatoes&#39;: 40, &#39;crumbled gorgonzola&#39;: 29, &#39;fontina cheese&#39;: 84, &#39;spinach leaves&#39;: 25, &#39;orange&#39;: 38, &#39;lamb shanks&#39;: 11, &#39;clove&#39;: 19, &#39;rosemary sprigs&#39;: 50, &#39;italian eggplant&#39;: 7, &#39;Sicilian olives&#39;: 7, &#39;mint sprigs&#39;: 28, &#39;peasant bread&#39;: 7, &#39;garlic powder&#39;: 252, &#39;Kraft Grated Parmesan Cheese&#39;: 10, &#39;peas&#39;: 42, &#39;Oscar Mayer Bacon&#39;: 1, &#39;Philadelphia Cream Cheese&#39;: 9, &#39;garnish&#39;: 3, &#39;dried navy beans&#39;: 5, &#39;celery&#39;: 155, &#39;diced onions&#39;: 42, &#39;wheat berries&#39;: 4, &#39;parsley sprigs&#39;: 38, &#39;thyme&#39;: 51, &#39;polenta prepar&#39;: 5, &#39;chicken breast halves&#39;: 35, &#39;dry sherry&#39;: 22, &#39;cocoa powder&#39;: 9, &#39;dried parsley&#39;: 90, &#39;chopped garlic&#39;: 69, &#39;baking soda&#39;: 68, &#39;coarse salt&#39;: 128, &#39;rigatoni&#39;: 60, &#39;nutmeg&#39;: 40, &#39;artichoke hearts&#39;: 99, &#39;pancetta&#39;: 153, &#39;parsnips&#39;: 7, &#39;whole wheat fettuccine&#39;: 4, &#39;ground sirloin&#39;: 20, &#39;red wine&#39;: 99, &#39;fresh marjoram&#39;: 29, &#39;castellane&#39;: 2, &#39;hot Italian sausages&#39;: 39, &#39;jumbo pasta shells&#39;: 37, &#39;fresh spinach&#39;: 87, &#39;chicken breasts&#39;: 93, &#39;gluten free blend&#39;: 1, &#39;paprika&#39;: 76, &#39;salt and ground black pepper&#39;: 66, &#39;bone in skinless chicken thigh&#39;: 1, &#39;figs&#39;: 10, &#39;egg substitute&#39;: 26, &#39;hot sauce&#39;: 20, &#39;ham&#39;: 42, &#39;balsamic vinaigrette&#39;: 6, &#39;pitted olives&#39;: 15, &#39;fresh chives&#39;: 24, &#39;white mushrooms&#39;: 20, &#39;haricots verts&#39;: 4, &#39;fresh peas&#39;: 16, &#39;bow-tie pasta&#39;: 86, &#39;asparagus tips&#39;: 3, &#39;green beans&#39;: 70, &#39;low-fat cottage cheese&#39;: 14, &#39;whole wheat lasagna noodles&#39;: 8, &#39;shredded parmesan cheese&#39;: 38, &#39;apricots&#39;: 5, &#39;grappa&#39;: 9, &#39;all purpose unbleached flour&#39;: 68, &#39;dry yeast&#39;: 71, &#39;worcestershire sauce&#39;: 54, &#39;amaretti&#39;: 6, &#39;frozen strawberries&#39;: 1, &#39;strawberries&#39;: 48, &#39;cooked ham&#39;: 9, &#39;Alfredo sauce&#39;: 43, &#39;chopped fresh sage&#39;: 91, &#39;cheese slices&#39;: 11, &#39;bread&#39;: 51, &#39;morel&#39;: 2, &#39;leeks&#39;: 106, &#39;1% low-fat cottage cheese&#39;: 10, &#39;feta cheese crumbles&#39;: 68, &#39;pork belly&#39;: 2, &#39;fresh sage&#39;: 29, &#39;pork loin&#39;: 3, &#39;sliced black olives&#39;: 49, &#39;medium shrimp&#39;: 70, &#39;red chili peppers&#39;: 22, &#39;parmigiano-reggiano cheese&#39;: 53, &#39;fresh mint&#39;: 53, &#39;baguette&#39;: 119, &#39;chicken legs&#39;: 7, &#39;baby spinach&#39;: 117, &#39;white beans&#39;: 38, &#39;ground pork&#39;: 72, &#39;romana&#39;: 1, &#39;fresh mushrooms&#39;: 117, &#39;crimini mushrooms&#39;: 26, &#39;navel oranges&#39;: 10, &#39;kalamata&#39;: 62, &#39;sea scallops&#39;: 31, &#39;ladyfingers&#39;: 40, &#39;reduced fat cream cheese&#39;: 3, &#39;whipped topping&#39;: 12, &#39;wine&#39;: 15, &#39;frozen broccoli&#39;: 5, &#39;nonfat ricotta cheese&#39;: 32, &#39;shells&#39;: 8, &#39;reduced-fat cheese&#39;: 1, &#39;cornflake cereal&#39;: 2, &#39;beef brisket&#39;: 2, &#39;dry pasta&#39;: 8, &#39;white bread&#39;: 35, &#39;pork&#39;: 10, &#39;sweet italian sausage&#39;: 72, &#39;prosecco&#39;: 10, &#39;bread dough&#39;: 16, &#39;baby lima beans&#39;: 3, &#39;whipped cream&#39;: 15, &#39;hot cocoa mix&#39;: 2, &#39;brewed coffee&#39;: 18, &#39;abbamele&#39;: 1, &#39;wild mushrooms&#39;: 30, &#39;chopped walnuts&#39;: 46, &#39;fregola&#39;: 4, &#39;savoy cabbage&#39;: 10, &#39;mushroom caps&#39;: 17, &#39;pinot grigio&#39;: 3, &#39;liquid egg substitute&#39;: 1, &#39;chocolate candy bars&#39;: 5, &#39;cooked rice&#39;: 5, &#39;bread crumbs&#39;: 103, &#39;banana peppers&#39;: 5, &#39;toasted walnuts&#39;: 7, &#39;dark rum&#39;: 22, &#39;dried fig&#39;: 11, &#39;chopped parsley&#39;: 99, &#39;ground white pepper&#39;: 33, &quot;soft goat&#39;s cheese&quot;: 10, &#39;truffle oil&#39;: 17, &#39;hazelnuts&#39;: 53, &#39;veal scallopini&#39;: 2, &#39;elbow macaroni&#39;: 19, &#39;bread flour&#39;: 77, &#39;bread yeast&#39;: 1, &#39;red potato&#39;: 26, &#39;vegan parmesan cheese&#39;: 4, &#39;ahi&#39;: 1, &#39;golden brown sugar&#39;: 14, &#39;anjou pears&#39;: 3, &#39;granny smith apples&#39;: 6, &#39;crystallized ginger&#39;: 10, &#39;ice water&#39;: 7, &#39;vegetable shortening&#39;: 5, &#39;chinese five-spice powder&#39;: 1, &#39;whole wheat flour&#39;: 21, &#39;large egg yolks&#39;: 142, &#39;lean ground beef&#39;: 135, &#39;sausage links&#39;: 32, &#39;pork chops&#39;: 8, &#39;rosemary&#39;: 57, &#39;nectarines&#39;: 5, &#39;sweet cherries&#39;: 12, &#39;orange zest&#39;: 28, &#39;lavender buds&#39;: 1, &#39;apricot halves&#39;: 4, &#39;diced tomatoes in juice&#39;: 7, &#39;scallions&#39;: 61, &#39;italian salad dressing&#39;: 41, &#39;short pasta&#39;: 7, &#39;lemon wedge&#39;: 58, &#39;porterhouse steaks&#39;: 4, &#39;1% low-fat milk&#39;: 68, &#39;golden raisins&#39;: 46, &#39;cinnamon sticks&#39;: 18, &#39;broccolini&#39;: 3, &#39;salted butter&#39;: 11, &#39;chicken stock&#39;: 142, &#39;coffee granules&#39;: 12, &#39;lemon rind&#39;: 34, &#39;baby portobello mushrooms&#39;: 10, &#39;broccoli florets&#39;: 64, &#39;orecchiette&#39;: 41, &#39;melted butter&#39;: 24, &#39;pizza shells&#39;: 3, &#39;frozen mixed thawed vegetables,&#39;: 2, &#39;ragu old world style pasta sauc&#39;: 11, &#39;loosely packed fresh basil leaves&#39;: 21, &#39;whole wheat spaghetti&#39;: 22, &#39;butternut squash&#39;: 62, &#39;wonton wrappers&#39;: 17, &#39;meat&#39;: 15, &#39;pear tomatoes&#39;: 6, &#39;gaeta olives&#39;: 5, &#39;yukon gold potatoes&#39;: 39, &#39;turbot&#39;: 2, &#39;ground lamb&#39;: 6, &#39;refrigerated pizza dough&#39;: 31, &#39;potato gnocchi&#39;: 20, &#39;cream&#39;: 27, &#39;angel hair&#39;: 60, &#39;lime juice&#39;: 6, &#39;gelato&#39;: 4, &#39;cherry preserves&#39;: 2, &#39;amaretto liqueur&#39;: 4, &#39;cherries&#39;: 14, &#39;instant espresso powder&#39;: 22, &#39;nuts&#39;: 3, &#39;brown sugar&#39;: 67, &#39;pie shell&#39;: 2, &#39;marsala wine&#39;: 105, &#39;basil leaves&#39;: 153, &#39;cake&#39;: 7, &#39;crabmeat&#39;: 11, &#39;chopped fresh herbs&#39;: 8, &#39;button mushrooms&#39;: 35, &#39;escarole&#39;: 47, &#39;chopped pecans&#39;: 28, &#39;chocolate bars&#39;: 2, &#39;coffee liqueur&#39;: 11, &#39;flat anchovy&#39;: 5, &#39;italian loaf&#39;: 4, &#39;salad dressing&#39;: 23, &#39;pitted black olives&#39;: 24, &#39;rotini&#39;: 46, &#39;frozen mixed vegetables&#39;: 4, &#39;canned tomatoes&#39;: 26, &#39;feta cheese&#39;: 36, &#39;anise seed&#39;: 13, &#39;top sirloin&#39;: 1, &#39;candy&#39;: 3, &#39;angel food cake mix&#39;: 2, &#39;buttercream frosting&#39;: 1, &#39;sour cream&#39;: 71, &#39;shredded cheddar cheese&#39;: 42, &#39;cottage cheese&#39;: 44, &#39;noodles&#39;: 38, &#39;lime&#39;: 5, &#39;watermelon&#39;: 5, &#39;risotto&#39;: 11, &#39;ice cubes&#39;: 8, &#39;peeled tomatoes&#39;: 32, &#39;cooked vermicelli&#39;: 5, &#39;buns&#39;: 1, &#39;apple cider vinegar&#39;: 7, &#39;ground red pepper&#39;: 34, &#39;yellow bell pepper&#39;: 85, &#39;active dry yeast&#39;: 128, &#39;baking potatoes&#39;: 34, &#39;grated romano cheese&#39;: 19, &#39;quinoa&#39;: 8, &#39;littleneck clams&#39;: 26, &#39;whole wheat bread&#39;: 8, &#39;cream cheese, soften&#39;: 40, &#39;salad seasoning mix&#39;: 4, &#39;cucumber&#39;: 25, &#39;pappardelle pasta&#39;: 6, &#39;fresh mozzarella&#39;: 95, &#39;tuna steaks&#39;: 16, &#39;shredded zucchini&#39;: 4, &#39;dried pasta&#39;: 13, &#39;berries&#39;: 5, &#39;clams&#39;: 50, &#39;ground round&#39;: 25, &#39;fettuccine, cook and drain&#39;: 4, &#39;tomato purée&#39;: 47, &#39;chopped celery&#39;: 103, &#39;ditalini pasta&#39;: 15, &#39;lobster&#39;: 11, &#39;almonds&#39;: 35, &#39;anise&#39;: 13, &#39;anise extract&#39;: 12, &#39;brandy&#39;: 25, &#39;boneless chicken breast&#39;: 19, &#39;buffalo sauce&#39;: 2, &#39;blue cheese dressing&#39;: 3, &#39;cannelloni shells&#39;: 2, &#39;butter cooking spray&#39;: 3, &#39;light alfredo sauce&#39;: 7, &#39;rice&#39;: 12, &#39;cream of chicken soup&#39;: 9, &#39;chees fresh mozzarella&#39;: 72, &#39;shrimp tails&#39;: 2, &#39;bay scallops&#39;: 12, &#39;lump crab meat&#39;: 17, &#39;fish stock&#39;: 11, &#39;capellini&#39;: 8, &#39;veal&#39;: 17, &#39;beef stock&#39;: 24, &#39;chard&#39;: 3, &#39;grated Gruyère cheese&#39;: 5, &#39;panko&#39;: 25, &#39;chickpeas&#39;: 37, &#39;white cornmeal&#39;: 4, &#39;red bell pepper, sliced&#39;: 2, &#39;herbs&#39;: 20, &#39;chicken thighs&#39;: 32, &#39;green bell pepper, slice&#39;: 13, &#39;vanilla&#39;: 28, &#39;liqueur&#39;: 10, &#39;aged balsamic vinegar&#39;: 8, &#39;limoncello&#39;: 6, &#39;golden beets&#39;: 4, &#39;pizza doughs&#39;: 103, &#39;black cod&#39;: 2, &#39;green cabbage&#39;: 12, &#39;cavolo nero&#39;: 4, &#39;winter squash&#39;: 1, &#39;thin pizza crust&#39;: 9, &#39;toasted pine nuts&#39;: 30, &#39;grated parmesan romano&#39;: 2, &#39;cumin seed&#39;: 4, &#39;cilantro leaves&#39;: 7, &#39;seasoning salt&#39;: 15, &#39;mixed greens&#39;: 16, &#39;turkey breast cutlets&#39;: 9, &#39;cod fillets&#39;: 8, &#39;barilla&#39;: 4, &#39;linguini&#39;: 9, &#39;perciatelli&#39;: 7, &#39;crumbled blue cheese&#39;: 16, &#39;black mission figs&#39;: 4, &#39;swordfish steaks&#39;: 11, &#39;anchovy paste&#39;: 47, &#39;chuck&#39;: 8, &#39;tomatoes with juice&#39;: 30, &#39;store bought low sodium chicken stock&#39;: 2, &#39;fresh lavender&#39;: 1, &#39;grated orange&#39;: 31, &#39;vanilla wafers&#39;: 3, &#39;amaretto&#39;: 19, &#39;toasted almonds&#39;: 6, &#39;light corn syrup&#39;: 8, &#39;focaccia&#39;: 10, &#39;oyster mushrooms&#39;: 6, &#39;shiitake mushroom caps&#39;: 16, &#39;onion powder&#39;: 39, &#39;sourdough&#39;: 7, &#39;orange bell pepper&#39;: 27, &#39;nonfat cottage cheese&#39;: 6, &#39;stewed tomatoes&#39;: 37, &#39;raspberries&#39;: 25, &#39;vanilla beans&#39;: 26, &#39;Frangelico&#39;: 9, &#39;vegetable oil spray&#39;: 22, &#39;table salt&#39;: 21, &#39;white peppercorns&#39;: 2, &#39;herb vinegar&#39;: 3, &#39;reduced fat sharp cheddar cheese&#39;: 8, &#39;deli ham&#39;: 4, &#39;ground turkey&#39;: 48, &#39;hot dogs&#39;: 2, &#39;italian style stewed tomatoes&#39;: 15, &#39;veal stock&#39;: 7, &#39;portabello mushroom&#39;: 41, &#39;rocket leaves&#39;: 35, &#39;country bread&#39;: 26, &#39;bottled balsamic vinaigrette&#39;: 1, &#39;scallops&#39;: 12, &#39;italian tomatoes&#39;: 16, &#39;peeled shrimp&#39;: 3, &#39;whole wheat pizza crust&#39;: 3, &#39;dried mixed herbs&#39;: 3, &#39;whole wheat pastry flour&#39;: 6, &#39;nonstick spray&#39;: 7, &#39;low-fat sour cream&#39;: 8, &#39;day old bread&#39;: 2, &#39;champagne vinegar&#39;: 9, &#39;pizza sauce&#39;: 71, &#39;red vermouth&#39;: 5, &#39;low sodium chicken stock&#39;: 6, &#39;peppercorns&#39;: 3, &#39;turkey stock&#39;: 3, &#39;cooked turkey&#39;: 6, &#39;green olives&#39;: 52, &#39;spaghettini&#39;: 23, &#39;minced onion&#39;: 41, &#39;beef broth&#39;: 62, &#39;dried mint flakes&#39;: 3, &#39;ravioli&#39;: 16, &#39;meat loaf mix&#39;: 1, &#39;rub&#39;: 2, &#39;clam juice&#39;: 26, &#39;bottled clam juice&#39;: 15, &#39;cayenne pepper&#39;: 50, &#39;pears&#39;: 19, &#39;gorgonzola&#39;: 27, &#39;duck&#39;: 2, &#39;pure vanilla extract&#39;: 22, &#39;panettone&#39;: 4, &#39;sambuca&#39;: 4, &#39;light brown sugar&#39;: 15, &#39;fat free frozen top whip&#39;: 5, &#39;frozen bread dough&#39;: 9, &#39;whole wheat breadcrumbs&#39;: 10, &#39;ground chuck&#39;: 16, &#39;bows&#39;: 3, &#39;sauce&#39;: 53, &#39;ground oregano&#39;: 4, &#39;lean ground turkey&#39;: 2, &#39;milk chocolate&#39;: 11, &#39;hazelnut butter&#39;: 4, &#39;fig jam&#39;: 2, &#39;crackers&#39;: 7, &#39;sun-dried tomatoes in oil&#39;: 28, &#39;dark chocolate&#39;: 7, &#39;vanilla ice cream&#39;: 17, &#39;chives&#39;: 31, &#39;radishes&#39;: 13, &#39;lettuce&#39;: 6, &#39;grilled chicken&#39;: 2, &#39;flatbread&#39;: 4, &#39;fresh parsley leaves&#39;: 56, &#39;lemon extract&#39;: 7, &#39;salad greens&#39;: 11, &#39;brown rice&#39;: 6, &#39;tortellini&#39;: 19, &#39;reduced fat alfredo sauce&#39;: 4, &#39;fresh asparagus&#39;: 24, &#39;round steaks&#39;: 5, &#39;chili powder&#39;: 26, &#39;ground cumin&#39;: 30, &#39;rolls&#39;: 21, &#39;prego traditional italian sauce&#39;: 4, &#39;brown hash potato&#39;: 5, &#39;nonfat milk&#39;: 7, &#39;frozen cheese ravioli&#39;: 6, &#39;mild Italian sausage&#39;: 17, &#39;sandwich rolls&#39;: 3, &#39;orange juice concentrate&#39;: 5, &#39;rotelle&#39;: 3, &#39;sweet italian sausag links, cut into&#39;: 2, &#39;idaho potatoes&#39;: 4, &#39;popcorn&#39;: 1, &#39;grated orange peel&#39;: 26, &#39;french fried onions&#39;: 1, &#39;turbinado&#39;: 7, &#39;cooked chicken breasts&#39;: 16, &#39;farro&#39;: 14, &#39;flour tortillas&#39;: 11, &#39;english cucumber&#39;: 7, &#39;minced peperoncini&#39;: 2, &#39;teleme&#39;: 2, &#39;brine cured green olives&#39;: 4, &#39;black forest ham&#39;: 1, &#39;frozen whole kernel corn&#39;: 5, &#39;sweet potatoes&#39;: 15, &#39;applewood smoked bacon&#39;: 7, &#39;panko breadcrumbs&#39;: 24, &#39;coarse kosher salt&#39;: 9, &#39;cornmeal&#39;: 72, &#39;dried sage&#39;: 28, &#39;dri leav thyme&#39;: 6, &#39;low sodium beef broth&#39;: 3, &#39;cabbage&#39;: 12, &#39;fresh shiitake mushrooms&#39;: 15, &#39;rabbit&#39;: 8, &#39;herbes de provence&#39;: 12, &#39;dressing&#39;: 7, &#39;chicken fingers&#39;: 3, &#39;reduced-fat sour cream&#39;: 13, &#39;dried fettuccine&#39;: 14, &#39;white pepper&#39;: 22, &#39;albacore tuna in water&#39;: 5, &#39;light mayonnaise&#39;: 7, &#39;turkey tenderloins&#39;: 4, &#39;almond flour&#39;: 8, &#39;raw almond&#39;: 3, &#39;fresh orange juice&#39;: 32, &#39;cranberries&#39;: 3, &#39;orange marmalade&#39;: 6, &#39;fresh lemon&#39;: 4, &#39;condensed chicken broth&#39;: 1, &#39;oil packed anchovy fillets&#39;: 3, &#39;boneless chicken skinless thigh&#39;: 20, &#39;sugar pea&#39;: 16, &#39;nonfat yogurt&#39;: 4, &#39;roast red peppers, drain&#39;: 24, &#39;french baguette&#39;: 25, &#39;ripe olives&#39;: 26, &#39;honey glazed ham&#39;: 2, &#39;chiles&#39;: 9, &#39;spring onions&#39;: 3, &#39;candied orange peel&#39;: 13, &#39;lard&#39;: 5, &#39;cinnamon&#39;: 23, &#39;semolina flour&#39;: 32, &#39;onion salt&#39;: 2, &#39;beef demi-glace&#39;: 1, &#39;veal shanks&#39;: 21, &#39;orange peel&#39;: 7, &#39;lemon peel&#39;: 11, &#39;plain yogurt&#39;: 7, &#39;Quinoa Flour&#39;: 1, &#39;spelt flour&#39;: 2, &#39;plums&#39;: 14, &#39;heirloom tomatoes&#39;: 21, &#39;fresh lime juice&#39;: 25, &#39;artichok heart marin&#39;: 33, &#39;bucatini&#39;: 16, &#39;processed cheese&#39;: 5, &#39;egg noodles, cooked and drained&#39;: 3, &#39;tapioca flour&#39;: 1, &#39;lasagna noodles, cooked and drained&#39;: 26, &#39;firm tofu&#39;: 9, &#39;sherry vinegar&#39;: 27, &#39;country style bread&#39;: 7, &#39;teardrop tomatoes&#39;: 2, &#39;ground sausage&#39;: 5, &#39;pita pockets&#39;: 1, &#39;orange juice&#39;: 19, &#39;chili pepper&#39;: 5, &#39;currant&#39;: 7, &#39;small capers, rins and drain&#39;: 2, &#39;filet&#39;: 4, &#39;lettuce leaves&#39;: 8, &#39;vodka&#39;: 27, &#39;stolichnaya&#39;: 1, &#39;romaine lettuce&#39;: 33, &#39;croutons&#39;: 18, &#39;pepperocini&#39;: 2, &#39;cherry peppers&#39;: 3, &#39;dandelion&#39;: 1, &#39;beans&#39;: 6, &#39;hot pepper sauce&#39;: 7, &#39;shredded Monterey Jack cheese&#39;: 13, &#39;cane sugar&#39;: 1, &#39;mixed nuts&#39;: 1, &#39;meatballs&#39;: 17, &#39;plain dry bread crumb&#39;: 11, &#39;spanish onion&#39;: 9, &#39;cuban peppers&#39;: 3, &#39;green tomatoes&#39;: 2, &#39;sesame seeds&#39;: 13, &#39;boneless beef chuck roast&#39;: 3, &#39;hard-boiled egg&#39;: 6, &#39;pork tenderloin&#39;: 24, &#39;bulk italian sausag&#39;: 21, &#39;beef bouillon granules&#39;: 3, &#39;prebaked pizza crusts&#39;: 13, &#39;buttermilk&#39;: 32, &#39;flaked coconut&#39;: 5, &#39;lower sodium chicken broth&#39;: 20, &#39;carnaroli rice&#39;: 13, &#39;fresh oregano leaves&#39;: 24, &#39;cavatappi&#39;: 9, &#39;cooking oil&#39;: 24, &#39;cayenne&#39;: 8, &#39;ground cloves&#39;: 30, &#39;corn syrup&#39;: 4, &#39;small pasta&#39;: 7, &#39;ground fennel&#39;: 7, &#39;low-fat buttermilk&#39;: 9, &#39;ice&#39;: 7, &#39;chicken bouillon granules&#39;: 6, &#39;poultry seasoning&#39;: 10, &#39;roast&#39;: 3, &#39;fusilli&#39;: 43, &#39;Italian herbs&#39;: 7, &#39;diced yellow onion&#39;: 1, &#39;manicotti pasta&#39;: 4, &#39;parsley flakes&#39;: 17, &#39;vinaigrette&#39;: 7, &#39;bread ciabatta&#39;: 16, &#39;kidney beans&#39;: 12, &#39;creole seasoning&#39;: 4, &#39;prepared pasta sauce&#39;: 5, &#39;small curd cottage cheese&#39;: 14, &#39;white sandwich bread&#39;: 9, &#39;mini chocolate chips&#39;: 5, &#39;lean beef&#39;: 2, &#39;breadstick&#39;: 13, &#39;pickled okra&#39;: 2, &#39;fronds&#39;: 15, &#39;thick-cut bacon&#39;: 11, &#39;boiling potatoes&#39;: 13, &#39;ditalini&#39;: 11, &#39;cranberry beans&#39;: 4, &#39;center cut bacon&#39;: 7, &#39;roasting chickens&#39;: 8, &#39;fleur de sel&#39;: 6, &#39;Margherita Pepperoni&#39;: 1, &#39;soft-shell clams&#39;: 1, &#39;liquid&#39;: 5, &#39;fresh chevre&#39;: 4, &#39;pork sausages&#39;: 14, &#39;dried minced onion&#39;: 5, &#39;msg&#39;: 2, &#39;pork stew meat&#39;: 1, &#39;beef stew meat&#39;: 3, &#39;ziti&#39;: 26, &#39;Balsamico Bianco&#39;: 2, &#39;ground mustard&#39;: 1, ...} ## 请不要修改下方代码# Finally, plot the 10 most used ingredientsplt.style.use(u'ggplot')fig = pd.DataFrame(sum_ingredients, index=[0]).transpose()[0].sort_values(ascending=False, inplace=False)[:10].plot(kind='barh')fig.invert_yaxis()fig = fig.get_figure()fig.tight_layout()fig.show() /opt/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:448: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure. % get_backend()) ## TODO: 统计意大利菜系中佐料出现次数，并赋值到italian_ingredients字典中italian_ingredients = &#123;&#125;train_content_italian = train_content[train_content['cuisine'] == 'italian']for l in train_content_italian['ingredients']: for a in l: if a not in italian_ingredients: italian_ingredients[a] = 1 elif a in sum_italian: italian_ingredients[a] += 1italian_ingredients {&#39;sugar&#39;: 760, &#39;pistachio nuts&#39;: 7, &#39;white almond bark&#39;: 1, &#39;flour&#39;: 142, &#39;vanilla extract&#39;: 219, &#39;olive oil&#39;: 3111, &#39;almond extract&#39;: 56, &#39;eggs&#39;: 627, &#39;baking powder&#39;: 186, &#39;dried cranberries&#39;: 8, &#39;chopped tomatoes&#39;: 37, &#39;fresh basil&#39;: 787, &#39;garlic&#39;: 1471, &#39;extra-virgin olive oil&#39;: 1362, &#39;kosher salt&#39;: 656, &#39;flat leaf parsley&#39;: 588, &#39;pimentos&#39;: 16, &#39;sweet pepper&#39;: 7, &#39;dried oregano&#39;: 626, &#39;sharp cheddar cheese&#39;: 9, &#39;pepper&#39;: 965, &#39;swiss cheese&#39;: 7, &#39;provolone cheese&#39;: 138, &#39;canola oil&#39;: 41, &#39;mushrooms&#39;: 184, &#39;black olives&#39;: 67, &#39;sausages&#39;: 58, &#39;Italian parsley leaves&#39;: 74, &#39;walnuts&#39;: 38, &#39;hot red pepper flakes&#39;: 76, &#39;fresh lemon juice&#39;: 471, &#39;trout fillet&#39;: 3, &#39;garlic cloves&#39;: 1619, &#39;chipotle chile&#39;: 2, &#39;fine sea salt&#39;: 77, &#39;fresh parmesan cheese&#39;: 251, &#39;butter&#39;: 1030, &#39;all-purpose flour&#39;: 918, &#39;fat free less sodium chicken broth&#39;: 158, &#39;chopped fresh chives&#39;: 68, &#39;gruyere cheese&#39;: 18, &#39;ground black pepper&#39;: 1444, &#39;bacon slices&#39;: 48, &#39;gnocchi&#39;: 41, &#39;fat free milk&#39;: 42, &#39;cooking spray&#39;: 491, &#39;salt&#39;: 3454, &#39;italian seasoning&#39;: 347, &#39;broiler-fryer chicken&#39;: 1, &#39;mayonaise&#39;: 63, &#39;zesty italian dressing&#39;: 11, &#39;marinara sauce&#39;: 222, &#39;linguine&#39;: 193, &#39;capers&#39;: 306, &#39;crushed red pepper flakes&#39;: 179, &#39;olives&#39;: 29, &#39;lemon zest&#39;: 98, &#39;sliced mushrooms&#39;: 131, &#39;sherry&#39;: 13, &#39;grated parmesan cheese&#39;: 1580, &#39;heavy cream&#39;: 300, &#39;spaghetti&#39;: 296, &#39;chicken broth&#39;: 245, &#39;cooked chicken&#39;: 33, &#39;yellow corn meal&#39;: 64, &#39;boiling water&#39;: 63, &#39;sea salt&#39;: 202, &#39;onions&#39;: 1240, &#39;crushed garlic&#39;: 20, &#39;green onions&#39;: 144, &#39;white sugar&#39;: 231, &#39;dried basil&#39;: 425, &#39;diced tomatoes&#39;: 429, &#39;bread slices&#39;: 15, &#39;great northern beans&#39;: 21, &#39;shrimp&#39;: 59, &#39;sage leaves&#39;: 89, &#39;Oscar Mayer Deli Fresh Smoked Ham&#39;: 1, &#39;hoagie rolls&#39;: 8, &#39;salami&#39;: 41, &#39;giardiniera&#39;: 5, &#39;mozzarella cheese&#39;: 396, &#39;pepperoni&#39;: 48, &#39;bay leaves&#39;: 107, &#39;crushed red pepper&#39;: 418, &#39;mussels&#39;: 38, &#39;basil&#39;: 174, &#39;black pepper&#39;: 636, &#39;dry white wine&#39;: 658, &#39;tomatoes&#39;: 601, &#39;finely chopped onion&#39;: 145, &#39;lemon&#39;: 236, &#39;pesto&#39;: 113, &#39;salmon fillets&#39;: 11, &#39;white wine&#39;: 176, &#39;pizza crust&#39;: 36, &#39;plum tomatoes&#39;: 340, &#39;part-skim mozzarella cheese&#39;: 201, &#39;crushed tomatoes&#39;: 241, &#39;fresh rosemary&#39;: 292, &#39;boneless pork loin&#39;: 9, &#39;pappardelle&#39;: 11, &#39;red pepper&#39;: 49, &#39;Italian bread&#39;: 89, &#39;balsamic vinegar&#39;: 348, &#39;sausage casings&#39;: 78, &#39;honey&#39;: 126, &#39;shredded mozzarella cheese&#39;: 413, &#39;roasted red peppers&#39;: 114, &#39;penne pasta&#39;: 146, &#39;spinach&#39;: 119, &#39;asiago&#39;: 72, &#39;whole wheat pasta&#39;: 13, &#39;sweet onion&#39;: 69, &#39;grape tomatoes&#39;: 98, &#39;chestnuts&#39;: 9, &#39;granulated sugar&#39;: 82, &#39;whole milk ricotta cheese&#39;: 47, &#39;coffee ice cream&#39;: 3, &#39;large eggs&#39;: 625, &#39;mascarpone&#39;: 124, &#39;rum&#39;: 12, &#39;powdered sugar&#39;: 69, &#39;semisweet chocolate&#39;: 46, &#39;chestnut flour&#39;: 1, &#39;starchy potatoes&#39;: 2, &#39;grated nutmeg&#39;: 64, &#39;blood orange&#39;: 5, &#39;freshly ground pepper&#39;: 316, &#39;fennel bulb&#39;: 103, &#39;low salt chicken broth&#39;: 138, &#39;dijon mustard&#39;: 99, &#39;corn starch&#39;: 83, &#39;white wine vinegar&#39;: 73, &#39;tomato sauce&#39;: 357, &#39;shredded carrots&#39;: 11, &#39;english muffins, split and toasted&#39;: 2, &#39;chopped onion&#39;: 327, &#39;vegetable oil cooking spray&#39;: 73, &#39;chopped green bell pepper&#39;: 39, &#39;cheddar cheese&#39;: 18, &#39;lasagna noodles&#39;: 196, &#39;ranch dressing&#39;: 5, &#39;evaporated milk&#39;: 13, &#39;fresh parsley&#39;: 631, &#39;fresh oregano&#39;: 209, &#39;cold water&#39;: 58, &#39;chocolate morsels&#39;: 4, &#39;cream sweeten whip&#39;: 3, &#39;instant espresso granules&#39;: 4, &#39;whipping cream&#39;: 162, &#39;kahlúa&#39;: 12, &#39;chocolate covered coffee beans&#39;: 1, &#39;unflavored gelatin&#39;: 48, &#39;pound cake&#39;: 7, &#39;pinenuts&#39;: 252, &#39;zucchini&#39;: 326, &#39;baby carrots&#39;: 15, &#39;fresh basil leaves&#39;: 352, &#39;asparagus spears&#39;: 24, &#39;white onion&#39;: 48, &#39;carrots&#39;: 379, &#39;frozen peas&#39;: 67, &#39;arborio rice&#39;: 261, &#39;yellow crookneck squash&#39;: 3, &#39;fresh leav spinach&#39;: 25, &#39;cheese tortellini&#39;: 53, &#39;cherry tomatoes&#39;: 170, &#39;navy beans&#39;: 8, &#39;pecorino romano cheese&#39;: 146, &#39;fresh fava bean&#39;: 12, &#39;italian sausage&#39;: 129, &#39;large garlic cloves&#39;: 293, &#39;pasta sheets&#39;: 9, &#39;water&#39;: 1052, &#39;Turkish bay leaves&#39;: 3, &#39;dried chickpeas&#39;: 1, &#39;celery ribs&#39;: 130, &#39;semolina&#39;: 14, &#39;warm water&#39;: 182, &#39;vine ripened tomatoes&#39;: 12, &#39;bittersweet chocolate&#39;: 42, &#39;fat free yogurt&#39;: 4, &#39;skim milk&#39;: 16, &#39;angel food cake&#39;: 4, &#39;unsweetened cocoa powder&#39;: 84, &#39;instant espresso&#39;: 7, &#39;garlic salt&#39;: 61, &#39;tomato paste&#39;: 376, &#39;veal cutlets&#39;: 18, &#39;broccoli rabe&#39;: 47, &#39;whole milk&#39;: 157, &#39;parmigiano reggiano cheese&#39;: 306, &#39;dry bread crumbs&#39;: 115, &#39;fontina&#39;: 27, &#39;unsalted butter&#39;: 564, &#39;pasta sauce&#39;: 246, &#39;olive oil flavored cooking spray&#39;: 29, &#39;frozen chopped spinach&#39;: 131, &#39;large egg whites&#39;: 138, &#39;part-skim ricotta cheese&#39;: 111, &#39;manicotti shells&#39;: 18, &#39;fettucine&#39;: 136, &#39;parmesan cheese&#39;: 474, &#39;large shrimp&#39;: 77, &#39;chicken bouillon&#39;: 10, &#39;cream of tartar&#39;: 9, &#39;orange liqueur&#39;: 15, &#39;heavy whipping cream&#39;: 73, &#39;semi-sweet chocolate morsels&#39;: 21, &#39;cake flour&#39;: 24, &#39;confectioners sugar&#39;: 94, &#39;ground cinnamon&#39;: 85, &#39;ricotta cheese&#39;: 333, &#39;cream cheese&#39;: 105, &#39;dried porcini mushrooms&#39;: 81, &#39;chopped fresh thyme&#39;: 151, &#39;beef rib short&#39;: 5, &#39;dry red wine&#39;: 131, &#39;hot water&#39;: 85, &#39;fat free less sodium beef broth&#39;: 7, &#39;cremini mushrooms&#39;: 67, &#39;pitted kalamata olives&#39;: 94, &#39;cauliflower&#39;: 34, &#39;whole peeled tomatoes&#39;: 60, &#39;swiss chard&#39;: 63, &#39;banana squash&#39;: 1, &#39;vegetable broth&#39;: 81, &#39;bay leaf&#39;: 130, &#39;cannellini beans&#39;: 161, &#39;boneless skinless chicken breast halves&#39;: 182, &#39;light sour cream&#39;: 5, &#39;fava beans&#39;: 13, &#39;finely chopped fresh parsley&#39;: 33, &#39;fresh tarragon&#39;: 33, &#39;grated lemon zest&#39;: 142, &#39;chopped almonds&#39;: 12, &#39;broccoli&#39;: 64, &#39;potatoes&#39;: 59, &#39;self rising flour&#39;: 1, &#39;eggplant&#39;: 204, &#39;chicken&#39;: 84, &#39;pitas&#39;: 5, &#39;goat cheese&#39;: 92, &#39;reduced fat milk&#39;: 29, &#39;smoked trout&#39;: 1, &#39;fresh dill&#39;: 16, &#39;asparagus&#39;: 123, &#39;oil&#39;: 92, &#39;ricotta salata&#39;: 22, &#39;pecorino cheese&#39;: 52, &#39;grana padano&#39;: 7, &#39;pasta&#39;: 255, &#39;cracked black pepper&#39;: 117, &#39;green bell pepper&#39;: 163, &#39;vidalia onion&#39;: 19, &#39;smoked gouda&#39;: 14, &#39;baby spinach leaves&#39;: 39, &#39;shredded sharp cheddar cheese&#39;: 17, &#39;small red potato&#39;: 13, &#39;Bertolli® Classico Olive Oil&#39;: 16, &#39;bacon, crisp-cooked and crumbled&#39;: 2, &#39;bertolli vineyard premium collect marinara with burgundi wine sauc&#39;: 5, &#39;bread crumb fresh&#39;: 74, &#39;( oz.) tomato sauce&#39;: 7, &#39;ground veal&#39;: 35, &#39;italian seasoning mix&#39;: 1, &#39;beef&#39;: 22, &#39;fat skimmed chicken broth&#39;: 28, &#39;solid pack pumpkin&#39;: 4, &#39;ground nutmeg&#39;: 127, &#39;white rice&#39;: 11, &#39;ground pepper&#39;: 77, &#39;fresh thyme leaves&#39;: 53, &#39;pistachios&#39;: 10, &#39;dried cherry&#39;: 6, &#39;prosciutto&#39;: 256, &#39;romano cheese&#39;: 89, &#39;parsley leaves&#39;: 23, &#39;shallots&#39;: 316, &#39;fresh raspberries&#39;: 14, &#39;dry vermouth&#39;: 18, &#39;canned low sodium chicken broth&#39;: 46, &#39;chicken livers&#39;: 14, &#39;raisins&#39;: 39, &#39;seasoned bread crumbs&#39;: 71, &#39;minced garlic&#39;: 334, &#39;half &amp; half&#39;: 89, &#39;ground beef&#39;: 237, &#39;jack cheese&#39;: 9, &#39;vegetables&#39;: 29, &#39;lemon juice&#39;: 216, &#39;ricotta&#39;: 95, &#39;yellow onion&#39;: 153, &#39;reduced sodium chicken broth&#39;: 36, &#39;chopped fresh mint&#39;: 54, &#39;purple onion&#39;: 350, &#39;low sodium chicken broth&#39;: 78, &#39;polenta&#39;: 132, &#39;bell pepper&#39;: 38, &#39;olive oil cooking spray&#39;: 13, &#39;bacon&#39;: 124, &#39;egg yolks&#39;: 82, &#39;duck breast halves&#39;: 1, &#39;fennel seeds&#39;: 114, &#39;roma tomatoes&#39;: 44, &#39;pesto sauce&#39;: 26, &#39;arugula&#39;: 125, &#39;summer squash&#39;: 9, &#39;red wine vinegar&#39;: 227, &#39;ciabatta&#39;: 21, &#39;juice&#39;: 72, &#39;fresh herbs&#39;: 14, &#39;grated lemon peel&#39;: 89, &#39;dough&#39;: 46, &#39;coarse sea salt&#39;: 17, &#39;rosemary leaves&#39;: 10, &#39;red pepper flakes&#39;: 206, &#39;brie cheese&#39;: 14, &#39;cheese ravioli&#39;: 18, &#39;Italian seasoned breadcrumbs&#39;: 23, &#39;cheese&#39;: 127, &#39;parsley&#39;: 95, &#39;fat-free cottage cheese&#39;: 11, &#39;oven-ready lasagna noodles&#39;: 43, &#39;margarine&#39;: 38, &#39;radicchio&#39;: 40, &#39;garbanzo beans&#39;: 18, &#39;orzo pasta&#39;: 16, &#39;rubbed sage&#39;: 17, &#39;dried rosemary&#39;: 96, &#39;canned beef broth&#39;: 4, &#39;kale leaves&#39;: 7, &#39;chicken noodle soup&#39;: 1, &#39;italian style rolls&#39;: 1, &#39;genoa salami&#39;: 22, &#39;oregano&#39;: 114, &#39;boiled ham&#39;: 1, &#39;capicola&#39;: 4, &#39;iceberg lettuce&#39;: 6, &#39;jalapeno chilies&#39;: 20, &#39;diced celery&#39;: 15, &#39;italian salad dressing mix&#39;: 7, &#39;chopped cilantro fresh&#39;: 25, &#39;cider vinegar&#39;: 14, &#39;red bell pepper&#39;: 354, &#39;sliced green onions&#39;: 40, &#39;barbecue sauce&#39;: 9, &#39;prepared pizza crust&#39;: 2, &#39;boneless skinless chicken breasts&#39;: 132, &#39;Sargento® Traditional Cut Shredded Mozzarella Cheese&#39;: 1, &#39;vegetable oil&#39;: 253, &#39;basil pesto sauce&#39;: 33, &#39;aioli&#39;: 3, &#39;cooked shrimp&#39;: 11, &#39;mozzarella balls&#39;: 4, &#39;sun-dried tomatoes&#39;: 118, &#39;milk&#39;: 277, &#39;condensed cream of mushroom soup&#39;: 16, &#39;fettuccine pasta&#39;: 24, &#39;frozen spinach&#39;: 24, &#39;lasagne&#39;: 6, &#39;passata&#39;: 3, &#39;fat free cream cheese&#39;: 8, &#39;french bread&#39;: 54, &#39;non-fat sour cream&#39;: 14, &#39;reduced fat swiss cheese&#39;: 3, &#39;fat-free mayonnaise&#39;: 8, &#39;roasted garlic&#39;: 7, &#39;seasoning&#39;: 12, &#39;kale&#39;: 58, &#39;sundried tomato paste&#39;: 4, &#39;dried thyme&#39;: 145, &#39;porcini&#39;: 7, &#39;fresh thyme&#39;: 62, &#39;sourdough loaf&#39;: 4, &#39;crust&#39;: 3, &#39;duck fat&#39;: 1, &#39;squabs&#39;: 1, &#39;confit&#39;: 2, &#39;aged gouda&#39;: 1, &#39;soppressata&#39;: 16, &#39;artichokes&#39;: 44, &#39;anchovy fillets&#39;: 124, &#39;lemon slices&#39;: 12, &#39;chicken cutlets&#39;: 23, &#39;saffron threads&#39;: 28, &#39;orzo&#39;: 51, &#39;veal chops&#39;: 4, &#39;oil cured olives&#39;: 3, &#39;pasta rotel&#39;: 2, &#39;pasta water&#39;: 10, &#39;cooking wine&#39;: 3, &#39;crusty bread&#39;: 10, &#39;fresh tomatoes&#39;: 32, &#39;uncooked rigatoni&#39;: 6, &#39;grating cheese&#39;: 6, &#39;vegetable stock&#39;: 29, &#39;freshly grated parmesan&#39;: 87, &#39;florets&#39;: 15, &#39;sliced almonds&#39;: 37, &#39;thyme sprigs&#39;: 45, &#39;egg whites&#39;: 95, &#39;greek yogurt&#39;: 6, &#39;penne&#39;: 99, &#39;fish fillets&#39;: 10, &#39;russet potatoes&#39;: 40, &#39;crumbled gorgonzola&#39;: 29, &#39;fontina cheese&#39;: 84, &#39;spinach leaves&#39;: 25, &#39;orange&#39;: 38, &#39;lamb shanks&#39;: 11, &#39;clove&#39;: 19, &#39;rosemary sprigs&#39;: 50, &#39;italian eggplant&#39;: 7, &#39;Sicilian olives&#39;: 7, &#39;mint sprigs&#39;: 28, &#39;peasant bread&#39;: 7, &#39;garlic powder&#39;: 252, &#39;Kraft Grated Parmesan Cheese&#39;: 10, &#39;peas&#39;: 42, &#39;Oscar Mayer Bacon&#39;: 1, &#39;Philadelphia Cream Cheese&#39;: 9, &#39;garnish&#39;: 3, &#39;dried navy beans&#39;: 5, &#39;celery&#39;: 155, &#39;diced onions&#39;: 42, &#39;wheat berries&#39;: 4, &#39;parsley sprigs&#39;: 38, &#39;thyme&#39;: 51, &#39;polenta prepar&#39;: 5, &#39;chicken breast halves&#39;: 35, &#39;dry sherry&#39;: 22, &#39;cocoa powder&#39;: 9, &#39;dried parsley&#39;: 90, &#39;chopped garlic&#39;: 69, &#39;baking soda&#39;: 68, &#39;coarse salt&#39;: 128, &#39;rigatoni&#39;: 60, &#39;nutmeg&#39;: 40, &#39;artichoke hearts&#39;: 99, &#39;pancetta&#39;: 153, &#39;parsnips&#39;: 7, &#39;whole wheat fettuccine&#39;: 4, &#39;ground sirloin&#39;: 20, &#39;red wine&#39;: 99, &#39;fresh marjoram&#39;: 29, &#39;castellane&#39;: 2, &#39;hot Italian sausages&#39;: 39, &#39;jumbo pasta shells&#39;: 37, &#39;fresh spinach&#39;: 87, &#39;chicken breasts&#39;: 93, &#39;gluten free blend&#39;: 1, &#39;paprika&#39;: 76, &#39;salt and ground black pepper&#39;: 66, &#39;bone in skinless chicken thigh&#39;: 1, &#39;figs&#39;: 10, &#39;egg substitute&#39;: 26, &#39;hot sauce&#39;: 20, &#39;ham&#39;: 42, &#39;balsamic vinaigrette&#39;: 6, &#39;pitted olives&#39;: 15, &#39;fresh chives&#39;: 24, &#39;white mushrooms&#39;: 20, &#39;haricots verts&#39;: 4, &#39;fresh peas&#39;: 16, &#39;bow-tie pasta&#39;: 86, &#39;asparagus tips&#39;: 3, &#39;green beans&#39;: 70, &#39;low-fat cottage cheese&#39;: 14, &#39;whole wheat lasagna noodles&#39;: 8, &#39;shredded parmesan cheese&#39;: 38, &#39;apricots&#39;: 5, &#39;grappa&#39;: 9, &#39;all purpose unbleached flour&#39;: 68, &#39;dry yeast&#39;: 71, &#39;worcestershire sauce&#39;: 54, &#39;amaretti&#39;: 6, &#39;frozen strawberries&#39;: 1, &#39;strawberries&#39;: 48, &#39;cooked ham&#39;: 9, &#39;Alfredo sauce&#39;: 43, &#39;chopped fresh sage&#39;: 91, &#39;cheese slices&#39;: 11, &#39;bread&#39;: 51, &#39;morel&#39;: 2, &#39;leeks&#39;: 106, &#39;1% low-fat cottage cheese&#39;: 10, &#39;feta cheese crumbles&#39;: 68, &#39;pork belly&#39;: 2, &#39;fresh sage&#39;: 29, &#39;pork loin&#39;: 3, &#39;sliced black olives&#39;: 49, &#39;medium shrimp&#39;: 70, &#39;red chili peppers&#39;: 22, &#39;parmigiano-reggiano cheese&#39;: 53, &#39;fresh mint&#39;: 53, &#39;baguette&#39;: 119, &#39;chicken legs&#39;: 7, &#39;baby spinach&#39;: 117, &#39;white beans&#39;: 38, &#39;ground pork&#39;: 72, &#39;romana&#39;: 1, &#39;fresh mushrooms&#39;: 117, &#39;crimini mushrooms&#39;: 26, &#39;navel oranges&#39;: 10, &#39;kalamata&#39;: 62, &#39;sea scallops&#39;: 31, &#39;ladyfingers&#39;: 40, &#39;reduced fat cream cheese&#39;: 3, &#39;whipped topping&#39;: 12, &#39;wine&#39;: 15, &#39;frozen broccoli&#39;: 5, &#39;nonfat ricotta cheese&#39;: 32, &#39;shells&#39;: 8, &#39;reduced-fat cheese&#39;: 1, &#39;cornflake cereal&#39;: 2, &#39;beef brisket&#39;: 2, &#39;dry pasta&#39;: 8, &#39;white bread&#39;: 35, &#39;pork&#39;: 10, &#39;sweet italian sausage&#39;: 72, &#39;prosecco&#39;: 10, &#39;bread dough&#39;: 16, &#39;baby lima beans&#39;: 3, &#39;whipped cream&#39;: 15, &#39;hot cocoa mix&#39;: 2, &#39;brewed coffee&#39;: 18, &#39;abbamele&#39;: 1, &#39;wild mushrooms&#39;: 30, &#39;chopped walnuts&#39;: 46, &#39;fregola&#39;: 4, &#39;savoy cabbage&#39;: 10, &#39;mushroom caps&#39;: 17, &#39;pinot grigio&#39;: 3, &#39;liquid egg substitute&#39;: 1, &#39;chocolate candy bars&#39;: 5, &#39;cooked rice&#39;: 5, &#39;bread crumbs&#39;: 103, &#39;banana peppers&#39;: 5, &#39;toasted walnuts&#39;: 7, &#39;dark rum&#39;: 22, &#39;dried fig&#39;: 11, &#39;chopped parsley&#39;: 99, &#39;ground white pepper&#39;: 33, &quot;soft goat&#39;s cheese&quot;: 10, &#39;truffle oil&#39;: 17, &#39;hazelnuts&#39;: 53, &#39;veal scallopini&#39;: 2, &#39;elbow macaroni&#39;: 19, &#39;bread flour&#39;: 77, &#39;bread yeast&#39;: 1, &#39;red potato&#39;: 26, &#39;vegan parmesan cheese&#39;: 4, &#39;ahi&#39;: 1, &#39;golden brown sugar&#39;: 14, &#39;anjou pears&#39;: 3, &#39;granny smith apples&#39;: 6, &#39;crystallized ginger&#39;: 10, &#39;ice water&#39;: 7, &#39;vegetable shortening&#39;: 5, &#39;chinese five-spice powder&#39;: 1, &#39;whole wheat flour&#39;: 21, &#39;large egg yolks&#39;: 142, &#39;lean ground beef&#39;: 135, &#39;sausage links&#39;: 32, &#39;pork chops&#39;: 8, &#39;rosemary&#39;: 57, &#39;nectarines&#39;: 5, &#39;sweet cherries&#39;: 12, &#39;orange zest&#39;: 28, &#39;lavender buds&#39;: 1, &#39;apricot halves&#39;: 4, &#39;diced tomatoes in juice&#39;: 7, &#39;scallions&#39;: 61, &#39;italian salad dressing&#39;: 41, &#39;short pasta&#39;: 7, &#39;lemon wedge&#39;: 58, &#39;porterhouse steaks&#39;: 4, &#39;1% low-fat milk&#39;: 68, &#39;golden raisins&#39;: 46, &#39;cinnamon sticks&#39;: 18, &#39;broccolini&#39;: 3, &#39;salted butter&#39;: 11, &#39;chicken stock&#39;: 142, &#39;coffee granules&#39;: 12, &#39;lemon rind&#39;: 34, &#39;baby portobello mushrooms&#39;: 10, &#39;broccoli florets&#39;: 64, &#39;orecchiette&#39;: 41, &#39;melted butter&#39;: 24, &#39;pizza shells&#39;: 3, &#39;frozen mixed thawed vegetables,&#39;: 2, &#39;ragu old world style pasta sauc&#39;: 11, &#39;loosely packed fresh basil leaves&#39;: 21, &#39;whole wheat spaghetti&#39;: 22, &#39;butternut squash&#39;: 62, &#39;wonton wrappers&#39;: 17, &#39;meat&#39;: 15, &#39;pear tomatoes&#39;: 6, &#39;gaeta olives&#39;: 5, &#39;yukon gold potatoes&#39;: 39, &#39;turbot&#39;: 2, &#39;ground lamb&#39;: 6, &#39;refrigerated pizza dough&#39;: 31, &#39;potato gnocchi&#39;: 20, &#39;cream&#39;: 27, &#39;angel hair&#39;: 60, &#39;lime juice&#39;: 6, &#39;gelato&#39;: 4, &#39;cherry preserves&#39;: 2, &#39;amaretto liqueur&#39;: 4, &#39;cherries&#39;: 14, &#39;instant espresso powder&#39;: 22, &#39;nuts&#39;: 3, &#39;brown sugar&#39;: 67, &#39;pie shell&#39;: 2, &#39;marsala wine&#39;: 105, &#39;basil leaves&#39;: 153, &#39;cake&#39;: 7, &#39;crabmeat&#39;: 11, &#39;chopped fresh herbs&#39;: 8, &#39;button mushrooms&#39;: 35, &#39;escarole&#39;: 47, &#39;chopped pecans&#39;: 28, &#39;chocolate bars&#39;: 2, &#39;coffee liqueur&#39;: 11, &#39;flat anchovy&#39;: 5, &#39;italian loaf&#39;: 4, &#39;salad dressing&#39;: 23, &#39;pitted black olives&#39;: 24, &#39;rotini&#39;: 46, &#39;frozen mixed vegetables&#39;: 4, &#39;canned tomatoes&#39;: 26, &#39;feta cheese&#39;: 36, &#39;anise seed&#39;: 13, &#39;top sirloin&#39;: 1, &#39;candy&#39;: 3, &#39;angel food cake mix&#39;: 2, &#39;buttercream frosting&#39;: 1, &#39;sour cream&#39;: 71, &#39;shredded cheddar cheese&#39;: 42, &#39;cottage cheese&#39;: 44, &#39;noodles&#39;: 38, &#39;lime&#39;: 5, &#39;watermelon&#39;: 5, &#39;risotto&#39;: 11, &#39;ice cubes&#39;: 8, &#39;peeled tomatoes&#39;: 32, &#39;cooked vermicelli&#39;: 5, &#39;buns&#39;: 1, &#39;apple cider vinegar&#39;: 7, &#39;ground red pepper&#39;: 34, &#39;yellow bell pepper&#39;: 85, &#39;active dry yeast&#39;: 128, &#39;baking potatoes&#39;: 34, &#39;grated romano cheese&#39;: 19, &#39;quinoa&#39;: 8, &#39;littleneck clams&#39;: 26, &#39;whole wheat bread&#39;: 8, &#39;cream cheese, soften&#39;: 40, &#39;salad seasoning mix&#39;: 4, &#39;cucumber&#39;: 25, &#39;pappardelle pasta&#39;: 6, &#39;fresh mozzarella&#39;: 95, &#39;tuna steaks&#39;: 16, &#39;shredded zucchini&#39;: 4, &#39;dried pasta&#39;: 13, &#39;berries&#39;: 5, &#39;clams&#39;: 50, &#39;ground round&#39;: 25, &#39;fettuccine, cook and drain&#39;: 4, &#39;tomato purée&#39;: 47, &#39;chopped celery&#39;: 103, &#39;ditalini pasta&#39;: 15, &#39;lobster&#39;: 11, &#39;almonds&#39;: 35, &#39;anise&#39;: 13, &#39;anise extract&#39;: 12, &#39;brandy&#39;: 25, &#39;boneless chicken breast&#39;: 19, &#39;buffalo sauce&#39;: 2, &#39;blue cheese dressing&#39;: 3, &#39;cannelloni shells&#39;: 2, &#39;butter cooking spray&#39;: 3, &#39;light alfredo sauce&#39;: 7, &#39;rice&#39;: 12, &#39;cream of chicken soup&#39;: 9, &#39;chees fresh mozzarella&#39;: 72, &#39;shrimp tails&#39;: 2, &#39;bay scallops&#39;: 12, &#39;lump crab meat&#39;: 17, &#39;fish stock&#39;: 11, &#39;capellini&#39;: 8, &#39;veal&#39;: 17, &#39;beef stock&#39;: 24, &#39;chard&#39;: 3, &#39;grated Gruyère cheese&#39;: 5, &#39;panko&#39;: 25, &#39;chickpeas&#39;: 37, &#39;white cornmeal&#39;: 4, &#39;red bell pepper, sliced&#39;: 2, &#39;herbs&#39;: 20, &#39;chicken thighs&#39;: 32, &#39;green bell pepper, slice&#39;: 13, &#39;vanilla&#39;: 28, &#39;liqueur&#39;: 10, &#39;aged balsamic vinegar&#39;: 8, &#39;limoncello&#39;: 6, &#39;golden beets&#39;: 4, &#39;pizza doughs&#39;: 103, &#39;black cod&#39;: 2, &#39;green cabbage&#39;: 12, &#39;cavolo nero&#39;: 4, &#39;winter squash&#39;: 1, &#39;thin pizza crust&#39;: 9, &#39;toasted pine nuts&#39;: 30, &#39;grated parmesan romano&#39;: 2, &#39;cumin seed&#39;: 4, &#39;cilantro leaves&#39;: 7, &#39;seasoning salt&#39;: 15, &#39;mixed greens&#39;: 16, &#39;turkey breast cutlets&#39;: 9, &#39;cod fillets&#39;: 8, &#39;barilla&#39;: 4, &#39;linguini&#39;: 9, &#39;perciatelli&#39;: 7, &#39;crumbled blue cheese&#39;: 16, &#39;black mission figs&#39;: 4, &#39;swordfish steaks&#39;: 11, &#39;anchovy paste&#39;: 47, &#39;chuck&#39;: 8, &#39;tomatoes with juice&#39;: 30, &#39;store bought low sodium chicken stock&#39;: 2, &#39;fresh lavender&#39;: 1, &#39;grated orange&#39;: 31, &#39;vanilla wafers&#39;: 3, &#39;amaretto&#39;: 19, &#39;toasted almonds&#39;: 6, &#39;light corn syrup&#39;: 8, &#39;focaccia&#39;: 10, &#39;oyster mushrooms&#39;: 6, &#39;shiitake mushroom caps&#39;: 16, &#39;onion powder&#39;: 39, &#39;sourdough&#39;: 7, &#39;orange bell pepper&#39;: 27, &#39;nonfat cottage cheese&#39;: 6, &#39;stewed tomatoes&#39;: 37, &#39;raspberries&#39;: 25, &#39;vanilla beans&#39;: 26, &#39;Frangelico&#39;: 9, &#39;vegetable oil spray&#39;: 22, &#39;table salt&#39;: 21, &#39;white peppercorns&#39;: 2, &#39;herb vinegar&#39;: 3, &#39;reduced fat sharp cheddar cheese&#39;: 8, &#39;deli ham&#39;: 4, &#39;ground turkey&#39;: 48, &#39;hot dogs&#39;: 2, &#39;italian style stewed tomatoes&#39;: 15, &#39;veal stock&#39;: 7, &#39;portabello mushroom&#39;: 41, &#39;rocket leaves&#39;: 35, &#39;country bread&#39;: 26, &#39;bottled balsamic vinaigrette&#39;: 1, &#39;scallops&#39;: 12, &#39;italian tomatoes&#39;: 16, &#39;peeled shrimp&#39;: 3, &#39;whole wheat pizza crust&#39;: 3, &#39;dried mixed herbs&#39;: 3, &#39;whole wheat pastry flour&#39;: 6, &#39;nonstick spray&#39;: 7, &#39;low-fat sour cream&#39;: 8, &#39;day old bread&#39;: 2, &#39;champagne vinegar&#39;: 9, &#39;pizza sauce&#39;: 71, &#39;red vermouth&#39;: 5, &#39;low sodium chicken stock&#39;: 6, &#39;peppercorns&#39;: 3, &#39;turkey stock&#39;: 3, &#39;cooked turkey&#39;: 6, &#39;green olives&#39;: 52, &#39;spaghettini&#39;: 23, &#39;minced onion&#39;: 41, &#39;beef broth&#39;: 62, &#39;dried mint flakes&#39;: 3, &#39;ravioli&#39;: 16, &#39;meat loaf mix&#39;: 1, &#39;rub&#39;: 2, &#39;clam juice&#39;: 26, &#39;bottled clam juice&#39;: 15, &#39;cayenne pepper&#39;: 50, &#39;pears&#39;: 19, &#39;gorgonzola&#39;: 27, &#39;duck&#39;: 2, &#39;pure vanilla extract&#39;: 22, &#39;panettone&#39;: 4, &#39;sambuca&#39;: 4, &#39;light brown sugar&#39;: 15, &#39;fat free frozen top whip&#39;: 5, &#39;frozen bread dough&#39;: 9, &#39;whole wheat breadcrumbs&#39;: 10, &#39;ground chuck&#39;: 16, &#39;bows&#39;: 3, &#39;sauce&#39;: 53, &#39;ground oregano&#39;: 4, &#39;lean ground turkey&#39;: 2, &#39;milk chocolate&#39;: 11, &#39;hazelnut butter&#39;: 4, &#39;fig jam&#39;: 2, &#39;crackers&#39;: 7, &#39;sun-dried tomatoes in oil&#39;: 28, &#39;dark chocolate&#39;: 7, &#39;vanilla ice cream&#39;: 17, &#39;chives&#39;: 31, &#39;radishes&#39;: 13, &#39;lettuce&#39;: 6, &#39;grilled chicken&#39;: 2, &#39;flatbread&#39;: 4, &#39;fresh parsley leaves&#39;: 56, &#39;lemon extract&#39;: 7, &#39;salad greens&#39;: 11, &#39;brown rice&#39;: 6, &#39;tortellini&#39;: 19, &#39;reduced fat alfredo sauce&#39;: 4, &#39;fresh asparagus&#39;: 24, &#39;round steaks&#39;: 5, &#39;chili powder&#39;: 26, &#39;ground cumin&#39;: 30, &#39;rolls&#39;: 21, &#39;prego traditional italian sauce&#39;: 4, &#39;brown hash potato&#39;: 5, &#39;nonfat milk&#39;: 7, &#39;frozen cheese ravioli&#39;: 6, &#39;mild Italian sausage&#39;: 17, &#39;sandwich rolls&#39;: 3, &#39;orange juice concentrate&#39;: 5, &#39;rotelle&#39;: 3, &#39;sweet italian sausag links, cut into&#39;: 2, &#39;idaho potatoes&#39;: 4, &#39;popcorn&#39;: 1, &#39;grated orange peel&#39;: 26, &#39;french fried onions&#39;: 1, &#39;turbinado&#39;: 7, &#39;cooked chicken breasts&#39;: 16, &#39;farro&#39;: 14, &#39;flour tortillas&#39;: 11, &#39;english cucumber&#39;: 7, &#39;minced peperoncini&#39;: 2, &#39;teleme&#39;: 2, &#39;brine cured green olives&#39;: 4, &#39;black forest ham&#39;: 1, &#39;frozen whole kernel corn&#39;: 5, &#39;sweet potatoes&#39;: 15, &#39;applewood smoked bacon&#39;: 7, &#39;panko breadcrumbs&#39;: 24, &#39;coarse kosher salt&#39;: 9, &#39;cornmeal&#39;: 72, &#39;dried sage&#39;: 28, &#39;dri leav thyme&#39;: 6, &#39;low sodium beef broth&#39;: 3, &#39;cabbage&#39;: 12, &#39;fresh shiitake mushrooms&#39;: 15, &#39;rabbit&#39;: 8, &#39;herbes de provence&#39;: 12, &#39;dressing&#39;: 7, &#39;chicken fingers&#39;: 3, &#39;reduced-fat sour cream&#39;: 13, &#39;dried fettuccine&#39;: 14, &#39;white pepper&#39;: 22, &#39;albacore tuna in water&#39;: 5, &#39;light mayonnaise&#39;: 7, &#39;turkey tenderloins&#39;: 4, &#39;almond flour&#39;: 8, &#39;raw almond&#39;: 3, &#39;fresh orange juice&#39;: 32, &#39;cranberries&#39;: 3, &#39;orange marmalade&#39;: 6, &#39;fresh lemon&#39;: 4, &#39;condensed chicken broth&#39;: 1, &#39;oil packed anchovy fillets&#39;: 3, &#39;boneless chicken skinless thigh&#39;: 20, &#39;sugar pea&#39;: 16, &#39;nonfat yogurt&#39;: 4, &#39;roast red peppers, drain&#39;: 24, &#39;french baguette&#39;: 25, &#39;ripe olives&#39;: 26, &#39;honey glazed ham&#39;: 2, &#39;chiles&#39;: 9, &#39;spring onions&#39;: 3, &#39;candied orange peel&#39;: 13, &#39;lard&#39;: 5, &#39;cinnamon&#39;: 23, &#39;semolina flour&#39;: 32, &#39;onion salt&#39;: 2, &#39;beef demi-glace&#39;: 1, &#39;veal shanks&#39;: 21, &#39;orange peel&#39;: 7, &#39;lemon peel&#39;: 11, &#39;plain yogurt&#39;: 7, &#39;Quinoa Flour&#39;: 1, &#39;spelt flour&#39;: 2, &#39;plums&#39;: 14, &#39;heirloom tomatoes&#39;: 21, &#39;fresh lime juice&#39;: 25, &#39;artichok heart marin&#39;: 33, &#39;bucatini&#39;: 16, &#39;processed cheese&#39;: 5, &#39;egg noodles, cooked and drained&#39;: 3, &#39;tapioca flour&#39;: 1, &#39;lasagna noodles, cooked and drained&#39;: 26, &#39;firm tofu&#39;: 9, &#39;sherry vinegar&#39;: 27, &#39;country style bread&#39;: 7, &#39;teardrop tomatoes&#39;: 2, &#39;ground sausage&#39;: 5, &#39;pita pockets&#39;: 1, &#39;orange juice&#39;: 19, &#39;chili pepper&#39;: 5, &#39;currant&#39;: 7, &#39;small capers, rins and drain&#39;: 2, &#39;filet&#39;: 4, &#39;lettuce leaves&#39;: 8, &#39;vodka&#39;: 27, &#39;stolichnaya&#39;: 1, &#39;romaine lettuce&#39;: 33, &#39;croutons&#39;: 18, &#39;pepperocini&#39;: 2, &#39;cherry peppers&#39;: 3, &#39;dandelion&#39;: 1, &#39;beans&#39;: 6, &#39;hot pepper sauce&#39;: 7, &#39;shredded Monterey Jack cheese&#39;: 13, &#39;cane sugar&#39;: 1, &#39;mixed nuts&#39;: 1, &#39;meatballs&#39;: 17, &#39;plain dry bread crumb&#39;: 11, &#39;spanish onion&#39;: 9, &#39;cuban peppers&#39;: 3, &#39;green tomatoes&#39;: 2, &#39;sesame seeds&#39;: 13, &#39;boneless beef chuck roast&#39;: 3, &#39;hard-boiled egg&#39;: 6, &#39;pork tenderloin&#39;: 24, &#39;bulk italian sausag&#39;: 21, &#39;beef bouillon granules&#39;: 3, &#39;prebaked pizza crusts&#39;: 13, &#39;buttermilk&#39;: 32, &#39;flaked coconut&#39;: 5, &#39;lower sodium chicken broth&#39;: 20, &#39;carnaroli rice&#39;: 13, &#39;fresh oregano leaves&#39;: 24, &#39;cavatappi&#39;: 9, &#39;cooking oil&#39;: 24, &#39;cayenne&#39;: 8, &#39;ground cloves&#39;: 30, &#39;corn syrup&#39;: 4, &#39;small pasta&#39;: 7, &#39;ground fennel&#39;: 7, &#39;low-fat buttermilk&#39;: 9, &#39;ice&#39;: 7, &#39;chicken bouillon granules&#39;: 6, &#39;poultry seasoning&#39;: 10, &#39;roast&#39;: 3, &#39;fusilli&#39;: 43, &#39;Italian herbs&#39;: 7, &#39;diced yellow onion&#39;: 1, &#39;manicotti pasta&#39;: 4, &#39;parsley flakes&#39;: 17, &#39;vinaigrette&#39;: 7, &#39;bread ciabatta&#39;: 16, &#39;kidney beans&#39;: 12, &#39;creole seasoning&#39;: 4, &#39;prepared pasta sauce&#39;: 5, &#39;small curd cottage cheese&#39;: 14, &#39;white sandwich bread&#39;: 9, &#39;mini chocolate chips&#39;: 5, &#39;lean beef&#39;: 2, &#39;breadstick&#39;: 13, &#39;pickled okra&#39;: 2, &#39;fronds&#39;: 15, &#39;thick-cut bacon&#39;: 11, &#39;boiling potatoes&#39;: 13, &#39;ditalini&#39;: 11, &#39;cranberry beans&#39;: 4, &#39;center cut bacon&#39;: 7, &#39;roasting chickens&#39;: 8, &#39;fleur de sel&#39;: 6, &#39;Margherita Pepperoni&#39;: 1, &#39;soft-shell clams&#39;: 1, &#39;liquid&#39;: 5, &#39;fresh chevre&#39;: 4, &#39;pork sausages&#39;: 14, &#39;dried minced onion&#39;: 5, &#39;msg&#39;: 2, &#39;pork stew meat&#39;: 1, &#39;beef stew meat&#39;: 3, &#39;ziti&#39;: 26, &#39;Balsamico Bianco&#39;: 2, &#39;ground mustard&#39;: 1, ...} ## 请不要修改下方代码# Finally, plot the 10 most used ingredientsfig = pd.DataFrame(italian_ingredients, index=[0]).transpose()[0].sort_values(ascending=False, inplace=False)[:10].plot(kind='barh')fig.invert_yaxis()fig = fig.get_figure()fig.tight_layout()fig.show() /opt/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:448: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure. % get_backend()) 若想要对数据分析做更深入的了解，可以参考数据分析师入门课程或者基于Python语言的人工智能Nano课程. 第三步. 建立模型在项目的第三步中，你需要了解必要的工具和技巧来让你的模型进行预测。用这些工具和技巧对每一个模型的表现做精确的衡量可以极大地增强你预测的信心。 3.1 单词清洗由于菜品包含的佐料众多，同一种佐料也可能有单复数、时态等变化，为了去除这类差异，我们考虑将ingredients 进行过滤 ## 请不要修改下方代码import refrom nltk.stem import WordNetLemmatizerimport numpy as npdef text_clean(ingredients): #去除单词的标点符号，只保留 a..z A...Z的单词字符 ingredients= np.array(ingredients).tolist() print(\"菜品佐料：\\n&#123;&#125;\".format(ingredients[9])) ingredients=[[re.sub('[^A-Za-z]', ' ', word) for word in component]for component in ingredients] print(\"去除标点符号之后的结果：\\n&#123;&#125;\".format(ingredients[9])) # 去除单词的单复数，时态，只保留单词的词干 lemma=WordNetLemmatizer() ingredients=[\" \".join([ \" \".join([lemma.lemmatize(w) for w in words.split(\" \")]) for words in component]) for component in ingredients] print(\"去除时态和单复数之后的结果：\\n&#123;&#125;\".format(ingredients[9])) return ingredientsprint(\"\\n处理训练集...\")train_ingredients = text_clean(train_content['ingredients'])print(\"\\n处理测试集...\")test_ingredients = text_clean(test_content['ingredients']) [nltk_data] Downloading package wordnet to [nltk_data] /Users/jindongwang/nltk_data... [nltk_data] Unzipping corpora/wordnet.zip. 处理训练集... 菜品佐料： [&#39;chopped tomatoes&#39;, &#39;fresh basil&#39;, &#39;garlic&#39;, &#39;extra-virgin olive oil&#39;, &#39;kosher salt&#39;, &#39;flat leaf parsley&#39;] 去除标点符号之后的结果： [&#39;chopped tomatoes&#39;, &#39;fresh basil&#39;, &#39;garlic&#39;, &#39;extra virgin olive oil&#39;, &#39;kosher salt&#39;, &#39;flat leaf parsley&#39;] 去除时态和单复数之后的结果： chopped tomato fresh basil garlic extra virgin olive oil kosher salt flat leaf parsley 处理测试集... 菜品佐料： [&#39;eggs&#39;, &#39;cherries&#39;, &#39;dates&#39;, &#39;dark muscovado sugar&#39;, &#39;ground cinnamon&#39;, &#39;mixed spice&#39;, &#39;cake&#39;, &#39;vanilla extract&#39;, &#39;self raising flour&#39;, &#39;sultana&#39;, &#39;rum&#39;, &#39;raisins&#39;, &#39;prunes&#39;, &#39;glace cherries&#39;, &#39;butter&#39;, &#39;port&#39;] 去除标点符号之后的结果： [&#39;eggs&#39;, &#39;cherries&#39;, &#39;dates&#39;, &#39;dark muscovado sugar&#39;, &#39;ground cinnamon&#39;, &#39;mixed spice&#39;, &#39;cake&#39;, &#39;vanilla extract&#39;, &#39;self raising flour&#39;, &#39;sultana&#39;, &#39;rum&#39;, &#39;raisins&#39;, &#39;prunes&#39;, &#39;glace cherries&#39;, &#39;butter&#39;, &#39;port&#39;] 去除时态和单复数之后的结果： egg cherry date dark muscovado sugar ground cinnamon mixed spice cake vanilla extract self raising flour sultana rum raisin prune glace cherry butter port 3.2 特征提取在该步骤中，我们将菜品的佐料转换成数值特征向量。考虑到绝大多数菜中都包含salt, water, sugar, butter等，采用one-hot的方法提取的向量将不能很好的对菜系作出区分。我们将考虑按照佐料出现的次数对佐料做一定的加权，即：佐料出现次数越多，佐料的区分性就越低。我们采用的特征为TF-IDF，相关介绍内容可以参考：TF-IDF与余弦相似性的应用（一）：自动提取关键词。 ## 请不要修改下方代码from sklearn.feature_extraction.text import TfidfVectorizer# 将佐料转换成特征向量# 处理 训练集vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 1), analyzer='word', max_df=.57, binary=False, token_pattern=r\"\\w+\",sublinear_tf=False)train_tfidf = vectorizer.fit_transform(train_ingredients).todense()## 处理 测试集test_tfidf = vectorizer.transform(test_ingredients) ## 请不要修改下方代码train_targets=np.array(train_content['cuisine']).tolist()train_targets[:10] [&#39;greek&#39;, &#39;southern_us&#39;, &#39;filipino&#39;, &#39;indian&#39;, &#39;indian&#39;, &#39;jamaican&#39;, &#39;spanish&#39;, &#39;italian&#39;, &#39;mexican&#39;, &#39;italian&#39;] 编程练习这里我们为了防止前面步骤中累积的错误，导致以下步骤无法正常运行。我们在此检查处理完的实验数据是否正确，请打印train_tfidf和train_targets中前五个数据。 # 你需要预览训练集train_tfidf,train_targets中前5条数据，试试Python的切片语法display(train_tfidf[:5])display(train_targets[:5]) matrix([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]) [&#39;greek&#39;, &#39;southern_us&#39;, &#39;filipino&#39;, &#39;indian&#39;, &#39;indian&#39;] 3.3 验证集划分为了在实验中大致估计模型的精确度我们将从原本的train_ingredients 划分出 20% 的数据用作valid_ingredients。 编程练习：数据分割与重排调用train_test_split函数将训练集划分为新的训练集和验证集，便于之后的模型精度观测。 从sklearn.model_selection中导入train_test_split 将train_tfidf和train_targets作为train_test_split的输入变量 设置test_size为0.2，划分出20%的验证集，80%的数据留作新的训练集。 设置random_state随机种子，以确保每一次运行都可以得到相同划分的结果。（随机种子固定，生成的随机序列就是确定的） ### TODO：划分出验证集from sklearn.model_selection import train_test_splitX_train , X_valid , y_train, y_valid = train_test_split(train_tfidf, train_targets, test_size=0.2, random_state=1) 3.2 建立模型调用 sklearn 中的逻辑回归模型（Logistic Regression）。 编程练习：训练模型 从sklearn.linear_model导入LogisticRegression 从sklearn.model_selection导入GridSearchCV, 参数自动搜索，只要把参数输进去，就能给出最优的结果和参数，这个方法适合小数据集。 定义parameters变量：为C参数创造一个字典，它的值是从1至10的数组; 定义classifier变量: 使用导入的LogisticRegression创建一个分类函数; 定义grid变量: 使用导入的GridSearchCV创建一个网格搜索对象；将变量’classifier’, ‘parameters’作为参数传至这个对象构造函数中； from sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import GridSearchCV## TODO: 建立逻辑回归模型parameters = &#123;'C':[1,2,3,4,5,6,7,8,9,10]&#125;classifier = LogisticRegression()grid = GridSearchCV(classifier, parameters)## 请不要修改下方代码grid = grid.fit(X_train, y_train) 模型训练结束之后，我们计算模型在验证集X_valid上预测结果，并计算模型的预测精度（与y_valid逐个比较）。 ## 请不要修改下方代码from sklearn.metrics import accuracy_score ## 计算模型的准确率valid_predict = grid.predict(X_valid)valid_score=accuracy_score(y_valid,valid_predict)print(\"验证集上的得分为：&#123;&#125;\".format(valid_score)) 验证集上的得分为：0.7912005028284098 第四步. 模型预测（可选）4.1 预测测试集编程练习 将模型grid对测试集test_tfidf做预测，然后查看预测结果。 ### TODO：预测测试结果predictions = grid.predict(test_tfidf)## 请不要修改下方代码print(\"预测的测试集个数为：&#123;&#125;\".format(len(predictions)))test_content['cuisine']=predictionstest_content.head(10) 预测的测试集个数为：9944 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id ingredients cuisine 0 18009 [baking powder, eggs, all-purpose flour, raisins, milk, white sugar] british 1 28583 [sugar, egg yolks, corn starch, cream of tartar, bananas, vanilla wafers, milk, vanilla extract, toasted pecans, egg... southern_us 2 41580 [sausage links, fennel bulb, fronds, olive oil, cuban peppers, onions] italian 3 29752 [meat cuts, file powder, smoked sausage, okra, shrimp, andouille sausage, water, paprika, hot sauce, garlic cloves, ... cajun_creole 4 35687 [ground black pepper, salt, sausage casings, leeks, parmigiano reggiano cheese, cornmeal, water, extra-virgin olive ... italian 5 38527 [baking powder, all-purpose flour, peach slices, corn starch, heavy cream, lemon juice, unsalted butter, salt, white... southern_us 6 19666 [grape juice, orange, white zinfandel] french 7 41217 [ground ginger, white pepper, green onions, orange juice, sugar, Sriracha, vegetable oil, orange zest, chicken broth... chinese 8 28753 [diced onions, taco seasoning mix, all-purpose flour, chopped cilantro fresh, ground cumin, ground cinnamon, vegetab... mexican 9 22659 [eggs, cherries, dates, dark muscovado sugar, ground cinnamon, mixed spice, cake, vanilla extract, self raising flou... british 4.2 提交结果为了更好的测试模型的效果，同时比较与其他人的差距，我们将模型的测试集上的结果提交至 kaggle What’s Cooking? （需要提前注册kaggle账号）。 注意：在提交作业时，请将提交排名得分截图，附在压缩包中。 ## 加载结果格式submit_frame = pd.read_csv(\"sample_submission.csv\")## 保存结果result = pd.merge(submit_frame, test_content, on=\"id\", how='left')result = result.rename(index=str, columns=&#123;\"cuisine_y\": \"cuisine\"&#125;)test_result_name = \"tfidf_cuisine_test.csv\"result[['id','cuisine']].to_csv(test_result_name,index=False) 将生成的 tfidf_cuisine_test.csv 提交至 https://www.kaggle.com/c/whats-cooking/submit 然后选择 Upload Submission File, 点击 Make submission即可。稍作等待，就可以看到右上角的评分结果（得分大致为：0.78580 左右）。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://jinyaxuan.github.io/categories/机器学习/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://jinyaxuan.github.io/tags/Machine-Learning/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://jinyaxuan.github.io/categories/机器学习/"}]},{"title":"Scorecard","slug":"Scorecard","date":"2019-02-15T10:40:26.000Z","updated":"2019-02-15T10:42:13.539Z","comments":false,"path":"2019/02/15/Scorecard/","link":"","permalink":"http://jinyaxuan.github.io/2019/02/15/Scorecard/","excerpt":"","text":"信用评分卡模型是一种比较成熟的预测方法，广泛应用于信用风险评估以及金融风险控制等领域，其基本原理是：将模型变量以WOE编码方式离散化之后运用 logistic 回归模型进行二分类变量的拟合及预测。信用评分卡一般可以分为申请评分卡、行为评分卡、催收评分卡等。本文主要讲述申请评分卡模型的建模分析过程。主要分以下几个步骤： 目标定义 数据获取 数据预处理 模型开发 模型评估 评分系统建立 1. 目标定义数据来源kaggle project: ‘give-me-some-credit-dataset’，找出关键的特征变量，建立信用评分模型 2. 数据获取#导入必要的库包import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.ensemble import RandomForestRegressorimport warningswarnings.filterwarnings('ignore')%matplotlib inline #读取数据data=pd.read_csv('cs-training.csv')data=data.drop(axis=1, columns=[data.columns[0]])data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SeriousDlqin2yrs RevolvingUtilizationOfUnsecuredLines age NumberOfTime30-59DaysPastDueNotWorse DebtRatio MonthlyIncome NumberOfOpenCreditLinesAndLoans NumberOfTimes90DaysLate NumberRealEstateLoansOrLines NumberOfTime60-89DaysPastDueNotWorse NumberOfDependents 0 1 0.766127 45 2 0.802982 9120.0 13 0 6 0 2.0 1 0 0.957151 40 0 0.121876 2600.0 4 0 0 0 1.0 2 0 0.658180 38 1 0.085113 3042.0 2 1 0 0 0.0 3 0 0.233810 30 0 0.036050 3300.0 5 0 0 0 0.0 4 0 0.907239 49 1 0.024926 63588.0 7 0 1 0 0.0 data.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 150000 entries, 0 to 149999 Data columns (total 11 columns): SeriousDlqin2yrs 150000 non-null int64 RevolvingUtilizationOfUnsecuredLines 150000 non-null float64 age 150000 non-null int64 NumberOfTime30-59DaysPastDueNotWorse 150000 non-null int64 DebtRatio 150000 non-null float64 MonthlyIncome 120269 non-null float64 NumberOfOpenCreditLinesAndLoans 150000 non-null int64 NumberOfTimes90DaysLate 150000 non-null int64 NumberRealEstateLoansOrLines 150000 non-null int64 NumberOfTime60-89DaysPastDueNotWorse 150000 non-null int64 NumberOfDependents 146076 non-null float64 dtypes: float64(4), int64(7) memory usage: 12.6 MB 3. 数据预处理#缺失值处理 #随机森林法填补MonthlyIncomeMI_df=data.iloc[:,0:10]MI_known=MI_df.loc[MI_df['MonthlyIncome'].notnull()]MI_unknown=MI_df.loc[MI_df['MonthlyIncome'].isnull()]X_known=MI_known.drop('MonthlyIncome',axis=1)y_known=MI_known['MonthlyIncome']X_unknown=MI_unknown.drop('MonthlyIncome',axis=1)rfr=RandomForestRegressor(random_state=0, n_estimators=100, max_depth=3, n_jobs=-1)rfr.fit(X_known, y_known)data.loc[MI_df['MonthlyIncome'].isnull(), 'MonthlyIncome']=rfr.predict(X_unknown).round(0)print('Done') Done #NumberOfDependents 缺失值较少，可以直接删除data.dropna(inplace=True)#去除重复值data.drop_duplicates(inplace=True) #异常值处理sns.boxplot(y='age', data=data) &lt;matplotlib.axes._subplots.AxesSubplot at 0x10e034cc0&gt; data=data[((data['age']&gt;0) &amp; (data['age']&lt;100))] #逾期次数sns.boxplot(data=data[['NumberOfTime30-59DaysPastDueNotWorse','NumberOfTimes90DaysLate','NumberOfTime60-89DaysPastDueNotWorse']], palette='Set2')plt.xticks(rotation=20) (array([0, 1, 2]), &lt;a list of 3 Text xticklabel objects&gt;) data=data[data['NumberOfTime30-59DaysPastDueNotWorse']&lt;80]#再次检查异常点sns.boxplot(data=data[['NumberOfTime30-59DaysPastDueNotWorse','NumberOfTimes90DaysLate','NumberOfTime60-89DaysPastDueNotWorse']], palette='Set2')plt.xticks(rotation=20) (array([0, 1, 2]), &lt;a list of 3 Text xticklabel objects&gt;) #月收入和年龄变量的分布sns.boxplot(x='MonthlyIncome', data=data) &lt;matplotlib.axes._subplots.AxesSubplot at 0x10f596b70&gt; print('%.2f%% of customers monthly income under 40000.' %(data.loc[data['MonthlyIncome']&lt;=40000].shape[0]*100/data.shape[0]))#月收入绝大部分集中在40000以下，可画出对应的月收入的分布sns.distplot(data.loc[data['MonthlyIncome']&lt;=40000,'MonthlyIncome'], bins=80, label='MonthlyIncome dist', kde=False) 99.69% of customers monthly income under 40000. &lt;matplotlib.axes._subplots.AxesSubplot at 0x107f14048&gt; sns.distplot(data['age'], bins=30, label='age dist', kde=True) &lt;matplotlib.axes._subplots.AxesSubplot at 0x10e075978&gt; 特征工程#训练集与测试集划分from sklearn.model_selection import train_test_splity = data['SeriousDlqin2yrs']X = data.iloc[:,1:]X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=10) # 定义最优分箱函数import scipy.stats as statsdef mono_bin(Y, X, n = 20): r = 0 total_bad=Y.sum() total_good=Y.count()-total_bad #print(total_bad, total_good) while np.abs(r) &lt; 1: d1 = pd.DataFrame(&#123;\"X\": X, \"Y\": Y, \"Bucket\": pd.qcut(X, n)&#125;) d2 = d1.groupby('Bucket', as_index = True) r, p = stats.spearmanr(d2.mean().X, d2.mean().Y) n = n - 1 d3 = pd.DataFrame(d2.X.min(), columns = ['min']) d3['min']=d2.min().X d3['max'] = d2.max().X d3['bad'] = d2.sum().Y d3['good'] = d2.count().Y-d3['bad'] d3['bad_rate'] = d2.mean().Y d3['woe']=np.log(((d2.count().Y-d2.sum().Y)/total_good)/(d2.sum().Y/total_bad)) iv = ((d3['good']/total_good - d3['bad']/total_bad)*d3['woe']).sum() d4 = (d3.sort_index(by = 'min')).reset_index(drop=True) woe = list(d4['woe'].round(3)) cut=[] cut.append(float('-inf')) for i in range(1, n+1): qua = X.quantile(i / (n+1)) cut.append(round(qua, 4)) cut.append(float('inf')) return d4, iv, cut, woe #举例，将‘age’最优分箱mono_bin(y, data['age'], n=10) ( min max bad good bad_rate woe 0 21 33 1816 14471 0.111500 -0.561845 1 34 40 1664 16073 0.093815 -0.369439 2 41 45 1360 14683 0.084772 -0.258150 3 46 49 1209 13619 0.081535 -0.215683 4 50 54 1298 16516 0.072864 -0.093851 5 55 59 913 15757 0.054769 0.210948 6 60 64 690 15923 0.041534 0.501473 7 65 71 414 14194 0.028341 0.897353 8 72 99 341 14403 0.023128 1.105954, 0.2414021266070617, [-inf, 33.0, 40.0, 45.0, 49.0, 54.0, 59.0, 64.0, 71.0, inf], [-0.562, -0.369, -0.258, -0.216, -0.094, 0.211, 0.501, 0.897, 1.106]) #每个变量的个数，从而确定连续变量与分类变量var_lst=data.columns[1:]var_num=&#123;&#125;for var in var_lst: var_num[var]=len(data[var].unique())var_num {&#39;RevolvingUtilizationOfUnsecuredLines&#39;: 122950, &#39;age&#39;: 79, &#39;NumberOfTime30-59DaysPastDueNotWorse&#39;: 14, &#39;DebtRatio&#39;: 114065, &#39;MonthlyIncome&#39;: 13592, &#39;NumberOfOpenCreditLinesAndLoans&#39;: 58, &#39;NumberOfTimes90DaysLate&#39;: 17, &#39;NumberRealEstateLoansOrLines&#39;: 28, &#39;NumberOfTime60-89DaysPastDueNotWorse&#39;: 11, &#39;NumberOfDependents&#39;: 13} #将四个连续变量最优分箱x1_df, x1_iv, x1_cut, x1_woe = mono_bin( y_train, X_train['RevolvingUtilizationOfUnsecuredLines'], n=10)x2_df, x2_iv, x2_cut, x2_woe = mono_bin( y_train, X_train['age'], n=10)x4_df, x4_iv, x4_cut, x4_woe = mono_bin( y_train, X_train['DebtRatio'], n=10)x5_df, x5_iv, x5_cut, x5_woe = mono_bin( y_train, X_train['MonthlyIncome'], n=10) #不能最优分箱的变量则进行手动分箱，WOE计算函数def woe_value(Y, X, cut): total_bad=Y.sum() total_good=Y.count()-total_bad d1 = pd.DataFrame(&#123;\"X\": X, \"Y\": Y, \"Bucket\": cut&#125;) d2 = d1.groupby('Bucket', as_index = True) d3 = pd.DataFrame(d2.X.min(), columns = ['min']) d3['min']=d2.min().X d3['max'] = d2.max().X d3['bad'] = d2.sum().Y d3['good'] = d2.count().Y-d3['bad'] d3['bad_rate'] = d2.mean().Y d3['woe']=np.log(((d2.count().Y-d2.sum().Y)/total_good)/(d2.sum().Y/total_bad)) iv = ((d3['good']/total_good - d3['bad']/total_bad)*d3['woe']).sum() d4 = (d3.sort_index(by = 'min')).reset_index(drop=True) woe = list(d4['woe'].round(3)) return d4, iv, woe x3_cut = [-np.Inf, 0, 1, 3, 5, np.Inf]x6_cut = [-np.Inf, 1, 2, 3, 5, np.Inf]x7_cut = [-np.Inf, 0, 1, 3, 5, np.Inf]x8_cut = [-np.Inf, 0, 1, 2, 3, np.Inf]x9_cut = [-np.Inf, 0, 1, 3, np.Inf]x10_cut = [-np.Inf, 0, 1, 2, 3, 5, np.Inf]x3_bin = pd.cut(X_train['NumberOfTime30-59DaysPastDueNotWorse'], bins= x3_cut)x6_bin = pd.cut(X_train['NumberOfOpenCreditLinesAndLoans'], bins= x6_cut)x7_bin = pd.cut(X_train['NumberOfTimes90DaysLate'], bins=x7_cut)x8_bin = pd.cut(X_train['NumberRealEstateLoansOrLines'], bins=x8_cut)x9_bin = pd.cut(X_train['NumberOfTime60-89DaysPastDueNotWorse'], bins=x9_cut)x10_bin = pd.cut(X_train['NumberOfDependents'], bins=x10_cut)x3_df, x3_iv, x3_woe = woe_value(y_train, X_train['NumberOfTime30-59DaysPastDueNotWorse'], x3_bin)x6_df, x6_iv, x6_woe = woe_value(y_train, X_train['NumberOfOpenCreditLinesAndLoans'], x6_bin)x7_df, x7_iv, x7_woe = woe_value(y_train, X_train['NumberOfTimes90DaysLate'], x7_bin)x8_df, x8_iv, x8_woe = woe_value(y_train, X_train['NumberRealEstateLoansOrLines'], x8_bin)x9_df, x9_iv, x9_woe = woe_value(y_train, X_train['NumberOfTime60-89DaysPastDueNotWorse'], x9_bin)x10_df, x10_iv, x10_woe = woe_value(y_train, X_train['NumberOfDependents'], x10_bin) #相关性分析corr = data.corr()xticks = ['x'+str(i) for i in range(12)]yticks = list(data.columns)fig = plt.figure()ax1 = fig.add_subplot(111)sns.heatmap(corr, cmap='GnBu', annot=True, ax= ax1, annot_kws=&#123;'size':6, 'color':'red'&#125;)ax1.set_xticklabels(xticks, rotation=0, fontsize=10)ax1.set_yticklabels(yticks, rotation=0, fontsize=10)plt.show() #从相关系数热力图可以看出，自变量之间的线性相关性比较弱#画出每个变量的IV值iv = [eval('x'+str(i)+'_iv') for i in range(1,11)]index=['x'+str(i) for i in range(1,11)]sns.barplot(x=index, y=iv)plt.ylabel('IV') Text(0,0.5,&#39;IV&#39;) #选x1, x2, x3, x7, x9， IV&gt;0.2高预测性#WOE编码x1= 'RevolvingUtilizationOfUnsecuredLines'x2= 'age'x3= 'NumberOfTime30-59DaysPastDueNotWorse'x7= 'NumberOfTimes90DaysLate'x9= 'NumberOfTime60-89DaysPastDueNotWorse'#定义WOE编码函数def woe_trans(data, var, woe, cut): woe_name = var+'_woe' for i in range(len(woe)): if i == 0: data.loc[(data[var]&lt;=cut[i+1]), woe_name] = woe[i] elif ((i&gt;0) and (i&lt;=(len(woe)-2))): data.loc[((data[var]&lt;=cut[i+1]) &amp; (data[var]&gt;cut[i])), woe_name] = woe[i] else: data.loc[(data[var]&gt;cut[i]), woe_name] = woe[i] return data for i in [1,2,3,7,9]: X_train = woe_trans(X_train, eval('x'+str(i)), eval('x'+str(i)+'_woe'), eval('x'+str(i)+'_cut')) #选取WOE编码之后的列作为训练数据集X_train = X_train.iloc[:, -5:] 4. 模型开发#建立逻辑回归模型import statsmodels.api as smX1=sm.add_constant(X_train)logit=sm.Logit(y_train, X1)result=logit.fit()print(result.summary()) Optimization terminated successfully. Current function value: 0.185840 Iterations 8 Logit Regression Results ============================================================================== Dep. Variable: SeriousDlqin2yrs No. Observations: 101740 Model: Logit Df Residuals: 101734 Method: MLE Df Model: 5 Date: Thu, 20 Sep 2018 Pseudo R-squ.: 0.2382 Time: 21:57:47 Log-Likelihood: -18907. converged: True LL-Null: -24820. LLR p-value: 0.000 ============================================================================================================ coef std err z P&gt;|z| [0.025 0.975] ------------------------------------------------------------------------------------------------------------ const -2.6195 0.015 -171.961 0.000 -2.649 -2.590 RevolvingUtilizationOfUnsecuredLines_woe -0.6441 0.016 -41.387 0.000 -0.675 -0.614 age_woe -0.4992 0.033 -15.294 0.000 -0.563 -0.435 NumberOfTime30-59DaysPastDueNotWorse_woe -0.5455 0.016 -34.470 0.000 -0.577 -0.515 NumberOfTimes90DaysLate_woe -0.5683 0.014 -41.833 0.000 -0.595 -0.542 NumberOfTime60-89DaysPastDueNotWorse_woe -0.4019 0.017 -23.032 0.000 -0.436 -0.368 ============================================================================================================ 5. 模型评估#对测试集进行WOE编码for i in [1,2,3,7,9]: X_test = woe_trans(X_test, eval('x'+str(i)), eval('x'+str(i)+'_woe'), eval('x'+str(i)+'_cut'))X_test = X_test.iloc[:, -5:] #绘制ROC曲线，计算AUCfrom sklearn import metricsX2=sm.add_constant(X_test)y_pred = result.predict(X2)fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)auc = metrics.auc(fpr, tpr)plt.plot(fpr, tpr, 'b', label='AUC=%.2f' %auc)plt.legend(loc='lower right')plt.plot([0,1], [0,1], 'r--')plt.xlim([0,1])plt.ylim([0,1])plt.xlabel('FPR')plt.ylabel('TPR')plt.show() 6. 评分系统建立#分数计算函数PDO=20base=600#all_woe是某一个体所有相关变量woe编码值构成的序列#total_score = base- PDO*(all_woe.dot(coef))/np.log(2)factor= -PDO/np.log(2)coef = result.paramsdef get_score(coef, woe, factor): scores=[round(coef*woe[i]*factor, 0) for i in range(len(woe))] return scores #计算各因子每个区间对应的分数x1_scores=get_score(coef[1], x1_woe, factor)print(x1_scores)x2_scores=get_score(coef[2], x2_woe, factor)x3_scores=get_score(coef[3], x3_woe, factor)x7_scores=get_score(coef[4], x7_woe, factor)x9_scores=get_score(coef[5], x9_woe, factor) [24.0, 23.0, 5.0, -20.0] data.head()datacopy=data data=datacopy for i in [1,2,3,7,9]: data = woe_trans(data, eval('x'+str(i)), eval('x'+str(i)+'_woe'), eval('x'+str(i)+'_cut'))data = data.iloc[:, -5:] #对整个data进行打分计算data_c=sm.add_constant(data) data['score']=500 + round(data_c.dot(coef)*factor, 0) data.head(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } RevolvingUtilizationOfUnsecuredLines_woe age_woe NumberOfTime30-59DaysPastDueNotWorse_woe NumberOfTimes90DaysLate_woe NumberOfTime60-89DaysPastDueNotWorse_woe score 0 -1.097 -0.264 -1.720 0.373 0.267 534.0 1 -1.097 -0.371 0.515 0.373 0.267 567.0 2 -1.097 -0.371 -0.878 -1.969 0.267 507.0 3 0.284 -0.564 0.515 0.373 0.267 590.0 4 -1.097 -0.184 -0.878 0.373 0.267 548.0 5 0.284 1.094 0.515 0.373 0.267 614.0 6 0.284 0.216 0.515 0.373 0.267 601.0 7 -1.097 -0.371 0.515 0.373 0.267 567.0 9 0.284 0.216 0.515 0.373 0.267 601.0 10 -1.097 -0.564 0.515 0.373 0.267 564.0","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://jinyaxuan.github.io/categories/机器学习/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://jinyaxuan.github.io/tags/Machine-Learning/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://jinyaxuan.github.io/categories/机器学习/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-02-15T08:09:15.599Z","updated":"2019-02-15T08:09:15.599Z","comments":true,"path":"2019/02/15/hello-world/","link":"","permalink":"http://jinyaxuan.github.io/2019/02/15/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","categories":[],"tags":[],"keywords":[]},{"title":"K-Means","slug":"k-Means","date":"2019-02-15T05:30:45.000Z","updated":"2019-02-15T08:09:15.599Z","comments":true,"path":"2019/02/15/k-Means/","link":"","permalink":"http://jinyaxuan.github.io/2019/02/15/k-Means/","excerpt":"","text":"电影评分的 k 均值聚类假设你是 Netflix 的一名数据分析师，你想要根据用户对不同电影的评分研究用户在电影品位上的相似和不同之处。了解这些评分对用户电影推荐系统有帮助吗？我们来研究下这方面的数据。 我们将使用的数据来自精彩的 MovieLens 用户评分数据集。我们稍后将在 notebook 中查看每个电影评分，先看看不同类型之间的评分比较情况。 数据集概述该数据集有两个文件。我们将这两个文件导入 pandas dataframe 中： import pandas as pdimport matplotlib.pyplot as pltimport numpy as npfrom scipy.sparse import csr_matriximport helper# Import the Movies datasetmovies = pd.read_csv('ml-latest-small/movies.csv')movies.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } movieId title genres 0 1 Toy Story (1995) Adventure|Animation|Children|Comedy|Fantasy 1 2 Jumanji (1995) Adventure|Children|Fantasy 2 3 Grumpier Old Men (1995) Comedy|Romance 3 4 Waiting to Exhale (1995) Comedy|Drama|Romance 4 5 Father of the Bride Part II (1995) Comedy # Import the ratings datasetratings = pd.read_csv('ml-latest-small/ratings.csv')ratings.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } userId movieId rating timestamp 0 1 31 2.5 1260759144 1 1 1029 3.0 1260759179 2 1 1061 3.0 1260759182 3 1 1129 2.0 1260759185 4 1 1172 4.0 1260759205 现在我们已经知道数据集的结构，每个表格中有多少条记录。 print('The dataset contains: ', len(ratings), ' ratings of ', len(movies), ' movies.') The dataset contains: 100004 ratings of 9125 movies. 爱情片与科幻片我们先查看一小部分用户，并看看他们喜欢什么类型的电影。我们将大部分数据预处理过程都隐藏在了辅助函数中，并重点研究聚类概念。在完成此 notebook 后，建议你快速浏览下 helper.py，了解这些辅助函数是如何实现的。 # Calculate the average rating of romance and scifi moviesgenre_ratings = helper.get_genre_ratings(ratings, movies, ['Romance', 'Sci-Fi'], ['avg_romance_rating', 'avg_scifi_rating'])genre_ratings.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } avg_romance_rating avg_scifi_rating userId 1 3.50 2.40 2 3.59 3.80 3 3.65 3.14 4 4.50 4.26 5 4.08 4.00 函数 get_genre_ratings 计算了每位用户对所有爱情片和科幻片的平均评分。我们对数据集稍微进行偏倚，删除同时喜欢科幻片和爱情片的用户，使聚类能够将他们定义为更喜欢其中一种类型。 biased_dataset = helper.bias_genre_rating_dataset(genre_ratings, 3.2, 2.5)print( \"Number of records: \", len(biased_dataset))biased_dataset.head() Number of records: 183 .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } userId avg_romance_rating avg_scifi_rating 0 1 3.50 2.40 1 3 3.65 3.14 2 6 2.90 2.75 3 7 2.93 3.36 4 12 2.89 2.62 可以看出我们有 183 位用户，对于每位用户，我们都得出了他们对看过的爱情片和科幻片的平均评分。 我们来绘制该数据集： %matplotlib inlinehelper.draw_scatterplot(biased_dataset['avg_scifi_rating'],'Avg scifi rating', biased_dataset['avg_romance_rating'], 'Avg romance rating') 我们可以在此样本中看到明显的偏差（我们故意创建的）。如果使用 k 均值将样本分成两组，效果如何？ # Let's turn our dataset into a listX = biased_dataset[['avg_scifi_rating','avg_romance_rating']].values 导入 KMeans 通过 n_clusters = 2 准备 KMeans 将数据集 X 传递给 KMeans 的 fit_predict 方法，并将聚类标签放入 predictions # TODO: Import KMeansfrom sklearn.cluster import KMeans# TODO: Create an instance of KMeans to find two clusterskmeans_1 = KMeans(n_clusters = 2)# TODO: use fit_predict to cluster the datasetpredictions = kmeans_1.fit_predict(X)# Plothelper.draw_clusters(biased_dataset, predictions) 可以看出分组的依据主要是每个人对爱情片的评分高低。如果爱情片的平均评分超过 3 星，则属于第一组，否则属于另一组。 如果分成三组，会发生什么？ # TODO: Create an instance of KMeans to find three clusterskmeans_2 = KMeans(n_clusters = 3)# TODO: use fit_predict to cluster the datasetpredictions_2 = kmeans_2.fit_predict(X)# Plothelper.draw_clusters(biased_dataset, predictions_2) 现在平均科幻片评分开始起作用了，分组情况如下所示： 喜欢爱情片但是不喜欢科幻片的用户 喜欢科幻片但是不喜欢爱情片的用户 即喜欢科幻片又喜欢爱情片的用户 再添加一组 # TODO: Create an instance of KMeans to find four clusterskmeans_3 = KMeans(n_clusters = 4)# TODO: use fit_predict to cluster the datasetpredictions_3 = kmeans_3.fit_predict(X)# Plothelper.draw_clusters(biased_dataset, predictions_3) 可以看出将数据集分成的聚类越多，每个聚类中用户的兴趣就相互之间越相似。 选择 K我们可以将数据点拆分为任何数量的聚类。对于此数据集来说，正确的聚类数量是多少？ 可以通过多种方式选择聚类 k。我们将研究一种简单的方式，叫做“肘部方法”。肘部方法会绘制 k 的上升值与使用该 k 值计算的总误差分布情况。 如何计算总误差？一种方法是计算平方误差。假设我们要计算 k=2 时的误差。有两个聚类，每个聚类有一个“图心”点。对于数据集中的每个点，我们将其坐标减去所属聚类的图心。然后将差值结果取平方（以便消除负值），并对结果求和。这样就可以获得每个点的误差值。如果将这些误差值求和，就会获得 k=2 时所有点的总误差。 现在的一个任务是对每个 k（介于 1 到数据集中的元素数量之间）执行相同的操作。 # Choose the range of k values to test.# We added a stride of 5 to improve performance. We don't need to calculate the error for every k valuepossible_k_values = range(2, len(X)+1, 5)# Calculate error values for all k values we're interested inerrors_per_k = [helper.clustering_errors(k, X) for k in possible_k_values] # Optional: Look at the values of K vs the silhouette score of running K-means with that value of klist(zip(possible_k_values, errors_per_k)) [(2, 0.35588178764728251), (7, 0.37324118163771741), (12, 0.35650856326047475), (17, 0.3741137698024623), (22, 0.37718217339438476), (27, 0.36071909992215945), (32, 0.37104279808464452), (37, 0.3649882241766923), (42, 0.36895091450195883), (47, 0.37696003940733186), (52, 0.38716548900081571), (57, 0.35079775582937778), (62, 0.34916584233387205), (67, 0.34839937724907), (72, 0.34907390154971468), (77, 0.34837739216196456), (82, 0.3309353056966266), (87, 0.34005916910201761), (92, 0.32494553685658306), (97, 0.32418331059507227), (102, 0.31329160485165003), (107, 0.29407239955320186), (112, 0.27366896911138017), (117, 0.28906341363336779), (122, 0.27342563040040624), (127, 0.25219179857975438), (132, 0.25320773897416415), (137, 0.2412264569953621), (142, 0.21855949198498667), (147, 0.19924498428850082), (152, 0.18722856283659275), (157, 0.16447514022082693), (162, 0.14697529680439808), (167, 0.12609539969216882), (172, 0.096865005870864829), (177, 0.064230120163174503), (182, 0.054644808743169397)] # Plot the each value of K vs. the silhouette score at that valuefig, ax = plt.subplots(figsize=(16, 6))ax.set_xlabel('K - number of clusters')ax.set_ylabel('Silhouette Score (higher is better)')ax.plot(possible_k_values, errors_per_k)# Ticks and gridxticks = np.arange(min(possible_k_values), max(possible_k_values)+1, 5.0)ax.set_xticks(xticks, minor=False)ax.set_xticks(xticks, minor=True)ax.xaxis.grid(True, which='both')yticks = np.arange(round(min(errors_per_k), 2), max(errors_per_k), .05)ax.set_yticks(yticks, minor=False)ax.set_yticks(yticks, minor=True)ax.yaxis.grid(True, which='both') 看了该图后发现，合适的 k 值包括 7、22、27、32 等（每次运行时稍微不同）。聚类 (k) 数量超过该范围将开始导致糟糕的聚类情况（根据轮廓分数） 我会选择 k=7，因为更容易可视化： # TODO: Create an instance of KMeans to find seven clusterskmeans_4 = KMeans(n_clusters=7)# TODO: use fit_predict to cluster the datasetpredictions_4 = kmeans_4.fit_predict(X)# plothelper.draw_clusters(biased_dataset, predictions_4, cmap='Accent') 注意：当你尝试绘制更大的 k 值（超过 10）时，需要确保你的绘制库没有对聚类重复使用相同的颜色。对于此图，我们需要使用 matplotlib colormap ‘Accent’，因为其他色图要么颜色之间的对比度不强烈，要么在超过 8 个或 10 个聚类后会重复利用某些颜色。 再加入动作片类型到目前为止，我们只查看了用户如何对爱情片和科幻片进行评分。我们再添加另一种类型，看看加入动作片类型后效果如何。 现在数据集如下所示： biased_dataset_3_genres = helper.get_genre_ratings(ratings, movies, ['Romance', 'Sci-Fi', 'Action'], ['avg_romance_rating', 'avg_scifi_rating', 'avg_action_rating'])biased_dataset_3_genres = helper.bias_genre_rating_dataset(biased_dataset_3_genres, 3.2, 2.5).dropna()print( \"Number of records: \", len(biased_dataset_3_genres))biased_dataset_3_genres.head() Number of records: 183 .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } userId avg_romance_rating avg_scifi_rating avg_action_rating 0 1 3.50 2.40 2.80 1 3 3.65 3.14 3.47 2 6 2.90 2.75 3.27 3 7 2.93 3.36 3.29 4 12 2.89 2.62 3.21 X_with_action = biased_dataset_3_genres[['avg_scifi_rating', 'avg_romance_rating', 'avg_action_rating']].values # TODO: Create an instance of KMeans to find seven clusterskmeans_5 = KMeans(n_clusters=7)# TODO: use fit_predict to cluster the datasetpredictions_5 = kmeans_5.fit_predict(X_with_action)# plothelper.draw_clusters_3d(biased_dataset_3_genres, predictions_5) 我们依然分别用 x 轴和 y 轴表示科幻片和爱情片。并用点的大小大致表示动作片评分情况（更大的点表示平均评分超过 3 颗星，更小的点表示不超过 3 颗星 ）。 可以看出添加类型后，用户的聚类分布发生了变化。为 k 均值提供的数据越多，每组中用户之间的兴趣越相似。但是如果继续这么绘制，我们将无法可视化二维或三维之外的情形。在下个部分，我们将使用另一种图表，看看多达 50 个维度的聚类情况。 电影级别的聚类现在我们已经知道 k 均值会如何根据用户的类型品位对用户进行聚类，我们再进一步分析，看看用户对单个影片的评分情况。为此，我们将数据集构建成 userId 与用户对每部电影的评分形式。例如，我们来看看以下数据集子集： # Merge the two tables then pivot so we have Users X Movies dataframeratings_title = pd.merge(ratings, movies[['movieId', 'title']], on='movieId' )user_movie_ratings = pd.pivot_table(ratings_title, index='userId', columns= 'title', values='rating')print('dataset dimensions: ', user_movie_ratings.shape, '\\n\\nSubset example:')user_movie_ratings.iloc[:6, :10] dataset dimensions: (671, 9064) Subset example: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } title \"Great Performances\" Cats (1998) $9.99 (2008) 'Hellboy': The Seeds of Creation (2004) 'Neath the Arizona Skies (1934) 'Round Midnight (1986) 'Salem's Lot (2004) 'Til There Was You (1997) 'burbs, The (1989) 'night Mother (1986) (500) Days of Summer (2009) userId 1 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 5 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 6 NaN NaN NaN NaN NaN NaN NaN 4.0 NaN NaN NaN 值的优势表明了第一个问题。大多数用户没有看过大部分电影，并且没有为这些电影评分。这种数据集称为“稀疏”数据集，因为只有少数单元格有值。 为了解决这一问题，我们按照获得评分次数最多的电影和对电影评分次数最多的用户排序。这样可以形成更“密集”的区域，使我们能够查看数据集的顶部数据。 如果我们要选择获得评分次数最多的电影和对电影评分次数最多的用户，则如下所示： n_movies = 30n_users = 18most_rated_movies_users_selection = helper.sort_by_rating_density(user_movie_ratings, n_movies, n_users)print('dataset dimensions: ', most_rated_movies_users_selection.shape)most_rated_movies_users_selection.head() dataset dimensions: (18, 30) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } title Forrest Gump (1994) Pulp Fiction (1994) Shawshank Redemption, The (1994) Silence of the Lambs, The (1991) Star Wars: Episode IV - A New Hope (1977) Jurassic Park (1993) Matrix, The (1999) Toy Story (1995) Schindler's List (1993) Terminator 2: Judgment Day (1991) ... Dances with Wolves (1990) Fight Club (1999) Usual Suspects, The (1995) Seven (a.k.a. Se7en) (1995) Lion King, The (1994) Godfather, The (1972) Lord of the Rings: The Fellowship of the Ring, The (2001) Apollo 13 (1995) True Lies (1994) Twelve Monkeys (a.k.a. 12 Monkeys) (1995) 29 5.0 5.0 5.0 4.0 4.0 4.0 3.0 4.0 5.0 4.0 ... 5.0 4.0 5.0 4.0 3.0 5.0 3.0 5.0 4.0 2.0 508 4.0 5.0 4.0 4.0 5.0 3.0 4.5 3.0 5.0 2.0 ... 5.0 4.0 5.0 4.0 3.5 5.0 4.5 3.0 2.0 4.0 14 1.0 5.0 2.0 5.0 5.0 3.0 5.0 2.0 4.0 4.0 ... 3.0 5.0 5.0 5.0 4.0 5.0 5.0 3.0 4.0 4.0 72 5.0 5.0 5.0 4.5 4.5 4.0 4.5 5.0 5.0 3.0 ... 4.5 5.0 5.0 5.0 5.0 5.0 5.0 3.5 3.0 5.0 653 4.0 5.0 5.0 4.5 5.0 4.5 5.0 5.0 5.0 5.0 ... 4.5 5.0 5.0 4.5 5.0 4.5 5.0 5.0 4.0 5.0 5 rows × 30 columns 这样更好分析。我们还需要指定一个可视化这些评分的良好方式，以便在查看更庞大的子集时能够直观地识别这些评分（稍后变成聚类）。 我们使用颜色代替评分数字： helper.draw_movies_heatmap(most_rated_movies_users_selection) 每列表示一部电影。每行表示一位用户。单元格的颜色根据图表右侧的刻度表示用户对该电影的评分情况。 注意到某些单元格是白色吗？表示相应用户没有对该电影进行评分。在现实中进行聚类时就会遇到这种问题。与一开始经过整理的示例不同，现实中的数据集经常比较稀疏，数据集中的部分单元格没有值。这样的话，直接根据电影评分对用户进行聚类不太方便，因为 k 均值通常不喜欢缺失值。 为了提高性能，我们将仅使用 1000 部电影的评分（数据集中一共有 9000 部以上）。 user_movie_ratings = pd.pivot_table(ratings_title, index='userId', columns= 'title', values='rating')most_rated_movies_1k = helper.get_most_rated_movies(user_movie_ratings, 1000) 为了使 sklearn 对像这样缺少值的数据集运行 k 均值聚类，我们首先需要将其转型为稀疏 csr 矩阵类型（如 SciPi 库中所定义）。 要从 pandas dataframe 转换为稀疏矩阵，我们需要先转换为 SparseDataFrame，然后使用 pandas 的 to_coo() 方法进行转换。 注意：只有较新版本的 pandas 具有to_coo()。如果你在下个单元格中遇到问题，确保你的 pandas 是最新版本。 sparse_ratings = csr_matrix(pd.SparseDataFrame(most_rated_movies_1k).to_coo()) 我们来聚类吧！对于 k 均值，我们需要指定 k，即聚类数量。我们随意地尝试 k=20（选择 k 的更佳方式如上述肘部方法所示。但是，该方法需要一定的运行时间。): # 20 clusterspredictions = KMeans(n_clusters=20, algorithm='full').fit_predict(sparse_ratings) 为了可视化其中一些聚类，我们需要将每个聚类绘制成热图： max_users = 70max_movies = 50clustered = pd.concat([most_rated_movies_1k.reset_index(), pd.DataFrame(&#123;'group':predictions&#125;)], axis=1)helper.draw_movie_clusters(clustered, max_users, max_movies) cluster # 7 # of users in cluster: 276. # of users in plot: 70 cluster # 16 # of users in cluster: 64. # of users in plot: 64 cluster # 0 # of users in cluster: 26. # of users in plot: 26 cluster # 2 # of users in cluster: 72. # of users in plot: 70 cluster # 6 # of users in cluster: 17. # of users in plot: 17 cluster # 3 # of users in cluster: 37. # of users in plot: 37 cluster # 11 # of users in cluster: 12. # of users in plot: 12 cluster # 18 # of users in cluster: 35. # of users in plot: 35 cluster # 9 # of users in cluster: 55. # of users in plot: 55 cluster # 8 # of users in cluster: 27. # of users in plot: 27 cluster # 15 # of users in cluster: 15. # of users in plot: 15 需要注意以下几个事项： 聚类中的评分越相似，你在该聚类中就越能发现颜色相似的垂直线。 在聚类中发现了非常有趣的规律： 某些聚类比其他聚类更稀疏，其中的用户可能比其他聚类中的用户看的电影更少，评分的电影也更少。 某些聚类主要是黄色，汇聚了非常喜欢特定类型电影的用户。其他聚类主要是绿色或海蓝色，表示这些用户都认为某些电影可以评 2-3 颗星。 注意每个聚类中的电影有何变化。图表对数据进行了过滤，仅显示评分最多的电影，然后按照平均评分排序。 能找到《指环王》在每个聚类中位于哪个位置吗？《星球大战》呢？ 很容易发现具有相似颜色的水平线，表示评分变化不大的用户。这可能是 Netflix 从基于星级的评分切换到喜欢/不喜欢评分的原因之一。四颗星评分对不同的人来说，含义不同。 我们在可视化聚类时，采取了一些措施（过滤/排序/切片）。因为这种数据集比较“稀疏”，大多数单元格没有值（因为大部分用户没有看过大部分电影）。 预测我们选择一个聚类和一位特定的用户，看看该聚类可以使我们执行哪些实用的操作。 首先选择一个聚类： # TODO: Pick a cluster ID from the clusters abovecluster_number = 11# Let's filter to only see the region of the dataset with the most number of values n_users = 75n_movies = 300cluster = clustered[clustered.group == cluster_number].drop(['index', 'group'], axis=1)cluster = helper.sort_by_rating_density(cluster, n_movies, n_users)helper.draw_movies_heatmap(cluster, axis_labels=False) 聚类中的实际评分如下所示： cluster.fillna('').head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Amadeus (1984) Annie Hall (1977) One Flew Over the Cuckoo's Nest (1975) Fargo (1996) Cool Hand Luke (1967) Chinatown (1974) North by Northwest (1959) Citizen Kane (1941) Wizard of Oz, The (1939) Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981) ... Sense and Sensibility (1995) Top Gun (1986) Flashdance (1983) Jerry Maguire (1996) Superman (1978) Abyss, The (1989) Devil in a Blue Dress (1995) Beetlejuice (1988) Dial M for Murder (1954) Broken Arrow (1996) 0 5.0 4.0 4.0 5 4 4 4 5 4 ... 3 3 3 4 3 1 4.0 4.0 4.0 4 5 5 3 5 4 3 ... 2 3 2 4 2 3 2 5.0 4.0 5.0 5 5 5 5 5 5 5 ... 3 4 5 4 8 2.0 5.0 2.0 5 3 5 3 4 5 3 ... 4.5 2 4 3 3 3 10 3.0 4.0 3.0 4 5 4 4 4 5 ... 5 4 3 2 5 rows × 300 columns 从表格中选择一个空白单元格。因为用户没有对该电影评分，所以是空白状态。能够预测她是否喜欢该电影吗？因为该用户属于似乎具有相似品位的用户聚类，我们可以计算该电影在此聚类中的平均评分，结果可以作为她是否喜欢该电影的合理预测依据。 # TODO: Fill in the name of the column/movie. e.g. 'Forrest Gump (1994)'# Pick a movie from the table above since we're looking at a subsetmovie_name = 'Forrest Gump (1994)'cluster[movie_name].mean() 3.6666666666666665 这就是我们关于她会如何对该电影进行评分的预测。 推荐我们回顾下上一步的操作。我们使用 k 均值根据用户的评分对用户进行聚类。这样就形成了具有相似评分的用户聚类，因此通常具有相似的电影品位。基于这一点，当某个用户对某部电影没有评分时，我们对该聚类中所有其他用户的评分取平均值，该平均值就是我们猜测该用户对该电影的喜欢程度。 根据这一逻辑，如果我们计算该聚类中每部电影的平均分数，就可以判断该“品位聚类”对数据集中每部电影的喜欢程度。 # The average rating of 20 movies as rated by the users in the clustercluster.mean().head(20) Amadeus (1984) 3.833333 Annie Hall (1977) 4.291667 One Flew Over the Cuckoo&#39;s Nest (1975) 4.208333 Fargo (1996) 4.454545 Cool Hand Luke (1967) 4.636364 Chinatown (1974) 4.454545 North by Northwest (1959) 4.409091 Citizen Kane (1941) 4.681818 Wizard of Oz, The (1939) 4.500000 Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981) 4.272727 Butch Cassidy and the Sundance Kid (1969) 4.045455 Star Wars: Episode V - The Empire Strikes Back (1980) 4.090909 Groundhog Day (1993) 3.727273 Gone with the Wind (1939) 4.272727 It&#39;s a Wonderful Life (1946) 4.272727 2001: A Space Odyssey (1968) 4.272727 Shawshank Redemption, The (1994) 4.363636 Philadelphia Story, The (1940) 4.409091 Bonnie and Clyde (1967) 4.150000 To Kill a Mockingbird (1962) 4.400000 dtype: float64 这对我们来说变得非常实用，因为现在我们可以使用它作为推荐引擎，使用户能够发现他们可能喜欢的电影。 当用户登录我们的应用时，现在我们可以向他们显示符合他们的兴趣品位的电影。推荐方式是选择聚类中该用户尚未评分的最高评分的电影。 # TODO: Pick a user ID from the dataset# Look at the table above outputted by the command \"cluster.fillna('').head()\" # and pick one of the user ids (the first column in the table)user_id = 11# Get all this user's ratingsuser_2_ratings = cluster.loc[user_id, :]# Which movies did they not rate? (We don't want to recommend movies they've already rated)user_2_unrated_movies = user_2_ratings[user_2_ratings.isnull()]# What are the ratings of these movies the user did not rate?avg_ratings = pd.concat([user_2_unrated_movies, cluster.mean()], axis=1, join='inner').loc[:,0]# Let's sort by rating so the highest rated movies are presented firstavg_ratings.sort_values(ascending=False)[:20] Remains of the Day, The (1993) 4.666667 Saving Private Ryan (1998) 4.642857 African Queen, The (1951) 4.625000 Lone Star (1996) 4.600000 Godfather: Part II, The (1974) 4.500000 Singin&#39; in the Rain (1952) 4.500000 My Cousin Vinny (1992) 4.500000 Raising Arizona (1987) 4.500000 Fargo (1996) 4.454545 Rain Man (1988) 4.400000 Full Metal Jacket (1987) 4.400000 Sense and Sensibility (1995) 4.375000 Fried Green Tomatoes (1991) 4.333333 Room with a View, A (1986) 4.300000 It&#39;s a Wonderful Life (1946) 4.272727 Dial M for Murder (1954) 4.250000 Laura (1944) 4.250000 American Graffiti (1973) 4.250000 Much Ado About Nothing (1993) 4.250000 Ordinary People (1980) 4.250000 Name: 0, dtype: float64 这些是向用户推荐的前 20 部电影！ 练习： 如果聚类中有一部电影只有一个评分，评分是 5 颗星。该电影在该聚类中的平均评分是多少？这会对我们的简单推荐引擎有何影响？你会如何调整推荐系统，以解决这一问题？ 关于协同过滤的更多信息 这是一个简单的推荐引擎，展示了“协同过滤”的最基本概念。有很多可以改进该引擎的启发法和方法。为了推动在这一领域的发展，Netflix 设立了 Netflix 奖项 ，他们会向对 Netflix 的推荐算法做出最大改进的算法奖励 1,000,000 美元。 在 2009 年，“BellKor’s Pragmatic Chaos”团队获得了这一奖项。这篇论文介绍了他们采用的方式，其中包含大量方法。 Netflix 最终并没有使用这个荣获 1,000,000 美元奖励的算法，因为他们采用了流式传输的方式，并产生了比电影评分要庞大得多的数据集——用户搜索了哪些内容？用户在此会话中试看了哪些其他电影？他们是否先看了一部电影，然后切换到了其他电影？这些新的数据点可以提供比评分本身更多的线索。 深入研究 该 notebook 显示了用户级推荐系统。我们实际上可以使用几乎一样的代码进行商品级推荐。例如亚马逊的“购买（评价或喜欢）此商品的客户也购买了（评价了或喜欢）以下商品：” 。我们可以在应用的每个电影页面显示这种推荐。为此，我们只需将数据集转置为“电影 X 用户”形状，然后根据评分之间的联系对电影（而不是用户）进行聚类。 我们从数据集 Movie Lens 中抽取了最小的子集，只包含 100,000 个评分。如果你想深入了解电影评分数据，可以查看他们的完整数据集，其中包含 2400 万个评分。","categories":[],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://jinyaxuan.github.io/tags/Machine-Learning/"}],"keywords":[]},{"title":"CNN网络图像识别","slug":"My-First-Post","date":"2019-02-15T05:05:16.000Z","updated":"2019-02-15T10:19:04.357Z","comments":true,"path":"2019/02/15/My-First-Post/","link":"","permalink":"http://jinyaxuan.github.io/2019/02/15/My-First-Post/","excerpt":"","text":"简介本文使用keras(2.1.4)——其他版本有坑. 网络框架搭建CNN网络，对cifar10数据集进行图像识别，cifar10是一种自带label的图像数据集，数据集种类十分丰富可以很好的检验网络性能，话不多说直接进入正题 第一步获取数据集通过keras可以直接下载cifar10数据集(数据集比较大可能需要一些时间)import keras#使用cifar10数据集from keras.datasets import cifar10(x_train, y_train), (x_test, y_test) = cifar10.load_data() 展示前24张图片观察数据集的部分样本别问为什么，要有一个程序员的严谨！！严谨！！严谨！！(重要的事说3遍)import numpy as npimport matplotlib.pyplot as pltfig = plt.figure(figsize=(20,5))for i in range(36): ax = fig.add_subplot(3, 12, i + 1, xticks=[], yticks=[]) ax.imshow(np.squeeze(x_train[i])) 所有数据集除以255重构图像因为图像单个像素中最大值为255，将其除以255是将每一个像素缩放到0-1之间，类似于标准化x_train = x_train.astype('float32')/255x_test = x_test.astype('float32')/255 将数据分解为测试集、训练集、验证集from keras.utils import np_utils# 将标签转化为one-hotnum_classes = len(np.unique(y_train))y_train = keras.utils.to_categorical(y_train, num_classes)y_test = keras.utils.to_categorical(y_test, num_classes)# 将数据分解为训练集和测试集(x_train, x_valid) = x_train[5000:], x_train[:5000](y_train, y_valid) = y_train[5000:], y_train[:5000]# 输出训练集形状print('x_train shape:', x_train.shape)# 输出每一个集合的长度print(x_train.shape[0], 'train samples')print(x_test.shape[0], 'test samples')print(x_valid.shape[0], 'validation samples') 开始构建卷积神经网络from keras.models import Sequentialfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout#初始化网络类型，选择顺序网络model = Sequential()#添加卷积层，使用same填充，relu激活model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(32, 32, 3)))#添加池化层model.add(MaxPooling2D(pool_size=2))model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))model.add(MaxPooling2D(pool_size=2))model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))model.add(MaxPooling2D(pool_size=2))#舍弃部分神经元，避免过拟合model.add(Dropout(0.3))#数据扁平化model.add(Flatten())model.add(Dense(500, activation='relu'))model.add(Dropout(0.4))model.add(Dense(10, activation='softmax'))#模型确认model.summary()#模型启动，定义损失函数，优化器，评分标准model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) 模型训练开始！心疼一波没有GPU的小伙伴。。。from keras.callbacks import ModelCheckpoint #训练模型checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, save_best_only=True)hist = model.fit(x_train, y_train, batch_size=32, epochs=100, validation_data=(x_valid, y_valid), callbacks=[checkpointer], verbose=2, shuffle=True) 测试集预测终于到了激动人心的时刻，想不想知道自己搭建的模型的性能? 等着吧！# 获取训练集预测y_hat = model.predict(x_test)# 定义文本标签--来源:(source: https://www.cs.toronto.edu/~kriz/cifar.html)cifar10_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'] 结果展示！！！！！# 展示样本训练结果fig = plt.figure(figsize=(20, 8))for i, idx in enumerate(np.random.choice(x_test.shape[0], size=32, replace=False)): ax = fig.add_subplot(4, 8, i + 1, xticks=[], yticks=[]) ax.imshow(np.squeeze(x_test[idx])) pred_idx = np.argmax(y_hat[idx]) true_idx = np.argmax(y_test[idx]) ax.set_title(\"&#123;&#125; (&#123;&#125;)\".format(cifar10_labels[pred_idx], cifar10_labels[true_idx]), color=(\"green\" if pred_idx == true_idx else \"red\")) 感言:说实话图像识别的发展是一个很漫长的过程，通过结果可以发现有时候我们确实有点为难机器了，不信你们自己看看那训练结果。。 有些图片你自己都不知道是什么东西。。 还有一点 感谢各位的支持 ！拜拜👋！ 还没完。 没有GPU的小伙伴可以去亚马逊申请免费的GPU服务器后 嘿嘿😁最后像提供数据集的前辈们致敬！","categories":[],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://jinyaxuan.github.io/tags/Deep-Learning/"}],"keywords":[]}]}