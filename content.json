{"meta":{"title":"单身程序员的小窝","subtitle":null,"description":null,"author":"Jindong","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"动态规划","slug":"动态规划","date":"2019-02-16T11:19:24.000Z","updated":"2019-02-16T14:48:20.115Z","comments":false,"path":"2019/02/16/动态规划/","link":"","permalink":"http://yoursite.com/2019/02/16/动态规划/","excerpt":"","text":"迷你项目：动态规划在此 notebook 中，你将自己编写很多经典动态规划算法的实现。 虽然我们提供了一些起始代码，但是你可以删掉这些提示并从头编写代码。 第 0 部分：探索 FrozenLakeEnv请使用以下代码单元格创建 FrozenLake 环境的实例。 123from frozenlake import FrozenLakeEnvenv = FrozenLakeEnv() 智能体将会在 $4 \\times 4$ 网格世界中移动，状态编号如下所示： [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15]] 智能体可以执行 4 个潜在动作： 1234LEFT = 0DOWN = 1RIGHT = 2UP = 3 因此，$\\mathcal{S}^+ = {0, 1, \\ldots, 15}$ 以及 $\\mathcal{A} = {0, 1, 2, 3}$。请通过运行以下代码单元格验证这一点。 1234567# print the state space and action spaceprint(env.observation_space)print(env.action_space)# print the total number of states and actionsprint(env.nS)print(env.nA) Discrete(16) Discrete(4) 16 4 动态规划假设智能体完全了解 MDP。我们已经修改了 frozenlake.py 文件以使智能体能够访问一步动态特性。 请执行以下代码单元格以返回特定状态和动作对应的一步动态特性。具体而言，当智能体在网格世界中以状态 1 向左移动时，env.P[1][0] 会返回每个潜在奖励的概率和下一个状态。 1env.P[0][0] [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)] 每个条目的格式如下所示 prob, next_state, reward, done 其中： prob 详细说明了相应的 (next_state, reward) 对的条件概率，以及 如果 next_state 是终止状态，则 done 是 True ，否则是 False。 因此，我们可以按照以下方式解析 env.P[1][0]： \\mathbb{P}(S_{t+1}=s',R_{t+1}=r|S_t=1,A_t=0) = \\begin{cases} \\frac{1}{3} \\text{ if } s'=1, r=0\\\\ \\frac{1}{3} \\text{ if } s'=0, r=0\\\\ \\frac{1}{3} \\text{ if } s'=5, r=0\\\\ 0 \\text{ else} \\end{cases}你可以随意更改上述代码单元格，以探索在其他（状态、动作）对下环境的行为是怎样的。 第 1 部分：迭代策略评估在此部分，你将自己编写迭代策略评估的实现。 你的算法应该有四个输入参数： env：这是 OpenAI Gym 环境的实例，其中 env.P 会返回一步动态特性。 policy：这是一个二维 numpy 数组，其中 policy.shape[0] 等于状态数量 (env.nS) ， policy.shape[1] 等于动作数量 (env.nA) 。policy[s][a] 返回智能体在状态 s 时根据该策略选择动作 a 的概率。 gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。 theta：这是一个非常小的正数，用于判断估算值是否足够地收敛于真值函数 (默认值为：1e-8）。 该算法会返回以下输出结果： V：这是一个一维numpy数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 在输入策略下的估算值。 请完成以下代码单元格中的函数。 12345678910111213141516171819import numpy as npdef policy_evaluation(env, policy, gamma=1, theta=1e-8): V = np.zeros(env.nS) ## TODO: complete the function while True: delta = 0 for s in range(env.nS): Vs = 0 for a, action_prob in enumerate(policy[s]): for prob, next_state, reward, done in env.P[s][a]: Vs += action_prob * prob * (reward + gamma * V[next_state]) delta = max(delta, np.abs(V[s]-Vs)) V[s] = Vs if delta &lt; theta: break return V 我们将评估等概率随机策略 $\\pi$，其中对于所有 $s\\in\\mathcal{S}$ 和 $a\\in\\mathcal{A}(s)$ ，$\\pi(a|s) = \\frac{1}{|\\mathcal{A}(s)|}$。 请使用以下代码单元格在变量 random_policy中指定该策略。 1random_policy = np.ones([env.nS, env.nA]) / env.nA 运行下个代码单元格以评估等概率随机策略并可视化输出结果。状态值函数已调整形状，以匹配网格世界的形状。 123456from plot_utils import plot_values# evaluate the policy V = policy_evaluation(env, random_policy)plot_values(V) 运行以下代码单元格以测试你的函数。如果代码单元格返回 PASSED，则表明你正确地实现了该函数！ 注意：为了确保结果准确，确保你的 policy_evaluation 函数满足上文列出的要求（具有四个输入、一个输出，并且没有更改输入参数的默认值）。 123import check_testcheck_test.run_check('policy_evaluation_check', policy_evaluation) PASSED 第 2 部分：通过 $v\\pi$ 获取 $q\\pi$在此部分，你将编写一个函数，该函数的输入是状态值函数估值以及一些状态 $s\\in\\mathcal{S}$。它会返回输入状态 $s\\in\\mathcal{S}$ 对应的动作值函数中的行。即你的函数应同时接受输入 $v\\pi$ 和 $s$，并针对所有 $a\\in\\mathcal{A}(s)$ 返回 $q\\pi(s,a)$。 你的算法应该有四个输入参数： env：这是 OpenAI Gym 环境的实例，其中 env.P 会返回一步动态特性。 V：这是一个一维 numpy 数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 的估值。 s：这是环境中的状态对应的整数。它应该是在 0 到 (env.nS)-1（含）之间的值。 gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。 该算法会返回以下输出结果： q：这是一个一维 numpy 数组，其中 q.shape[0] 等于动作数量 (env.nA)。q[a] 包含状态 s 和动作 a 的（估算）值。 请完成以下代码单元格中的函数。 12345678def q_from_v(env, V, s, gamma=1): q = np.zeros(env.nA) ## TODO: complete the function for a in range(env.nA): for prob, next_state, reward, done in env.P[s][a]: q[a] += prob * (reward + gamma * V[next_state]) return q 请运行以下代码单元格以输出上述状态值函数对应的动作值函数。 12345Q = np.zeros([env.nS, env.nA])for s in range(env.nS): Q[s] = q_from_v(env, V, s)print(\"Action-Value Function:\")print(Q) Action-Value Function: [[ 0.0147094 0.01393978 0.01393978 0.01317015] [ 0.00852356 0.01163091 0.0108613 0.01550788] [ 0.02444514 0.02095298 0.02406033 0.01435346] [ 0.01047649 0.01047649 0.00698432 0.01396865] [ 0.02166487 0.01701828 0.01624865 0.01006281] [ 0. 0. 0. 0. ] [ 0.05433538 0.04735105 0.05433538 0.00698432] [ 0. 0. 0. 0. ] [ 0.01701828 0.04099204 0.03480619 0.04640826] [ 0.07020885 0.11755991 0.10595784 0.05895312] [ 0.18940421 0.17582037 0.16001424 0.04297382] [ 0. 0. 0. 0. ] [ 0. 0. 0. 0. ] [ 0.08799677 0.20503718 0.23442716 0.17582037] [ 0.25238823 0.53837051 0.52711478 0.43929118] [ 0. 0. 0. 0. ]] 运行以下代码单元格以测试你的函数。如果代码单元格返回 PASSED，则表明你正确地实现了该函数！ 注意：为了确保结果准确，确保 q_from_v 函数满足上文列出的要求（具有四个输入、一个输出，并且没有更改输入参数的默认值）。 1check_test.run_check('q_from_v_check', q_from_v) PASSED 第 3 部分：策略改进在此部分，你将自己编写策略改进实现。 你的算法应该有三个输入参数： env：这是 OpenAI Gym 环境的实例，其中 env.P 会返回一步动态特性。 V：这是一个一维 numpy 数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 的估值。 gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。 该算法会返回以下输出结果： policy：这是一个二维 numpy 数组，其中 policy.shape[0] 等于状态数量 (env.nS) ， policy.shape[1] 等于动作数量 (env.nA) 。policy[s][a] 返回智能体在状态 s 时根据该策略选择动作 a 的概率。 请完成以下代码单元格中的函数。建议你使用你在上文实现的 q_from_v 函数。 123456789101112def policy_improvement(env, V, gamma=1): policy = np.zeros([env.nS, env.nA]) / env.nA ## TODO: complete the function for s in range(env.nS): q = q_from_v(env, V, s, gamma) # best_a = np.argwhere(q==np.max(q)).flatten()# policy[s] = np.sum([np.eye(env.nA)[i] for i in best_a], axis=0)/len(best_a) policy[s][np.argmax(q)] = 1 return policy 运行以下代码单元格以测试你的函数。如果代码单元格返回 PASSED，则表明你正确地实现了该函数！ 注意：为了确保结果准确，确保 policy_improvement 函数满足上文列出的要求（具有三个输入、一个输出，并且没有更改输入参数的默认值）。 在继续转到该 notebook 的下个部分之前，强烈建议你参阅 Dynamic_Programming_Solution.ipynb 中的解决方案。该函数有很多正确的实现方式！ 1check_test.run_check('policy_improvement_check', policy_improvement) PASSED 第 4 部分：策略迭代在此部分，你将自己编写策略迭代的实现。该算法会返回最优策略，以及相应的状态值函数。 你的算法应该有三个输入参数： env：这是 OpenAI Gym 环境的实例，其中 env.P 会返回一步动态特性。 gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。 theta：这是一个非常小的正数，用于判断策略评估步骤是否足够地收敛于真值函数 (默认值为：1e-8）。 该算法会返回以下输出结果： policy：这是一个二维 numpy 数组，其中 policy.shape[0] 等于状态数量 (env.nS) ， policy.shape[1] 等于动作数量 (env.nA) 。policy[s][a] 返回智能体在状态 s 时根据该策略选择动作 a 的概率。 V：这是一个一维 numpy 数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 的估值。 请完成以下代码单元格中的函数。强烈建议你使用你在上文实现的 policy_evaluation 和 policy_improvement 函数。 1234567891011121314import copydef policy_iteration(env, gamma=1, theta=1e-8): policy = np.ones([env.nS, env.nA]) / env.nA while True: V = policy_evaluation(env, policy, gamma, theta) new_policy = policy_improvement(env, V) ## TODO: complete the function if np.max(abs(policy_evaluation(env, policy) - policy_evaluation(env, new_policy))) &lt; theta*1e2: break policy = copy.copy(new_policy) return policy, V 运行下个代码单元格以解决该 MDP 并可视化输出结果。最优状态值函数已调整形状，以匹配网格世界的形状。 将该最优状态值函数与此 notebook 第 1 部分的状态值函数进行比较。最优状态值函数一直都大于或等于等概率随机策略的状态值函数吗？ 12345678# obtain the optimal policy and optimal state-value functionpolicy_pi, V_pi = policy_iteration(env)# print the optimal policyprint(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")print(policy_pi,\"\\n\")plot_values(V_pi) Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3): [[ 1. 0. 0. 0.] [ 0. 0. 0. 1.] [ 0. 0. 0. 1.] [ 0. 0. 0. 1.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 0. 0. 0. 1.] [ 0. 1. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 0. 0. 1. 0.] [ 0. 1. 0. 0.] [ 1. 0. 0. 0.]] 运行以下代码单元格以测试你的函数。如果代码单元格返回 PASSED，则表明你正确地实现了该函数！ 注意：为了确保结果准确，确保 policy_iteratio 函数满足上文列出的要求（具有三个输入、两个输出，并且没有更改输入参数的默认值）。 1check_test.run_check('policy_iteration_check', policy_iteration) PASSED 第 5 部分：截断策略迭代在此部分，你将自己编写截断策略迭代的实现。 首先，你将实现截断策略评估。你的算法应该有五个输入参数： env：这是 OpenAI Gym 环境的实例，其中 env.P 会返回一步动态特性。 policy：这是一个二维 numpy 数组，其中 policy.shape[0] 等于状态数量 (env.nS) ， policy.shape[1] 等于动作数量 (env.nA) 。policy[s][a] 返回智能体在状态 s 时根据该策略选择动作 a 的概率。 V：这是一个一维 numpy 数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 的估值。 max_it：这是一个正整数，对应的是经历状态空间的次数（默认值为：1）。 gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。 该算法会返回以下输出结果： V：这是一个一维 numpy 数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 的估值。 请完成以下代码单元格中的函数。 12345678910111213def truncated_policy_evaluation(env, policy, V, max_it=1, gamma=1): ## TODO: complete the function counter = 0 while counter &lt; max_it: for s in range(env.nS): v = 0 q = q_from_v(env, V, s, gamma) for a, action_prob in enumerate(policy[s]): v += action_prob * q[a] V[s] = v counter += 1 return V 接着，你将实现截断策略迭代。你的算法应该接受四个输入参数： env：这是 OpenAI Gym 环境的实例，其中 env.P 会返回一步动态特性。 max_it：这是一个正整数，对应的是经历状态空间的次数（默认值为：1）。 gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。 theta：这是一个非常小的正整数，用作停止条件（默认值为：1e-8）。 该算法会返回以下输出结果： policy：这是一个二维 numpy 数组，其中 policy.shape[0] 等于状态数量 (env.nS) ， policy.shape[1] 等于动作数量 (env.nA) 。policy[s][a] 返回智能体在状态 s 时根据该策略选择动作 a 的概率。 V：这是一个一维 numpy 数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 的估值。 请完成以下代码单元格中的函数。 123456789101112def truncated_policy_iteration(env, max_it=1, gamma=1, theta=1e-8): V = np.zeros(env.nS) policy = np.zeros([env.nS, env.nA]) / env.nA ## TODO: complete the function while True: policy = policy_improvement(env, V) old_V = copy.copy(V) V = policy_evaluation(env, policy, gamma=1, theta=1e-8) if max(abs(V - old_V)) &lt; theta: break return policy, V 运行下个代码单元格以解决该 MDP 并可视化输出结果。状态值函数已调整形状，以匹配网格世界的形状。 请实验不同的 max_it 参数值。始终都能获得最优状态值函数吗？ 12345678policy_tpi, V_tpi = truncated_policy_iteration(env, max_it=2)# print the optimal policyprint(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")print(policy_tpi,\"\\n\")# plot the optimal state-value functionplot_values(V_tpi) Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3): [[ 1. 0. 0. 0.] [ 0. 0. 0. 1.] [ 0. 0. 0. 1.] [ 0. 0. 0. 1.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 0. 0. 0. 1.] [ 0. 1. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 0. 0. 1. 0.] [ 0. 1. 0. 0.] [ 1. 0. 0. 0.]] 运行以下代码单元格以测试你的函数。如果代码单元格返回 PASSED，则表明你正确地实现了该函数！ 注意：为了确保结果准确，确保 truncated_policy_iteration 函数满足上文列出的要求（具有四个输入、两个输出，并且没有更改输入参数的默认值）。 1check_test.run_check('truncated_policy_iteration_check', truncated_policy_iteration) PASSED 第 6 部分：值迭代在此部分，你将自己编写值迭代的实现。 你的算法应该接受三个输入参数： env：这是 OpenAI Gym 环境的实例，其中 env.P 会返回一步动态特性。 gamma：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：1。 theta：这是一个非常小的正整数，用作停止条件（默认值为：1e-8）。 该算法会返回以下输出结果： policy：这是一个二维 numpy 数组，其中 policy.shape[0] 等于状态数量 (env.nS) ， policy.shape[1] 等于动作数量 (env.nA) 。policy[s][a] 返回智能体在状态 s 时根据该策略选择动作 a 的概率。 V：这是一个一维 numpy 数组，其中 V.shape[0] 等于状态数量 (env.nS)。V[s] 包含状态 s 的估值。 12345678910111213141516def value_iteration(env, gamma=1, theta=1e-8): V = np.zeros(env.nS) ## TODO: complete the function while True: delta = 0 for s in range(env.nS): v = copy.copy(V[s]) V[s] = max(q_from_v(env, V, s, gamma)) delta = max(delta, abs(v-V[s])) if delta &lt; theta: break policy = policy_improvement(env, V, gamma) return policy, V 运行下个代码单元格以解决该 MDP 并可视化输出结果。状态值函数已调整形状，以匹配网格世界的形状。 12345678policy_vi, V_vi = value_iteration(env)# print the optimal policyprint(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")print(policy_vi,\"\\n\")# plot the optimal state-value functionplot_values(V_vi) Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3): [[ 1. 0. 0. 0.] [ 0. 0. 0. 1.] [ 0. 0. 0. 1.] [ 0. 0. 0. 1.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 0. 0. 0. 1.] [ 0. 1. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 1. 0. 0. 0.] [ 0. 0. 1. 0.] [ 0. 1. 0. 0.] [ 1. 0. 0. 0.]] 运行以下代码单元格以测试你的函数。如果代码单元格返回 PASSED，则表明你正确地实现了该函数！ 注意：为了确保结果准确，确保 truncated_policy_iteration 函数满足上文列出的要求（具有三个输入、两个输出，并且没有更改输入参数的默认值）。 1check_test.run_check('value_iteration_check', value_iteration) PASSED 12","categories":[{"name":"强化学习","slug":"强化学习","permalink":"http://yoursite.com/categories/强化学习/"}],"tags":[{"name":"Reinforcement learning","slug":"Reinforcement-learning","permalink":"http://yoursite.com/tags/Reinforcement-learning/"}],"keywords":[{"name":"强化学习","slug":"强化学习","permalink":"http://yoursite.com/categories/强化学习/"}]},{"title":"Customer Setments","slug":"Customer-Setments","date":"2019-02-16T11:16:05.000Z","updated":"2019-02-16T11:16:49.399Z","comments":false,"path":"2019/02/16/Customer-Setments/","link":"","permalink":"http://yoursite.com/2019/02/16/Customer-Setments/","excerpt":"","text":"机器学习纳米学位非监督学习项目 3: 创建用户分类欢迎来到机器学习工程师纳米学位的第三个项目！在这个 notebook 文件中，有些模板代码已经提供给你，但你还需要实现更多的功能来完成这个项目。除非有明确要求，你无须修改任何已给出的代码。以‘练习’开始的标题表示接下来的代码部分中有你必须要实现的功能。每一部分都会有详细的指导，需要实现的部分也会在注释中以 ‘TODO’ 标出。请仔细阅读所有的提示！ 除了实现代码外，你还必须回答一些与项目和你的实现有关的问题。每一个需要你回答的问题都会以‘问题 X’为标题。请仔细阅读每个问题，并且在问题后的‘回答’文字框中写出完整的答案。我们将根据你对问题的回答和撰写代码所实现的功能来对你提交的项目进行评分。 提示：Code 和 Markdown 区域可通过 Shift + Enter 快捷键运行。此外，Markdown 可以通过双击进入编辑模式。 开始在这个项目中，你将分析一个数据集的内在结构，这个数据集包含很多客户真对不同类型产品的年度采购额（用金额表示）。这个项目的任务之一是如何最好地描述一个批发商不同种类顾客之间的差异。这样做将能够使得批发商能够更好的组织他们的物流服务以满足每个客户的需求。 这个项目的数据集能够在UCI机器学习信息库中找到.因为这个项目的目的，分析将不会包括 ‘Channel’ 和 ‘Region’ 这两个特征——重点集中在6个记录的客户购买的产品类别上。 运行下面的的代码单元以载入整个客户数据集和一些这个项目需要的 Python 库。如果你的数据集载入成功，你将看到后面输出数据集的大小。 1234# 检查你的Python版本from sys import version_infoif version_info.major != 3: raise Exception('请使用Python 3.x 来完成此项目') 1import seaborn as sns 123456789101112131415161718# 引入这个项目需要的库import numpy as npimport pandas as pdimport visuals as vsfrom IPython.display import display # 使得我们可以对DataFrame使用display()函数# 设置以内联的形式显示matplotlib绘制的图片（在notebook中显示更美观）%matplotlib inline# 高分辨率显示%config InlineBackend.figure_format='retina'# 载入整个客户数据集try: data = pd.read_csv(\"customers.csv\") data.drop(['Region', 'Channel'], axis = 1, inplace = True) print(\"Wholesale customers dataset has &#123;&#125; samples with &#123;&#125; features each.\".format(*data.shape))except: print(\"Dataset could not be loaded. Is the dataset missing?\") Wholesale customers dataset has 440 samples with 6 features each. 分析数据在这部分，你将开始分析数据，通过可视化和代码来理解每一个特征和其他特征的联系。你会看到关于数据集的统计描述，考虑每一个属性的相关性，然后从数据集中选择若干个样本数据点，你将在整个项目中一直跟踪研究这几个数据点。 运行下面的代码单元给出数据集的一个统计描述。注意这个数据集包含了6个重要的产品类型：‘Fresh’, ‘Milk’, ‘Grocery’, ‘Frozen’, ‘Detergents_Paper’和 ‘Delicatessen’。想一下这里每一个类型代表你会购买什么样的产品。 12# 显示数据集的一个描述display(data.describe()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen count 440.000000 440.000000 440.000000 440.000000 440.000000 440.000000 mean 12000.297727 5796.265909 7951.277273 3071.931818 2881.493182 1524.870455 std 12647.328865 7380.377175 9503.162829 4854.673333 4767.854448 2820.105937 min 3.000000 55.000000 3.000000 25.000000 3.000000 3.000000 25% 3127.750000 1533.000000 2153.000000 742.250000 256.750000 408.250000 50% 8504.000000 3627.000000 4755.500000 1526.000000 816.500000 965.500000 75% 16933.750000 7190.250000 10655.750000 3554.250000 3922.000000 1820.250000 max 112151.000000 73498.000000 92780.000000 60869.000000 40827.000000 47943.000000 练习: 选择样本为了对客户有一个更好的了解，并且了解代表他们的数据将会在这个分析过程中如何变换。最好是选择几个样本数据点，并且更为详细地分析它们。在下面的代码单元中，选择三个索引加入到索引列表indices中，这三个索引代表你要追踪的客户。我们建议你不断尝试，直到找到三个明显不同的客户。 1234567# TODO：从数据集中选择三个你希望抽样的数据点的索引indices = [1, 14, 168]# 为选择的样本建立一个DataFramesamples = pd.DataFrame(data.loc[indices], columns = data.keys()).reset_index(drop = True)print(\"Chosen samples of wholesale customers dataset:\")display(samples) Chosen samples of wholesale customers dataset: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 0 7057 9810 9568 1762 3293 1776 1 24653 9465 12091 294 5058 2168 2 5809 735 803 1393 79 429 问题 1在你看来你选择的这三个样本点分别代表什么类型的企业（客户）？对每一个你选择的样本客户，通过它在每一种产品类型上的花费与数据集的统计描述进行比较，给出你做上述判断的理由。 提示： 企业的类型包括超市、咖啡馆、零售商以及其他。注意不要使用具体企业的名字，比如说在描述一个餐饮业客户时，你不能使用麦当劳。 回答:第一个可能是咖啡厅 Milk 和 Grocery的购买需求高于平均值 第二个可能是餐厅 Fresh Milk Grocery Detergents_Paper Delicatessen都高于平均值 第三个可能是生鲜超市 Fresh 和 Frozen的需求要大些 练习: 特征相关性一个有趣的想法是，考虑这六个类别中的一个（或者多个）产品类别，是否对于理解客户的购买行为具有实际的相关性。也就是说，当用户购买了一定数量的某一类产品，我们是否能够确定他们必然会成比例地购买另一种类的产品。有一个简单的方法可以检测相关性：我们用移除了某一个特征之后的数据集来构建一个监督学习（回归）模型，然后用这个模型去预测那个被移除的特征，再对这个预测结果进行评分，看看预测结果如何。 在下面的代码单元中，你需要实现以下的功能： 使用 DataFrame.drop 函数移除数据集中你选择的不需要的特征，并将移除后的结果赋值给 new_data 。 使用 sklearn.model_selection.train_test_split 将数据集分割成训练集和测试集。 使用移除的特征作为你的目标标签。设置 test_size 为 0.25 并设置一个 random_state 。 导入一个 DecisionTreeRegressor （决策树回归器），设置一个 random_state，然后用训练集训练它。 使用回归器的 score 函数输出模型在测试集上的预测得分。 1234567891011121314# TODO：为DataFrame创建一个副本，用'drop'函数丢弃一个特征# TODO： new_data = data.drop('Detergents_Paper', axis=1)# TODO：使用给定的特征作为目标，将数据分割成训练集和测试集from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(new_data, data['Detergents_Paper'], test_size=0.25, random_state=40)# TODO：创建一个DecisionTreeRegressor（决策树回归器）并在训练集上训练它from sklearn.tree import DecisionTreeRegressorregressor = DecisionTreeRegressor(random_state=0)regressor.fit(X_train, y_train)# TODO：输出在测试集上的预测得分score = regressor.score(X_test, y_test)print(score) 0.7879942418323117 问题 2你尝试预测哪一个特征？预测的得分是多少？这个特征对于区分用户的消费习惯来说必要吗？为什么？提示： 决定系数（coefficient of determination），$R^2$ 结果在0到1之间，1表示完美拟合，一个负的 $R^2$ 表示模型不能够拟合数据。 回答:尝试预测Detergents_Paper 得分为0.787 不必要 该特征与其他特征有一定相关性 决定系数较高 可通过其他特征预测 可视化特征分布为了能够对这个数据集有一个更好的理解，我们可以对数据集中的每一个产品特征构建一个散布矩阵（scatter matrix）。如果你发现你在上面尝试预测的特征对于区分一个特定的用户来说是必须的，那么这个特征和其它的特征可能不会在下面的散射矩阵中显示任何关系。相反的，如果你认为这个特征对于识别一个特定的客户是没有作用的，那么通过散布矩阵可以看出在这个数据特征和其它特征中有关联性。运行下面的代码以创建一个散布矩阵。 12# 对于数据中的每一对特征构造一个散布矩阵pd.scatter_matrix(data, alpha = 0.3, figsize = (14,8), diagonal = 'kde'); C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: pandas.scatter_matrix is deprecated, use pandas.plotting.scatter_matrix instead 问题 3这里是否存在一些特征他们彼此之间存在一定程度相关性？如果有请列出。这个结果是验证了还是否认了你尝试预测的那个特征的相关性？这些特征的数据是怎么分布的？ 提示： 这些数据是正态分布（normally distributed）的吗？大多数的数据点分布在哪？ 回答:特征之间存在一定相关性 在矩阵对角线出现正偏态分布 数据预处理在这个部分，你将通过在数据上做一个合适的缩放，并检测异常点（你可以选择性移除）将数据预处理成一个更好的代表客户的形式。预处理数据是保证你在分析中能够得到显著且有意义的结果的重要环节。 练习: 特征缩放如果数据不是正态分布的，尤其是数据的平均数和中位数相差很大的时候（表示数据非常歪斜）。这时候通常用一个非线性的缩放是很合适的，（英文原文） — 尤其是对于金融数据。一种实现这个缩放的方法是使用 Box-Cox 变换，这个方法能够计算出能够最佳减小数据倾斜的指数变换方法。一个比较简单的并且在大多数情况下都适用的方法是使用自然对数。 在下面的代码单元中，你将需要实现以下功能： 使用 np.log 函数在数据 data 上做一个对数缩放，然后将它的副本（不改变原始data的值）赋值给 log_data。 使用 np.log 函数在样本数据 samples 上做一个对数缩放，然后将它的副本赋值给 log_samples。 12345678# TODO：使用自然对数缩放数据log_data = np.log(data)# TODO：使用自然对数缩放样本数据log_samples = np.log(samples)# 为每一对新产生的特征制作一个散射矩阵pd.scatter_matrix(log_data, alpha = 0.3, figsize = (14,8), diagonal = 'kde'); C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: pandas.scatter_matrix is deprecated, use pandas.plotting.scatter_matrix instead 观察在使用了一个自然对数的缩放之后，数据的各个特征会显得更加的正态分布。对于任意的你以前发现有相关关系的特征对，观察他们的相关关系是否还是存在的（并且尝试观察，他们的相关关系相比原来是变强了还是变弱了）。 运行下面的代码以观察样本数据在进行了自然对数转换之后如何改变了。 12# 展示经过对数变换后的样本数据display(log_samples) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 0 8.861775 9.191158 9.166179 7.474205 8.099554 7.482119 1 10.112654 9.155356 9.400217 5.683580 8.528726 7.681560 2 8.667164 6.599870 6.688355 7.239215 4.369448 6.061457 练习: 异常值检测对于任何的分析，在数据预处理的过程中检测数据中的异常值都是非常重要的一步。异常值的出现会使得把这些值考虑进去后结果出现倾斜。这里有很多关于怎样定义什么是数据集中的异常值的经验法则。这里我们将使用 Tukey 的定义异常值的方法：一个异常阶（outlier step）被定义成1.5倍的四分位距（interquartile range，IQR）。一个数据点如果某个特征包含在该特征的 IQR 之外的特征，那么该数据点被认定为异常点。 在下面的代码单元中，你需要完成下面的功能： 将指定特征的 25th 分位点的值分配给 Q1 。使用 np.percentile 来完成这个功能。 将指定特征的 75th 分位点的值分配给 Q3 。同样的，使用 np.percentile 来完成这个功能。 将指定特征的异常阶的计算结果赋值给 step。 选择性地通过将索引添加到 outliers 列表中，以移除异常值。 注意： 如果你选择移除异常值，请保证你选择的样本点不在这些移除的点当中！一旦你完成了这些功能，数据集将存储在 good_data 中。 123456789101112131415161718192021# 对于每一个特征，找到值异常高或者是异常低的数据点for feature in log_data.keys(): # TODO: 计算给定特征的Q1（数据的25th分位点） Q1 = np.percentile(log_data[feature], 25) # TODO: 计算给定特征的Q3（数据的75th分位点） Q3 = np.percentile(log_data[feature], 75) # TODO: 使用四分位范围计算异常阶（1.5倍的四分位距） step = (Q3 - Q1) * 1.5 # 显示异常点 print(\"Data points considered outliers for the feature '&#123;&#125;':\".format(feature)) display(log_data[~((log_data[feature] &gt;= Q1 - step) &amp; (log_data[feature] &lt;= Q3 + step))]) # TODO(可选): 选择你希望移除的数据点的索引outliers = [65,66,75,128,154]# 以下代码会移除outliers中索引的数据点, 并储存在good_data中good_data = log_data.drop(log_data.index[outliers]).reset_index(drop = True) Data points considered outliers for the feature &#39;Fresh&#39;: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 65 4.442651 9.950323 10.732651 3.583519 10.095388 7.260523 66 2.197225 7.335634 8.911530 5.164786 8.151333 3.295837 81 5.389072 9.163249 9.575192 5.645447 8.964184 5.049856 95 1.098612 7.979339 8.740657 6.086775 5.407172 6.563856 96 3.135494 7.869402 9.001839 4.976734 8.262043 5.379897 128 4.941642 9.087834 8.248791 4.955827 6.967909 1.098612 171 5.298317 10.160530 9.894245 6.478510 9.079434 8.740337 193 5.192957 8.156223 9.917982 6.865891 8.633731 6.501290 218 2.890372 8.923191 9.629380 7.158514 8.475746 8.759669 304 5.081404 8.917311 10.117510 6.424869 9.374413 7.787382 305 5.493061 9.468001 9.088399 6.683361 8.271037 5.351858 338 1.098612 5.808142 8.856661 9.655090 2.708050 6.309918 353 4.762174 8.742574 9.961898 5.429346 9.069007 7.013016 355 5.247024 6.588926 7.606885 5.501258 5.214936 4.844187 357 3.610918 7.150701 10.011086 4.919981 8.816853 4.700480 412 4.574711 8.190077 9.425452 4.584967 7.996317 4.127134 Data points considered outliers for the feature &#39;Milk&#39;: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 86 10.039983 11.205013 10.377047 6.894670 9.906981 6.805723 98 6.220590 4.718499 6.656727 6.796824 4.025352 4.882802 154 6.432940 4.007333 4.919981 4.317488 1.945910 2.079442 356 10.029503 4.897840 5.384495 8.057377 2.197225 6.306275 Data points considered outliers for the feature &#39;Grocery&#39;: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 75 9.923192 7.036148 1.098612 8.390949 1.098612 6.882437 154 6.432940 4.007333 4.919981 4.317488 1.945910 2.079442 Data points considered outliers for the feature &#39;Frozen&#39;: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 38 8.431853 9.663261 9.723703 3.496508 8.847360 6.070738 57 8.597297 9.203618 9.257892 3.637586 8.932213 7.156177 65 4.442651 9.950323 10.732651 3.583519 10.095388 7.260523 145 10.000569 9.034080 10.457143 3.737670 9.440738 8.396155 175 7.759187 8.967632 9.382106 3.951244 8.341887 7.436617 264 6.978214 9.177714 9.645041 4.110874 8.696176 7.142827 325 10.395650 9.728181 9.519735 11.016479 7.148346 8.632128 420 8.402007 8.569026 9.490015 3.218876 8.827321 7.239215 429 9.060331 7.467371 8.183118 3.850148 4.430817 7.824446 439 7.932721 7.437206 7.828038 4.174387 6.167516 3.951244 Data points considered outliers for the feature &#39;Detergents_Paper&#39;: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 75 9.923192 7.036148 1.098612 8.390949 1.098612 6.882437 161 9.428190 6.291569 5.645447 6.995766 1.098612 7.711101 Data points considered outliers for the feature &#39;Delicatessen&#39;: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 66 2.197225 7.335634 8.911530 5.164786 8.151333 3.295837 109 7.248504 9.724899 10.274568 6.511745 6.728629 1.098612 128 4.941642 9.087834 8.248791 4.955827 6.967909 1.098612 137 8.034955 8.997147 9.021840 6.493754 6.580639 3.583519 142 10.519646 8.875147 9.018332 8.004700 2.995732 1.098612 154 6.432940 4.007333 4.919981 4.317488 1.945910 2.079442 183 10.514529 10.690808 9.911952 10.505999 5.476464 10.777768 184 5.789960 6.822197 8.457443 4.304065 5.811141 2.397895 187 7.798933 8.987447 9.192075 8.743372 8.148735 1.098612 203 6.368187 6.529419 7.703459 6.150603 6.860664 2.890372 233 6.871091 8.513988 8.106515 6.842683 6.013715 1.945910 285 10.602965 6.461468 8.188689 6.948897 6.077642 2.890372 289 10.663966 5.655992 6.154858 7.235619 3.465736 3.091042 343 7.431892 8.848509 10.177932 7.283448 9.646593 3.610918 1sns.boxplot(data=data, x='Fresh', orient='v') &lt;matplotlib.axes._subplots.AxesSubplot at 0x256682c49b0&gt; 问题 4请列出所有在多于一个特征下被看作是异常的数据点。这些点应该被从数据集中移除吗？为什么？把你认为需要移除的数据点全部加入到到 outliers 变量中。 回答:不应该 共有48个异常值 对于本样本数据一共有400左右样本 占比过大 如果全部移除会对结果有较大影响 会造成数据损失 因此 判断有两个以上异常值的数据再进行移除 所以移除65,66,75,128,154 这5个样本 特征转换在这个部分中你将使用主成分分析（PCA）来分析批发商客户数据的内在结构。由于使用PCA在一个数据集上会计算出最大化方差的维度，我们将找出哪一个特征组合能够最好的描绘客户。 练习: 主成分分析（PCA）既然数据被缩放到一个更加正态分布的范围中并且我们也移除了需要移除的异常点，我们现在就能够在 good_data 上使用PCA算法以发现数据的哪一个维度能够最大化特征的方差。除了找到这些维度，PCA 也将报告每一个维度的解释方差比（explained variance ratio）—这个数据有多少方差能够用这个单独的维度来解释。注意 PCA 的一个组成部分（维度）能够被看做这个空间中的一个新的“特征”，但是它是原来数据中的特征构成的。 在下面的代码单元中，你将要实现下面的功能： 导入 sklearn.decomposition.PCA 并且将 good_data 用 PCA 并且使用6个维度进行拟合后的结果保存到 pca 中。 使用 pca.transform 将 log_samples 进行转换，并将结果存储到 pca_samples 中。 12345678910# TODO：通过在good data上进行PCA，将其转换成6个维度from sklearn.decomposition import PCApca = PCA(n_components=6)pca.fit(good_data)# TODO：使用上面的PCA拟合将变换施加在log_samples上pca_samples = pca.transform(good_data)# 生成PCA的结果图pca_results = vs.pca_results(good_data, pca) 问题 5数据的第一个和第二个主成分总共表示了多少的方差？ 前四个主成分呢？使用上面提供的可视化图像，从用户花费的角度来讨论前四个主要成分中每个主成分代表的消费行为并给出你做出判断的理由。 提示： 对每个主成分中的特征分析权重的正负和大小。 结合每个主成分权重的正负讨论消费行为。 某一特定维度上的正向增长对应正权特征的增长和负权特征的减少。增长和减少的速率和每个特征的权重相关。参考资料：Interpretation of the Principal Components 回答:0.72 0.93 第一个 咖啡厅 最大的负权特征是洗涤类商品 第二个 超市 生鲜冻品和鱼的权值最大 第三个 鱼类零售店 鱼的权值最大 第四个 熟食店 熟食的权值占比最大 观察运行下面的代码，查看经过对数转换的样本数据在进行一个6个维度的主成分分析（PCA）之后会如何改变。观察样本数据的前四个维度的数值。考虑这和你初始对样本点的解释是否一致。 12# 展示经过PCA转换的sample log-datadisplay(pd.DataFrame(np.round(pca_samples, 4), columns = pca_results.index.values)) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Dimension 1 Dimension 2 Dimension 3 Dimension 4 Dimension 5 Dimension 6 0 -1.7580 0.0097 -0.9590 -1.6824 0.2680 -0.3891 1 -1.7887 -0.8123 0.2315 -0.0036 0.1194 -0.2106 2 -1.8834 -1.5991 1.3204 -0.5432 -0.3934 -0.3117 3 1.1553 -1.4052 0.5422 0.4127 -0.6865 0.6409 4 -0.7848 -2.3943 0.4798 -0.3483 -0.3191 0.0613 5 -1.0850 -0.3243 -0.2635 -0.8812 0.1862 -0.5347 6 -1.1286 0.2629 -1.3162 -0.5369 -0.4836 0.1097 7 -1.5672 -0.9010 0.3684 -0.2682 -0.4571 0.1526 8 -0.8636 0.6650 -0.5376 -0.7922 -0.1551 0.0344 9 -2.8734 -0.6774 0.1330 -0.1802 -0.0250 0.1224 10 -2.0887 -0.7006 0.8537 1.0105 -0.5587 0.2495 11 1.0120 -0.0103 -0.7516 -0.0545 -0.4333 0.6602 12 -2.2406 -1.2419 -1.0729 -1.9589 0.2160 -0.1782 13 -1.8891 -1.3001 -1.1945 0.9689 -0.2426 0.2970 14 -2.3388 -0.9013 -1.1515 -1.6713 -0.0485 -0.0739 15 0.4258 0.8803 -1.2189 -0.7945 -0.7319 0.3868 16 -2.7939 2.0377 0.3420 -1.2847 0.1457 -0.1353 17 0.2575 -0.5179 1.1702 -1.5806 0.4159 -0.5328 18 -1.3906 -1.8004 0.0301 -0.3807 -0.2116 0.1467 19 -0.9992 0.4720 -0.9332 -0.1723 -0.4229 0.5269 20 -0.8375 -1.0765 -0.3684 -0.8111 -0.5111 -0.3040 21 1.7467 0.1939 0.2753 0.6012 -0.7470 0.1974 22 -0.1419 -2.7722 0.3293 0.3928 -1.3904 0.2012 23 -2.8096 -3.6459 1.0567 -0.5186 0.6999 -0.1811 24 -2.0709 -2.4853 0.2692 -0.4013 -0.1917 0.1027 25 -1.2292 1.5540 -3.2462 0.0043 0.1124 -0.0697 26 1.9083 -0.3765 0.1924 0.1502 -0.3852 0.5367 27 2.4162 0.6069 -0.7652 -1.3209 0.1614 0.8089 28 -3.5695 -0.9977 0.9477 -0.5400 0.2579 0.0323 29 0.5684 -1.0850 -1.4044 -0.5784 -0.6738 -0.2157 ... ... ... ... ... ... ... 405 -0.4777 -0.3472 0.3116 -0.3929 -0.9402 0.1142 406 0.7424 0.0059 2.1018 -0.9815 0.2461 -0.0371 407 -2.1528 5.3859 0.0930 0.4023 0.3577 0.3111 408 -0.0741 -1.6911 1.6461 1.4172 0.0587 0.1461 409 0.5554 -0.0029 -0.2440 1.6316 -0.4586 -0.0161 410 -1.5972 -0.8047 0.1483 -0.0839 -0.3191 -0.0512 411 -2.5495 0.1090 -0.1921 -0.0073 -0.0074 -0.3327 412 -1.9218 0.5424 -0.4014 -0.8837 -0.1168 0.1580 413 -3.2940 2.4621 0.3317 -0.9146 0.1175 0.1444 414 -0.3359 -0.0856 -0.1970 -1.0176 -0.6091 -0.7771 415 -3.0266 1.8034 -1.1358 -2.9577 -0.4264 0.1528 416 -1.4571 -1.0316 -0.5676 -0.6119 -0.4132 0.1330 417 0.4422 -0.7142 -0.9356 -0.9923 -0.7925 0.4116 418 -0.4191 -0.4595 -1.0590 -0.2384 -0.2853 -0.1660 419 -1.0698 0.0957 -1.8679 0.3247 -0.2282 0.6291 420 2.3699 -1.7726 1.3282 0.7617 0.4672 0.1593 421 -2.0740 -1.5983 -0.0683 0.4013 -0.0483 0.0983 422 0.4545 -2.6564 0.0980 1.1628 1.4384 -0.5162 423 -0.1223 0.6924 0.0667 0.9488 0.6363 -0.2967 424 1.4269 1.2099 -0.1030 -3.9222 0.6257 0.5211 425 -0.0856 0.4483 1.0450 -1.3292 1.1732 1.1231 426 -0.2111 -1.6998 0.8104 1.4239 -0.0610 -0.2023 427 0.1304 0.5643 -1.9278 -1.1452 -0.7829 0.4971 428 0.9382 0.6387 1.3840 -0.3231 -0.0505 -0.7708 429 -1.0055 -0.3825 -1.0855 -0.6019 -0.2346 0.1880 430 0.6448 -2.8583 0.6377 0.5879 1.9515 0.7170 431 3.1848 -1.9448 0.2677 -0.6799 -0.2663 -0.5194 432 -3.7425 -0.8561 -0.9885 -0.8879 0.0503 0.2058 433 1.6691 -0.3980 0.5161 -1.3189 0.0913 0.0056 434 0.7390 3.6914 -2.0335 -0.9927 0.3109 -0.1734 435 rows × 6 columns 练习：降维当使用主成分分析的时候，一个主要的目的是减少数据的维度，这实际上降低了问题的复杂度。当然降维也是需要一定代价的：更少的维度能够表示的数据中的总方差更少。因为这个，累计解释方差比（cumulative explained variance ratio）对于我们确定这个问题需要多少维度非常重要。另外，如果大部分的方差都能够通过两个或者是三个维度进行表示的话，降维之后的数据能够被可视化。 在下面的代码单元中，你将实现下面的功能： 将 good_data 用两个维度的PCA进行拟合，并将结果存储到 pca 中去。 使用 pca.transform 将 good_data 进行转换，并将结果存储在 reduced_data 中。 使用 pca.transform 将 log_samples 进行转换，并将结果存储在 pca_samples 中。 1234567891011# TODO：通过在good data上进行PCA，将其转换成两个维度pca = PCA(n_components=2)pca.fit(good_data)# TODO：使用上面训练的PCA将good data进行转换reduced_data = pca.transform(good_data)# TODO：使用上面训练的PCA将log_samples进行转换pca_samples = pca.transform(log_samples)# 为降维后的数据创建一个DataFramereduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2']) 观察运行以下代码观察当仅仅使用两个维度进行 PCA 转换后，这个对数样本数据将怎样变化。观察这里的结果与一个使用六个维度的 PCA 转换相比较时，前两维的数值是保持不变的。 12# 展示经过两个维度的PCA转换之后的样本log-datadisplay(pd.DataFrame(np.round(pca_samples, 4), columns = ['Dimension 1', 'Dimension 2'])) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Dimension 1 Dimension 2 0 -1.7887 -0.8123 1 -2.3388 -0.9013 2 3.2785 0.9078 可视化一个双标图（Biplot）双标图是一个散点图，每个数据点的位置由它所在主成分的分数确定。坐标系是主成分（这里是 Dimension 1 和 Dimension 2）。此外，双标图还展示出初始特征在主成分上的投影。一个双标图可以帮助我们理解降维后的数据，发现主成分和初始特征之间的关系。 运行下面的代码来创建一个降维后数据的双标图。 12# 可视化双标图vs.biplot(good_data, reduced_data, pca) &lt;matplotlib.axes._subplots.AxesSubplot at 0x256683adc88&gt; 观察一旦我们有了原始特征的投影（红色箭头），就能更加容易的理解散点图每个数据点的相对位置。 在这个双标图中，哪些初始特征与第一个主成分有强关联？哪些初始特征与第二个主成分相关联？你观察到的是否与之前得到的 pca_results 图相符？ 聚类在这个部分，你讲选择使用 K-Means 聚类算法或者是高斯混合模型聚类算法以发现数据中隐藏的客户分类。然后，你将从簇中恢复一些特定的关键数据点，通过将它们转换回原始的维度和规模，从而理解他们的含义。 问题 6使用 K-Means 聚类算法的优点是什么？使用高斯混合模型聚类算法的优点是什么？基于你现在对客户数据的观察结果，你选用了这两个算法中的哪一个，为什么？ 回答:K-Means优点是：计算速度快、时间短，易解释，聚类效果还不错；但缺点主要是需要提前确定K值，对异常值极度敏感。 高斯混合模型聚类算法的优点是聚类输出的信息量更大，理论上可以拟合任何连续的概率密度函数。 我会选高斯混合模型，因为两种算法聚类得分差异很小，且GMM能输出数据点属于某一类别的概率，因此输出的信息丰富程度大大高于K-means算法 练习: 创建聚类针对不同情况，有些问题你需要的聚类数目可能是已知的。但是在聚类数目不作为一个先验知道的情况下，我们并不能够保证某个聚类的数目对这个数据是最优的，因为我们对于数据的结构（如果存在的话）是不清楚的。但是，我们可以通过计算每一个簇中点的轮廓系数来衡量聚类的质量。数据点的轮廓系数衡量了它与分配给他的簇的相似度，这个值范围在-1（不相似）到1（相似）。平均轮廓系数为我们提供了一种简单地度量聚类质量的方法。 在接下来的代码单元中，你将实现下列功能： 在 reduced_data 上使用一个聚类算法，并将结果赋值到 clusterer，需要设置 random_state 使得结果可以复现。 使用 clusterer.predict 预测 reduced_data 中的每一个点的簇，并将结果赋值到 preds。 使用算法的某个属性值找到聚类中心，并将它们赋值到 centers。 预测 pca_samples 中的每一个样本点的类别并将结果赋值到 sample_preds。 导入 sklearn.metrics.silhouette_score 包并计算 reduced_data 相对于 preds 的轮廓系数。 将轮廓系数赋值给 score 并输出结果。 12345678910111213141516# TODO：在降维后的数据上使用你选择的聚类算法from sklearn.mixture import GaussianMixtureclusterer = GaussianMixture(n_components=2, random_state=40)clusterer.fit(reduced_data)# TODO：预测每一个点的簇preds = clusterer.predict(reduced_data)# TODO：找到聚类中心centers = clusterer.means_# TODO：预测在每一个转换后的样本点的类sample_preds = clusterer.predict(pca_samples)from sklearn.metrics import silhouette_score# TODO：计算选择的类别的平均轮廓系数（mean silhouette coefficient）score = silhouette_score(reduced_data, preds)print(score) 0.4219168464626149 问题 7汇报你尝试的不同的聚类数对应的轮廓系数。在这些当中哪一个聚类的数目能够得到最佳的轮廓系数？ 回答:聚类为2时 0.421 聚类为3时 0.375 聚类为4时0.248 所以聚类为2时效果最佳 聚类可视化一旦你选好了通过上面的评价函数得到的算法的最佳聚类数目，你就能够通过使用下面的代码块可视化来得到的结果。作为实验，你可以试着调整你的聚类算法的聚类的数量来看一下不同的可视化结果。但是你提供的最终的可视化图像必须和你选择的最优聚类数目一致。 12# 从已有的实现中展示聚类的结果vs.cluster_results(reduced_data, preds, centers, pca_samples) 练习: 数据恢复上面的可视化图像中提供的每一个聚类都有一个中心点。这些中心（或者叫平均点）并不是数据中真实存在的点，但是是所有预测在这个簇中的数据点的平均。对于创建客户分类的问题，一个簇的中心对应于那个分类的平均用户。因为这个数据现在进行了降维并缩放到一定的范围，我们可以通过施加一个反向的转换恢复这个点所代表的用户的花费。 在下面的代码单元中，你将实现下列的功能： 使用 pca.inverse_transform 将 centers 反向转换，并将结果存储在 log_centers 中。 使用 np.log 的反函数 np.exp 反向转换 log_centers 并将结果存储到 true_centers 中。 1234567891011# TODO：反向转换中心点log_centers = pca.inverse_transform(centers)# TODO：对中心点做指数转换true_centers = np.exp(log_centers)# 显示真实的中心点segments = ['Segment &#123;&#125;'.format(i) for i in range(0,len(centers))]true_centers = pd.DataFrame(np.round(true_centers), columns = data.keys())true_centers.index = segmentsdisplay(true_centers) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Fresh Milk Grocery Frozen Detergents_Paper Delicatessen Segment 0 3552.0 7837.0 12219.0 870.0 4696.0 962.0 Segment 1 8953.0 2114.0 2765.0 2075.0 353.0 732.0 问题 8考虑上面的代表性数据点在每一个产品类型的花费总数，你认为这些客户分类代表了哪类客户？为什么？需要参考在项目最开始得到的统计值来给出理由。 提示： 一个被分到&#39;Cluster X&#39;的客户最好被用 &#39;Segment X&#39;中的特征集来标识的企业类型表示。 回答:Segment 0 代表餐厅 食品类出售比重较大 Segment 1 代表超市 基本符合超市出售商品特征 问题 9对于每一个样本点问题 8 中的哪一个分类能够最好的表示它？你之前对样本的预测和现在的结果相符吗？ 运行下面的代码单元以找到每一个样本点被预测到哪一个簇中去。 123# 显示预测结果for i, pred in enumerate(sample_preds): print(\"Sample point\", i, \"predicted to be in Cluster\", pred) Sample point 0 predicted to be in Cluster 0 Sample point 1 predicted to be in Cluster 0 Sample point 2 predicted to be in Cluster 1 回答:cluster0 结果不太相符 原数据分类比较细致 这里我们只是使用了2个簇 结论在最后一部分中，你要学习如何使用已经被分类的数据。首先，你要考虑不同组的客户客户分类，针对不同的派送策略受到的影响会有什么不同。其次，你要考虑到，每一个客户都被打上了标签（客户属于哪一个分类）可以给客户数据提供一个多一个特征。最后，你会把客户分类与一个数据中的隐藏变量做比较，看一下这个分类是否辨识了特定的关系。 问题 10在对他们的服务或者是产品做细微的改变的时候，公司经常会使用 A/B tests 以确定这些改变会对客户产生积极作用还是消极作用。这个批发商希望考虑将他的派送服务从每周5天变为每周3天，但是他只会对他客户当中对此有积极反馈的客户采用。这个批发商应该如何利用客户分类来知道哪些客户对它的这个派送策略的改变有积极的反馈，如果有的话？你需要给出在这个情形下A/B 测试具体的实现方法，以及最终得出结论的依据是什么？ 提示： 我们能假设这个改变对所有的客户影响都一致吗？我们怎样才能够确定它对于哪个类型的客户影响最大？ 回答：对两组客户分别为A1;A2,B1;B2 A1参照组 A2实验组 分别应用每周5天和每周3天的服务 分别记录每组的周均销售额 如果A2组的销售额较高 则说明该组对该类客户组有积极作用 然后对A1 A2组 使用新的策略否则保持不变 同理B1,B2采取同样的方式 问题 11通过聚类技术，我们能够将原有的没有标记的数据集中的附加结构分析出来。因为每一个客户都有一个最佳的划分（取决于你选择使用的聚类算法），我们可以把用户分类作为数据的一个工程特征。假设批发商最近迎来十位新顾客，并且他已经为每位顾客每个产品类别年度采购额进行了预估。进行了这些估算之后，批发商该如何运用它的预估和非监督学习的结果来对这十个新的客户进行更好的预测？ 提示：在下面的代码单元中，我们提供了一个已经做好聚类的数据（聚类结果为数据中的cluster属性），我们将在这个数据集上做一个小实验。尝试运行下面的代码看看我们尝试预测‘Region’的时候，如果存在聚类特征’cluster’与不存在相比对最终的得分会有什么影响？这对你有什么启发？ 12345678910111213141516171819202122232425from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import train_test_split# 读取包含聚类结果的数据cluster_data = pd.read_csv(\"cluster.csv\")y = cluster_data['Region']X = cluster_data.drop(['Region'], axis = 1)# 划分训练集测试集X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24)clf = RandomForestClassifier(random_state=24)clf.fit(X_train, y_train)score_with_cluster = clf.score(X_test, y_test)# 移除cluster特征X_train = X_train.copy()X_train.drop(['cluster'], axis=1, inplace=True)X_test = X_test.copy()X_test.drop(['cluster'], axis=1, inplace=True)clf.fit(X_train, y_train)score_no_cluster = clf.score(X_test, y_test)print(\"不使用cluster特征的得分: %.4f\"%score_no_cluster)print(\"使用cluster特征的得分: %.4f\"%score_with_cluster) 不使用cluster特征的得分: 0.6437 使用cluster特征的得分: 0.6667 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release. from numpy.core.umath_tests import inner1d 回答：cluster特征对用户的影响较小 但使用caluster有益于精度提高 可视化内在的分布在这个项目的开始，我们讨论了从数据集中移除 &#39;Channel&#39; 和 &#39;Region&#39; 特征，这样在分析过程中我们就会着重分析用户产品类别。通过重新引入 Channel 这个特征到数据集中，并施加和原来数据集同样的 PCA 变换的时候我们将能够发现数据集产生一个有趣的结构。 运行下面的代码单元以查看哪一个数据点在降维的空间中被标记为 &#39;HoReCa&#39; (旅馆/餐馆/咖啡厅)或者 &#39;Retail&#39;。另外，你将发现样本点在图中被圈了出来，用以显示他们的标签。 12# 根据‘Channel‘数据显示聚类的结果vs.channel_results(reduced_data, outliers, pca_samples) 问题 12你选择的聚类算法和聚类点的数目，与内在的旅馆/餐馆/咖啡店和零售商的分布相比，有足够好吗？根据这个分布有没有哪个簇能够刚好划分成’零售商’或者是’旅馆/饭店/咖啡馆’？你觉得这个分类和前面你对于用户分类的定义是一致的吗？ 回答: 基本一致 零售类也属于超市的范围 所有都一样因为特征不多 很多分类比较模糊 注意: 当你写完了所有的代码，并且回答了所有的问题。你就可以把你的 iPython Notebook 导出成 HTML 文件。你可以在菜单栏，这样导出File -&gt; Download as -&gt; HTML (.html)把这个 HTML 和这个 iPython notebook 一起做为你的作业提交。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://yoursite.com/tags/Machine-Learning/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}]},{"title":"finding donors","slug":"finding-donors","date":"2019-02-16T11:13:44.000Z","updated":"2019-02-16T11:14:39.171Z","comments":false,"path":"2019/02/16/finding-donors/","link":"","permalink":"http://yoursite.com/2019/02/16/finding-donors/","excerpt":"","text":"机器学习纳米学位监督学习项目2: 为CharityML寻找捐献者欢迎来到机器学习工程师纳米学位的第二个项目！在此文件中，有些示例代码已经提供给你，但你还需要实现更多的功能让项目成功运行。除非有明确要求，你无须修改任何已给出的代码。以‘练习’开始的标题表示接下来的代码部分中有你必须要实现的功能。每一部分都会有详细的指导，需要实现的部分也会在注释中以’TODO’标出。请仔细阅读所有的提示！ 除了实现代码外，你还必须回答一些与项目和你的实现有关的问题。每一个需要你回答的问题都会以‘问题 X’为标题。请仔细阅读每个问题，并且在问题后的‘回答’文字框中写出完整的答案。我们将根据你对问题的回答和撰写代码所实现的功能来对你提交的项目进行评分。 提示：Code 和 Markdown 区域可通过Shift + Enter快捷键运行。此外，Markdown可以通过双击进入编辑模式。 开始在这个项目中，你将使用1994年美国人口普查收集的数据，选用几个监督学习算法以准确地建模被调查者的收入。然后，你将根据初步结果从中选择出最佳的候选算法，并进一步优化该算法以最好地建模这些数据。你的目标是建立一个能够准确地预测被调查者年收入是否超过50000美元的模型。这种类型的任务会出现在那些依赖于捐款而存在的非营利性组织。了解人群的收入情况可以帮助一个非营利性的机构更好地了解他们要多大的捐赠，或是否他们应该接触这些人。虽然我们很难直接从公开的资源中推断出一个人的一般收入阶层，但是我们可以（也正是我们将要做的）从其他的一些公开的可获得的资源中获得一些特征从而推断出该值。 这个项目的数据集来自UCI机器学习知识库。这个数据集是由Ron Kohavi和Barry Becker在发表文章“Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid”之后捐赠的，你可以在Ron Kohavi提供的在线版本中找到这个文章。我们在这里探索的数据集相比于原有的数据集有一些小小的改变，比如说移除了特征&#39;fnlwgt&#39; 以及一些遗失的或者是格式不正确的记录。 探索数据运行下面的代码单元以载入需要的Python库并导入人口普查数据。注意数据集的最后一列&#39;income&#39;将是我们需要预测的列（表示被调查者的年收入会大于或者是最多50,000美元），人口普查数据中的每一列都将是关于被调查者的特征。 1234567891011121314151617# 为这个项目导入需要的库import numpy as npimport pandas as pdfrom time import timefrom IPython.display import display # 允许为DataFrame使用display()# 导入附加的可视化代码visuals.pyimport visuals as vs# 为notebook提供更加漂亮的可视化%matplotlib inline# 导入人口普查数据data = pd.read_csv(\"census.csv\")# 成功 - 显示第一条记录display(data.head(n=1)) age workclass education_level education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country income 0 39 State-gov Bachelors 13.0 Never-married Adm-clerical Not-in-family White Male 2174.0 0.0 40.0 United-States &lt;=50K 练习：数据探索首先我们对数据集进行一个粗略的探索，我们将看看每一个类别里会有多少被调查者？并且告诉我们这些里面多大比例是年收入大于50,000美元的。在下面的代码单元中，你将需要计算以下量： 总的记录数量，&#39;n_records&#39; 年收入大于50,000美元的人数，&#39;n_greater_50k&#39;. 年收入最多为50,000美元的人数 &#39;n_at_most_50k&#39;. 年收入大于50,000美元的人所占的比例， &#39;greater_percent&#39;. 提示： 您可能需要查看上面的生成的表，以了解&#39;income&#39;条目的格式是什么样的。 1234567891011121314151617# TODO：总的记录数n_records = data.shape[0]# TODO：被调查者的收入大于$50,000的人数n_greater_50k = len(data[data.income == '&gt;50K'])# TODO：被调查者的收入最多为$50,000的人数n_at_most_50k = len(data[data.income == '&lt;=50K'])# TODO：被调查者收入大于$50,000所占的比例greater_percent = float(n_greater_50k) / n_records * 100# 打印结果print (\"Total number of records: &#123;&#125;\".format(n_records))print (\"Individuals making more than $50,000: &#123;&#125;\".format(n_greater_50k))print (\"Individuals making at most $50,000: &#123;&#125;\".format(n_at_most_50k))print (\"Percentage of individuals making more than $50,000: &#123;:.2f&#125;%\".format(greater_percent)) Total number of records: 45222 Individuals making more than $50,000: 11208 Individuals making at most $50,000: 34014 Percentage of individuals making more than $50,000: 24.78% 准备数据在数据能够被作为输入提供给机器学习算法之前，它经常需要被清洗，格式化，和重新组织 - 这通常被叫做预处理。幸运的是，对于这个数据集，没有我们必须处理的无效或丢失的条目，然而，由于某一些特征存在的特性我们必须进行一定的调整。这个预处理都可以极大地帮助我们提升几乎所有的学习算法的结果和预测能力。 获得特征和标签income 列是我们需要的标签，记录一个人的年收入是否高于50K。 因此我们应该把他从数据中剥离出来，单独存放。 123# 将数据切分成特征和对应的标签income_raw = data['income']features_raw = data.drop('income', axis = 1) 转换倾斜的连续特征一个数据集有时可能包含至少一个靠近某个数字的特征，但有时也会有一些相对来说存在极大值或者极小值的不平凡分布的的特征。算法对这种分布的数据会十分敏感，并且如果这种数据没有能够很好地规一化处理会使得算法表现不佳。在人口普查数据集的两个特征符合这个描述：’capital-gain&#39;和&#39;capital-loss&#39;。 运行下面的代码单元以创建一个关于这两个特征的条形图。请注意当前的值的范围和它们是如何分布的。 12# 可视化 'capital-gain'和'capital-loss' 两个特征vs.distribution(features_raw) 对于高度倾斜分布的特征如&#39;capital-gain&#39;和&#39;capital-loss&#39;，常见的做法是对数据施加一个对数转换，将数据转换成对数，这样非常大和非常小的值不会对学习算法产生负面的影响。并且使用对数变换显著降低了由于异常值所造成的数据范围异常。但是在应用这个变换时必须小心：因为0的对数是没有定义的，所以我们必须先将数据处理成一个比0稍微大一点的数以成功完成对数转换。 运行下面的代码单元来执行数据的转换和可视化结果。再次，注意值的范围和它们是如何分布的。 123456# 对于倾斜的数据使用Log转换skewed = ['capital-gain', 'capital-loss']features_raw[skewed] = data[skewed].apply(lambda x: np.log(x + 1))# 可视化对数转换后 'capital-gain'和'capital-loss' 两个特征vs.distribution(features_raw, transformed = True) 规一化数字特征除了对于高度倾斜的特征施加转换，对数值特征施加一些形式的缩放通常会是一个好的习惯。在数据上面施加一个缩放并不会改变数据分布的形式（比如上面说的’capital-gain’ or ‘capital-loss’）；但是，规一化保证了每一个特征在使用监督学习器的时候能够被平等的对待。注意一旦使用了缩放，观察数据的原始形式不再具有它本来的意义了，就像下面的例子展示的。 运行下面的代码单元来规一化每一个数字特征。我们将使用sklearn.preprocessing.MinMaxScaler来完成这个任务。 123456789from sklearn.preprocessing import MinMaxScaler# 初始化一个 scaler，并将它施加到特征上scaler = MinMaxScaler()numerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']features_raw[numerical] = scaler.fit_transform(data[numerical])# 显示一个经过缩放的样例记录display(features_raw.head(n = 5)) age workclass education_level education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country 0 0.301370 State-gov Bachelors 0.800000 Never-married Adm-clerical Not-in-family White Male 0.02174 0.0 0.397959 United-States 1 0.452055 Self-emp-not-inc Bachelors 0.800000 Married-civ-spouse Exec-managerial Husband White Male 0.00000 0.0 0.122449 United-States 2 0.287671 Private HS-grad 0.533333 Divorced Handlers-cleaners Not-in-family White Male 0.00000 0.0 0.397959 United-States 3 0.493151 Private 11th 0.400000 Married-civ-spouse Handlers-cleaners Husband Black Male 0.00000 0.0 0.397959 United-States 4 0.150685 Private Bachelors 0.800000 Married-civ-spouse Prof-specialty Wife Black Female 0.00000 0.0 0.397959 Cuba 练习：数据预处理从上面的数据探索中的表中，我们可以看到有几个属性的每一条记录都是非数字的。通常情况下，学习算法期望输入是数字的，这要求非数字的特征（称为类别变量）被转换。转换类别变量的一种流行的方法是使用独热编码方案。独热编码为每一个非数字特征的每一个可能的类别创建一个“虚拟”变量。例如，假设someFeature有三个可能的取值A，B或者C，。我们将把这个特征编码成someFeature_A, someFeature_B和someFeature_C. 特征X 特征X_A 特征X_B 特征X_C B 0 1 0 C ——&gt; 独热编码 ——&gt; 0 0 1 A 1 0 0 此外，对于非数字的特征，我们需要将非数字的标签&#39;income&#39;转换成数值以保证学习算法能够正常工作。因为这个标签只有两种可能的类别（”&lt;=50K”和”&gt;50K”），我们不必要使用独热编码，可以直接将他们编码分别成两个类0和1，在下面的代码单元中你将实现以下功能： 使用pandas.get_dummies()对&#39;features_raw&#39;数据来施加一个独热编码。 将目标标签&#39;income_raw&#39;转换成数字项。 将”&lt;=50K”转换成0；将”&gt;50K”转换成1。 12345678910111213# TODO：使用pandas.get_dummies()对'features_raw'数据进行独热编码features = pd.get_dummies(features_raw)# TODO：将'income_raw'编码成数字值from sklearn import preprocessingincome = pd.Series(preprocessing.LabelEncoder().fit_transform(income_raw))# 打印经过独热编码之后的特征数量encoded = list(features.columns)print (\"&#123;&#125; total features after one-hot encoding.\".format(len(encoded)))# 移除下面一行的注释以观察编码的特征名字print (encoded) 103 total features after one-hot encoding. [&#39;age&#39;, &#39;education-num&#39;, &#39;capital-gain&#39;, &#39;capital-loss&#39;, &#39;hours-per-week&#39;, &#39;workclass_ Federal-gov&#39;, &#39;workclass_ Local-gov&#39;, &#39;workclass_ Private&#39;, &#39;workclass_ Self-emp-inc&#39;, &#39;workclass_ Self-emp-not-inc&#39;, &#39;workclass_ State-gov&#39;, &#39;workclass_ Without-pay&#39;, &#39;education_level_ 10th&#39;, &#39;education_level_ 11th&#39;, &#39;education_level_ 12th&#39;, &#39;education_level_ 1st-4th&#39;, &#39;education_level_ 5th-6th&#39;, &#39;education_level_ 7th-8th&#39;, &#39;education_level_ 9th&#39;, &#39;education_level_ Assoc-acdm&#39;, &#39;education_level_ Assoc-voc&#39;, &#39;education_level_ Bachelors&#39;, &#39;education_level_ Doctorate&#39;, &#39;education_level_ HS-grad&#39;, &#39;education_level_ Masters&#39;, &#39;education_level_ Preschool&#39;, &#39;education_level_ Prof-school&#39;, &#39;education_level_ Some-college&#39;, &#39;marital-status_ Divorced&#39;, &#39;marital-status_ Married-AF-spouse&#39;, &#39;marital-status_ Married-civ-spouse&#39;, &#39;marital-status_ Married-spouse-absent&#39;, &#39;marital-status_ Never-married&#39;, &#39;marital-status_ Separated&#39;, &#39;marital-status_ Widowed&#39;, &#39;occupation_ Adm-clerical&#39;, &#39;occupation_ Armed-Forces&#39;, &#39;occupation_ Craft-repair&#39;, &#39;occupation_ Exec-managerial&#39;, &#39;occupation_ Farming-fishing&#39;, &#39;occupation_ Handlers-cleaners&#39;, &#39;occupation_ Machine-op-inspct&#39;, &#39;occupation_ Other-service&#39;, &#39;occupation_ Priv-house-serv&#39;, &#39;occupation_ Prof-specialty&#39;, &#39;occupation_ Protective-serv&#39;, &#39;occupation_ Sales&#39;, &#39;occupation_ Tech-support&#39;, &#39;occupation_ Transport-moving&#39;, &#39;relationship_ Husband&#39;, &#39;relationship_ Not-in-family&#39;, &#39;relationship_ Other-relative&#39;, &#39;relationship_ Own-child&#39;, &#39;relationship_ Unmarried&#39;, &#39;relationship_ Wife&#39;, &#39;race_ Amer-Indian-Eskimo&#39;, &#39;race_ Asian-Pac-Islander&#39;, &#39;race_ Black&#39;, &#39;race_ Other&#39;, &#39;race_ White&#39;, &#39;sex_ Female&#39;, &#39;sex_ Male&#39;, &#39;native-country_ Cambodia&#39;, &#39;native-country_ Canada&#39;, &#39;native-country_ China&#39;, &#39;native-country_ Columbia&#39;, &#39;native-country_ Cuba&#39;, &#39;native-country_ Dominican-Republic&#39;, &#39;native-country_ Ecuador&#39;, &#39;native-country_ El-Salvador&#39;, &#39;native-country_ England&#39;, &#39;native-country_ France&#39;, &#39;native-country_ Germany&#39;, &#39;native-country_ Greece&#39;, &#39;native-country_ Guatemala&#39;, &#39;native-country_ Haiti&#39;, &#39;native-country_ Holand-Netherlands&#39;, &#39;native-country_ Honduras&#39;, &#39;native-country_ Hong&#39;, &#39;native-country_ Hungary&#39;, &#39;native-country_ India&#39;, &#39;native-country_ Iran&#39;, &#39;native-country_ Ireland&#39;, &#39;native-country_ Italy&#39;, &#39;native-country_ Jamaica&#39;, &#39;native-country_ Japan&#39;, &#39;native-country_ Laos&#39;, &#39;native-country_ Mexico&#39;, &#39;native-country_ Nicaragua&#39;, &#39;native-country_ Outlying-US(Guam-USVI-etc)&#39;, &#39;native-country_ Peru&#39;, &#39;native-country_ Philippines&#39;, &#39;native-country_ Poland&#39;, &#39;native-country_ Portugal&#39;, &#39;native-country_ Puerto-Rico&#39;, &#39;native-country_ Scotland&#39;, &#39;native-country_ South&#39;, &#39;native-country_ Taiwan&#39;, &#39;native-country_ Thailand&#39;, &#39;native-country_ Trinadad&amp;Tobago&#39;, &#39;native-country_ United-States&#39;, &#39;native-country_ Vietnam&#39;, &#39;native-country_ Yugoslavia&#39;] 混洗和切分数据现在所有的 类别变量 已被转换成数值特征，而且所有的数值特征已被规一化。和我们一般情况下做的一样，我们现在将数据（包括特征和它们的标签）切分成训练和测试集。其中80%的数据将用于训练和20%的数据用于测试。然后再进一步把训练数据分为训练集和验证集，用来选择和优化模型。 运行下面的代码单元来完成切分。 1234567891011121314# 导入 train_test_splitfrom sklearn.model_selection import train_test_split# 将'features'和'income'数据切分成训练集和测试集X_train, X_test, y_train, y_test = train_test_split(features, income, test_size = 0.2, random_state = 0, stratify = income)# 将'X_train'和'y_train'进一步切分为训练集和验证集X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0, stratify = y_train)# 显示切分的结果print (\"Training set has &#123;&#125; samples.\".format(X_train.shape[0]))print (\"Validation set has &#123;&#125; samples.\".format(X_val.shape[0]))print (\"Testing set has &#123;&#125; samples.\".format(X_test.shape[0])) Training set has 28941 samples. Validation set has 7236 samples. Testing set has 9045 samples. 评价模型性能在这一部分中，我们将尝试四种不同的算法，并确定哪一个能够最好地建模数据。四种算法包含一个天真的预测器 和三个你选择的监督学习器。 评价方法和朴素的预测器CharityML通过他们的研究人员知道被调查者的年收入大于$50,000最有可能向他们捐款。因为这个原因CharityML对于准确预测谁能够获得$50,000以上收入尤其有兴趣。这样看起来使用准确率作为评价模型的标准是合适的。另外，把没有收入大于$50,000的人识别成年收入大于$50,000对于CharityML来说是有害的，因为他想要找到的是有意愿捐款的用户。这样，我们期望的模型具有准确预测那些能够年收入大于$50,000的能力比模型去查全这些被调查者更重要。我们能够使用F-beta score作为评价指标，这样能够同时考虑查准率和查全率： F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{precision \\cdot recall}{\\left( \\beta^2 \\cdot precision \\right) + recall}尤其是，当 $\\beta = 0.5$ 的时候更多的强调查准率，这叫做F$_{0.5}$ score （或者为了简单叫做F-score）。 问题 1 - 天真的预测器的性能通过查看收入超过和不超过 $50,000 的人数，我们能发现多数被调查者年收入没有超过 $50,000。如果我们简单地预测说“这个人的收入没有超过 $50,000”，我们就可以得到一个 准确率超过 50% 的预测。这样我们甚至不用看数据就能做到一个准确率超过 50%。这样一个预测被称作是天真的。通常对数据使用一个天真的预测器是十分重要的，这样能够帮助建立一个模型表现是否好的基准。 使用下面的代码单元计算天真的预测器的相关性能。将你的计算结果赋值给&#39;accuracy&#39;, ‘precision’, ‘recall’ 和 &#39;fscore&#39;，这些值会在后面被使用，请注意这里不能使用scikit-learn，你需要根据公式自己实现相关计算。 如果我们选择一个无论什么情况都预测被调查者年收入大于 $50,000 的模型，那么这个模型在验证集上的准确率，查准率，查全率和 F-score是多少？ 12345678910111213141516171819202122#不能使用scikit-learn，你需要根据公式自己实现相关计算。TP = float(len(y_val[y_val == 1]))FP = float(len(y_val[y_val == 0]))FN = 0#TODO： 计算准确率accuracy = float(TP)/len(y_val)# TODO： 计算查准率 Precisionprecision = TP/(TP+FP)# TODO： 计算查全率 Recallrecall = TP/(TP+FN)# TODO： 使用上面的公式，设置beta=0.5，计算F-scorefscore = (1+0.5**2)*((precision*recall)) / (0.5**2*precision+recall)# 打印结果print (\"Naive Predictor on validation data: \\n \\ Accuracy score: &#123;:.4f&#125; \\n \\ Precision: &#123;:.4f&#125; \\n \\ Recall: &#123;:.4f&#125; \\n \\ F-score: &#123;:.4f&#125;\".format(accuracy, precision, recall, fscore)) Naive Predictor on validation data: Accuracy score: 0.2478 Precision: 0.2478 Recall: 1.0000 F-score: 0.2917 监督学习模型问题 2 - 模型应用你能够在 scikit-learn 中选择以下监督学习模型 高斯朴素贝叶斯 (GaussianNB) 决策树 (DecisionTree) 集成方法 (Bagging, AdaBoost, Random Forest, Gradient Boosting) K近邻 (K Nearest Neighbors) 随机梯度下降分类器 (SGDC) 支撑向量机 (SVM) Logistic回归（LogisticRegression） 从上面的监督学习模型中选择三个适合我们这个问题的模型，并回答相应问题。 模型1模型名称 回答：SVM 描述一个该模型在真实世界的一个应用场景。（你需要为此做点研究，并给出你的引用出处） 回答：人类行为认知的辨别，根据图像判断人物在做什么 这个模型的优势是什么？他什么情况下表现最好？ 回答：丰富的核函数可以灵活解决回归问题和分类问题，当特征大于样本数且样本总数较小的时候表现优异 这个模型的缺点是什么？什么条件下它表现很差？ 回答：当需要计算大量样本时丰富的核函数因为没有通用标准计算量大 且表现平平 根据我们当前数据集的特点，为什么这个模型适合这个问题。 回答：当前数据集含有大量特征 且样本总数适中 而且svm丰富的核函数可以很好的满足该项目需求 模型2模型名称 回答：Random Forest 描述一个该模型在真实世界的一个应用场景。（你需要为此做点研究，并给出你的引用出处） 回答：随机森林分类器对土地覆盖进行分类 这个模型的优势是什么？他什么情况下表现最好？ 回答：简单直观 便于理解 提前归一化 以及处理缺失值 可以解决任何类型的数据集 且不需要对数据预处理 通过网格搜索选择超参数 高泛化能力 当特征和样本数量比列保持平衡时表现优异 这个模型的缺点是什么？什么条件下它表现很差？ 回答：容易过拟合 当某叶子结点发生变化时整体结构也发生变化 且当特征的样本比例不平衡的时候容易出现偏向 根据我们当前数据集的特点，为什么这个模型适合这个问题。 回答：特征唯独高 评估各个特征重要性 小范围噪声不会过拟合 模型3模型名称 回答：K近邻 (K Nearest Neighbors) 描述一个该模型在真实世界的一个应用场景。（你需要为此做点研究，并给出你的引用出处） 回答：使用k近邻法估计和绘制森林林分密度，体积和覆盖类型 这个模型的优势是什么？他什么情况下表现最好？ 回答：思想简单 容易理解 聚类效果较优 这个模型的缺点是什么？什么条件下它表现很差？ 回答：对异常值敏感 提前判断K值 局部最优 复杂度高不易控制 迭代次数较多 根据我们当前数据集的特点，为什么这个模型适合这个问题。 回答：该项目数据适中且属于二分类为题 练习 - 创建一个训练和预测的流水线为了正确评估你选择的每一个模型的性能，创建一个能够帮助你快速有效地使用不同大小的训练集并在验证集上做预测的训练和验证的流水线是十分重要的。你在这里实现的功能将会在接下来的部分中被用到。在下面的代码单元中，你将实现以下功能： 从sklearn.metrics中导入fbeta_score和accuracy_score。 用训练集拟合学习器，并记录训练时间。 对训练集的前300个数据点和验证集进行预测并记录预测时间。 计算预测训练集的前300个数据点的准确率和F-score。 计算预测验证集的准确率和F-score。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# TODO：从sklearn中导入两个评价指标 - fbeta_score和accuracy_scorefrom sklearn.metrics import fbeta_score, accuracy_scoredef train_predict(learner, sample_size, X_train, y_train, X_val, y_val): ''' inputs: - learner: the learning algorithm to be trained and predicted on - sample_size: the size of samples (number) to be drawn from training set - X_train: features training set - y_train: income training set - X_val: features validation set - y_val: income validation set ''' results = &#123;&#125; # TODO：使用sample_size大小的训练数据来拟合学习器 # TODO: Fit the learner to the training data using slicing with 'sample_size' start = time() # 获得程序开始时间 learner = learner.fit(X_train[:sample_size], y_train[:sample_size]) end = time() # 获得程序结束时间 # TODO：计算训练时间 results['train_time'] = end - start # TODO: 得到在验证集上的预测值 # 然后得到对前300个训练数据的预测结果 start = time() # 获得程序开始时间 predictions_val = learner.predict(X_val) predictions_train = learner.predict(X_train[:300]) end = time() # 获得程序结束时间 # TODO：计算预测用时 results['pred_time'] = end - start # TODO：计算在最前面的300个训练数据的准确率 results['acc_train'] = accuracy_score(y_test[:300], predictions_train) # TODO：计算在验证上的准确率 results['acc_val'] = accuracy_score(y_val, predictions_val) # TODO：计算在最前面300个训练数据上的F-score results['f_train'] = fbeta_score(y_train[:300], predictions_train, beta=0.5) # TODO：计算验证集上的F-score results['f_val'] = fbeta_score(y_val, predictions_val, beta=0.5) # 成功 print (\"&#123;&#125; trained on &#123;&#125; samples.\".format(learner.__class__.__name__, sample_size)) # 返回结果 return results 练习：初始模型的评估在下面的代码单元中，您将需要实现以下功能： 导入你在前面讨论的三个监督学习模型。 初始化三个模型并存储在&#39;clf_A&#39;，&#39;clf_B&#39;和&#39;clf_C&#39;中。 使用模型的默认参数值，在接下来的部分中你将需要对某一个模型的参数进行调整。 设置random_state (如果有这个参数)。 计算1%， 10%， 100%的训练数据分别对应多少个数据点，并将这些值存储在&#39;samples_1&#39;, &#39;samples_10&#39;, &#39;samples_100&#39;中 注意：取决于你选择的算法，下面实现的代码可能需要一些时间来运行！ 123456789101112131415161718192021222324# TODO：从sklearn中导入三个监督学习模型from sklearn.neighbors import KNeighborsClassifierfrom sklearn.svm import SVCfrom sklearn.ensemble import RandomForestClassifier# TODO：初始化三个模型clf_A = KNeighborsClassifier(n_neighbors=2)clf_B = SVC(kernel='linear')clf_C = RandomForestClassifier(random_state=0)# TODO：计算1%， 10%， 100%的训练数据分别对应多少点samples_1 = int(len(X_train) * 0.01)samples_10 = int(len(X_train) * 0.10)samples_100 = len(X_train)# 收集学习器的结果results = &#123;&#125;for clf in [clf_A, clf_B, clf_C]: clf_name = clf.__class__.__name__ results[clf_name] = &#123;&#125; for i, samples in enumerate([samples_1, samples_10, samples_100]): results[clf_name][i] = train_predict(clf, samples, X_train, y_train, X_val, y_val)# 对选择的三个模型得到的评价结果进行可视化vs.evaluate(results, accuracy, fscore) KNeighborsClassifier trained on 289 samples. KNeighborsClassifier trained on 2894 samples. KNeighborsClassifier trained on 28941 samples. SVC trained on 289 samples. SVC trained on 2894 samples. SVC trained on 28941 samples. RandomForestClassifier trained on 289 samples. RandomForestClassifier trained on 2894 samples. RandomForestClassifier trained on 28941 samples. 提高效果在这最后一节中，您将从三个有监督的学习模型中选择 最好的 模型来使用学生数据。你将在整个训练集（X_train和y_train）上使用网格搜索优化至少调节一个参数以获得一个比没有调节之前更好的 F-score。 问题 3 - 选择最佳的模型基于你前面做的评价，用一到两段话向 CharityML 解释这三个模型中哪一个对于判断被调查者的年收入大于 $50,000 是最合适的。提示：你的答案应该包括评价指标，预测/训练时间，以及该算法是否适合这里的数据。 回答：从F-score来看，RandomForest在所有训练集上和1%、10%的验证集表现最好，SVM在100%的验证集表现最好从准确度上来看，RandomForest在所有训练集上和1%、10%的验证集表现最好，SVM在100%的验证集表现最好从训练时间上来看，SVM明显多与其他两种算法从预测时间上来看，KNeighbors最多，SVM次之，RandomForest最少KNeighbors由于初始点选择的问题可能会导致分类效果不固定综上所述RandomForest综合表现较好，时间短分类准，我认为最合适，有调优的空间。如果不考虑时间SVM也有可能调出更优化的结果。 问题 4 - 用通俗的话解释模型用一到两段话，向 CharityML 用外行也听得懂的话来解释最终模型是如何工作的。你需要解释所选模型的主要特点。例如，这个模型是怎样被训练的，它又是如何做出预测的。避免使用高级的数学或技术术语，不要使用公式或特定的算法名词。 回答： 训练 根据所有数据依次找到能最大区分当前数据的一个特征，进行数据分割，然后对分割的数据接着重复上述步骤，直到所有的特征都判断完，这样一系列的判断对应的结果为数据的类别，这样的判断构成的就是决策树 再多次执行上一步，每次执行的时候对样本进行随机有放回的抽样，构成多个不一样的决策树，这些决策树合并起来就是随机森林 预测 对于新来的样本，每个决策树做一个分类结果进行相等权重投票，然后以多数者投票的结果作为该样本的分类结果 练习：模型调优调节选择的模型的参数。使用网格搜索（GridSearchCV）来至少调整模型的重要参数（至少调整一个），这个参数至少需尝试3个不同的值。你要使用整个训练集来完成这个过程。在接下来的代码单元中，你需要实现以下功能： 导入sklearn.model_selection.GridSearchCV 和 sklearn.metrics.make_scorer. 初始化你选择的分类器，并将其存储在clf中。 设置random_state (如果有这个参数)。 创建一个对于这个模型你希望调整参数的字典。 例如: parameters = {‘parameter’ : [list of values]}。 注意： 如果你的学习器有 max_features 参数，请不要调节它！ 使用make_scorer来创建一个fbeta_score评分对象（设置$\\beta = 0.5$）。 在分类器clf上用’scorer’作为评价函数运行网格搜索，并将结果存储在grid_obj中。 用训练集（X_train, y_train）训练grid search object,并将结果存储在grid_fit中。 注意： 取决于你选择的参数列表，下面实现的代码可能需要花一些时间运行！ 1234567891011121314151617181920212223242526272829303132333435# TODO：导入'GridSearchCV', 'make_scorer'和其他一些需要的库from sklearn.model_selection import GridSearchCVfrom sklearn.metrics import make_scorer# TODO：初始化分类器clf = RandomForestClassifier(random_state=0)# TODO：创建你希望调节的参数列表parameters = &#123;'n_estimators':[10,50,100,150]&#125;# TODO：创建一个fbeta_score打分对象scorer = make_scorer(fbeta_score, beta=0.5)# TODO：在分类器上使用网格搜索，使用'scorer'作为评价函数grid_obj = GridSearchCV(clf, parameters, scorer)# TODO：用训练数据拟合网格搜索对象并找到最佳参数grid_obj.fit(X_train, y_train)# 得到estimatorbest_clf = grid_obj.best_estimator_# 使用没有调优的模型做预测predictions = (clf.fit(X_train, y_train)).predict(X_val)best_predictions = best_clf.predict(X_val)# 汇报调优后的模型print (\"best_clf\\n------\")print (best_clf)# 汇报调参前和调参后的分数print (\"\\nUnoptimized model\\n------\")print (\"Accuracy score on validation data: &#123;:.4f&#125;\".format(accuracy_score(y_val, predictions)))print (\"F-score on validation data: &#123;:.4f&#125;\".format(fbeta_score(y_val, predictions, beta = 0.5)))print (\"\\nOptimized Model\\n------\")print (\"Final accuracy score on the validation data: &#123;:.4f&#125;\".format(accuracy_score(y_val, best_predictions)))print (\"Final F-score on the validation data: &#123;:.4f&#125;\".format(fbeta_score(y_val, best_predictions, beta = 0.5))) best_clf ------ RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1, oob_score=False, random_state=0, verbose=0, warm_start=False) Unoptimized model ------ Accuracy score on validation data: 0.8389 F-score on validation data: 0.6812 Optimized Model ------ Final accuracy score on the validation data: 0.8456 Final F-score on the validation data: 0.6940 问题 5 - 最终模型评估你的最优模型在测试数据上的准确率和 F-score 是多少？这些分数比没有优化的模型好还是差？注意：请在下面的表格中填写你的结果，然后在答案框中提供讨论。 结果: 评价指标 未优化的模型 优化的模型 准确率 0.8389 0.8456 F-score 0.6812 0.6940 回答：相比较没有优化的模型有少许提升 特征的重要性在数据上（比如我们这里使用的人口普查的数据）使用监督学习算法的一个重要的任务是决定哪些特征能够提供最强的预测能力。专注于少量的有效特征和标签之间的关系，我们能够更加简单地理解这些现象，这在很多情况下都是十分有用的。在这个项目的情境下这表示我们希望选择一小部分特征，这些特征能够在预测被调查者是否年收入大于$50,000这个问题上有很强的预测能力。 选择一个有 &#39;feature_importance_&#39; 属性的scikit学习分类器（例如 AdaBoost，随机森林）。&#39;feature_importance_&#39; 属性是对特征的重要性排序的函数。在下一个代码单元中用这个分类器拟合训练集数据并使用这个属性来决定人口普查数据中最重要的5个特征。 问题 6 - 观察特征相关性当探索数据的时候，它显示在这个人口普查数据集中每一条记录我们有十三个可用的特征。在这十三个记录中，你认为哪五个特征对于预测是最重要的，选择每个特征的理由是什么？你会怎样对他们排序？ 回答： 特征1:age 年龄的增长事业和收入也会增长 特征2:hours-per-week 理论上工作时间越多收入越高 特征3:captial-gain 拥有其他资本收益代表经济条件更好 特征4:martial-status 收入大雨50K说明拥有更好的承担能力 特征5:education-num 教育程度越高 越有可能高收入 练习 - 提取特征重要性选择一个scikit-learn中有feature_importance_属性的监督学习分类器，这个属性是一个在做预测的时候根据所选择的算法来对特征重要性进行排序的功能。 在下面的代码单元中，你将要实现以下功能： 如果这个模型和你前面使用的三个模型不一样的话从sklearn中导入一个监督学习模型。 在整个训练集上训练一个监督学习模型。 使用模型中的 &#39;feature_importances_&#39;提取特征的重要性。 12345678910# TODO：导入一个有'feature_importances_'的监督学习模型# TODO：在训练集上训练一个监督学习模型model = best_clf# TODO： 提取特征重要性importances = model.feature_importances_# 绘图best_clfvs.feature_plot(importances, X_train, y_train) 问题 7 - 提取特征重要性观察上面创建的展示五个用于预测被调查者年收入是否大于$50,000最相关的特征的可视化图像。 这五个特征的权重加起来是否超过了0.5?这五个特征和你在问题 6中讨论的特征比较怎么样？如果说你的答案和这里的相近，那么这个可视化怎样佐证了你的想法？如果你的选择不相近，那么为什么你觉得这些特征更加相关？ 回答：0.24+0.12+0.10+0.07+0.06&gt;0.5 基本一致。权重越高说明重要性越高 特征选择如果我们只是用可用特征的一个子集的话模型表现会怎么样？通过使用更少的特征来训练，在评价指标的角度来看我们的期望是训练和预测的时间会更少。从上面的可视化来看，我们可以看到前五个最重要的特征贡献了数据中所有特征中超过一半的重要性。这提示我们可以尝试去减小特征空间，简化模型需要学习的信息。下面代码单元将使用你前面发现的优化模型，并只使用五个最重要的特征在相同的训练集上训练模型。 1234567891011121314151617181920# 导入克隆模型的功能from sklearn.base import clone# 减小特征空间X_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::-1])[:5]]]X_val_reduced = X_val[X_val.columns.values[(np.argsort(importances)[::-1])[:5]]]# 在前面的网格搜索的基础上训练一个“最好的”模型clf_on_reduced = (clone(best_clf)).fit(X_train_reduced, y_train)# 做一个新的预测reduced_predictions = clf_on_reduced.predict(X_val_reduced)# 对于每一个版本的数据汇报最终模型的分数print (\"Final Model trained on full data\\n------\")print (\"Accuracy on validation data: &#123;:.4f&#125;\".format(accuracy_score(y_val, best_predictions)))print (\"F-score on validation data: &#123;:.4f&#125;\".format(fbeta_score(y_val, best_predictions, beta = 0.5)))print (\"\\nFinal Model trained on reduced data\\n------\")print (\"Accuracy on validation data: &#123;:.4f&#125;\".format(accuracy_score(y_val, reduced_predictions)))print (\"F-score on validation data: &#123;:.4f&#125;\".format(fbeta_score(y_val, reduced_predictions, beta = 0.5))) Final Model trained on full data ------ Accuracy on validation data: 0.8456 F-score on validation data: 0.6940 Final Model trained on reduced data ------ Accuracy on validation data: 0.8333 F-score on validation data: 0.6678 问题 8 - 特征选择的影响最终模型在只是用五个特征的数据上和使用所有的特征数据上的 F-score 和准确率相比怎么样？如果训练时间是一个要考虑的因素，你会考虑使用部分特征的数据作为你的训练集吗？ 回答： 评价指标 使用所有特征 使用部分特征 准确率 0.8456 0.8333 F-score 0.6940 0.6678 使用部分特征和所有特征相比准确率和F1值出现了下降现象 如果考虑时间问题我会考虑使用部分特征 会缩短大量训练时间 问题 9 - 在测试集上测试你的模型终于到了测试的时候，记住，测试集只能用一次。 使用你最有信心的模型，在测试集上测试，计算出准确率和 F-score。简述你选择这个模型的原因，并分析测试结果 123456#TODO test your model on testing data and report accuracy and F scoreres = best_clf.predict(X_test)from sklearn.metrics import fbeta_score,accuracy_scoreprint('accuracy score:&#123;&#125;'.format(accuracy_score(y_test, res)))print('fbeta score:&#123;&#125;'.format(fbeta_score(y_test, res, beta=0.5))) accuracy score:0.8383637368711996 fbeta score:0.6780398221534892 注意： 当你写完了所有的代码，并且回答了所有的问题。你就可以把你的 iPython Notebook 导出成 HTML 文件。你可以在菜单栏，这样导出File -&gt; Download as -&gt; HTML (.html)把这个 HTML 和这个 iPython notebook 一起做为你的作业提交。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://yoursite.com/tags/Machine-Learning/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}]},{"title":"dog_app by CNN","slug":"dog-app-for-CNN","date":"2019-02-15T13:34:56.000Z","updated":"2019-02-16T11:07:20.711Z","comments":false,"path":"2019/02/15/dog-app-for-CNN/","link":"","permalink":"http://yoursite.com/2019/02/15/dog-app-for-CNN/","excerpt":"","text":"卷积神经网络（Convolutional Neural Network, CNN）项目：实现一个狗品种识别算法App在这个notebook文件中，有些模板代码已经提供给你，但你还需要实现更多的功能来完成这个项目。除非有明确要求，你无须修改任何已给出的代码。以‘(练习)’开始的标题表示接下来的代码部分中有你需要实现的功能。这些部分都配有详细的指导，需要实现的部分也会在注释中以’TODO’标出。请仔细阅读所有的提示。 除了实现代码外，你还需要回答一些与项目及代码相关的问题。每个需要回答的问题都会以 ‘问题 X’ 标记。请仔细阅读每个问题，并且在问题后的 ‘回答’ 部分写出完整的答案。我们将根据 你对问题的回答 和 撰写代码实现的功能 来对你提交的项目进行评分。 提示：Code 和 Markdown 区域可通过 Shift + Enter 快捷键运行。此外，Markdown可以通过双击进入编辑模式。 项目中显示为选做的部分可以帮助你的项目脱颖而出，而不是仅仅达到通过的最低要求。如果你决定追求更高的挑战，请在此 notebook 中完成选做部分的代码。 让我们开始吧在这个notebook中，你将迈出第一步，来开发可以作为移动端或 Web应用程序一部分的算法。在这个项目的最后，你的程序将能够把用户提供的任何一个图像作为输入。如果可以从图像中检测到一只狗，它会输出对狗品种的预测。如果图像中是一个人脸，它会预测一个与其最相似的狗的种类。下面这张图展示了完成项目后可能的输出结果。（……实际上我们希望每个学生的输出结果不相同！） 在现实世界中，你需要拼凑一系列的模型来完成不同的任务；举个例子，用来预测狗种类的算法会与预测人类的算法不同。在做项目的过程中，你可能会遇到不少失败的预测，因为并不存在完美的算法和模型。你最终提交的不完美的解决方案也一定会给你带来一个有趣的学习经验！ 项目内容我们将这个notebook分为不同的步骤，你可以使用下面的链接来浏览此notebook。 Step 0: 导入数据集 Step 1: 检测人脸 Step 2: 检测狗狗 Step 3: 从头创建一个CNN来分类狗品种 Step 4: 使用一个CNN来区分狗的品种(使用迁移学习) Step 5: 建立一个CNN来分类狗的品种（使用迁移学习） Step 6: 完成你的算法 Step 7: 测试你的算法 在该项目中包含了如下的问题： 问题 1 问题 2 问题 3 问题 4 问题 5 问题 6 问题 7 问题 8 问题 9 问题 10 问题 11 步骤 0: 导入数据集导入狗数据集在下方的代码单元（cell）中，我们导入了一个狗图像的数据集。我们使用 scikit-learn 库中的 load_files 函数来获取一些变量： train_files, valid_files, test_files - 包含图像的文件路径的numpy数组 train_targets, valid_targets, test_targets - 包含独热编码分类标签的numpy数组 dog_names - 由字符串构成的与标签相对应的狗的种类 1234567891011121314151617181920212223242526from sklearn.datasets import load_files from keras.utils import np_utilsimport numpy as npfrom glob import glob# 定义函数来加载train，test和validation数据集def load_dataset(path): data = load_files(path) dog_files = np.array(data['filenames']) dog_targets = np_utils.to_categorical(np.array(data['target']), 133) return dog_files, dog_targets# 加载train，test和validation数据集train_files, train_targets = load_dataset('dogImages/dogImages/train')valid_files, valid_targets = load_dataset('dogImages/dogImages/valid')test_files, test_targets = load_dataset('dogImages/dogImages/test')# 加载狗品种列表dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/dogImages/train/*/\"))]# 打印数据统计描述print('There are %d total dog categories.' % len(dog_names))print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))print('There are %d training dog images.' % len(train_files))print('There are %d validation dog images.' % len(valid_files))print('There are %d test dog images.'% len(test_files)) Using TensorFlow backend. There are 133 total dog categories. There are 8351 total dog images. There are 6680 training dog images. There are 835 validation dog images. There are 836 test dog images. 导入人脸数据集在下方的代码单元中，我们导入人脸图像数据集，文件所在路径存储在名为 human_files 的 numpy 数组。 123456789import randomrandom.seed(8675309)# 加载打乱后的人脸数据集的文件名human_files = np.array(glob(\"lfw/lfw/*/*\"))random.shuffle(human_files)# 打印数据集的数据量print('There are %d total human images.' % len(human_files)) There are 13233 total human images. 步骤1：检测人脸我们将使用 OpenCV 中的 Haar feature-based cascade classifiers 来检测图像中的人脸。OpenCV 提供了很多预训练的人脸检测模型，它们以XML文件保存在 github。我们已经下载了其中一个检测模型，并且把它存储在 haarcascades 的目录中。 在如下代码单元中，我们将演示如何使用这个检测模型在样本图像中找到人脸。 123456789101112131415161718192021222324252627282930import cv2 import matplotlib.pyplot as plt %matplotlib inline # 提取预训练的人脸检测模型face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')# 加载彩色（通道顺序为BGR）图像img = cv2.imread(human_files[3])# 将BGR图像进行灰度处理gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)# 在图像中找出脸faces = face_cascade.detectMultiScale(gray)# 打印图像中检测到的脸的个数print('Number of faces detected:', len(faces))# 获取每一个所检测到的脸的识别框for (x,y,w,h) in faces: # 在人脸图像中绘制出识别框 cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) # 将BGR图像转变为RGB图像以打印cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)# 展示含有识别框的图像plt.imshow(cv_rgb)plt.show() Number of faces detected: 1 在使用任何一个检测模型之前，将图像转换为灰度图是常用过程。detectMultiScale 函数使用储存在 face_cascade 中的的数据，对输入的灰度图像进行分类。 在上方的代码中，faces 以 numpy 数组的形式，保存了识别到的面部信息。它其中每一行表示一个被检测到的脸，该数据包括如下四个信息：前两个元素 x、y 代表识别框左上角的 x 和 y 坐标（参照上图，注意 y 坐标的方向和我们默认的方向不同）；后两个元素代表识别框在 x 和 y 轴两个方向延伸的长度 w 和 d。 写一个人脸识别器我们可以将这个程序封装为一个函数。该函数的输入为人脸图像的路径，当图像中包含人脸时，该函数返回 True，反之返回 False。该函数定义如下所示。 123456# 如果img_path路径表示的图像检测到了脸，返回\"True\" def face_detector(img_path): img = cv2.imread(img_path) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) faces = face_cascade.detectMultiScale(gray) return len(faces) &gt; 0 【练习】 评估人脸检测模型 问题 1:在下方的代码块中，使用 face_detector 函数，计算： human_files 的前100张图像中，能够检测到人脸的图像占比多少？ dog_files 的前100张图像中，能够检测到人脸的图像占比多少？ 理想情况下，人图像中检测到人脸的概率应当为100%，而狗图像中检测到人脸的概率应该为0%。你会发现我们的算法并非完美，但结果仍然是可以接受的。我们从每个数据集中提取前100个图像的文件路径，并将它们存储在human_files_short和dog_files_short中。 12345678910human_files_short = human_files[:100]dog_files_short = train_files[:100]## 请不要修改上方代码def detect(detector, files): return np.mean(list(map(detector, files)))## TODO: 基于human_files_short和dog_files_short## 中的图像测试face_detector的表现print('humna: &#123;:.2%&#125;'.format(detect(face_detector,human_files_short)))print('dog: &#123;:.2%&#125;'.format(detect(face_detector, dog_files_short))) humna: 99.00% dog: 12.00% 问题 2:就算法而言，该算法成功与否的关键在于，用户能否提供含有清晰面部特征的人脸图像。那么你认为，这样的要求在实际使用中对用户合理吗？如果你觉得不合理，你能否想到一个方法，即使图像中并没有清晰的面部特征，也能够检测到人脸？ 回答: 不合理 轮廓检测 压缩图像特征 通过正图像和负图像训练分类器 给予权重 确定位置 最后保留正图像 选做：我们建议在你的算法中使用opencv的人脸检测模型去检测人类图像，不过你可以自由地探索其他的方法，尤其是尝试使用深度学习来解决它:)。请用下方的代码单元来设计和测试你的面部监测算法。如果你决定完成这个选做任务，你需要报告算法在每一个数据集上的表现。 1234567891011121314151617181920212223242526272829303132## (选做) TODO: 报告另一个面部检测算法在LFW数据集上的表现### 你可以随意使用所需的代码单元数import cv2 import matplotlib.pyplot as plt %matplotlib inline # 提取预训练的人脸检测模型face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt2.xml')# 加载彩色（通道顺序为BGR）图像img = cv2.imread(human_files[4])# 将BGR图像进行灰度处理gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)# 在图像中找出脸faces = face_cascade.detectMultiScale(gray)# 打印图像中检测到的脸的个数print('Number of faces detected:', len(faces))# 获取每一个所检测到的脸的识别框for (x,y,w,h) in faces: # 在人脸图像中绘制出识别框 cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) # 将BGR图像转变为RGB图像以打印cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)# 展示含有识别框的图像plt.imshow(cv_rgb)plt.show() Number of faces detected: 1 步骤 2: 检测狗狗在这个部分中，我们使用预训练的 ResNet-50 模型去检测图像中的狗。下方的第一行代码就是下载了 ResNet-50 模型的网络结构参数，以及基于 ImageNet 数据集的预训练权重。 ImageNet 这目前一个非常流行的数据集，常被用来测试图像分类等计算机视觉任务相关的算法。它包含超过一千万个 URL，每一个都链接到 1000 categories 中所对应的一个物体的图像。任给输入一个图像，该 ResNet-50 模型会返回一个对图像中物体的预测结果。 1234from keras.applications.resnet50 import ResNet50# 定义ResNet50模型ResNet50_model = ResNet50(weights='imagenet') 数据预处理 在使用 TensorFlow 作为后端的时候，在 Keras 中，CNN 的输入是一个4维数组（也被称作4维张量），它的各维度尺寸为 (nb_samples, rows, columns, channels)。其中 nb_samples 表示图像（或者样本）的总数，rows, columns, 和 channels 分别表示图像的行数、列数和通道数。 下方的 path_to_tensor 函数实现如下将彩色图像的字符串型的文件路径作为输入，返回一个4维张量，作为 Keras CNN 输入。因为我们的输入图像是彩色图像，因此它们具有三个通道（ channels 为 3）。 该函数首先读取一张图像，然后将其缩放为 224×224 的图像。 随后，该图像被调整为具有4个维度的张量。 对于任一输入图像，最后返回的张量的维度是：(1, 224, 224, 3)。 paths_to_tensor 函数将图像路径的字符串组成的 numpy 数组作为输入，并返回一个4维张量，各维度尺寸为 (nb_samples, 224, 224, 3)。 在这里，nb_samples是提供的图像路径的数据中的样本数量或图像数量。你也可以将 nb_samples 理解为数据集中3维张量的个数（每个3维张量表示一个不同的图像。 1234567891011121314from keras.preprocessing import image from tqdm import tqdmdef path_to_tensor(img_path): # 用PIL加载RGB图像为PIL.Image.Image类型 img = image.load_img(img_path, target_size=(224, 224)) # 将PIL.Image.Image类型转化为格式为(224, 224, 3)的3维张量 x = image.img_to_array(img) # 将3维张量转化为格式为(1, 224, 224, 3)的4维张量并返回 return np.expand_dims(x, axis=0)def paths_to_tensor(img_paths): list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)] return np.vstack(list_of_tensors) 基于 ResNet-50 架构进行预测对于通过上述步骤得到的四维张量，在把它们输入到 ResNet-50 网络、或 Keras 中其他类似的预训练模型之前，还需要进行一些额外的处理： 首先，这些图像的通道顺序为 RGB，我们需要重排他们的通道顺序为 BGR。 其次，预训练模型的输入都进行了额外的归一化过程。因此我们在这里也要对这些张量进行归一化，即对所有图像所有像素都减去像素均值 [103.939, 116.779, 123.68]（以 RGB 模式表示，根据所有的 ImageNet 图像算出）。 导入的 preprocess_input 函数实现了这些功能。如果你对此很感兴趣，可以在 这里 查看 preprocess_input的代码。 在实现了图像处理的部分之后，我们就可以使用模型来进行预测。这一步通过 predict 方法来实现，它返回一个向量，向量的第 i 个元素表示该图像属于第 i 个 ImageNet 类别的概率。这通过如下的 ResNet50_predict_labels 函数实现。 通过对预测出的向量取用 argmax 函数（找到有最大概率值的下标序号），我们可以得到一个整数，即模型预测到的物体的类别。进而根据这个 清单，我们能够知道这具体是哪个品种的狗狗。 12345from keras.applications.resnet50 import preprocess_input, decode_predictionsdef ResNet50_predict_labels(img_path): # 返回img_path路径的图像的预测向量 img = preprocess_input(path_to_tensor(img_path)) return np.argmax(ResNet50_model.predict(img)) 完成狗检测模型在研究该 清单 的时候，你会注意到，狗类别对应的序号为151-268。因此，在检查预训练模型判断图像是否包含狗的时候，我们只需要检查如上的 ResNet50_predict_labels 函数是否返回一个介于151和268之间（包含区间端点）的值。 我们通过这些想法来完成下方的 dog_detector 函数，如果从图像中检测到狗就返回 True，否则返回 False。 123def dog_detector(img_path): prediction = ResNet50_predict_labels(img_path) return ((prediction &lt;= 268) &amp; (prediction &gt;= 151)) 【作业】评估狗狗检测模型 问题 3:在下方的代码块中，使用 dog_detector 函数，计算： human_files_short中图像检测到狗狗的百分比？ dog_files_short中图像检测到狗狗的百分比？ 123### TODO: 测试dog_detector函数在human_files_short和dog_files_short的表现print('humna: &#123;:.2%&#125;'.format(detect(dog_detector,human_files_short)))print('dog: &#123;:.2%&#125;'.format(detect(dog_detector, dog_files_short))) humna: 2.00% dog: 100.00% 步骤 3: 从头开始创建一个CNN来分类狗品种现在我们已经实现了一个函数，能够在图像中识别人类及狗狗。但我们需要更进一步的方法，来对狗的类别进行识别。在这一步中，你需要实现一个卷积神经网络来对狗的品种进行分类。你需要从头实现你的卷积神经网络（在这一阶段，你还不能使用迁移学习），并且你需要达到超过1%的测试集准确率。在本项目的步骤五种，你还有机会使用迁移学习来实现一个准确率大大提高的模型。 在添加卷积层的时候，注意不要加上太多的（可训练的）层。更多的参数意味着更长的训练时间，也就是说你更可能需要一个 GPU 来加速训练过程。万幸的是，Keras 提供了能够轻松预测每次迭代（epoch）花费时间所需的函数。你可以据此推断你算法所需的训练时间。 值得注意的是，对狗的图像进行分类是一项极具挑战性的任务。因为即便是一个正常人，也很难区分布列塔尼犬和威尔士史宾格犬。 布列塔尼犬（Brittany） 威尔士史宾格犬（Welsh Springer Spaniel） 不难发现其他的狗品种会有很小的类间差别（比如金毛寻回犬和美国水猎犬）。 金毛寻回犬（Curly-Coated Retriever） 美国水猎犬（American Water Spaniel） 同样，拉布拉多犬（labradors）有黄色、棕色和黑色这三种。那么你设计的基于视觉的算法将不得不克服这种较高的类间差别，以达到能够将这些不同颜色的同类狗分到同一个品种中。 黄色拉布拉多犬（Yellow Labrador） 棕色拉布拉多犬（Chocolate Labrador） 黑色拉布拉多犬（Black Labrador） 我们也提到了随机分类将得到一个非常低的结果：不考虑品种略有失衡的影响，随机猜测到正确品种的概率是1/133，相对应的准确率是低于1%的。 请记住，在深度学习领域，实践远远高于理论。大量尝试不同的框架吧，相信你的直觉！当然，玩得开心！ 数据预处理通过对每张图像的像素值除以255，我们对图像实现了归一化处理。 1234567from PIL import ImageFile ImageFile.LOAD_TRUNCATED_IMAGES = True # Keras中的数据预处理过程train_tensors = paths_to_tensor(train_files).astype('float32')/255valid_tensors = paths_to_tensor(valid_files).astype('float32')/255test_tensors = paths_to_tensor(test_files).astype('float32')/255 100%|██████████| 6680/6680 [00:56&lt;00:00, 117.40it/s] 100%|██████████| 835/835 [00:32&lt;00:00, 25.94it/s] 100%|██████████| 836/836 [00:15&lt;00:00, 54.77it/s] 【练习】模型架构创建一个卷积神经网络来对狗品种进行分类。在你代码块的最后，执行 model.summary() 来输出你模型的总结信息。 我们已经帮你导入了一些所需的 Python 库，如有需要你可以自行导入。如果你在过程中遇到了困难，如下是给你的一点小提示——该模型能够在5个 epoch 内取得超过1%的测试准确率，并且能在CPU上很快地训练。 问题 4:在下方的代码块中尝试使用 Keras 搭建卷积网络的架构，并回答相关的问题。 你可以尝试自己搭建一个卷积网络的模型，那么你需要回答你搭建卷积网络的具体步骤（用了哪些层）以及为什么这样搭建。 你也可以根据上图提示的步骤搭建卷积网络，那么请说明为何如上的架构能够在该问题上取得很好的表现。 回答: 建立一个卷基层 建立一个池化层 再建立一个卷基层 再建立一个池化层 再建立一个卷基层 再建立一个池化层 添加一个全局池化层 再添加一个全连接层 可以对图像进行更细致的处理 但最终结果不会因为层数越深处理越好 123456789101112131415161718192021from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Flatten, BatchNormalization, Activationfrom keras.layers import Dropout, Flatten, Densefrom keras.models import Sequentialmodel = Sequential()### TODO: 定义你的网络架构model.add(Conv2D(filters=3, kernel_size=2, padding='same',activation='relu',input_shape=(224,224,3)))model.add(MaxPooling2D(pool_size=2))model.add(Dropout(0.2))model.add(Conv2D(filters=6, kernel_size=2, padding='same',activation='relu'))model.add(MaxPooling2D(pool_size=2))model.add(Dropout(0.2))model.add(Conv2D(16, (3, 3), strides=(1, 1), padding='valid'))model.add(MaxPooling2D((2, 2)))model.add(BatchNormalization())model.add(Activation('relu'))model.add(Dropout(0.2))model.add(GlobalAveragePooling2D())model.add(Dense(133 ,activation='softmax'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 224, 224, 3) 39 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 112, 112, 3) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 112, 112, 3) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 112, 112, 6) 78 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 56, 56, 6) 0 _________________________________________________________________ dropout_2 (Dropout) (None, 56, 56, 6) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 54, 54, 16) 880 _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 27, 27, 16) 0 _________________________________________________________________ batch_normalization_1 (Batch (None, 27, 27, 16) 64 _________________________________________________________________ activation_50 (Activation) (None, 27, 27, 16) 0 _________________________________________________________________ dropout_3 (Dropout) (None, 27, 27, 16) 0 _________________________________________________________________ global_average_pooling2d_1 ( (None, 16) 0 _________________________________________________________________ dense_1 (Dense) (None, 133) 2261 ================================================================= Total params: 3,322 Trainable params: 3,290 Non-trainable params: 32 _________________________________________________________________ 123## 编译模型model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) 【练习】训练模型 问题 5:在下方代码单元训练模型。使用模型检查点（model checkpointing）来储存具有最低验证集 loss 的模型。 可选题：你也可以对训练集进行 数据增强，来优化模型的表现。 1234567891011121314from keras.callbacks import ModelCheckpoint ### TODO: 设置训练模型的epochs的数量epochs = 35### 不要修改下方代码checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', verbose=1, save_best_only=True)model.fit(train_tensors, train_targets, validation_data=(valid_tensors, valid_targets), epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1) Train on 6680 samples, validate on 835 samples Epoch 1/35 6680/6680 [==============================] - 77s 12ms/step - loss: 4.8886 - acc: 0.0073 - val_loss: 4.8812 - val_acc: 0.0108 Epoch 00001: val_loss improved from inf to 4.88124, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 2/35 6680/6680 [==============================] - 67s 10ms/step - loss: 4.8673 - acc: 0.0099 - val_loss: 4.8750 - val_acc: 0.0108 - ETA: 1:06 - loss: 4.8847 - acc: 0.0000e+00 Epoch 00002: val_loss improved from 4.88124 to 4.87504, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 3/35 6680/6680 [==============================] - 67s 10ms/step - loss: 4.8608 - acc: 0.0121 - val_loss: 4.8750 - val_acc: 0.0132 - ETA: 51s - loss: 4.8555 - acc: 0.0149 Epoch 00003: val_loss did not improve Epoch 4/35 6680/6680 [==============================] - 67s 10ms/step - loss: 4.8569 - acc: 0.0123 - val_loss: 4.8735 - val_acc: 0.0156 Epoch 00004: val_loss improved from 4.87504 to 4.87351, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 5/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.8508 - acc: 0.0144 - val_loss: 4.8756 - val_acc: 0.0120 Epoch 00005: val_loss did not improve Epoch 6/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.8452 - acc: 0.0138 - val_loss: 4.8623 - val_acc: 0.0156 Epoch 00006: val_loss improved from 4.87351 to 4.86231, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 7/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.8388 - acc: 0.0163 - val_loss: 4.8579 - val_acc: 0.0144 Epoch 00007: val_loss improved from 4.86231 to 4.85791, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 8/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.8278 - acc: 0.0196 - val_loss: 4.8477 - val_acc: 0.0240 - ETA: 55s - loss: 4.8244 - acc: 0.0224 - ETA: 41s - loss: 4.8203 - acc: 0.0230 Epoch 00008: val_loss improved from 4.85791 to 4.84765, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 9/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.8167 - acc: 0.0220 - val_loss: 4.8453 - val_acc: 0.0204 - ETA: 31s - loss: 4.8156 - acc: 0.0231 Epoch 00009: val_loss improved from 4.84765 to 4.84525, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 10/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.8049 - acc: 0.0228 - val_loss: 4.8268 - val_acc: 0.0275 - ETA: 1:02 - loss: 4.7917 - acc: 0.0214 - ETA: 49s - loss: 4.8110 - acc: 0.0207 - ETA: 5s - loss: 4.8058 - acc: 0.0220 Epoch 00010: val_loss improved from 4.84525 to 4.82678, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 11/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.7932 - acc: 0.0216 - val_loss: 4.8224 - val_acc: 0.0287 - ETA: 2s - loss: 4.7937 - acc: 0.0218 Epoch 00011: val_loss improved from 4.82678 to 4.82244, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 12/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.7788 - acc: 0.0234 - val_loss: 4.8113 - val_acc: 0.0216 Epoch 00012: val_loss improved from 4.82244 to 4.81131, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 13/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.7625 - acc: 0.0238 - val_loss: 4.8150 - val_acc: 0.0263 - ETA: 37s - loss: 4.7487 - acc: 0.0236 Epoch 00013: val_loss did not improve Epoch 14/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.7494 - acc: 0.0256 - val_loss: 4.8236 - val_acc: 0.0204 - ETA: 30s - loss: 4.7515 - acc: 0.0266 Epoch 00014: val_loss did not improve Epoch 15/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.7343 - acc: 0.0244 - val_loss: 4.8024 - val_acc: 0.0180 - ETA: 36s - loss: 4.7482 - acc: 0.0243 - ETA: 32s - loss: 4.7423 - acc: 0.0234 - ETA: 15s - loss: 4.7333 - acc: 0.0235 Epoch 00015: val_loss improved from 4.81131 to 4.80236, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 16/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.7182 - acc: 0.0295 - val_loss: 4.7744 - val_acc: 0.0263 - ETA: 12s - loss: 4.7098 - acc: 0.0305 Epoch 00016: val_loss improved from 4.80236 to 4.77445, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 17/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.7020 - acc: 0.0299 - val_loss: 4.7623 - val_acc: 0.0311 - ETA: 1:00 - loss: 4.6920 - acc: 0.0222 - ETA: 35s - loss: 4.7126 - acc: 0.0307 - ETA: 30s - loss: 4.7073 - acc: 0.0305 Epoch 00017: val_loss improved from 4.77445 to 4.76233, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 18/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.6870 - acc: 0.0331 - val_loss: 4.7441 - val_acc: 0.0323 - ETA: 59s - loss: 4.6866 - acc: 0.0357 - ETA: 55s - loss: 4.6717 - acc: 0.0327 - ETA: 42s - loss: 4.6893 - acc: 0.0343 - ETA: 17s - loss: 4.6844 - acc: 0.0329 Epoch 00018: val_loss improved from 4.76233 to 4.74409, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 19/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.6697 - acc: 0.0332 - val_loss: 4.7471 - val_acc: 0.0299 - ETA: 57s - loss: 4.6509 - acc: 0.0463 - ETA: 5s - loss: 4.6693 - acc: 0.0331 Epoch 00019: val_loss did not improve Epoch 20/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.6548 - acc: 0.0349 - val_loss: 4.7546 - val_acc: 0.0299 - ETA: 9s - loss: 4.6577 - acc: 0.0352 - ETA: 0s - loss: 4.6559 - acc: 0.0350 Epoch 00020: val_loss did not improve Epoch 21/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.6410 - acc: 0.0365 - val_loss: 4.7458 - val_acc: 0.0323 Epoch 00021: val_loss did not improve Epoch 22/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.6292 - acc: 0.0380 - val_loss: 4.7081 - val_acc: 0.0371 - ETA: 29s - loss: 4.6224 - acc: 0.0403 Epoch 00022: val_loss improved from 4.74409 to 4.70811, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 23/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.6140 - acc: 0.0362 - val_loss: 4.7357 - val_acc: 0.0180 - ETA: 32s - loss: 4.6099 - acc: 0.0361 Epoch 00023: val_loss did not improve Epoch 24/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.6050 - acc: 0.0365 - val_loss: 4.6911 - val_acc: 0.0347 Epoch 00024: val_loss improved from 4.70811 to 4.69114, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 25/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.5925 - acc: 0.0379 - val_loss: 4.6984 - val_acc: 0.0287 Epoch 00025: val_loss did not improve Epoch 26/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.5854 - acc: 0.0391 - val_loss: 4.6992 - val_acc: 0.0323 - ETA: 1:03 - loss: 4.6114 - acc: 0.0250 Epoch 00026: val_loss did not improve Epoch 27/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.5708 - acc: 0.0397 - val_loss: 4.7011 - val_acc: 0.0311 - ETA: 55s - loss: 4.5531 - acc: 0.0390 - ETA: 37s - loss: 4.5666 - acc: 0.0388 - ETA: 25s - loss: 4.5774 - acc: 0.0397 Epoch 00027: val_loss did not improve Epoch 28/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.5707 - acc: 0.0400 - val_loss: 4.6913 - val_acc: 0.0287 - ETA: 22s - loss: 4.5671 - acc: 0.0396 - ETA: 10s - loss: 4.5716 - acc: 0.0389 - ETA: 4s - loss: 4.5709 - acc: 0.0400 Epoch 00028: val_loss did not improve Epoch 29/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.5552 - acc: 0.0428 - val_loss: 4.6915 - val_acc: 0.0311 Epoch 00029: val_loss did not improve Epoch 30/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.5480 - acc: 0.0394 - val_loss: 4.6864 - val_acc: 0.0359 - ETA: 45s - loss: 4.5450 - acc: 0.0424 Epoch 00030: val_loss improved from 4.69114 to 4.68639, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 31/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.5408 - acc: 0.0431 - val_loss: 4.6938 - val_acc: 0.0311 - ETA: 30s - loss: 4.5420 - acc: 0.0447 - ETA: 1s - loss: 4.5420 - acc: 0.0434 Epoch 00031: val_loss did not improve Epoch 32/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.5377 - acc: 0.0424 - val_loss: 4.6689 - val_acc: 0.0311 Epoch 00032: val_loss improved from 4.68639 to 4.66890, saving model to saved_models/weights.best.from_scratch.hdf5 Epoch 33/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.5289 - acc: 0.0434 - val_loss: 4.6723 - val_acc: 0.0323 - ETA: 46s - loss: 4.5195 - acc: 0.0368 Epoch 00033: val_loss did not improve Epoch 34/35 6680/6680 [==============================] - 68s 10ms/step - loss: 4.5216 - acc: 0.0436 - val_loss: 4.7055 - val_acc: 0.0263 - ETA: 55s - loss: 4.5110 - acc: 0.0490 - ETA: 26s - loss: 4.5125 - acc: 0.0468 Epoch 00034: val_loss did not improve Epoch 35/35 6680/6680 [==============================] - 69s 10ms/step - loss: 4.5189 - acc: 0.0443 - val_loss: 4.6770 - val_acc: 0.0287 Epoch 00035: val_loss did not improve &lt;keras.callbacks.History at 0x7f9b4cfe3da0&gt; 123## 加载具有最好验证loss的模型model.load_weights('saved_models/weights.best.from_scratch.hdf5') 测试模型在狗图像的测试数据集上试用你的模型。确保测试准确率大于1%。 123456# 获取测试数据集中每一个图像所预测的狗品种的indexdog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]# 报告测试准确率test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)print('Test accuracy: %.4f%%' % test_accuracy) Test accuracy: 3.1100% 步骤 4: 使用一个CNN来区分狗的品种使用 迁移学习（Transfer Learning）的方法，能帮助我们在不损失准确率的情况下大大减少训练时间。在以下步骤中，你可以尝试使用迁移学习来训练你自己的CNN。 得到从图像中提取的特征向量（Bottleneck Features）1234bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')train_VGG16 = bottleneck_features['train']valid_VGG16 = bottleneck_features['valid']test_VGG16 = bottleneck_features['test'] 模型架构该模型使用预训练的 VGG-16 模型作为固定的图像特征提取器，其中 VGG-16 最后一层卷积层的输出被直接输入到我们的模型。我们只需要添加一个全局平均池化层以及一个全连接层，其中全连接层使用 softmax 激活函数，对每一个狗的种类都包含一个节点。 12345VGG16_model = Sequential()VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))VGG16_model.add(Dense(133, activation='softmax'))VGG16_model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= global_average_pooling2d_2 ( (None, 512) 0 _________________________________________________________________ dense_2 (Dense) (None, 133) 68229 ================================================================= Total params: 68,229 Trainable params: 68,229 Non-trainable params: 0 _________________________________________________________________ 123## 编译模型VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) 12345678## 训练模型checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', verbose=1, save_best_only=True)VGG16_model.fit(train_VGG16, train_targets, validation_data=(valid_VGG16, valid_targets), epochs=20, batch_size=20, callbacks=[checkpointer], verbose=10) Train on 6680 samples, validate on 835 samples Epoch 1/20 Epoch 00001: val_loss improved from inf to 10.56545, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 2/20 Epoch 00002: val_loss improved from 10.56545 to 9.88948, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 3/20 Epoch 00003: val_loss improved from 9.88948 to 9.41489, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 4/20 Epoch 00004: val_loss improved from 9.41489 to 9.28659, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 5/20 Epoch 00005: val_loss improved from 9.28659 to 9.20906, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 6/20 Epoch 00006: val_loss improved from 9.20906 to 9.11723, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 7/20 Epoch 00007: val_loss improved from 9.11723 to 9.03948, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 8/20 Epoch 00008: val_loss improved from 9.03948 to 8.85330, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 9/20 Epoch 00009: val_loss did not improve Epoch 10/20 Epoch 00010: val_loss improved from 8.85330 to 8.65410, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 11/20 Epoch 00011: val_loss improved from 8.65410 to 8.40498, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 12/20 Epoch 00012: val_loss improved from 8.40498 to 8.39156, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 13/20 Epoch 00013: val_loss improved from 8.39156 to 8.36496, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 14/20 Epoch 00014: val_loss improved from 8.36496 to 8.30643, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 15/20 Epoch 00015: val_loss improved from 8.30643 to 8.16639, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 16/20 Epoch 00016: val_loss improved from 8.16639 to 8.06120, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 17/20 Epoch 00017: val_loss improved from 8.06120 to 7.94562, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 18/20 Epoch 00018: val_loss improved from 7.94562 to 7.90346, saving model to saved_models/weights.best.VGG16.hdf5 Epoch 19/20 Epoch 00019: val_loss did not improve Epoch 20/20 Epoch 00020: val_loss improved from 7.90346 to 7.84868, saving model to saved_models/weights.best.VGG16.hdf5 &lt;keras.callbacks.History at 0x7f9b4c84af28&gt; 123## 加载具有最好验证loss的模型VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5') 测试模型现在，我们可以测试此CNN在狗图像测试数据集中识别品种的效果如何。我们在下方打印出测试准确率。 123456# 获取测试数据集中每一个图像所预测的狗品种的indexVGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]# 报告测试准确率test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)print('Test accuracy: %.4f%%' % test_accuracy) Test accuracy: 42.8230% 使用模型预测狗的品种123456789from extract_bottleneck_features import *def VGG16_predict_breed(img_path): # 提取bottleneck特征 bottleneck_feature = extract_VGG16(path_to_tensor(img_path)) # 获取预测向量 predicted_vector = VGG16_model.predict(bottleneck_feature) # 返回此模型预测的狗的品种 return dog_names[np.argmax(predicted_vector)] 步骤 5: 建立一个CNN来分类狗的品种（使用迁移学习）现在你将使用迁移学习来建立一个CNN，从而可以从图像中识别狗的品种。你的 CNN 在测试集上的准确率必须至少达到60%。 在步骤4中，我们使用了迁移学习来创建一个使用基于 VGG-16 提取的特征向量来搭建一个 CNN。在本部分内容中，你必须使用另一个预训练模型来搭建一个 CNN。为了让这个任务更易实现，我们已经预先对目前 keras 中可用的几种网络进行了预训练： VGG-19 bottleneck features ResNet-50 bottleneck features Inception bottleneck features Xception bottleneck features 这些文件被命名为为： Dog{network}Data.npz 其中 {network} 可以是 VGG19、Resnet50、InceptionV3 或 Xception 中的一个。选择上方网络架构中的一个，下载相对应的bottleneck特征，并将所下载的文件保存在目录 bottleneck_features/ 中。 【练习】获取模型的特征向量在下方代码块中，通过运行下方代码提取训练、测试与验证集相对应的bottleneck特征。 bottleneck_features = np.load(&#39;bottleneck_features/Dog{network}Data.npz&#39;) train_{network} = bottleneck_features[&#39;train&#39;] valid_{network} = bottleneck_features[&#39;valid&#39;] test_{network} = bottleneck_features[&#39;test&#39;] 12345### TODO: 从另一个预训练的CNN获取bottleneck特征bottleneck_features = np.load('bottleneck_features/DogResnet50Data.npz')train_Resnet = bottleneck_features['train']valid_Resnet = bottleneck_features['valid']test_Resnet = bottleneck_features['test'] 【练习】模型架构建立一个CNN来分类狗品种。在你的代码单元块的最后，通过运行如下代码输出网络的结构： &lt;your model&#39;s name&gt;.summary() 问题 6:在下方的代码块中尝试使用 Keras 搭建最终的网络架构，并回答你实现最终 CNN 架构的步骤与每一步的作用，并描述你在迁移学习过程中，使用该网络架构的原因。 回答:Resnet50网络相较于其他net 速度更快 准确率更高 且实验证明 经过多种类训练过的网络比训练单一种类识别的网络效果更优 所以选择resent50网络来进行迁移学习 12345### TODO: 定义你的框架model = Sequential()model.add(GlobalAveragePooling2D(input_shape=train_Resnet.shape[1:]))model.add(Dense(133, activation='softmax'))model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= global_average_pooling2d_3 ( (None, 2048) 0 _________________________________________________________________ dense_3 (Dense) (None, 133) 272517 ================================================================= Total params: 272,517 Trainable params: 272,517 Non-trainable params: 0 _________________________________________________________________ 123### TODO: 编译模型model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) 【练习】训练模型 问题 7:在下方代码单元中训练你的模型。使用模型检查点（model checkpointing）来储存具有最低验证集 loss 的模型。 当然，你也可以对训练集进行 数据增强 以优化模型的表现，不过这不是必须的步骤。 12345678### TODO: 训练模型checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Resnet50.hdf5', verbose=10, save_best_only=True)model.fit(train_Resnet, train_targets, validation_data=(valid_Resnet, valid_targets), epochs=20, batch_size=20, callbacks=[checkpointer], verbose=10) Train on 6680 samples, validate on 835 samples Epoch 1/20 Epoch 00001: val_loss improved from inf to 0.83976, saving model to saved_models/weights.best.Resnet50.hdf5 Epoch 2/20 Epoch 00002: val_loss improved from 0.83976 to 0.73381, saving model to saved_models/weights.best.Resnet50.hdf5 Epoch 3/20 Epoch 00003: val_loss improved from 0.73381 to 0.67062, saving model to saved_models/weights.best.Resnet50.hdf5 Epoch 4/20 Epoch 00004: val_loss improved from 0.67062 to 0.64336, saving model to saved_models/weights.best.Resnet50.hdf5 Epoch 5/20 Epoch 00005: val_loss did not improve Epoch 6/20 Epoch 00006: val_loss improved from 0.64336 to 0.55801, saving model to saved_models/weights.best.Resnet50.hdf5 Epoch 7/20 Epoch 00007: val_loss did not improve Epoch 8/20 Epoch 00008: val_loss did not improve Epoch 9/20 Epoch 00009: val_loss did not improve Epoch 10/20 Epoch 00010: val_loss did not improve Epoch 11/20 Epoch 00011: val_loss did not improve Epoch 12/20 Epoch 00012: val_loss did not improve Epoch 13/20 Epoch 00013: val_loss did not improve Epoch 14/20 Epoch 00014: val_loss did not improve Epoch 15/20 Epoch 00015: val_loss did not improve Epoch 16/20 Epoch 00016: val_loss did not improve Epoch 17/20 Epoch 00017: val_loss did not improve Epoch 18/20 Epoch 00018: val_loss did not improve Epoch 19/20 Epoch 00019: val_loss did not improve Epoch 20/20 Epoch 00020: val_loss did not improve &lt;keras.callbacks.History at 0x7f9b4c0d0c88&gt; 12### TODO: 加载具有最佳验证loss的模型权重model.load_weights('saved_models/weights.best.Resnet50.hdf5') 【练习】测试模型 问题 8:在狗图像的测试数据集上试用你的模型。确保测试准确率大于60%。 1234567891011121314151617181920212223242526### TODO: 在测试集上计算分类准确率model_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet]# 报告测试准确率test_accuracy = 100*np.sum(np.array(model_predictions)==np.argmax(test_targets, axis=1))/len(model_predictions)print('Test accuracy: %.4f%%' % test_accuracy)# Fit the modelhistory = model.fit(train_Resnet, train_targets, validation_split=0.33, epochs=150, batch_size=10, verbose=0)# list all data in historyprint(history.history.keys())# summarize history for accuracyplt.plot(history.history['acc'])plt.plot(history.history['val_acc'])plt.title('model accuracy')plt.ylabel('accuracy')plt.xlabel('epoch')plt.legend(['train', 'test'], loc='upper left')plt.show()# summarize history for lossplt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.title('model loss')plt.ylabel('loss')plt.xlabel('epoch')plt.legend(['train', 'test'], loc='upper left')plt.show() Test accuracy: 82.7751% dict_keys([&#39;val_loss&#39;, &#39;val_acc&#39;, &#39;loss&#39;, &#39;acc&#39;]) 【练习】使用模型测试狗的品种实现一个函数，它的输入为图像路径，功能为预测对应图像的类别，输出为你模型预测出的狗类别（Affenpinscher, Afghan_hound 等）。 与步骤5中的模拟函数类似，你的函数应当包含如下三个步骤： 根据选定的模型载入图像特征（bottleneck features） 将图像特征输输入到你的模型中，并返回预测向量。注意，在该向量上使用 argmax 函数可以返回狗种类的序号。 使用在步骤0中定义的 dog_names 数组来返回对应的狗种类名称。 提取图像特征过程中使用到的函数可以在 extract_bottleneck_features.py 中找到。同时，他们应已在之前的代码块中被导入。根据你选定的 CNN 网络，你可以使用 extract_{network} 函数来获得对应的图像特征，其中 {network} 代表 VGG19, Resnet50, InceptionV3, 或 Xception 中的一个。 问题 9:1234567### TODO: 写一个函数，该函数将图像的路径作为输入### 然后返回此模型所预测的狗的品种from extract_bottleneck_features import *def Resnet50_predict_breed(img_path): bottleneck_features = extract_Resnet50(path_to_tensor(img_path)) predicted_vector = model.predict(bottleneck_features) return dog_names[np.argmax(predicted_vector)] 步骤 6: 完成你的算法实现一个算法，它的输入为图像的路径，它能够区分图像是否包含一个人、狗或两者都不包含，然后： 如果从图像中检测到一只狗，返回被预测的品种。 如果从图像中检测到人，返回最相像的狗品种。 如果两者都不能在图像中检测到，输出错误提示。 我们非常欢迎你来自己编写检测图像中人类与狗的函数，你可以随意地使用上方完成的 face_detector 和 dog_detector 函数。你需要在步骤5使用你的CNN来预测狗品种。 下面提供了算法的示例输出，但你可以自由地设计自己的模型！ 问题 10:在下方代码块中完成你的代码。 123456789101112131415### TODO: 设计你的算法### 自由地使用所需的代码单元数吧def Predict(path): image = cv2.imread(path) cv = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) plt.imshow(cv) plt.show() if face_detector(path) &gt; 0: print('you look like a ...' + Resnet50_predict_breed(path)) elif dog_detector(path) == True: print('you are a &#123;&#125; dog'.format(Resnet50_predict_breed(path))) else: print(\"sorry\") 步骤 7: 测试你的算法在这个部分中，你将尝试一下你的新算法！算法认为你看起来像什么类型的狗？如果你有一只狗，它可以准确地预测你的狗的品种吗？如果你有一只猫，它会将你的猫误判为一只狗吗？ 问题 11:在下方编写代码，用至少6张现实中的图片来测试你的算法。你可以使用任意照片，不过请至少使用两张人类图片（要征得当事人同意哦）和两张狗的图片。同时请回答如下问题： 输出结果比你预想的要好吗 :) ？或者更糟 :( ？：输出结果比我预想的要好 提出至少三点改进你的模型的想法。添加dropout防止过拟合 减少或增加隐藏层层数 压缩input时图像的像素 123456## TODO: 在你的电脑上，在步骤6中，至少在6张图片上运行你的算法。## 自由地使用所需的代码单元数吧import osjpg = os.listdir('images')for i in jpg[:6]: Predict('images/'+i) you are a train/130.Welsh_springer_spaniel dog you are a train/009.American_water_spaniel dog you look like a ...train/037.Brittany you are a train/055.Curly-coated_retriever dog you are a train/096.Labrador_retriever dog sorry 注意: 当你写完了所有的代码，并且回答了所有的问题。你就可以把你的 iPython Notebook 导出成 HTML 文件。你可以在菜单栏，这样导出File -&gt; Download as -&gt; HTML (.html)把这个 HTML 和这个 iPython notebook 一起做为你的作业提交。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}]},{"title":"PredictYourCuisine","slug":"PredictYourCuisine","date":"2019-02-15T13:06:53.000Z","updated":"2019-02-16T11:18:20.791Z","comments":false,"path":"2019/02/15/PredictYourCuisine/","link":"","permalink":"http://yoursite.com/2019/02/15/PredictYourCuisine/","excerpt":"","text":"机器学习工程师纳米学位项目 0: 预测你的下一道世界料理欢迎来到机器学习的预测烹饪菜系项目！在此文件中，有些示例代码已经提供给你，但你还需要实现更多的功能来让项目成功运行。除非有明确要求，你无须修改任何已给出的代码。以编程练习开始的标题表示接下来的内容中有需要你必须实现的功能。每一部分都会有详细的指导，需要实现的部分也会在注释中以TODO标出。请仔细阅读所有的提示！ 实验任务：给定佐料名称，预测菜品所属的菜系。 实验步骤：菜品数据载入；佐料名称预处理，并预览数据集结构；载入逻辑回归模型，并训练；结果测试并提交，查看实验分数。 提示：Code 和 Markdown 区域可通过 Shift + Enter 快捷键运行。此外，Markdown可以通过双击进入编辑模式。 第一步. 下载并导入数据在这个项目中，你将利用Yummly所提供的数据集来训练和测试一个模型，并对模型的性能和预测能力进行测试。通过该数据训练后的好的模型可以被用来对菜系进行预测。 此项目的数据集来自Kaggle What’s Cooking 竞赛。共 39774/9944 个训练和测试数据点，涵盖了中国菜、越南菜、法国菜等的信息。数据集包含以下特征： ‘id’：24717, 数据编号 ‘cuisine’：”indian”, 菜系 ‘ingredients’：[“tumeric”, “vegetable stock”, …] 此菜所包含的佐料 首先你需要前往此 菜系数据集 下载(选择 Download All )。如果不能正常下载，请参考教室中的下载教程。然后运行下面区域的代码以载入数据集，以及一些此项目所需的 Python 库。如果成功返回数据集的大小，表示数据集已载入成功。 1.1 配置环境首先按照本目录中README.md文件中的第一部分内容，配置实验开发环境和所需库函数。 1.2 加载数据其次，在下载完实验数据集后，我们将其解压至当前目录中(即：MLND-cn-trial\\目录下面)， 然后依次输入以下代码，加载本次实验的训练集和测试集。 12345678910111213141516171819202122## 请不要修改下方代码# 导入依赖库import jsonimport codecsimport pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline# 加载数据集train_filename='all/train.json'train_content = pd.read_json(codecs.open(train_filename, mode='r', encoding='utf-8'))test_filename = 'all/test.json'test_content = pd.read_json(codecs.open(test_filename, mode='r', encoding='utf-8')) # 打印加载的数据集数量print(\"菜名数据集一共包含 &#123;&#125; 训练数据 和 &#123;&#125; 测试样例。\\n\".format(len(train_content), len(test_content)))if len(train_content)==39774 and len(test_content)==9944: print(\"数据成功载入！\")else: print(\"数据载入有问题，请检查文件路径！\") 菜名数据集一共包含 39774 训练数据 和 9944 测试样例。 数据成功载入！ 1.3 数据预览为了查看我们的数据集的分布和菜品总共的种类，我们打印出部分数据样例。 12## 请不要修改下方代码pd.set_option('display.max_colwidth',120) 编程练习你需要通过head()函数来预览训练集train_content数据。（输出前5条） 12### TODO：打印train_content中前5个数据样例以预览数据train_content['cuisine'] 0 greek 1 southern_us 2 filipino 3 indian 4 indian 5 jamaican 6 spanish 7 italian 8 mexican 9 italian 10 italian 11 chinese 12 italian 13 mexican 14 italian 15 indian 16 british 17 italian 18 thai 19 vietnamese 20 thai 21 mexican 22 southern_us 23 chinese 24 italian 25 chinese 26 cajun_creole 27 italian 28 chinese 29 mexican ... 39744 greek 39745 spanish 39746 indian 39747 moroccan 39748 italian 39749 mexican 39750 mexican 39751 moroccan 39752 southern_us 39753 italian 39754 vietnamese 39755 indian 39756 mexican 39757 greek 39758 greek 39759 korean 39760 southern_us 39761 chinese 39762 indian 39763 italian 39764 mexican 39765 indian 39766 irish 39767 italian 39768 mexican 39769 irish 39770 italian 39771 irish 39772 chinese 39773 mexican Name: cuisine, Length: 39774, dtype: object 1234## 请不要修改下方代码## 查看总共菜品分类categories=np.unique(train_content['cuisine'])print(\"一共包含 &#123;&#125; 种菜品，分别是:\\n&#123;&#125;\".format(len(categories),categories)) 一共包含 20 种菜品，分别是: [&#39;brazilian&#39; &#39;british&#39; &#39;cajun_creole&#39; &#39;chinese&#39; &#39;filipino&#39; &#39;french&#39; &#39;greek&#39; &#39;indian&#39; &#39;irish&#39; &#39;italian&#39; &#39;jamaican&#39; &#39;japanese&#39; &#39;korean&#39; &#39;mexican&#39; &#39;moroccan&#39; &#39;russian&#39; &#39;southern_us&#39; &#39;spanish&#39; &#39;thai&#39; &#39;vietnamese&#39;] 第二步. 分析数据在项目的第二个部分，你会对菜肴数据进行初步的观察并给出你的分析。通过对数据的探索来熟悉数据可以让你更好地理解和解释你的结果。 由于这个项目的最终目标是建立一个预测世界菜系的模型，我们需要将数据集分为特征(Features)和目标变量(Target Variables)。 特征: &#39;ingredients&#39;，给我们提供了每个菜品所包含的佐料名称。 目标变量：&#39;cuisine&#39;，是我们希望预测的菜系分类。 他们分别被存在 train_ingredients 和 train_targets 两个变量名中。 编程练习：数据提取 将train_content中的ingredients赋值到train_integredients 将train_content中的cuisine赋值到train_targets 1234567### TODO：将特征与目标变量分别赋值train_ingredients = train_content['ingredients']train_targets = train_content['cuisine']### TODO: 打印结果，检查是否正确赋值display(train_ingredients)display(train_targets) 0 [romaine lettuce, black olives, grape tomatoes, garlic, pepper, purple onion, seasoning, garbanzo beans, feta cheese... 1 [plain flour, ground pepper, salt, tomatoes, ground black pepper, thyme, eggs, green tomatoes, yellow corn meal, mil... 2 [eggs, pepper, salt, mayonaise, cooking oil, green chilies, grilled chicken breasts, garlic powder, yellow onion, so... 3 [water, vegetable oil, wheat, salt] 4 [black pepper, shallots, cornflour, cayenne pepper, onions, garlic paste, milk, butter, salt, lemon juice, water, ch... 5 [plain flour, sugar, butter, eggs, fresh ginger root, salt, ground cinnamon, milk, vanilla extract, ground ginger, p... 6 [olive oil, salt, medium shrimp, pepper, garlic, chopped cilantro, jalapeno chilies, flat leaf parsley, skirt steak,... 7 [sugar, pistachio nuts, white almond bark, flour, vanilla extract, olive oil, almond extract, eggs, baking powder, d... 8 [olive oil, purple onion, fresh pineapple, pork, poblano peppers, corn tortillas, cheddar cheese, ground black peppe... 9 [chopped tomatoes, fresh basil, garlic, extra-virgin olive oil, kosher salt, flat leaf parsley] 10 [pimentos, sweet pepper, dried oregano, olive oil, garlic, sharp cheddar cheese, pepper, swiss cheese, provolone che... 11 [low sodium soy sauce, fresh ginger, dry mustard, green beans, white pepper, sesame oil, scallions, canola oil, suga... 12 [Italian parsley leaves, walnuts, hot red pepper flakes, extra-virgin olive oil, fresh lemon juice, trout fillet, ga... 13 [ground cinnamon, fresh cilantro, chili powder, ground coriander, kosher salt, ground black pepper, garlic, plum tom... 14 [fresh parmesan cheese, butter, all-purpose flour, fat free less sodium chicken broth, chopped fresh chives, gruyere... 15 [tumeric, vegetable stock, tomatoes, garam masala, naan, red lentils, red chili peppers, onions, spinach, sweet pota... 16 [greek yogurt, lemon curd, confectioners sugar, raspberries] 17 [italian seasoning, broiler-fryer chicken, mayonaise, zesty italian dressing] 18 [sugar, hot chili, asian fish sauce, lime juice] 19 [soy sauce, vegetable oil, red bell pepper, chicken broth, yellow squash, garlic chili sauce, sliced green onions, b... 20 [pork loin, roasted peanuts, chopped cilantro fresh, hoisin sauce, creamy peanut butter, chopped fresh mint, thai ba... 21 [roma tomatoes, kosher salt, purple onion, jalapeno chilies, lime, chopped cilantro] 22 [low-fat mayonnaise, pepper, salt, baking potatoes, eggs, spicy brown mustard] 23 [sesame seeds, red pepper, yellow peppers, water, extra firm tofu, broccoli, soy sauce, orange bell pepper, arrowroo... 24 [marinara sauce, flat leaf parsley, olive oil, linguine, capers, crushed red pepper flakes, olives, lemon zest, garlic] 25 [sugar, lo mein noodles, salt, chicken broth, light soy sauce, flank steak, beansprouts, dried black mushrooms, pepp... 26 [herbs, lemon juice, fresh tomatoes, paprika, mango, stock, chile pepper, onions, red chili peppers, oil] 27 [ground black pepper, butter, sliced mushrooms, sherry, salt, grated parmesan cheese, heavy cream, spaghetti, chicke... 28 [green bell pepper, egg roll wrappers, sweet and sour sauce, corn starch, molasses, vegetable oil, oil, soy sauce, s... 29 [flour tortillas, cheese, breakfast sausages, large eggs] ... 39744 [extra-virgin olive oil, oregano, potatoes, garlic cloves, pepper, salt, yellow mustard, fresh lemon juice] 39745 [quinoa, extra-virgin olive oil, fresh thyme leaves, scallion greens] 39746 [clove, bay leaves, ginger, chopped cilantro, ground turmeric, white onion, cinnamon, cardamom pods, serrano chile, ... 39747 [water, sugar, grated lemon zest, butter, pitted date, blanched almonds] 39748 [sea salt, pizza doughs, all-purpose flour, cornmeal, extra-virgin olive oil, shredded mozzarella cheese, kosher sal... 39749 [kosher salt, minced onion, tortilla chips, sugar, tomato juice, cilantro leaves, avocado, lime juice, roma tomatoes... 39750 [ground black pepper, chicken breasts, salsa, cheddar cheese, pepper jack, heavy cream, red enchilada sauce, unsalte... 39751 [olive oil, cayenne pepper, chopped cilantro fresh, boneless chicken skinless thigh, fine sea salt, low salt chicken... 39752 [self rising flour, milk, white sugar, butter, peaches in light syrup] 39753 [rosemary sprigs, lemon zest, garlic cloves, ground black pepper, vegetable broth, fresh basil leaves, minced garlic... 39754 [jasmine rice, bay leaves, sticky rice, rotisserie chicken, chopped cilantro, large eggs, vegetable oil, yellow onio... 39755 [mint leaves, cilantro leaves, ghee, tomatoes, cinnamon, oil, basmati rice, garlic paste, salt, coconut milk, clove,... 39756 [vegetable oil, cinnamon sticks, water, all-purpose flour, piloncillo, salt, orange zest, baking powder, hot water] 39757 [red bell pepper, garlic cloves, extra-virgin olive oil, feta cheese crumbles] 39758 [milk, salt, ground cayenne pepper, ground lamb, ground cinnamon, ground black pepper, pomegranate, chopped fresh mi... 39759 [red chili peppers, sea salt, onions, water, chilli bean sauce, caster sugar, garlic, white vinegar, chili oil, cucu... 39760 [butter, large eggs, cornmeal, baking powder, boiling water, milk, salt] 39761 [honey, chicken breast halves, cilantro leaves, carrots, soy sauce, Sriracha, wonton wrappers, freshly ground pepper... 39762 [curry powder, salt, chicken, water, vegetable oil, basmati rice, eggs, finely chopped onion, lemon juice, pepper, m... 39763 [fettuccine pasta, low-fat cream cheese, garlic, nonfat evaporated milk, grated parmesan cheese, corn starch, nonfat... 39764 [chili powder, worcestershire sauce, celery, red kidney beans, lean ground beef, stewed tomatoes, dried parsley, pep... 39765 [coconut, unsweetened coconut milk, mint leaves, plain yogurt] 39766 [rutabaga, ham, thick-cut bacon, potatoes, fresh parsley, salt, onions, pepper, carrots, pork sausages] 39767 [low-fat sour cream, grated parmesan cheese, salt, dried oregano, low-fat cottage cheese, butter, onions, olive oil,... 39768 [shredded cheddar cheese, crushed cheese crackers, cheddar cheese soup, cream of chicken soup, hot sauce, diced gree... 39769 [light brown sugar, granulated sugar, butter, warm water, large eggs, all-purpose flour, whole wheat flour, cooking ... 39770 [KRAFT Zesty Italian Dressing, purple onion, broccoli florets, rotini, pitted black olives, Kraft Grated Parmesan Ch... 39771 [eggs, citrus fruit, raisins, sourdough starter, flour, hot tea, sugar, ground nutmeg, salt, ground cinnamon, milk, ... 39772 [boneless chicken skinless thigh, minced garlic, steamed white rice, baking powder, corn starch, dark soy sauce, kos... 39773 [green chile, jalapeno chilies, onions, ground black pepper, salt, chopped cilantro fresh, green bell pepper, garlic... Name: ingredients, Length: 39774, dtype: object 0 greek 1 southern_us 2 filipino 3 indian 4 indian 5 jamaican 6 spanish 7 italian 8 mexican 9 italian 10 italian 11 chinese 12 italian 13 mexican 14 italian 15 indian 16 british 17 italian 18 thai 19 vietnamese 20 thai 21 mexican 22 southern_us 23 chinese 24 italian 25 chinese 26 cajun_creole 27 italian 28 chinese 29 mexican ... 39744 greek 39745 spanish 39746 indian 39747 moroccan 39748 italian 39749 mexican 39750 mexican 39751 moroccan 39752 southern_us 39753 italian 39754 vietnamese 39755 indian 39756 mexican 39757 greek 39758 greek 39759 korean 39760 southern_us 39761 chinese 39762 indian 39763 italian 39764 mexican 39765 indian 39766 irish 39767 italian 39768 mexican 39769 irish 39770 italian 39771 irish 39772 chinese 39773 mexican Name: cuisine, Length: 39774, dtype: object 编程练习：基础统计运算你的第一个编程练习是计算有关菜系佐料的统计数据。我们已为你导入了 numpy，你需要使用这个库来执行必要的计算。这些统计数据对于分析模型的预测结果非常重要的。在下面的代码中，你要做的是： 使用最频繁的佐料前10分别有哪些？ 意大利菜中最常见的10个佐料有哪些？ 123456789## TODO: 统计佐料出现次数，并赋值到sum_ingredients字典中sum_ingredients = &#123;&#125;for i in train_content['ingredients']: for a in i: if a not in sum_ingredients: sum_ingredients[a] = 1 elif a in sum_ingredients: sum_ingredients[a] += 1sum_ingredients {&#39;sugar&#39;: 760, &#39;pistachio nuts&#39;: 7, &#39;white almond bark&#39;: 1, &#39;flour&#39;: 142, &#39;vanilla extract&#39;: 219, &#39;olive oil&#39;: 3111, &#39;almond extract&#39;: 56, &#39;eggs&#39;: 627, &#39;baking powder&#39;: 186, &#39;dried cranberries&#39;: 8, &#39;chopped tomatoes&#39;: 37, &#39;fresh basil&#39;: 787, &#39;garlic&#39;: 1471, &#39;extra-virgin olive oil&#39;: 1362, &#39;kosher salt&#39;: 656, &#39;flat leaf parsley&#39;: 588, &#39;pimentos&#39;: 16, &#39;sweet pepper&#39;: 7, &#39;dried oregano&#39;: 626, &#39;sharp cheddar cheese&#39;: 9, &#39;pepper&#39;: 965, &#39;swiss cheese&#39;: 7, &#39;provolone cheese&#39;: 138, &#39;canola oil&#39;: 41, &#39;mushrooms&#39;: 184, &#39;black olives&#39;: 67, &#39;sausages&#39;: 58, &#39;Italian parsley leaves&#39;: 74, &#39;walnuts&#39;: 38, &#39;hot red pepper flakes&#39;: 76, &#39;fresh lemon juice&#39;: 471, &#39;trout fillet&#39;: 3, &#39;garlic cloves&#39;: 1619, &#39;chipotle chile&#39;: 2, &#39;fine sea salt&#39;: 77, &#39;fresh parmesan cheese&#39;: 251, &#39;butter&#39;: 1030, &#39;all-purpose flour&#39;: 918, &#39;fat free less sodium chicken broth&#39;: 158, &#39;chopped fresh chives&#39;: 68, &#39;gruyere cheese&#39;: 18, &#39;ground black pepper&#39;: 1444, &#39;bacon slices&#39;: 48, &#39;gnocchi&#39;: 41, &#39;fat free milk&#39;: 42, &#39;cooking spray&#39;: 491, &#39;salt&#39;: 3454, &#39;italian seasoning&#39;: 347, &#39;broiler-fryer chicken&#39;: 1, &#39;mayonaise&#39;: 63, &#39;zesty italian dressing&#39;: 11, &#39;marinara sauce&#39;: 222, &#39;linguine&#39;: 193, &#39;capers&#39;: 306, &#39;crushed red pepper flakes&#39;: 179, &#39;olives&#39;: 29, &#39;lemon zest&#39;: 98, &#39;sliced mushrooms&#39;: 131, &#39;sherry&#39;: 13, &#39;grated parmesan cheese&#39;: 1580, &#39;heavy cream&#39;: 300, &#39;spaghetti&#39;: 296, &#39;chicken broth&#39;: 245, &#39;cooked chicken&#39;: 33, &#39;yellow corn meal&#39;: 64, &#39;boiling water&#39;: 63, &#39;sea salt&#39;: 202, &#39;onions&#39;: 1240, &#39;crushed garlic&#39;: 20, &#39;green onions&#39;: 144, &#39;white sugar&#39;: 231, &#39;dried basil&#39;: 425, &#39;diced tomatoes&#39;: 429, &#39;bread slices&#39;: 15, &#39;great northern beans&#39;: 21, &#39;shrimp&#39;: 59, &#39;sage leaves&#39;: 89, &#39;Oscar Mayer Deli Fresh Smoked Ham&#39;: 1, &#39;hoagie rolls&#39;: 8, &#39;salami&#39;: 41, &#39;giardiniera&#39;: 5, &#39;mozzarella cheese&#39;: 396, &#39;pepperoni&#39;: 48, &#39;bay leaves&#39;: 107, &#39;crushed red pepper&#39;: 418, &#39;mussels&#39;: 38, &#39;basil&#39;: 174, &#39;black pepper&#39;: 636, &#39;dry white wine&#39;: 658, &#39;tomatoes&#39;: 601, &#39;finely chopped onion&#39;: 145, &#39;lemon&#39;: 236, &#39;pesto&#39;: 113, &#39;salmon fillets&#39;: 11, &#39;white wine&#39;: 176, &#39;pizza crust&#39;: 36, &#39;plum tomatoes&#39;: 340, &#39;part-skim mozzarella cheese&#39;: 201, &#39;crushed tomatoes&#39;: 241, &#39;fresh rosemary&#39;: 292, &#39;boneless pork loin&#39;: 9, &#39;pappardelle&#39;: 11, &#39;red pepper&#39;: 49, &#39;Italian bread&#39;: 89, &#39;balsamic vinegar&#39;: 348, &#39;sausage casings&#39;: 78, &#39;honey&#39;: 126, &#39;shredded mozzarella cheese&#39;: 413, &#39;roasted red peppers&#39;: 114, &#39;penne pasta&#39;: 146, &#39;spinach&#39;: 119, &#39;asiago&#39;: 72, &#39;whole wheat pasta&#39;: 13, &#39;sweet onion&#39;: 69, &#39;grape tomatoes&#39;: 98, &#39;chestnuts&#39;: 9, &#39;granulated sugar&#39;: 82, &#39;whole milk ricotta cheese&#39;: 47, &#39;coffee ice cream&#39;: 3, &#39;large eggs&#39;: 625, &#39;mascarpone&#39;: 124, &#39;rum&#39;: 12, &#39;powdered sugar&#39;: 69, &#39;semisweet chocolate&#39;: 46, &#39;chestnut flour&#39;: 1, &#39;starchy potatoes&#39;: 2, &#39;grated nutmeg&#39;: 64, &#39;blood orange&#39;: 5, &#39;freshly ground pepper&#39;: 316, &#39;fennel bulb&#39;: 103, &#39;low salt chicken broth&#39;: 138, &#39;dijon mustard&#39;: 99, &#39;corn starch&#39;: 83, &#39;white wine vinegar&#39;: 73, &#39;tomato sauce&#39;: 357, &#39;shredded carrots&#39;: 11, &#39;english muffins, split and toasted&#39;: 2, &#39;chopped onion&#39;: 327, &#39;vegetable oil cooking spray&#39;: 73, &#39;chopped green bell pepper&#39;: 39, &#39;cheddar cheese&#39;: 18, &#39;lasagna noodles&#39;: 196, &#39;ranch dressing&#39;: 5, &#39;evaporated milk&#39;: 13, &#39;fresh parsley&#39;: 631, &#39;fresh oregano&#39;: 209, &#39;cold water&#39;: 58, &#39;chocolate morsels&#39;: 4, &#39;cream sweeten whip&#39;: 3, &#39;instant espresso granules&#39;: 4, &#39;whipping cream&#39;: 162, &#39;kahlúa&#39;: 12, &#39;chocolate covered coffee beans&#39;: 1, &#39;unflavored gelatin&#39;: 48, &#39;pound cake&#39;: 7, &#39;pinenuts&#39;: 252, &#39;zucchini&#39;: 326, &#39;baby carrots&#39;: 15, &#39;fresh basil leaves&#39;: 352, &#39;asparagus spears&#39;: 24, &#39;white onion&#39;: 48, &#39;carrots&#39;: 379, &#39;frozen peas&#39;: 67, &#39;arborio rice&#39;: 261, &#39;yellow crookneck squash&#39;: 3, &#39;fresh leav spinach&#39;: 25, &#39;cheese tortellini&#39;: 53, &#39;cherry tomatoes&#39;: 170, &#39;navy beans&#39;: 8, &#39;pecorino romano cheese&#39;: 146, &#39;fresh fava bean&#39;: 12, &#39;italian sausage&#39;: 129, &#39;large garlic cloves&#39;: 293, &#39;pasta sheets&#39;: 9, &#39;water&#39;: 1052, &#39;Turkish bay leaves&#39;: 3, &#39;dried chickpeas&#39;: 1, &#39;celery ribs&#39;: 130, &#39;semolina&#39;: 14, &#39;warm water&#39;: 182, &#39;vine ripened tomatoes&#39;: 12, &#39;bittersweet chocolate&#39;: 42, &#39;fat free yogurt&#39;: 4, &#39;skim milk&#39;: 16, &#39;angel food cake&#39;: 4, &#39;unsweetened cocoa powder&#39;: 84, &#39;instant espresso&#39;: 7, &#39;garlic salt&#39;: 61, &#39;tomato paste&#39;: 376, &#39;veal cutlets&#39;: 18, &#39;broccoli rabe&#39;: 47, &#39;whole milk&#39;: 157, &#39;parmigiano reggiano cheese&#39;: 306, &#39;dry bread crumbs&#39;: 115, &#39;fontina&#39;: 27, &#39;unsalted butter&#39;: 564, &#39;pasta sauce&#39;: 246, &#39;olive oil flavored cooking spray&#39;: 29, &#39;frozen chopped spinach&#39;: 131, &#39;large egg whites&#39;: 138, &#39;part-skim ricotta cheese&#39;: 111, &#39;manicotti shells&#39;: 18, &#39;fettucine&#39;: 136, &#39;parmesan cheese&#39;: 474, &#39;large shrimp&#39;: 77, &#39;chicken bouillon&#39;: 10, &#39;cream of tartar&#39;: 9, &#39;orange liqueur&#39;: 15, &#39;heavy whipping cream&#39;: 73, &#39;semi-sweet chocolate morsels&#39;: 21, &#39;cake flour&#39;: 24, &#39;confectioners sugar&#39;: 94, &#39;ground cinnamon&#39;: 85, &#39;ricotta cheese&#39;: 333, &#39;cream cheese&#39;: 105, &#39;dried porcini mushrooms&#39;: 81, &#39;chopped fresh thyme&#39;: 151, &#39;beef rib short&#39;: 5, &#39;dry red wine&#39;: 131, &#39;hot water&#39;: 85, &#39;fat free less sodium beef broth&#39;: 7, &#39;cremini mushrooms&#39;: 67, &#39;pitted kalamata olives&#39;: 94, &#39;cauliflower&#39;: 34, &#39;whole peeled tomatoes&#39;: 60, &#39;swiss chard&#39;: 63, &#39;banana squash&#39;: 1, &#39;vegetable broth&#39;: 81, &#39;bay leaf&#39;: 130, &#39;cannellini beans&#39;: 161, &#39;boneless skinless chicken breast halves&#39;: 182, &#39;light sour cream&#39;: 5, &#39;fava beans&#39;: 13, &#39;finely chopped fresh parsley&#39;: 33, &#39;fresh tarragon&#39;: 33, &#39;grated lemon zest&#39;: 142, &#39;chopped almonds&#39;: 12, &#39;broccoli&#39;: 64, &#39;potatoes&#39;: 59, &#39;self rising flour&#39;: 1, &#39;eggplant&#39;: 204, &#39;chicken&#39;: 84, &#39;pitas&#39;: 5, &#39;goat cheese&#39;: 92, &#39;reduced fat milk&#39;: 29, &#39;smoked trout&#39;: 1, &#39;fresh dill&#39;: 16, &#39;asparagus&#39;: 123, &#39;oil&#39;: 92, &#39;ricotta salata&#39;: 22, &#39;pecorino cheese&#39;: 52, &#39;grana padano&#39;: 7, &#39;pasta&#39;: 255, &#39;cracked black pepper&#39;: 117, &#39;green bell pepper&#39;: 163, &#39;vidalia onion&#39;: 19, &#39;smoked gouda&#39;: 14, &#39;baby spinach leaves&#39;: 39, &#39;shredded sharp cheddar cheese&#39;: 17, &#39;small red potato&#39;: 13, &#39;Bertolli® Classico Olive Oil&#39;: 16, &#39;bacon, crisp-cooked and crumbled&#39;: 2, &#39;bertolli vineyard premium collect marinara with burgundi wine sauc&#39;: 5, &#39;bread crumb fresh&#39;: 74, &#39;( oz.) tomato sauce&#39;: 7, &#39;ground veal&#39;: 35, &#39;italian seasoning mix&#39;: 1, &#39;beef&#39;: 22, &#39;fat skimmed chicken broth&#39;: 28, &#39;solid pack pumpkin&#39;: 4, &#39;ground nutmeg&#39;: 127, &#39;white rice&#39;: 11, &#39;ground pepper&#39;: 77, &#39;fresh thyme leaves&#39;: 53, &#39;pistachios&#39;: 10, &#39;dried cherry&#39;: 6, &#39;prosciutto&#39;: 256, &#39;romano cheese&#39;: 89, &#39;parsley leaves&#39;: 23, &#39;shallots&#39;: 316, &#39;fresh raspberries&#39;: 14, &#39;dry vermouth&#39;: 18, &#39;canned low sodium chicken broth&#39;: 46, &#39;chicken livers&#39;: 14, &#39;raisins&#39;: 39, &#39;seasoned bread crumbs&#39;: 71, &#39;minced garlic&#39;: 334, &#39;half &amp; half&#39;: 89, &#39;ground beef&#39;: 237, &#39;jack cheese&#39;: 9, &#39;vegetables&#39;: 29, &#39;lemon juice&#39;: 216, &#39;ricotta&#39;: 95, &#39;yellow onion&#39;: 153, &#39;reduced sodium chicken broth&#39;: 36, &#39;chopped fresh mint&#39;: 54, &#39;purple onion&#39;: 350, &#39;low sodium chicken broth&#39;: 78, &#39;polenta&#39;: 132, &#39;bell pepper&#39;: 38, &#39;olive oil cooking spray&#39;: 13, &#39;bacon&#39;: 124, &#39;egg yolks&#39;: 82, &#39;duck breast halves&#39;: 1, &#39;fennel seeds&#39;: 114, &#39;roma tomatoes&#39;: 44, &#39;pesto sauce&#39;: 26, &#39;arugula&#39;: 125, &#39;summer squash&#39;: 9, &#39;red wine vinegar&#39;: 227, &#39;ciabatta&#39;: 21, &#39;juice&#39;: 72, &#39;fresh herbs&#39;: 14, &#39;grated lemon peel&#39;: 89, &#39;dough&#39;: 46, &#39;coarse sea salt&#39;: 17, &#39;rosemary leaves&#39;: 10, &#39;red pepper flakes&#39;: 206, &#39;brie cheese&#39;: 14, &#39;cheese ravioli&#39;: 18, &#39;Italian seasoned breadcrumbs&#39;: 23, &#39;cheese&#39;: 127, &#39;parsley&#39;: 95, &#39;fat-free cottage cheese&#39;: 11, &#39;oven-ready lasagna noodles&#39;: 43, &#39;margarine&#39;: 38, &#39;radicchio&#39;: 40, &#39;garbanzo beans&#39;: 18, &#39;orzo pasta&#39;: 16, &#39;rubbed sage&#39;: 17, &#39;dried rosemary&#39;: 96, &#39;canned beef broth&#39;: 4, &#39;kale leaves&#39;: 7, &#39;chicken noodle soup&#39;: 1, &#39;italian style rolls&#39;: 1, &#39;genoa salami&#39;: 22, &#39;oregano&#39;: 114, &#39;boiled ham&#39;: 1, &#39;capicola&#39;: 4, &#39;iceberg lettuce&#39;: 6, &#39;jalapeno chilies&#39;: 20, &#39;diced celery&#39;: 15, &#39;italian salad dressing mix&#39;: 7, &#39;chopped cilantro fresh&#39;: 25, &#39;cider vinegar&#39;: 14, &#39;red bell pepper&#39;: 354, &#39;sliced green onions&#39;: 40, &#39;barbecue sauce&#39;: 9, &#39;prepared pizza crust&#39;: 2, &#39;boneless skinless chicken breasts&#39;: 132, &#39;Sargento® Traditional Cut Shredded Mozzarella Cheese&#39;: 1, &#39;vegetable oil&#39;: 253, &#39;basil pesto sauce&#39;: 33, &#39;aioli&#39;: 3, &#39;cooked shrimp&#39;: 11, &#39;mozzarella balls&#39;: 4, &#39;sun-dried tomatoes&#39;: 118, &#39;milk&#39;: 277, &#39;condensed cream of mushroom soup&#39;: 16, &#39;fettuccine pasta&#39;: 24, &#39;frozen spinach&#39;: 24, &#39;lasagne&#39;: 6, &#39;passata&#39;: 3, &#39;fat free cream cheese&#39;: 8, &#39;french bread&#39;: 54, &#39;non-fat sour cream&#39;: 14, &#39;reduced fat swiss cheese&#39;: 3, &#39;fat-free mayonnaise&#39;: 8, &#39;roasted garlic&#39;: 7, &#39;seasoning&#39;: 12, &#39;kale&#39;: 58, &#39;sundried tomato paste&#39;: 4, &#39;dried thyme&#39;: 145, &#39;porcini&#39;: 7, &#39;fresh thyme&#39;: 62, &#39;sourdough loaf&#39;: 4, &#39;crust&#39;: 3, &#39;duck fat&#39;: 1, &#39;squabs&#39;: 1, &#39;confit&#39;: 2, &#39;aged gouda&#39;: 1, &#39;soppressata&#39;: 16, &#39;artichokes&#39;: 44, &#39;anchovy fillets&#39;: 124, &#39;lemon slices&#39;: 12, &#39;chicken cutlets&#39;: 23, &#39;saffron threads&#39;: 28, &#39;orzo&#39;: 51, &#39;veal chops&#39;: 4, &#39;oil cured olives&#39;: 3, &#39;pasta rotel&#39;: 2, &#39;pasta water&#39;: 10, &#39;cooking wine&#39;: 3, &#39;crusty bread&#39;: 10, &#39;fresh tomatoes&#39;: 32, &#39;uncooked rigatoni&#39;: 6, &#39;grating cheese&#39;: 6, &#39;vegetable stock&#39;: 29, &#39;freshly grated parmesan&#39;: 87, &#39;florets&#39;: 15, &#39;sliced almonds&#39;: 37, &#39;thyme sprigs&#39;: 45, &#39;egg whites&#39;: 95, &#39;greek yogurt&#39;: 6, &#39;penne&#39;: 99, &#39;fish fillets&#39;: 10, &#39;russet potatoes&#39;: 40, &#39;crumbled gorgonzola&#39;: 29, &#39;fontina cheese&#39;: 84, &#39;spinach leaves&#39;: 25, &#39;orange&#39;: 38, &#39;lamb shanks&#39;: 11, &#39;clove&#39;: 19, &#39;rosemary sprigs&#39;: 50, &#39;italian eggplant&#39;: 7, &#39;Sicilian olives&#39;: 7, &#39;mint sprigs&#39;: 28, &#39;peasant bread&#39;: 7, &#39;garlic powder&#39;: 252, &#39;Kraft Grated Parmesan Cheese&#39;: 10, &#39;peas&#39;: 42, &#39;Oscar Mayer Bacon&#39;: 1, &#39;Philadelphia Cream Cheese&#39;: 9, &#39;garnish&#39;: 3, &#39;dried navy beans&#39;: 5, &#39;celery&#39;: 155, &#39;diced onions&#39;: 42, &#39;wheat berries&#39;: 4, &#39;parsley sprigs&#39;: 38, &#39;thyme&#39;: 51, &#39;polenta prepar&#39;: 5, &#39;chicken breast halves&#39;: 35, &#39;dry sherry&#39;: 22, &#39;cocoa powder&#39;: 9, &#39;dried parsley&#39;: 90, &#39;chopped garlic&#39;: 69, &#39;baking soda&#39;: 68, &#39;coarse salt&#39;: 128, &#39;rigatoni&#39;: 60, &#39;nutmeg&#39;: 40, &#39;artichoke hearts&#39;: 99, &#39;pancetta&#39;: 153, &#39;parsnips&#39;: 7, &#39;whole wheat fettuccine&#39;: 4, &#39;ground sirloin&#39;: 20, &#39;red wine&#39;: 99, &#39;fresh marjoram&#39;: 29, &#39;castellane&#39;: 2, &#39;hot Italian sausages&#39;: 39, &#39;jumbo pasta shells&#39;: 37, &#39;fresh spinach&#39;: 87, &#39;chicken breasts&#39;: 93, &#39;gluten free blend&#39;: 1, &#39;paprika&#39;: 76, &#39;salt and ground black pepper&#39;: 66, &#39;bone in skinless chicken thigh&#39;: 1, &#39;figs&#39;: 10, &#39;egg substitute&#39;: 26, &#39;hot sauce&#39;: 20, &#39;ham&#39;: 42, &#39;balsamic vinaigrette&#39;: 6, &#39;pitted olives&#39;: 15, &#39;fresh chives&#39;: 24, &#39;white mushrooms&#39;: 20, &#39;haricots verts&#39;: 4, &#39;fresh peas&#39;: 16, &#39;bow-tie pasta&#39;: 86, &#39;asparagus tips&#39;: 3, &#39;green beans&#39;: 70, &#39;low-fat cottage cheese&#39;: 14, &#39;whole wheat lasagna noodles&#39;: 8, &#39;shredded parmesan cheese&#39;: 38, &#39;apricots&#39;: 5, &#39;grappa&#39;: 9, &#39;all purpose unbleached flour&#39;: 68, &#39;dry yeast&#39;: 71, &#39;worcestershire sauce&#39;: 54, &#39;amaretti&#39;: 6, &#39;frozen strawberries&#39;: 1, &#39;strawberries&#39;: 48, &#39;cooked ham&#39;: 9, &#39;Alfredo sauce&#39;: 43, &#39;chopped fresh sage&#39;: 91, &#39;cheese slices&#39;: 11, &#39;bread&#39;: 51, &#39;morel&#39;: 2, &#39;leeks&#39;: 106, &#39;1% low-fat cottage cheese&#39;: 10, &#39;feta cheese crumbles&#39;: 68, &#39;pork belly&#39;: 2, &#39;fresh sage&#39;: 29, &#39;pork loin&#39;: 3, &#39;sliced black olives&#39;: 49, &#39;medium shrimp&#39;: 70, &#39;red chili peppers&#39;: 22, &#39;parmigiano-reggiano cheese&#39;: 53, &#39;fresh mint&#39;: 53, &#39;baguette&#39;: 119, &#39;chicken legs&#39;: 7, &#39;baby spinach&#39;: 117, &#39;white beans&#39;: 38, &#39;ground pork&#39;: 72, &#39;romana&#39;: 1, &#39;fresh mushrooms&#39;: 117, &#39;crimini mushrooms&#39;: 26, &#39;navel oranges&#39;: 10, &#39;kalamata&#39;: 62, &#39;sea scallops&#39;: 31, &#39;ladyfingers&#39;: 40, &#39;reduced fat cream cheese&#39;: 3, &#39;whipped topping&#39;: 12, &#39;wine&#39;: 15, &#39;frozen broccoli&#39;: 5, &#39;nonfat ricotta cheese&#39;: 32, &#39;shells&#39;: 8, &#39;reduced-fat cheese&#39;: 1, &#39;cornflake cereal&#39;: 2, &#39;beef brisket&#39;: 2, &#39;dry pasta&#39;: 8, &#39;white bread&#39;: 35, &#39;pork&#39;: 10, &#39;sweet italian sausage&#39;: 72, &#39;prosecco&#39;: 10, &#39;bread dough&#39;: 16, &#39;baby lima beans&#39;: 3, &#39;whipped cream&#39;: 15, &#39;hot cocoa mix&#39;: 2, &#39;brewed coffee&#39;: 18, &#39;abbamele&#39;: 1, &#39;wild mushrooms&#39;: 30, &#39;chopped walnuts&#39;: 46, &#39;fregola&#39;: 4, &#39;savoy cabbage&#39;: 10, &#39;mushroom caps&#39;: 17, &#39;pinot grigio&#39;: 3, &#39;liquid egg substitute&#39;: 1, &#39;chocolate candy bars&#39;: 5, &#39;cooked rice&#39;: 5, &#39;bread crumbs&#39;: 103, &#39;banana peppers&#39;: 5, &#39;toasted walnuts&#39;: 7, &#39;dark rum&#39;: 22, &#39;dried fig&#39;: 11, &#39;chopped parsley&#39;: 99, &#39;ground white pepper&#39;: 33, &quot;soft goat&#39;s cheese&quot;: 10, &#39;truffle oil&#39;: 17, &#39;hazelnuts&#39;: 53, &#39;veal scallopini&#39;: 2, &#39;elbow macaroni&#39;: 19, &#39;bread flour&#39;: 77, &#39;bread yeast&#39;: 1, &#39;red potato&#39;: 26, &#39;vegan parmesan cheese&#39;: 4, &#39;ahi&#39;: 1, &#39;golden brown sugar&#39;: 14, &#39;anjou pears&#39;: 3, &#39;granny smith apples&#39;: 6, &#39;crystallized ginger&#39;: 10, &#39;ice water&#39;: 7, &#39;vegetable shortening&#39;: 5, &#39;chinese five-spice powder&#39;: 1, &#39;whole wheat flour&#39;: 21, &#39;large egg yolks&#39;: 142, &#39;lean ground beef&#39;: 135, &#39;sausage links&#39;: 32, &#39;pork chops&#39;: 8, &#39;rosemary&#39;: 57, &#39;nectarines&#39;: 5, &#39;sweet cherries&#39;: 12, &#39;orange zest&#39;: 28, &#39;lavender buds&#39;: 1, &#39;apricot halves&#39;: 4, &#39;diced tomatoes in juice&#39;: 7, &#39;scallions&#39;: 61, &#39;italian salad dressing&#39;: 41, &#39;short pasta&#39;: 7, &#39;lemon wedge&#39;: 58, &#39;porterhouse steaks&#39;: 4, &#39;1% low-fat milk&#39;: 68, &#39;golden raisins&#39;: 46, &#39;cinnamon sticks&#39;: 18, &#39;broccolini&#39;: 3, &#39;salted butter&#39;: 11, &#39;chicken stock&#39;: 142, &#39;coffee granules&#39;: 12, &#39;lemon rind&#39;: 34, &#39;baby portobello mushrooms&#39;: 10, &#39;broccoli florets&#39;: 64, &#39;orecchiette&#39;: 41, &#39;melted butter&#39;: 24, &#39;pizza shells&#39;: 3, &#39;frozen mixed thawed vegetables,&#39;: 2, &#39;ragu old world style pasta sauc&#39;: 11, &#39;loosely packed fresh basil leaves&#39;: 21, &#39;whole wheat spaghetti&#39;: 22, &#39;butternut squash&#39;: 62, &#39;wonton wrappers&#39;: 17, &#39;meat&#39;: 15, &#39;pear tomatoes&#39;: 6, &#39;gaeta olives&#39;: 5, &#39;yukon gold potatoes&#39;: 39, &#39;turbot&#39;: 2, &#39;ground lamb&#39;: 6, &#39;refrigerated pizza dough&#39;: 31, &#39;potato gnocchi&#39;: 20, &#39;cream&#39;: 27, &#39;angel hair&#39;: 60, &#39;lime juice&#39;: 6, &#39;gelato&#39;: 4, &#39;cherry preserves&#39;: 2, &#39;amaretto liqueur&#39;: 4, &#39;cherries&#39;: 14, &#39;instant espresso powder&#39;: 22, &#39;nuts&#39;: 3, &#39;brown sugar&#39;: 67, &#39;pie shell&#39;: 2, &#39;marsala wine&#39;: 105, &#39;basil leaves&#39;: 153, &#39;cake&#39;: 7, &#39;crabmeat&#39;: 11, &#39;chopped fresh herbs&#39;: 8, &#39;button mushrooms&#39;: 35, &#39;escarole&#39;: 47, &#39;chopped pecans&#39;: 28, &#39;chocolate bars&#39;: 2, &#39;coffee liqueur&#39;: 11, &#39;flat anchovy&#39;: 5, &#39;italian loaf&#39;: 4, &#39;salad dressing&#39;: 23, &#39;pitted black olives&#39;: 24, &#39;rotini&#39;: 46, &#39;frozen mixed vegetables&#39;: 4, &#39;canned tomatoes&#39;: 26, &#39;feta cheese&#39;: 36, &#39;anise seed&#39;: 13, &#39;top sirloin&#39;: 1, &#39;candy&#39;: 3, &#39;angel food cake mix&#39;: 2, &#39;buttercream frosting&#39;: 1, &#39;sour cream&#39;: 71, &#39;shredded cheddar cheese&#39;: 42, &#39;cottage cheese&#39;: 44, &#39;noodles&#39;: 38, &#39;lime&#39;: 5, &#39;watermelon&#39;: 5, &#39;risotto&#39;: 11, &#39;ice cubes&#39;: 8, &#39;peeled tomatoes&#39;: 32, &#39;cooked vermicelli&#39;: 5, &#39;buns&#39;: 1, &#39;apple cider vinegar&#39;: 7, &#39;ground red pepper&#39;: 34, &#39;yellow bell pepper&#39;: 85, &#39;active dry yeast&#39;: 128, &#39;baking potatoes&#39;: 34, &#39;grated romano cheese&#39;: 19, &#39;quinoa&#39;: 8, &#39;littleneck clams&#39;: 26, &#39;whole wheat bread&#39;: 8, &#39;cream cheese, soften&#39;: 40, &#39;salad seasoning mix&#39;: 4, &#39;cucumber&#39;: 25, &#39;pappardelle pasta&#39;: 6, &#39;fresh mozzarella&#39;: 95, &#39;tuna steaks&#39;: 16, &#39;shredded zucchini&#39;: 4, &#39;dried pasta&#39;: 13, &#39;berries&#39;: 5, &#39;clams&#39;: 50, &#39;ground round&#39;: 25, &#39;fettuccine, cook and drain&#39;: 4, &#39;tomato purée&#39;: 47, &#39;chopped celery&#39;: 103, &#39;ditalini pasta&#39;: 15, &#39;lobster&#39;: 11, &#39;almonds&#39;: 35, &#39;anise&#39;: 13, &#39;anise extract&#39;: 12, &#39;brandy&#39;: 25, &#39;boneless chicken breast&#39;: 19, &#39;buffalo sauce&#39;: 2, &#39;blue cheese dressing&#39;: 3, &#39;cannelloni shells&#39;: 2, &#39;butter cooking spray&#39;: 3, &#39;light alfredo sauce&#39;: 7, &#39;rice&#39;: 12, &#39;cream of chicken soup&#39;: 9, &#39;chees fresh mozzarella&#39;: 72, &#39;shrimp tails&#39;: 2, &#39;bay scallops&#39;: 12, &#39;lump crab meat&#39;: 17, &#39;fish stock&#39;: 11, &#39;capellini&#39;: 8, &#39;veal&#39;: 17, &#39;beef stock&#39;: 24, &#39;chard&#39;: 3, &#39;grated Gruyère cheese&#39;: 5, &#39;panko&#39;: 25, &#39;chickpeas&#39;: 37, &#39;white cornmeal&#39;: 4, &#39;red bell pepper, sliced&#39;: 2, &#39;herbs&#39;: 20, &#39;chicken thighs&#39;: 32, &#39;green bell pepper, slice&#39;: 13, &#39;vanilla&#39;: 28, &#39;liqueur&#39;: 10, &#39;aged balsamic vinegar&#39;: 8, &#39;limoncello&#39;: 6, &#39;golden beets&#39;: 4, &#39;pizza doughs&#39;: 103, &#39;black cod&#39;: 2, &#39;green cabbage&#39;: 12, &#39;cavolo nero&#39;: 4, &#39;winter squash&#39;: 1, &#39;thin pizza crust&#39;: 9, &#39;toasted pine nuts&#39;: 30, &#39;grated parmesan romano&#39;: 2, &#39;cumin seed&#39;: 4, &#39;cilantro leaves&#39;: 7, &#39;seasoning salt&#39;: 15, &#39;mixed greens&#39;: 16, &#39;turkey breast cutlets&#39;: 9, &#39;cod fillets&#39;: 8, &#39;barilla&#39;: 4, &#39;linguini&#39;: 9, &#39;perciatelli&#39;: 7, &#39;crumbled blue cheese&#39;: 16, &#39;black mission figs&#39;: 4, &#39;swordfish steaks&#39;: 11, &#39;anchovy paste&#39;: 47, &#39;chuck&#39;: 8, &#39;tomatoes with juice&#39;: 30, &#39;store bought low sodium chicken stock&#39;: 2, &#39;fresh lavender&#39;: 1, &#39;grated orange&#39;: 31, &#39;vanilla wafers&#39;: 3, &#39;amaretto&#39;: 19, &#39;toasted almonds&#39;: 6, &#39;light corn syrup&#39;: 8, &#39;focaccia&#39;: 10, &#39;oyster mushrooms&#39;: 6, &#39;shiitake mushroom caps&#39;: 16, &#39;onion powder&#39;: 39, &#39;sourdough&#39;: 7, &#39;orange bell pepper&#39;: 27, &#39;nonfat cottage cheese&#39;: 6, &#39;stewed tomatoes&#39;: 37, &#39;raspberries&#39;: 25, &#39;vanilla beans&#39;: 26, &#39;Frangelico&#39;: 9, &#39;vegetable oil spray&#39;: 22, &#39;table salt&#39;: 21, &#39;white peppercorns&#39;: 2, &#39;herb vinegar&#39;: 3, &#39;reduced fat sharp cheddar cheese&#39;: 8, &#39;deli ham&#39;: 4, &#39;ground turkey&#39;: 48, &#39;hot dogs&#39;: 2, &#39;italian style stewed tomatoes&#39;: 15, &#39;veal stock&#39;: 7, &#39;portabello mushroom&#39;: 41, &#39;rocket leaves&#39;: 35, &#39;country bread&#39;: 26, &#39;bottled balsamic vinaigrette&#39;: 1, &#39;scallops&#39;: 12, &#39;italian tomatoes&#39;: 16, &#39;peeled shrimp&#39;: 3, &#39;whole wheat pizza crust&#39;: 3, &#39;dried mixed herbs&#39;: 3, &#39;whole wheat pastry flour&#39;: 6, &#39;nonstick spray&#39;: 7, &#39;low-fat sour cream&#39;: 8, &#39;day old bread&#39;: 2, &#39;champagne vinegar&#39;: 9, &#39;pizza sauce&#39;: 71, &#39;red vermouth&#39;: 5, &#39;low sodium chicken stock&#39;: 6, &#39;peppercorns&#39;: 3, &#39;turkey stock&#39;: 3, &#39;cooked turkey&#39;: 6, &#39;green olives&#39;: 52, &#39;spaghettini&#39;: 23, &#39;minced onion&#39;: 41, &#39;beef broth&#39;: 62, &#39;dried mint flakes&#39;: 3, &#39;ravioli&#39;: 16, &#39;meat loaf mix&#39;: 1, &#39;rub&#39;: 2, &#39;clam juice&#39;: 26, &#39;bottled clam juice&#39;: 15, &#39;cayenne pepper&#39;: 50, &#39;pears&#39;: 19, &#39;gorgonzola&#39;: 27, &#39;duck&#39;: 2, &#39;pure vanilla extract&#39;: 22, &#39;panettone&#39;: 4, &#39;sambuca&#39;: 4, &#39;light brown sugar&#39;: 15, &#39;fat free frozen top whip&#39;: 5, &#39;frozen bread dough&#39;: 9, &#39;whole wheat breadcrumbs&#39;: 10, &#39;ground chuck&#39;: 16, &#39;bows&#39;: 3, &#39;sauce&#39;: 53, &#39;ground oregano&#39;: 4, &#39;lean ground turkey&#39;: 2, &#39;milk chocolate&#39;: 11, &#39;hazelnut butter&#39;: 4, &#39;fig jam&#39;: 2, &#39;crackers&#39;: 7, &#39;sun-dried tomatoes in oil&#39;: 28, &#39;dark chocolate&#39;: 7, &#39;vanilla ice cream&#39;: 17, &#39;chives&#39;: 31, &#39;radishes&#39;: 13, &#39;lettuce&#39;: 6, &#39;grilled chicken&#39;: 2, &#39;flatbread&#39;: 4, &#39;fresh parsley leaves&#39;: 56, &#39;lemon extract&#39;: 7, &#39;salad greens&#39;: 11, &#39;brown rice&#39;: 6, &#39;tortellini&#39;: 19, &#39;reduced fat alfredo sauce&#39;: 4, &#39;fresh asparagus&#39;: 24, &#39;round steaks&#39;: 5, &#39;chili powder&#39;: 26, &#39;ground cumin&#39;: 30, &#39;rolls&#39;: 21, &#39;prego traditional italian sauce&#39;: 4, &#39;brown hash potato&#39;: 5, &#39;nonfat milk&#39;: 7, &#39;frozen cheese ravioli&#39;: 6, &#39;mild Italian sausage&#39;: 17, &#39;sandwich rolls&#39;: 3, &#39;orange juice concentrate&#39;: 5, &#39;rotelle&#39;: 3, &#39;sweet italian sausag links, cut into&#39;: 2, &#39;idaho potatoes&#39;: 4, &#39;popcorn&#39;: 1, &#39;grated orange peel&#39;: 26, &#39;french fried onions&#39;: 1, &#39;turbinado&#39;: 7, &#39;cooked chicken breasts&#39;: 16, &#39;farro&#39;: 14, &#39;flour tortillas&#39;: 11, &#39;english cucumber&#39;: 7, &#39;minced peperoncini&#39;: 2, &#39;teleme&#39;: 2, &#39;brine cured green olives&#39;: 4, &#39;black forest ham&#39;: 1, &#39;frozen whole kernel corn&#39;: 5, &#39;sweet potatoes&#39;: 15, &#39;applewood smoked bacon&#39;: 7, &#39;panko breadcrumbs&#39;: 24, &#39;coarse kosher salt&#39;: 9, &#39;cornmeal&#39;: 72, &#39;dried sage&#39;: 28, &#39;dri leav thyme&#39;: 6, &#39;low sodium beef broth&#39;: 3, &#39;cabbage&#39;: 12, &#39;fresh shiitake mushrooms&#39;: 15, &#39;rabbit&#39;: 8, &#39;herbes de provence&#39;: 12, &#39;dressing&#39;: 7, &#39;chicken fingers&#39;: 3, &#39;reduced-fat sour cream&#39;: 13, &#39;dried fettuccine&#39;: 14, &#39;white pepper&#39;: 22, &#39;albacore tuna in water&#39;: 5, &#39;light mayonnaise&#39;: 7, &#39;turkey tenderloins&#39;: 4, &#39;almond flour&#39;: 8, &#39;raw almond&#39;: 3, &#39;fresh orange juice&#39;: 32, &#39;cranberries&#39;: 3, &#39;orange marmalade&#39;: 6, &#39;fresh lemon&#39;: 4, &#39;condensed chicken broth&#39;: 1, &#39;oil packed anchovy fillets&#39;: 3, &#39;boneless chicken skinless thigh&#39;: 20, &#39;sugar pea&#39;: 16, &#39;nonfat yogurt&#39;: 4, &#39;roast red peppers, drain&#39;: 24, &#39;french baguette&#39;: 25, &#39;ripe olives&#39;: 26, &#39;honey glazed ham&#39;: 2, &#39;chiles&#39;: 9, &#39;spring onions&#39;: 3, &#39;candied orange peel&#39;: 13, &#39;lard&#39;: 5, &#39;cinnamon&#39;: 23, &#39;semolina flour&#39;: 32, &#39;onion salt&#39;: 2, &#39;beef demi-glace&#39;: 1, &#39;veal shanks&#39;: 21, &#39;orange peel&#39;: 7, &#39;lemon peel&#39;: 11, &#39;plain yogurt&#39;: 7, &#39;Quinoa Flour&#39;: 1, &#39;spelt flour&#39;: 2, &#39;plums&#39;: 14, &#39;heirloom tomatoes&#39;: 21, &#39;fresh lime juice&#39;: 25, &#39;artichok heart marin&#39;: 33, &#39;bucatini&#39;: 16, &#39;processed cheese&#39;: 5, &#39;egg noodles, cooked and drained&#39;: 3, &#39;tapioca flour&#39;: 1, &#39;lasagna noodles, cooked and drained&#39;: 26, &#39;firm tofu&#39;: 9, &#39;sherry vinegar&#39;: 27, &#39;country style bread&#39;: 7, &#39;teardrop tomatoes&#39;: 2, &#39;ground sausage&#39;: 5, &#39;pita pockets&#39;: 1, &#39;orange juice&#39;: 19, &#39;chili pepper&#39;: 5, &#39;currant&#39;: 7, &#39;small capers, rins and drain&#39;: 2, &#39;filet&#39;: 4, &#39;lettuce leaves&#39;: 8, &#39;vodka&#39;: 27, &#39;stolichnaya&#39;: 1, &#39;romaine lettuce&#39;: 33, &#39;croutons&#39;: 18, &#39;pepperocini&#39;: 2, &#39;cherry peppers&#39;: 3, &#39;dandelion&#39;: 1, &#39;beans&#39;: 6, &#39;hot pepper sauce&#39;: 7, &#39;shredded Monterey Jack cheese&#39;: 13, &#39;cane sugar&#39;: 1, &#39;mixed nuts&#39;: 1, &#39;meatballs&#39;: 17, &#39;plain dry bread crumb&#39;: 11, &#39;spanish onion&#39;: 9, &#39;cuban peppers&#39;: 3, &#39;green tomatoes&#39;: 2, &#39;sesame seeds&#39;: 13, &#39;boneless beef chuck roast&#39;: 3, &#39;hard-boiled egg&#39;: 6, &#39;pork tenderloin&#39;: 24, &#39;bulk italian sausag&#39;: 21, &#39;beef bouillon granules&#39;: 3, &#39;prebaked pizza crusts&#39;: 13, &#39;buttermilk&#39;: 32, &#39;flaked coconut&#39;: 5, &#39;lower sodium chicken broth&#39;: 20, &#39;carnaroli rice&#39;: 13, &#39;fresh oregano leaves&#39;: 24, &#39;cavatappi&#39;: 9, &#39;cooking oil&#39;: 24, &#39;cayenne&#39;: 8, &#39;ground cloves&#39;: 30, &#39;corn syrup&#39;: 4, &#39;small pasta&#39;: 7, &#39;ground fennel&#39;: 7, &#39;low-fat buttermilk&#39;: 9, &#39;ice&#39;: 7, &#39;chicken bouillon granules&#39;: 6, &#39;poultry seasoning&#39;: 10, &#39;roast&#39;: 3, &#39;fusilli&#39;: 43, &#39;Italian herbs&#39;: 7, &#39;diced yellow onion&#39;: 1, &#39;manicotti pasta&#39;: 4, &#39;parsley flakes&#39;: 17, &#39;vinaigrette&#39;: 7, &#39;bread ciabatta&#39;: 16, &#39;kidney beans&#39;: 12, &#39;creole seasoning&#39;: 4, &#39;prepared pasta sauce&#39;: 5, &#39;small curd cottage cheese&#39;: 14, &#39;white sandwich bread&#39;: 9, &#39;mini chocolate chips&#39;: 5, &#39;lean beef&#39;: 2, &#39;breadstick&#39;: 13, &#39;pickled okra&#39;: 2, &#39;fronds&#39;: 15, &#39;thick-cut bacon&#39;: 11, &#39;boiling potatoes&#39;: 13, &#39;ditalini&#39;: 11, &#39;cranberry beans&#39;: 4, &#39;center cut bacon&#39;: 7, &#39;roasting chickens&#39;: 8, &#39;fleur de sel&#39;: 6, &#39;Margherita Pepperoni&#39;: 1, &#39;soft-shell clams&#39;: 1, &#39;liquid&#39;: 5, &#39;fresh chevre&#39;: 4, &#39;pork sausages&#39;: 14, &#39;dried minced onion&#39;: 5, &#39;msg&#39;: 2, &#39;pork stew meat&#39;: 1, &#39;beef stew meat&#39;: 3, &#39;ziti&#39;: 26, &#39;Balsamico Bianco&#39;: 2, &#39;ground mustard&#39;: 1, ...} 12345678## 请不要修改下方代码# Finally, plot the 10 most used ingredientsplt.style.use(u'ggplot')fig = pd.DataFrame(sum_ingredients, index=[0]).transpose()[0].sort_values(ascending=False, inplace=False)[:10].plot(kind='barh')fig.invert_yaxis()fig = fig.get_figure()fig.tight_layout()fig.show() /opt/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:448: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure. % get_backend()) 12345678910## TODO: 统计意大利菜系中佐料出现次数，并赋值到italian_ingredients字典中italian_ingredients = &#123;&#125;train_content_italian = train_content[train_content['cuisine'] == 'italian']for l in train_content_italian['ingredients']: for a in l: if a not in italian_ingredients: italian_ingredients[a] = 1 elif a in sum_italian: italian_ingredients[a] += 1italian_ingredients {&#39;sugar&#39;: 760, &#39;pistachio nuts&#39;: 7, &#39;white almond bark&#39;: 1, &#39;flour&#39;: 142, &#39;vanilla extract&#39;: 219, &#39;olive oil&#39;: 3111, &#39;almond extract&#39;: 56, &#39;eggs&#39;: 627, &#39;baking powder&#39;: 186, &#39;dried cranberries&#39;: 8, &#39;chopped tomatoes&#39;: 37, &#39;fresh basil&#39;: 787, &#39;garlic&#39;: 1471, &#39;extra-virgin olive oil&#39;: 1362, &#39;kosher salt&#39;: 656, &#39;flat leaf parsley&#39;: 588, &#39;pimentos&#39;: 16, &#39;sweet pepper&#39;: 7, &#39;dried oregano&#39;: 626, &#39;sharp cheddar cheese&#39;: 9, &#39;pepper&#39;: 965, &#39;swiss cheese&#39;: 7, &#39;provolone cheese&#39;: 138, &#39;canola oil&#39;: 41, &#39;mushrooms&#39;: 184, &#39;black olives&#39;: 67, &#39;sausages&#39;: 58, &#39;Italian parsley leaves&#39;: 74, &#39;walnuts&#39;: 38, &#39;hot red pepper flakes&#39;: 76, &#39;fresh lemon juice&#39;: 471, &#39;trout fillet&#39;: 3, &#39;garlic cloves&#39;: 1619, &#39;chipotle chile&#39;: 2, &#39;fine sea salt&#39;: 77, &#39;fresh parmesan cheese&#39;: 251, &#39;butter&#39;: 1030, &#39;all-purpose flour&#39;: 918, &#39;fat free less sodium chicken broth&#39;: 158, &#39;chopped fresh chives&#39;: 68, &#39;gruyere cheese&#39;: 18, &#39;ground black pepper&#39;: 1444, &#39;bacon slices&#39;: 48, &#39;gnocchi&#39;: 41, &#39;fat free milk&#39;: 42, &#39;cooking spray&#39;: 491, &#39;salt&#39;: 3454, &#39;italian seasoning&#39;: 347, &#39;broiler-fryer chicken&#39;: 1, &#39;mayonaise&#39;: 63, &#39;zesty italian dressing&#39;: 11, &#39;marinara sauce&#39;: 222, &#39;linguine&#39;: 193, &#39;capers&#39;: 306, &#39;crushed red pepper flakes&#39;: 179, &#39;olives&#39;: 29, &#39;lemon zest&#39;: 98, &#39;sliced mushrooms&#39;: 131, &#39;sherry&#39;: 13, &#39;grated parmesan cheese&#39;: 1580, &#39;heavy cream&#39;: 300, &#39;spaghetti&#39;: 296, &#39;chicken broth&#39;: 245, &#39;cooked chicken&#39;: 33, &#39;yellow corn meal&#39;: 64, &#39;boiling water&#39;: 63, &#39;sea salt&#39;: 202, &#39;onions&#39;: 1240, &#39;crushed garlic&#39;: 20, &#39;green onions&#39;: 144, &#39;white sugar&#39;: 231, &#39;dried basil&#39;: 425, &#39;diced tomatoes&#39;: 429, &#39;bread slices&#39;: 15, &#39;great northern beans&#39;: 21, &#39;shrimp&#39;: 59, &#39;sage leaves&#39;: 89, &#39;Oscar Mayer Deli Fresh Smoked Ham&#39;: 1, &#39;hoagie rolls&#39;: 8, &#39;salami&#39;: 41, &#39;giardiniera&#39;: 5, &#39;mozzarella cheese&#39;: 396, &#39;pepperoni&#39;: 48, &#39;bay leaves&#39;: 107, &#39;crushed red pepper&#39;: 418, &#39;mussels&#39;: 38, &#39;basil&#39;: 174, &#39;black pepper&#39;: 636, &#39;dry white wine&#39;: 658, &#39;tomatoes&#39;: 601, &#39;finely chopped onion&#39;: 145, &#39;lemon&#39;: 236, &#39;pesto&#39;: 113, &#39;salmon fillets&#39;: 11, &#39;white wine&#39;: 176, &#39;pizza crust&#39;: 36, &#39;plum tomatoes&#39;: 340, &#39;part-skim mozzarella cheese&#39;: 201, &#39;crushed tomatoes&#39;: 241, &#39;fresh rosemary&#39;: 292, &#39;boneless pork loin&#39;: 9, &#39;pappardelle&#39;: 11, &#39;red pepper&#39;: 49, &#39;Italian bread&#39;: 89, &#39;balsamic vinegar&#39;: 348, &#39;sausage casings&#39;: 78, &#39;honey&#39;: 126, &#39;shredded mozzarella cheese&#39;: 413, &#39;roasted red peppers&#39;: 114, &#39;penne pasta&#39;: 146, &#39;spinach&#39;: 119, &#39;asiago&#39;: 72, &#39;whole wheat pasta&#39;: 13, &#39;sweet onion&#39;: 69, &#39;grape tomatoes&#39;: 98, &#39;chestnuts&#39;: 9, &#39;granulated sugar&#39;: 82, &#39;whole milk ricotta cheese&#39;: 47, &#39;coffee ice cream&#39;: 3, &#39;large eggs&#39;: 625, &#39;mascarpone&#39;: 124, &#39;rum&#39;: 12, &#39;powdered sugar&#39;: 69, &#39;semisweet chocolate&#39;: 46, &#39;chestnut flour&#39;: 1, &#39;starchy potatoes&#39;: 2, &#39;grated nutmeg&#39;: 64, &#39;blood orange&#39;: 5, &#39;freshly ground pepper&#39;: 316, &#39;fennel bulb&#39;: 103, &#39;low salt chicken broth&#39;: 138, &#39;dijon mustard&#39;: 99, &#39;corn starch&#39;: 83, &#39;white wine vinegar&#39;: 73, &#39;tomato sauce&#39;: 357, &#39;shredded carrots&#39;: 11, &#39;english muffins, split and toasted&#39;: 2, &#39;chopped onion&#39;: 327, &#39;vegetable oil cooking spray&#39;: 73, &#39;chopped green bell pepper&#39;: 39, &#39;cheddar cheese&#39;: 18, &#39;lasagna noodles&#39;: 196, &#39;ranch dressing&#39;: 5, &#39;evaporated milk&#39;: 13, &#39;fresh parsley&#39;: 631, &#39;fresh oregano&#39;: 209, &#39;cold water&#39;: 58, &#39;chocolate morsels&#39;: 4, &#39;cream sweeten whip&#39;: 3, &#39;instant espresso granules&#39;: 4, &#39;whipping cream&#39;: 162, &#39;kahlúa&#39;: 12, &#39;chocolate covered coffee beans&#39;: 1, &#39;unflavored gelatin&#39;: 48, &#39;pound cake&#39;: 7, &#39;pinenuts&#39;: 252, &#39;zucchini&#39;: 326, &#39;baby carrots&#39;: 15, &#39;fresh basil leaves&#39;: 352, &#39;asparagus spears&#39;: 24, &#39;white onion&#39;: 48, &#39;carrots&#39;: 379, &#39;frozen peas&#39;: 67, &#39;arborio rice&#39;: 261, &#39;yellow crookneck squash&#39;: 3, &#39;fresh leav spinach&#39;: 25, &#39;cheese tortellini&#39;: 53, &#39;cherry tomatoes&#39;: 170, &#39;navy beans&#39;: 8, &#39;pecorino romano cheese&#39;: 146, &#39;fresh fava bean&#39;: 12, &#39;italian sausage&#39;: 129, &#39;large garlic cloves&#39;: 293, &#39;pasta sheets&#39;: 9, &#39;water&#39;: 1052, &#39;Turkish bay leaves&#39;: 3, &#39;dried chickpeas&#39;: 1, &#39;celery ribs&#39;: 130, &#39;semolina&#39;: 14, &#39;warm water&#39;: 182, &#39;vine ripened tomatoes&#39;: 12, &#39;bittersweet chocolate&#39;: 42, &#39;fat free yogurt&#39;: 4, &#39;skim milk&#39;: 16, &#39;angel food cake&#39;: 4, &#39;unsweetened cocoa powder&#39;: 84, &#39;instant espresso&#39;: 7, &#39;garlic salt&#39;: 61, &#39;tomato paste&#39;: 376, &#39;veal cutlets&#39;: 18, &#39;broccoli rabe&#39;: 47, &#39;whole milk&#39;: 157, &#39;parmigiano reggiano cheese&#39;: 306, &#39;dry bread crumbs&#39;: 115, &#39;fontina&#39;: 27, &#39;unsalted butter&#39;: 564, &#39;pasta sauce&#39;: 246, &#39;olive oil flavored cooking spray&#39;: 29, &#39;frozen chopped spinach&#39;: 131, &#39;large egg whites&#39;: 138, &#39;part-skim ricotta cheese&#39;: 111, &#39;manicotti shells&#39;: 18, &#39;fettucine&#39;: 136, &#39;parmesan cheese&#39;: 474, &#39;large shrimp&#39;: 77, &#39;chicken bouillon&#39;: 10, &#39;cream of tartar&#39;: 9, &#39;orange liqueur&#39;: 15, &#39;heavy whipping cream&#39;: 73, &#39;semi-sweet chocolate morsels&#39;: 21, &#39;cake flour&#39;: 24, &#39;confectioners sugar&#39;: 94, &#39;ground cinnamon&#39;: 85, &#39;ricotta cheese&#39;: 333, &#39;cream cheese&#39;: 105, &#39;dried porcini mushrooms&#39;: 81, &#39;chopped fresh thyme&#39;: 151, &#39;beef rib short&#39;: 5, &#39;dry red wine&#39;: 131, &#39;hot water&#39;: 85, &#39;fat free less sodium beef broth&#39;: 7, &#39;cremini mushrooms&#39;: 67, &#39;pitted kalamata olives&#39;: 94, &#39;cauliflower&#39;: 34, &#39;whole peeled tomatoes&#39;: 60, &#39;swiss chard&#39;: 63, &#39;banana squash&#39;: 1, &#39;vegetable broth&#39;: 81, &#39;bay leaf&#39;: 130, &#39;cannellini beans&#39;: 161, &#39;boneless skinless chicken breast halves&#39;: 182, &#39;light sour cream&#39;: 5, &#39;fava beans&#39;: 13, &#39;finely chopped fresh parsley&#39;: 33, &#39;fresh tarragon&#39;: 33, &#39;grated lemon zest&#39;: 142, &#39;chopped almonds&#39;: 12, &#39;broccoli&#39;: 64, &#39;potatoes&#39;: 59, &#39;self rising flour&#39;: 1, &#39;eggplant&#39;: 204, &#39;chicken&#39;: 84, &#39;pitas&#39;: 5, &#39;goat cheese&#39;: 92, &#39;reduced fat milk&#39;: 29, &#39;smoked trout&#39;: 1, &#39;fresh dill&#39;: 16, &#39;asparagus&#39;: 123, &#39;oil&#39;: 92, &#39;ricotta salata&#39;: 22, &#39;pecorino cheese&#39;: 52, &#39;grana padano&#39;: 7, &#39;pasta&#39;: 255, &#39;cracked black pepper&#39;: 117, &#39;green bell pepper&#39;: 163, &#39;vidalia onion&#39;: 19, &#39;smoked gouda&#39;: 14, &#39;baby spinach leaves&#39;: 39, &#39;shredded sharp cheddar cheese&#39;: 17, &#39;small red potato&#39;: 13, &#39;Bertolli® Classico Olive Oil&#39;: 16, &#39;bacon, crisp-cooked and crumbled&#39;: 2, &#39;bertolli vineyard premium collect marinara with burgundi wine sauc&#39;: 5, &#39;bread crumb fresh&#39;: 74, &#39;( oz.) tomato sauce&#39;: 7, &#39;ground veal&#39;: 35, &#39;italian seasoning mix&#39;: 1, &#39;beef&#39;: 22, &#39;fat skimmed chicken broth&#39;: 28, &#39;solid pack pumpkin&#39;: 4, &#39;ground nutmeg&#39;: 127, &#39;white rice&#39;: 11, &#39;ground pepper&#39;: 77, &#39;fresh thyme leaves&#39;: 53, &#39;pistachios&#39;: 10, &#39;dried cherry&#39;: 6, &#39;prosciutto&#39;: 256, &#39;romano cheese&#39;: 89, &#39;parsley leaves&#39;: 23, &#39;shallots&#39;: 316, &#39;fresh raspberries&#39;: 14, &#39;dry vermouth&#39;: 18, &#39;canned low sodium chicken broth&#39;: 46, &#39;chicken livers&#39;: 14, &#39;raisins&#39;: 39, &#39;seasoned bread crumbs&#39;: 71, &#39;minced garlic&#39;: 334, &#39;half &amp; half&#39;: 89, &#39;ground beef&#39;: 237, &#39;jack cheese&#39;: 9, &#39;vegetables&#39;: 29, &#39;lemon juice&#39;: 216, &#39;ricotta&#39;: 95, &#39;yellow onion&#39;: 153, &#39;reduced sodium chicken broth&#39;: 36, &#39;chopped fresh mint&#39;: 54, &#39;purple onion&#39;: 350, &#39;low sodium chicken broth&#39;: 78, &#39;polenta&#39;: 132, &#39;bell pepper&#39;: 38, &#39;olive oil cooking spray&#39;: 13, &#39;bacon&#39;: 124, &#39;egg yolks&#39;: 82, &#39;duck breast halves&#39;: 1, &#39;fennel seeds&#39;: 114, &#39;roma tomatoes&#39;: 44, &#39;pesto sauce&#39;: 26, &#39;arugula&#39;: 125, &#39;summer squash&#39;: 9, &#39;red wine vinegar&#39;: 227, &#39;ciabatta&#39;: 21, &#39;juice&#39;: 72, &#39;fresh herbs&#39;: 14, &#39;grated lemon peel&#39;: 89, &#39;dough&#39;: 46, &#39;coarse sea salt&#39;: 17, &#39;rosemary leaves&#39;: 10, &#39;red pepper flakes&#39;: 206, &#39;brie cheese&#39;: 14, &#39;cheese ravioli&#39;: 18, &#39;Italian seasoned breadcrumbs&#39;: 23, &#39;cheese&#39;: 127, &#39;parsley&#39;: 95, &#39;fat-free cottage cheese&#39;: 11, &#39;oven-ready lasagna noodles&#39;: 43, &#39;margarine&#39;: 38, &#39;radicchio&#39;: 40, &#39;garbanzo beans&#39;: 18, &#39;orzo pasta&#39;: 16, &#39;rubbed sage&#39;: 17, &#39;dried rosemary&#39;: 96, &#39;canned beef broth&#39;: 4, &#39;kale leaves&#39;: 7, &#39;chicken noodle soup&#39;: 1, &#39;italian style rolls&#39;: 1, &#39;genoa salami&#39;: 22, &#39;oregano&#39;: 114, &#39;boiled ham&#39;: 1, &#39;capicola&#39;: 4, &#39;iceberg lettuce&#39;: 6, &#39;jalapeno chilies&#39;: 20, &#39;diced celery&#39;: 15, &#39;italian salad dressing mix&#39;: 7, &#39;chopped cilantro fresh&#39;: 25, &#39;cider vinegar&#39;: 14, &#39;red bell pepper&#39;: 354, &#39;sliced green onions&#39;: 40, &#39;barbecue sauce&#39;: 9, &#39;prepared pizza crust&#39;: 2, &#39;boneless skinless chicken breasts&#39;: 132, &#39;Sargento® Traditional Cut Shredded Mozzarella Cheese&#39;: 1, &#39;vegetable oil&#39;: 253, &#39;basil pesto sauce&#39;: 33, &#39;aioli&#39;: 3, &#39;cooked shrimp&#39;: 11, &#39;mozzarella balls&#39;: 4, &#39;sun-dried tomatoes&#39;: 118, &#39;milk&#39;: 277, &#39;condensed cream of mushroom soup&#39;: 16, &#39;fettuccine pasta&#39;: 24, &#39;frozen spinach&#39;: 24, &#39;lasagne&#39;: 6, &#39;passata&#39;: 3, &#39;fat free cream cheese&#39;: 8, &#39;french bread&#39;: 54, &#39;non-fat sour cream&#39;: 14, &#39;reduced fat swiss cheese&#39;: 3, &#39;fat-free mayonnaise&#39;: 8, &#39;roasted garlic&#39;: 7, &#39;seasoning&#39;: 12, &#39;kale&#39;: 58, &#39;sundried tomato paste&#39;: 4, &#39;dried thyme&#39;: 145, &#39;porcini&#39;: 7, &#39;fresh thyme&#39;: 62, &#39;sourdough loaf&#39;: 4, &#39;crust&#39;: 3, &#39;duck fat&#39;: 1, &#39;squabs&#39;: 1, &#39;confit&#39;: 2, &#39;aged gouda&#39;: 1, &#39;soppressata&#39;: 16, &#39;artichokes&#39;: 44, &#39;anchovy fillets&#39;: 124, &#39;lemon slices&#39;: 12, &#39;chicken cutlets&#39;: 23, &#39;saffron threads&#39;: 28, &#39;orzo&#39;: 51, &#39;veal chops&#39;: 4, &#39;oil cured olives&#39;: 3, &#39;pasta rotel&#39;: 2, &#39;pasta water&#39;: 10, &#39;cooking wine&#39;: 3, &#39;crusty bread&#39;: 10, &#39;fresh tomatoes&#39;: 32, &#39;uncooked rigatoni&#39;: 6, &#39;grating cheese&#39;: 6, &#39;vegetable stock&#39;: 29, &#39;freshly grated parmesan&#39;: 87, &#39;florets&#39;: 15, &#39;sliced almonds&#39;: 37, &#39;thyme sprigs&#39;: 45, &#39;egg whites&#39;: 95, &#39;greek yogurt&#39;: 6, &#39;penne&#39;: 99, &#39;fish fillets&#39;: 10, &#39;russet potatoes&#39;: 40, &#39;crumbled gorgonzola&#39;: 29, &#39;fontina cheese&#39;: 84, &#39;spinach leaves&#39;: 25, &#39;orange&#39;: 38, &#39;lamb shanks&#39;: 11, &#39;clove&#39;: 19, &#39;rosemary sprigs&#39;: 50, &#39;italian eggplant&#39;: 7, &#39;Sicilian olives&#39;: 7, &#39;mint sprigs&#39;: 28, &#39;peasant bread&#39;: 7, &#39;garlic powder&#39;: 252, &#39;Kraft Grated Parmesan Cheese&#39;: 10, &#39;peas&#39;: 42, &#39;Oscar Mayer Bacon&#39;: 1, &#39;Philadelphia Cream Cheese&#39;: 9, &#39;garnish&#39;: 3, &#39;dried navy beans&#39;: 5, &#39;celery&#39;: 155, &#39;diced onions&#39;: 42, &#39;wheat berries&#39;: 4, &#39;parsley sprigs&#39;: 38, &#39;thyme&#39;: 51, &#39;polenta prepar&#39;: 5, &#39;chicken breast halves&#39;: 35, &#39;dry sherry&#39;: 22, &#39;cocoa powder&#39;: 9, &#39;dried parsley&#39;: 90, &#39;chopped garlic&#39;: 69, &#39;baking soda&#39;: 68, &#39;coarse salt&#39;: 128, &#39;rigatoni&#39;: 60, &#39;nutmeg&#39;: 40, &#39;artichoke hearts&#39;: 99, &#39;pancetta&#39;: 153, &#39;parsnips&#39;: 7, &#39;whole wheat fettuccine&#39;: 4, &#39;ground sirloin&#39;: 20, &#39;red wine&#39;: 99, &#39;fresh marjoram&#39;: 29, &#39;castellane&#39;: 2, &#39;hot Italian sausages&#39;: 39, &#39;jumbo pasta shells&#39;: 37, &#39;fresh spinach&#39;: 87, &#39;chicken breasts&#39;: 93, &#39;gluten free blend&#39;: 1, &#39;paprika&#39;: 76, &#39;salt and ground black pepper&#39;: 66, &#39;bone in skinless chicken thigh&#39;: 1, &#39;figs&#39;: 10, &#39;egg substitute&#39;: 26, &#39;hot sauce&#39;: 20, &#39;ham&#39;: 42, &#39;balsamic vinaigrette&#39;: 6, &#39;pitted olives&#39;: 15, &#39;fresh chives&#39;: 24, &#39;white mushrooms&#39;: 20, &#39;haricots verts&#39;: 4, &#39;fresh peas&#39;: 16, &#39;bow-tie pasta&#39;: 86, &#39;asparagus tips&#39;: 3, &#39;green beans&#39;: 70, &#39;low-fat cottage cheese&#39;: 14, &#39;whole wheat lasagna noodles&#39;: 8, &#39;shredded parmesan cheese&#39;: 38, &#39;apricots&#39;: 5, &#39;grappa&#39;: 9, &#39;all purpose unbleached flour&#39;: 68, &#39;dry yeast&#39;: 71, &#39;worcestershire sauce&#39;: 54, &#39;amaretti&#39;: 6, &#39;frozen strawberries&#39;: 1, &#39;strawberries&#39;: 48, &#39;cooked ham&#39;: 9, &#39;Alfredo sauce&#39;: 43, &#39;chopped fresh sage&#39;: 91, &#39;cheese slices&#39;: 11, &#39;bread&#39;: 51, &#39;morel&#39;: 2, &#39;leeks&#39;: 106, &#39;1% low-fat cottage cheese&#39;: 10, &#39;feta cheese crumbles&#39;: 68, &#39;pork belly&#39;: 2, &#39;fresh sage&#39;: 29, &#39;pork loin&#39;: 3, &#39;sliced black olives&#39;: 49, &#39;medium shrimp&#39;: 70, &#39;red chili peppers&#39;: 22, &#39;parmigiano-reggiano cheese&#39;: 53, &#39;fresh mint&#39;: 53, &#39;baguette&#39;: 119, &#39;chicken legs&#39;: 7, &#39;baby spinach&#39;: 117, &#39;white beans&#39;: 38, &#39;ground pork&#39;: 72, &#39;romana&#39;: 1, &#39;fresh mushrooms&#39;: 117, &#39;crimini mushrooms&#39;: 26, &#39;navel oranges&#39;: 10, &#39;kalamata&#39;: 62, &#39;sea scallops&#39;: 31, &#39;ladyfingers&#39;: 40, &#39;reduced fat cream cheese&#39;: 3, &#39;whipped topping&#39;: 12, &#39;wine&#39;: 15, &#39;frozen broccoli&#39;: 5, &#39;nonfat ricotta cheese&#39;: 32, &#39;shells&#39;: 8, &#39;reduced-fat cheese&#39;: 1, &#39;cornflake cereal&#39;: 2, &#39;beef brisket&#39;: 2, &#39;dry pasta&#39;: 8, &#39;white bread&#39;: 35, &#39;pork&#39;: 10, &#39;sweet italian sausage&#39;: 72, &#39;prosecco&#39;: 10, &#39;bread dough&#39;: 16, &#39;baby lima beans&#39;: 3, &#39;whipped cream&#39;: 15, &#39;hot cocoa mix&#39;: 2, &#39;brewed coffee&#39;: 18, &#39;abbamele&#39;: 1, &#39;wild mushrooms&#39;: 30, &#39;chopped walnuts&#39;: 46, &#39;fregola&#39;: 4, &#39;savoy cabbage&#39;: 10, &#39;mushroom caps&#39;: 17, &#39;pinot grigio&#39;: 3, &#39;liquid egg substitute&#39;: 1, &#39;chocolate candy bars&#39;: 5, &#39;cooked rice&#39;: 5, &#39;bread crumbs&#39;: 103, &#39;banana peppers&#39;: 5, &#39;toasted walnuts&#39;: 7, &#39;dark rum&#39;: 22, &#39;dried fig&#39;: 11, &#39;chopped parsley&#39;: 99, &#39;ground white pepper&#39;: 33, &quot;soft goat&#39;s cheese&quot;: 10, &#39;truffle oil&#39;: 17, &#39;hazelnuts&#39;: 53, &#39;veal scallopini&#39;: 2, &#39;elbow macaroni&#39;: 19, &#39;bread flour&#39;: 77, &#39;bread yeast&#39;: 1, &#39;red potato&#39;: 26, &#39;vegan parmesan cheese&#39;: 4, &#39;ahi&#39;: 1, &#39;golden brown sugar&#39;: 14, &#39;anjou pears&#39;: 3, &#39;granny smith apples&#39;: 6, &#39;crystallized ginger&#39;: 10, &#39;ice water&#39;: 7, &#39;vegetable shortening&#39;: 5, &#39;chinese five-spice powder&#39;: 1, &#39;whole wheat flour&#39;: 21, &#39;large egg yolks&#39;: 142, &#39;lean ground beef&#39;: 135, &#39;sausage links&#39;: 32, &#39;pork chops&#39;: 8, &#39;rosemary&#39;: 57, &#39;nectarines&#39;: 5, &#39;sweet cherries&#39;: 12, &#39;orange zest&#39;: 28, &#39;lavender buds&#39;: 1, &#39;apricot halves&#39;: 4, &#39;diced tomatoes in juice&#39;: 7, &#39;scallions&#39;: 61, &#39;italian salad dressing&#39;: 41, &#39;short pasta&#39;: 7, &#39;lemon wedge&#39;: 58, &#39;porterhouse steaks&#39;: 4, &#39;1% low-fat milk&#39;: 68, &#39;golden raisins&#39;: 46, &#39;cinnamon sticks&#39;: 18, &#39;broccolini&#39;: 3, &#39;salted butter&#39;: 11, &#39;chicken stock&#39;: 142, &#39;coffee granules&#39;: 12, &#39;lemon rind&#39;: 34, &#39;baby portobello mushrooms&#39;: 10, &#39;broccoli florets&#39;: 64, &#39;orecchiette&#39;: 41, &#39;melted butter&#39;: 24, &#39;pizza shells&#39;: 3, &#39;frozen mixed thawed vegetables,&#39;: 2, &#39;ragu old world style pasta sauc&#39;: 11, &#39;loosely packed fresh basil leaves&#39;: 21, &#39;whole wheat spaghetti&#39;: 22, &#39;butternut squash&#39;: 62, &#39;wonton wrappers&#39;: 17, &#39;meat&#39;: 15, &#39;pear tomatoes&#39;: 6, &#39;gaeta olives&#39;: 5, &#39;yukon gold potatoes&#39;: 39, &#39;turbot&#39;: 2, &#39;ground lamb&#39;: 6, &#39;refrigerated pizza dough&#39;: 31, &#39;potato gnocchi&#39;: 20, &#39;cream&#39;: 27, &#39;angel hair&#39;: 60, &#39;lime juice&#39;: 6, &#39;gelato&#39;: 4, &#39;cherry preserves&#39;: 2, &#39;amaretto liqueur&#39;: 4, &#39;cherries&#39;: 14, &#39;instant espresso powder&#39;: 22, &#39;nuts&#39;: 3, &#39;brown sugar&#39;: 67, &#39;pie shell&#39;: 2, &#39;marsala wine&#39;: 105, &#39;basil leaves&#39;: 153, &#39;cake&#39;: 7, &#39;crabmeat&#39;: 11, &#39;chopped fresh herbs&#39;: 8, &#39;button mushrooms&#39;: 35, &#39;escarole&#39;: 47, &#39;chopped pecans&#39;: 28, &#39;chocolate bars&#39;: 2, &#39;coffee liqueur&#39;: 11, &#39;flat anchovy&#39;: 5, &#39;italian loaf&#39;: 4, &#39;salad dressing&#39;: 23, &#39;pitted black olives&#39;: 24, &#39;rotini&#39;: 46, &#39;frozen mixed vegetables&#39;: 4, &#39;canned tomatoes&#39;: 26, &#39;feta cheese&#39;: 36, &#39;anise seed&#39;: 13, &#39;top sirloin&#39;: 1, &#39;candy&#39;: 3, &#39;angel food cake mix&#39;: 2, &#39;buttercream frosting&#39;: 1, &#39;sour cream&#39;: 71, &#39;shredded cheddar cheese&#39;: 42, &#39;cottage cheese&#39;: 44, &#39;noodles&#39;: 38, &#39;lime&#39;: 5, &#39;watermelon&#39;: 5, &#39;risotto&#39;: 11, &#39;ice cubes&#39;: 8, &#39;peeled tomatoes&#39;: 32, &#39;cooked vermicelli&#39;: 5, &#39;buns&#39;: 1, &#39;apple cider vinegar&#39;: 7, &#39;ground red pepper&#39;: 34, &#39;yellow bell pepper&#39;: 85, &#39;active dry yeast&#39;: 128, &#39;baking potatoes&#39;: 34, &#39;grated romano cheese&#39;: 19, &#39;quinoa&#39;: 8, &#39;littleneck clams&#39;: 26, &#39;whole wheat bread&#39;: 8, &#39;cream cheese, soften&#39;: 40, &#39;salad seasoning mix&#39;: 4, &#39;cucumber&#39;: 25, &#39;pappardelle pasta&#39;: 6, &#39;fresh mozzarella&#39;: 95, &#39;tuna steaks&#39;: 16, &#39;shredded zucchini&#39;: 4, &#39;dried pasta&#39;: 13, &#39;berries&#39;: 5, &#39;clams&#39;: 50, &#39;ground round&#39;: 25, &#39;fettuccine, cook and drain&#39;: 4, &#39;tomato purée&#39;: 47, &#39;chopped celery&#39;: 103, &#39;ditalini pasta&#39;: 15, &#39;lobster&#39;: 11, &#39;almonds&#39;: 35, &#39;anise&#39;: 13, &#39;anise extract&#39;: 12, &#39;brandy&#39;: 25, &#39;boneless chicken breast&#39;: 19, &#39;buffalo sauce&#39;: 2, &#39;blue cheese dressing&#39;: 3, &#39;cannelloni shells&#39;: 2, &#39;butter cooking spray&#39;: 3, &#39;light alfredo sauce&#39;: 7, &#39;rice&#39;: 12, &#39;cream of chicken soup&#39;: 9, &#39;chees fresh mozzarella&#39;: 72, &#39;shrimp tails&#39;: 2, &#39;bay scallops&#39;: 12, &#39;lump crab meat&#39;: 17, &#39;fish stock&#39;: 11, &#39;capellini&#39;: 8, &#39;veal&#39;: 17, &#39;beef stock&#39;: 24, &#39;chard&#39;: 3, &#39;grated Gruyère cheese&#39;: 5, &#39;panko&#39;: 25, &#39;chickpeas&#39;: 37, &#39;white cornmeal&#39;: 4, &#39;red bell pepper, sliced&#39;: 2, &#39;herbs&#39;: 20, &#39;chicken thighs&#39;: 32, &#39;green bell pepper, slice&#39;: 13, &#39;vanilla&#39;: 28, &#39;liqueur&#39;: 10, &#39;aged balsamic vinegar&#39;: 8, &#39;limoncello&#39;: 6, &#39;golden beets&#39;: 4, &#39;pizza doughs&#39;: 103, &#39;black cod&#39;: 2, &#39;green cabbage&#39;: 12, &#39;cavolo nero&#39;: 4, &#39;winter squash&#39;: 1, &#39;thin pizza crust&#39;: 9, &#39;toasted pine nuts&#39;: 30, &#39;grated parmesan romano&#39;: 2, &#39;cumin seed&#39;: 4, &#39;cilantro leaves&#39;: 7, &#39;seasoning salt&#39;: 15, &#39;mixed greens&#39;: 16, &#39;turkey breast cutlets&#39;: 9, &#39;cod fillets&#39;: 8, &#39;barilla&#39;: 4, &#39;linguini&#39;: 9, &#39;perciatelli&#39;: 7, &#39;crumbled blue cheese&#39;: 16, &#39;black mission figs&#39;: 4, &#39;swordfish steaks&#39;: 11, &#39;anchovy paste&#39;: 47, &#39;chuck&#39;: 8, &#39;tomatoes with juice&#39;: 30, &#39;store bought low sodium chicken stock&#39;: 2, &#39;fresh lavender&#39;: 1, &#39;grated orange&#39;: 31, &#39;vanilla wafers&#39;: 3, &#39;amaretto&#39;: 19, &#39;toasted almonds&#39;: 6, &#39;light corn syrup&#39;: 8, &#39;focaccia&#39;: 10, &#39;oyster mushrooms&#39;: 6, &#39;shiitake mushroom caps&#39;: 16, &#39;onion powder&#39;: 39, &#39;sourdough&#39;: 7, &#39;orange bell pepper&#39;: 27, &#39;nonfat cottage cheese&#39;: 6, &#39;stewed tomatoes&#39;: 37, &#39;raspberries&#39;: 25, &#39;vanilla beans&#39;: 26, &#39;Frangelico&#39;: 9, &#39;vegetable oil spray&#39;: 22, &#39;table salt&#39;: 21, &#39;white peppercorns&#39;: 2, &#39;herb vinegar&#39;: 3, &#39;reduced fat sharp cheddar cheese&#39;: 8, &#39;deli ham&#39;: 4, &#39;ground turkey&#39;: 48, &#39;hot dogs&#39;: 2, &#39;italian style stewed tomatoes&#39;: 15, &#39;veal stock&#39;: 7, &#39;portabello mushroom&#39;: 41, &#39;rocket leaves&#39;: 35, &#39;country bread&#39;: 26, &#39;bottled balsamic vinaigrette&#39;: 1, &#39;scallops&#39;: 12, &#39;italian tomatoes&#39;: 16, &#39;peeled shrimp&#39;: 3, &#39;whole wheat pizza crust&#39;: 3, &#39;dried mixed herbs&#39;: 3, &#39;whole wheat pastry flour&#39;: 6, &#39;nonstick spray&#39;: 7, &#39;low-fat sour cream&#39;: 8, &#39;day old bread&#39;: 2, &#39;champagne vinegar&#39;: 9, &#39;pizza sauce&#39;: 71, &#39;red vermouth&#39;: 5, &#39;low sodium chicken stock&#39;: 6, &#39;peppercorns&#39;: 3, &#39;turkey stock&#39;: 3, &#39;cooked turkey&#39;: 6, &#39;green olives&#39;: 52, &#39;spaghettini&#39;: 23, &#39;minced onion&#39;: 41, &#39;beef broth&#39;: 62, &#39;dried mint flakes&#39;: 3, &#39;ravioli&#39;: 16, &#39;meat loaf mix&#39;: 1, &#39;rub&#39;: 2, &#39;clam juice&#39;: 26, &#39;bottled clam juice&#39;: 15, &#39;cayenne pepper&#39;: 50, &#39;pears&#39;: 19, &#39;gorgonzola&#39;: 27, &#39;duck&#39;: 2, &#39;pure vanilla extract&#39;: 22, &#39;panettone&#39;: 4, &#39;sambuca&#39;: 4, &#39;light brown sugar&#39;: 15, &#39;fat free frozen top whip&#39;: 5, &#39;frozen bread dough&#39;: 9, &#39;whole wheat breadcrumbs&#39;: 10, &#39;ground chuck&#39;: 16, &#39;bows&#39;: 3, &#39;sauce&#39;: 53, &#39;ground oregano&#39;: 4, &#39;lean ground turkey&#39;: 2, &#39;milk chocolate&#39;: 11, &#39;hazelnut butter&#39;: 4, &#39;fig jam&#39;: 2, &#39;crackers&#39;: 7, &#39;sun-dried tomatoes in oil&#39;: 28, &#39;dark chocolate&#39;: 7, &#39;vanilla ice cream&#39;: 17, &#39;chives&#39;: 31, &#39;radishes&#39;: 13, &#39;lettuce&#39;: 6, &#39;grilled chicken&#39;: 2, &#39;flatbread&#39;: 4, &#39;fresh parsley leaves&#39;: 56, &#39;lemon extract&#39;: 7, &#39;salad greens&#39;: 11, &#39;brown rice&#39;: 6, &#39;tortellini&#39;: 19, &#39;reduced fat alfredo sauce&#39;: 4, &#39;fresh asparagus&#39;: 24, &#39;round steaks&#39;: 5, &#39;chili powder&#39;: 26, &#39;ground cumin&#39;: 30, &#39;rolls&#39;: 21, &#39;prego traditional italian sauce&#39;: 4, &#39;brown hash potato&#39;: 5, &#39;nonfat milk&#39;: 7, &#39;frozen cheese ravioli&#39;: 6, &#39;mild Italian sausage&#39;: 17, &#39;sandwich rolls&#39;: 3, &#39;orange juice concentrate&#39;: 5, &#39;rotelle&#39;: 3, &#39;sweet italian sausag links, cut into&#39;: 2, &#39;idaho potatoes&#39;: 4, &#39;popcorn&#39;: 1, &#39;grated orange peel&#39;: 26, &#39;french fried onions&#39;: 1, &#39;turbinado&#39;: 7, &#39;cooked chicken breasts&#39;: 16, &#39;farro&#39;: 14, &#39;flour tortillas&#39;: 11, &#39;english cucumber&#39;: 7, &#39;minced peperoncini&#39;: 2, &#39;teleme&#39;: 2, &#39;brine cured green olives&#39;: 4, &#39;black forest ham&#39;: 1, &#39;frozen whole kernel corn&#39;: 5, &#39;sweet potatoes&#39;: 15, &#39;applewood smoked bacon&#39;: 7, &#39;panko breadcrumbs&#39;: 24, &#39;coarse kosher salt&#39;: 9, &#39;cornmeal&#39;: 72, &#39;dried sage&#39;: 28, &#39;dri leav thyme&#39;: 6, &#39;low sodium beef broth&#39;: 3, &#39;cabbage&#39;: 12, &#39;fresh shiitake mushrooms&#39;: 15, &#39;rabbit&#39;: 8, &#39;herbes de provence&#39;: 12, &#39;dressing&#39;: 7, &#39;chicken fingers&#39;: 3, &#39;reduced-fat sour cream&#39;: 13, &#39;dried fettuccine&#39;: 14, &#39;white pepper&#39;: 22, &#39;albacore tuna in water&#39;: 5, &#39;light mayonnaise&#39;: 7, &#39;turkey tenderloins&#39;: 4, &#39;almond flour&#39;: 8, &#39;raw almond&#39;: 3, &#39;fresh orange juice&#39;: 32, &#39;cranberries&#39;: 3, &#39;orange marmalade&#39;: 6, &#39;fresh lemon&#39;: 4, &#39;condensed chicken broth&#39;: 1, &#39;oil packed anchovy fillets&#39;: 3, &#39;boneless chicken skinless thigh&#39;: 20, &#39;sugar pea&#39;: 16, &#39;nonfat yogurt&#39;: 4, &#39;roast red peppers, drain&#39;: 24, &#39;french baguette&#39;: 25, &#39;ripe olives&#39;: 26, &#39;honey glazed ham&#39;: 2, &#39;chiles&#39;: 9, &#39;spring onions&#39;: 3, &#39;candied orange peel&#39;: 13, &#39;lard&#39;: 5, &#39;cinnamon&#39;: 23, &#39;semolina flour&#39;: 32, &#39;onion salt&#39;: 2, &#39;beef demi-glace&#39;: 1, &#39;veal shanks&#39;: 21, &#39;orange peel&#39;: 7, &#39;lemon peel&#39;: 11, &#39;plain yogurt&#39;: 7, &#39;Quinoa Flour&#39;: 1, &#39;spelt flour&#39;: 2, &#39;plums&#39;: 14, &#39;heirloom tomatoes&#39;: 21, &#39;fresh lime juice&#39;: 25, &#39;artichok heart marin&#39;: 33, &#39;bucatini&#39;: 16, &#39;processed cheese&#39;: 5, &#39;egg noodles, cooked and drained&#39;: 3, &#39;tapioca flour&#39;: 1, &#39;lasagna noodles, cooked and drained&#39;: 26, &#39;firm tofu&#39;: 9, &#39;sherry vinegar&#39;: 27, &#39;country style bread&#39;: 7, &#39;teardrop tomatoes&#39;: 2, &#39;ground sausage&#39;: 5, &#39;pita pockets&#39;: 1, &#39;orange juice&#39;: 19, &#39;chili pepper&#39;: 5, &#39;currant&#39;: 7, &#39;small capers, rins and drain&#39;: 2, &#39;filet&#39;: 4, &#39;lettuce leaves&#39;: 8, &#39;vodka&#39;: 27, &#39;stolichnaya&#39;: 1, &#39;romaine lettuce&#39;: 33, &#39;croutons&#39;: 18, &#39;pepperocini&#39;: 2, &#39;cherry peppers&#39;: 3, &#39;dandelion&#39;: 1, &#39;beans&#39;: 6, &#39;hot pepper sauce&#39;: 7, &#39;shredded Monterey Jack cheese&#39;: 13, &#39;cane sugar&#39;: 1, &#39;mixed nuts&#39;: 1, &#39;meatballs&#39;: 17, &#39;plain dry bread crumb&#39;: 11, &#39;spanish onion&#39;: 9, &#39;cuban peppers&#39;: 3, &#39;green tomatoes&#39;: 2, &#39;sesame seeds&#39;: 13, &#39;boneless beef chuck roast&#39;: 3, &#39;hard-boiled egg&#39;: 6, &#39;pork tenderloin&#39;: 24, &#39;bulk italian sausag&#39;: 21, &#39;beef bouillon granules&#39;: 3, &#39;prebaked pizza crusts&#39;: 13, &#39;buttermilk&#39;: 32, &#39;flaked coconut&#39;: 5, &#39;lower sodium chicken broth&#39;: 20, &#39;carnaroli rice&#39;: 13, &#39;fresh oregano leaves&#39;: 24, &#39;cavatappi&#39;: 9, &#39;cooking oil&#39;: 24, &#39;cayenne&#39;: 8, &#39;ground cloves&#39;: 30, &#39;corn syrup&#39;: 4, &#39;small pasta&#39;: 7, &#39;ground fennel&#39;: 7, &#39;low-fat buttermilk&#39;: 9, &#39;ice&#39;: 7, &#39;chicken bouillon granules&#39;: 6, &#39;poultry seasoning&#39;: 10, &#39;roast&#39;: 3, &#39;fusilli&#39;: 43, &#39;Italian herbs&#39;: 7, &#39;diced yellow onion&#39;: 1, &#39;manicotti pasta&#39;: 4, &#39;parsley flakes&#39;: 17, &#39;vinaigrette&#39;: 7, &#39;bread ciabatta&#39;: 16, &#39;kidney beans&#39;: 12, &#39;creole seasoning&#39;: 4, &#39;prepared pasta sauce&#39;: 5, &#39;small curd cottage cheese&#39;: 14, &#39;white sandwich bread&#39;: 9, &#39;mini chocolate chips&#39;: 5, &#39;lean beef&#39;: 2, &#39;breadstick&#39;: 13, &#39;pickled okra&#39;: 2, &#39;fronds&#39;: 15, &#39;thick-cut bacon&#39;: 11, &#39;boiling potatoes&#39;: 13, &#39;ditalini&#39;: 11, &#39;cranberry beans&#39;: 4, &#39;center cut bacon&#39;: 7, &#39;roasting chickens&#39;: 8, &#39;fleur de sel&#39;: 6, &#39;Margherita Pepperoni&#39;: 1, &#39;soft-shell clams&#39;: 1, &#39;liquid&#39;: 5, &#39;fresh chevre&#39;: 4, &#39;pork sausages&#39;: 14, &#39;dried minced onion&#39;: 5, &#39;msg&#39;: 2, &#39;pork stew meat&#39;: 1, &#39;beef stew meat&#39;: 3, &#39;ziti&#39;: 26, &#39;Balsamico Bianco&#39;: 2, &#39;ground mustard&#39;: 1, ...} 1234567## 请不要修改下方代码# Finally, plot the 10 most used ingredientsfig = pd.DataFrame(italian_ingredients, index=[0]).transpose()[0].sort_values(ascending=False, inplace=False)[:10].plot(kind='barh')fig.invert_yaxis()fig = fig.get_figure()fig.tight_layout()fig.show() /opt/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:448: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure. % get_backend()) 若想要对数据分析做更深入的了解，可以参考数据分析师入门课程或者基于Python语言的人工智能Nano课程. 第三步. 建立模型在项目的第三步中，你需要了解必要的工具和技巧来让你的模型进行预测。用这些工具和技巧对每一个模型的表现做精确的衡量可以极大地增强你预测的信心。 3.1 单词清洗由于菜品包含的佐料众多，同一种佐料也可能有单复数、时态等变化，为了去除这类差异，我们考虑将ingredients 进行过滤 12345678910111213141516171819202122## 请不要修改下方代码import refrom nltk.stem import WordNetLemmatizerimport numpy as npdef text_clean(ingredients): #去除单词的标点符号，只保留 a..z A...Z的单词字符 ingredients= np.array(ingredients).tolist() print(\"菜品佐料：\\n&#123;&#125;\".format(ingredients[9])) ingredients=[[re.sub('[^A-Za-z]', ' ', word) for word in component]for component in ingredients] print(\"去除标点符号之后的结果：\\n&#123;&#125;\".format(ingredients[9])) # 去除单词的单复数，时态，只保留单词的词干 lemma=WordNetLemmatizer() ingredients=[\" \".join([ \" \".join([lemma.lemmatize(w) for w in words.split(\" \")]) for words in component]) for component in ingredients] print(\"去除时态和单复数之后的结果：\\n&#123;&#125;\".format(ingredients[9])) return ingredientsprint(\"\\n处理训练集...\")train_ingredients = text_clean(train_content['ingredients'])print(\"\\n处理测试集...\")test_ingredients = text_clean(test_content['ingredients']) [nltk_data] Downloading package wordnet to [nltk_data] /Users/jindongwang/nltk_data... [nltk_data] Unzipping corpora/wordnet.zip. 处理训练集... 菜品佐料： [&#39;chopped tomatoes&#39;, &#39;fresh basil&#39;, &#39;garlic&#39;, &#39;extra-virgin olive oil&#39;, &#39;kosher salt&#39;, &#39;flat leaf parsley&#39;] 去除标点符号之后的结果： [&#39;chopped tomatoes&#39;, &#39;fresh basil&#39;, &#39;garlic&#39;, &#39;extra virgin olive oil&#39;, &#39;kosher salt&#39;, &#39;flat leaf parsley&#39;] 去除时态和单复数之后的结果： chopped tomato fresh basil garlic extra virgin olive oil kosher salt flat leaf parsley 处理测试集... 菜品佐料： [&#39;eggs&#39;, &#39;cherries&#39;, &#39;dates&#39;, &#39;dark muscovado sugar&#39;, &#39;ground cinnamon&#39;, &#39;mixed spice&#39;, &#39;cake&#39;, &#39;vanilla extract&#39;, &#39;self raising flour&#39;, &#39;sultana&#39;, &#39;rum&#39;, &#39;raisins&#39;, &#39;prunes&#39;, &#39;glace cherries&#39;, &#39;butter&#39;, &#39;port&#39;] 去除标点符号之后的结果： [&#39;eggs&#39;, &#39;cherries&#39;, &#39;dates&#39;, &#39;dark muscovado sugar&#39;, &#39;ground cinnamon&#39;, &#39;mixed spice&#39;, &#39;cake&#39;, &#39;vanilla extract&#39;, &#39;self raising flour&#39;, &#39;sultana&#39;, &#39;rum&#39;, &#39;raisins&#39;, &#39;prunes&#39;, &#39;glace cherries&#39;, &#39;butter&#39;, &#39;port&#39;] 去除时态和单复数之后的结果： egg cherry date dark muscovado sugar ground cinnamon mixed spice cake vanilla extract self raising flour sultana rum raisin prune glace cherry butter port 3.2 特征提取在该步骤中，我们将菜品的佐料转换成数值特征向量。考虑到绝大多数菜中都包含salt, water, sugar, butter等，采用one-hot的方法提取的向量将不能很好的对菜系作出区分。我们将考虑按照佐料出现的次数对佐料做一定的加权，即：佐料出现次数越多，佐料的区分性就越低。我们采用的特征为TF-IDF，相关介绍内容可以参考：TF-IDF与余弦相似性的应用（一）：自动提取关键词。 123456789101112## 请不要修改下方代码from sklearn.feature_extraction.text import TfidfVectorizer# 将佐料转换成特征向量# 处理 训练集vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 1), analyzer='word', max_df=.57, binary=False, token_pattern=r\"\\w+\",sublinear_tf=False)train_tfidf = vectorizer.fit_transform(train_ingredients).todense()## 处理 测试集test_tfidf = vectorizer.transform(test_ingredients) 123## 请不要修改下方代码train_targets=np.array(train_content['cuisine']).tolist()train_targets[:10] [&#39;greek&#39;, &#39;southern_us&#39;, &#39;filipino&#39;, &#39;indian&#39;, &#39;indian&#39;, &#39;jamaican&#39;, &#39;spanish&#39;, &#39;italian&#39;, &#39;mexican&#39;, &#39;italian&#39;] 编程练习这里我们为了防止前面步骤中累积的错误，导致以下步骤无法正常运行。我们在此检查处理完的实验数据是否正确，请打印train_tfidf和train_targets中前五个数据。 123# 你需要预览训练集train_tfidf,train_targets中前5条数据，试试Python的切片语法display(train_tfidf[:5])display(train_targets[:5]) matrix([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]) [&#39;greek&#39;, &#39;southern_us&#39;, &#39;filipino&#39;, &#39;indian&#39;, &#39;indian&#39;] 3.3 验证集划分为了在实验中大致估计模型的精确度我们将从原本的train_ingredients 划分出 20% 的数据用作valid_ingredients。 编程练习：数据分割与重排调用train_test_split函数将训练集划分为新的训练集和验证集，便于之后的模型精度观测。 从sklearn.model_selection中导入train_test_split 将train_tfidf和train_targets作为train_test_split的输入变量 设置test_size为0.2，划分出20%的验证集，80%的数据留作新的训练集。 设置random_state随机种子，以确保每一次运行都可以得到相同划分的结果。（随机种子固定，生成的随机序列就是确定的） 12345### TODO：划分出验证集from sklearn.model_selection import train_test_splitX_train , X_valid , y_train, y_valid = train_test_split(train_tfidf, train_targets, test_size=0.2, random_state=1) 3.2 建立模型调用 sklearn 中的逻辑回归模型（Logistic Regression）。 编程练习：训练模型 从sklearn.linear_model导入LogisticRegression 从sklearn.model_selection导入GridSearchCV, 参数自动搜索，只要把参数输进去，就能给出最优的结果和参数，这个方法适合小数据集。 定义parameters变量：为C参数创造一个字典，它的值是从1至10的数组; 定义classifier变量: 使用导入的LogisticRegression创建一个分类函数; 定义grid变量: 使用导入的GridSearchCV创建一个网格搜索对象；将变量’classifier’, ‘parameters’作为参数传至这个对象构造函数中； 12345678910111213from sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import GridSearchCV## TODO: 建立逻辑回归模型parameters = &#123;'C':[1,2,3,4,5,6,7,8,9,10]&#125;classifier = LogisticRegression()grid = GridSearchCV(classifier, parameters)## 请不要修改下方代码grid = grid.fit(X_train, y_train) 模型训练结束之后，我们计算模型在验证集X_valid上预测结果，并计算模型的预测精度（与y_valid逐个比较）。 1234567## 请不要修改下方代码from sklearn.metrics import accuracy_score ## 计算模型的准确率valid_predict = grid.predict(X_valid)valid_score=accuracy_score(y_valid,valid_predict)print(\"验证集上的得分为：&#123;&#125;\".format(valid_score)) 验证集上的得分为：0.7912005028284098 第四步. 模型预测（可选）4.1 预测测试集编程练习 将模型grid对测试集test_tfidf做预测，然后查看预测结果。 1234567### TODO：预测测试结果predictions = grid.predict(test_tfidf)## 请不要修改下方代码print(\"预测的测试集个数为：&#123;&#125;\".format(len(predictions)))test_content['cuisine']=predictionstest_content.head(10) 预测的测试集个数为：9944 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id ingredients cuisine 0 18009 [baking powder, eggs, all-purpose flour, raisins, milk, white sugar] british 1 28583 [sugar, egg yolks, corn starch, cream of tartar, bananas, vanilla wafers, milk, vanilla extract, toasted pecans, egg... southern_us 2 41580 [sausage links, fennel bulb, fronds, olive oil, cuban peppers, onions] italian 3 29752 [meat cuts, file powder, smoked sausage, okra, shrimp, andouille sausage, water, paprika, hot sauce, garlic cloves, ... cajun_creole 4 35687 [ground black pepper, salt, sausage casings, leeks, parmigiano reggiano cheese, cornmeal, water, extra-virgin olive ... italian 5 38527 [baking powder, all-purpose flour, peach slices, corn starch, heavy cream, lemon juice, unsalted butter, salt, white... southern_us 6 19666 [grape juice, orange, white zinfandel] french 7 41217 [ground ginger, white pepper, green onions, orange juice, sugar, Sriracha, vegetable oil, orange zest, chicken broth... chinese 8 28753 [diced onions, taco seasoning mix, all-purpose flour, chopped cilantro fresh, ground cumin, ground cinnamon, vegetab... mexican 9 22659 [eggs, cherries, dates, dark muscovado sugar, ground cinnamon, mixed spice, cake, vanilla extract, self raising flou... british 4.2 提交结果为了更好的测试模型的效果，同时比较与其他人的差距，我们将模型的测试集上的结果提交至 kaggle What’s Cooking? （需要提前注册kaggle账号）。 注意：在提交作业时，请将提交排名得分截图，附在压缩包中。 1234567## 加载结果格式submit_frame = pd.read_csv(\"sample_submission.csv\")## 保存结果result = pd.merge(submit_frame, test_content, on=\"id\", how='left')result = result.rename(index=str, columns=&#123;\"cuisine_y\": \"cuisine\"&#125;)test_result_name = \"tfidf_cuisine_test.csv\"result[['id','cuisine']].to_csv(test_result_name,index=False) 将生成的 tfidf_cuisine_test.csv 提交至 https://www.kaggle.com/c/whats-cooking/submit 然后选择 Upload Submission File, 点击 Make submission即可。稍作等待，就可以看到右上角的评分结果（得分大致为：0.78580 左右）。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://yoursite.com/tags/Machine-Learning/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}]},{"title":"Scorecard","slug":"Scorecard","date":"2019-02-15T10:40:26.000Z","updated":"2019-02-15T10:42:13.539Z","comments":false,"path":"2019/02/15/Scorecard/","link":"","permalink":"http://yoursite.com/2019/02/15/Scorecard/","excerpt":"","text":"信用评分卡模型是一种比较成熟的预测方法，广泛应用于信用风险评估以及金融风险控制等领域，其基本原理是：将模型变量以WOE编码方式离散化之后运用 logistic 回归模型进行二分类变量的拟合及预测。信用评分卡一般可以分为申请评分卡、行为评分卡、催收评分卡等。本文主要讲述申请评分卡模型的建模分析过程。主要分以下几个步骤： 目标定义 数据获取 数据预处理 模型开发 模型评估 评分系统建立 1. 目标定义数据来源kaggle project: ‘give-me-some-credit-dataset’，找出关键的特征变量，建立信用评分模型 2. 数据获取12345678910#导入必要的库包import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.ensemble import RandomForestRegressorimport warningswarnings.filterwarnings('ignore')%matplotlib inline 1234#读取数据data=pd.read_csv('cs-training.csv')data=data.drop(axis=1, columns=[data.columns[0]])data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SeriousDlqin2yrs RevolvingUtilizationOfUnsecuredLines age NumberOfTime30-59DaysPastDueNotWorse DebtRatio MonthlyIncome NumberOfOpenCreditLinesAndLoans NumberOfTimes90DaysLate NumberRealEstateLoansOrLines NumberOfTime60-89DaysPastDueNotWorse NumberOfDependents 0 1 0.766127 45 2 0.802982 9120.0 13 0 6 0 2.0 1 0 0.957151 40 0 0.121876 2600.0 4 0 0 0 1.0 2 0 0.658180 38 1 0.085113 3042.0 2 1 0 0 0.0 3 0 0.233810 30 0 0.036050 3300.0 5 0 0 0 0.0 4 0 0.907239 49 1 0.024926 63588.0 7 0 1 0 0.0 1data.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 150000 entries, 0 to 149999 Data columns (total 11 columns): SeriousDlqin2yrs 150000 non-null int64 RevolvingUtilizationOfUnsecuredLines 150000 non-null float64 age 150000 non-null int64 NumberOfTime30-59DaysPastDueNotWorse 150000 non-null int64 DebtRatio 150000 non-null float64 MonthlyIncome 120269 non-null float64 NumberOfOpenCreditLinesAndLoans 150000 non-null int64 NumberOfTimes90DaysLate 150000 non-null int64 NumberRealEstateLoansOrLines 150000 non-null int64 NumberOfTime60-89DaysPastDueNotWorse 150000 non-null int64 NumberOfDependents 146076 non-null float64 dtypes: float64(4), int64(7) memory usage: 12.6 MB 3. 数据预处理1234567891011121314#缺失值处理 #随机森林法填补MonthlyIncomeMI_df=data.iloc[:,0:10]MI_known=MI_df.loc[MI_df['MonthlyIncome'].notnull()]MI_unknown=MI_df.loc[MI_df['MonthlyIncome'].isnull()]X_known=MI_known.drop('MonthlyIncome',axis=1)y_known=MI_known['MonthlyIncome']X_unknown=MI_unknown.drop('MonthlyIncome',axis=1)rfr=RandomForestRegressor(random_state=0, n_estimators=100, max_depth=3, n_jobs=-1)rfr.fit(X_known, y_known)data.loc[MI_df['MonthlyIncome'].isnull(), 'MonthlyIncome']=rfr.predict(X_unknown).round(0)print('Done') Done 1234#NumberOfDependents 缺失值较少，可以直接删除data.dropna(inplace=True)#去除重复值data.drop_duplicates(inplace=True) 12#异常值处理sns.boxplot(y='age', data=data) &lt;matplotlib.axes._subplots.AxesSubplot at 0x10e034cc0&gt; 1data=data[((data['age']&gt;0) &amp; (data['age']&lt;100))] 123#逾期次数sns.boxplot(data=data[['NumberOfTime30-59DaysPastDueNotWorse','NumberOfTimes90DaysLate','NumberOfTime60-89DaysPastDueNotWorse']], palette='Set2')plt.xticks(rotation=20) (array([0, 1, 2]), &lt;a list of 3 Text xticklabel objects&gt;) 1234data=data[data['NumberOfTime30-59DaysPastDueNotWorse']&lt;80]#再次检查异常点sns.boxplot(data=data[['NumberOfTime30-59DaysPastDueNotWorse','NumberOfTimes90DaysLate','NumberOfTime60-89DaysPastDueNotWorse']], palette='Set2')plt.xticks(rotation=20) (array([0, 1, 2]), &lt;a list of 3 Text xticklabel objects&gt;) 12#月收入和年龄变量的分布sns.boxplot(x='MonthlyIncome', data=data) &lt;matplotlib.axes._subplots.AxesSubplot at 0x10f596b70&gt; 123print('%.2f%% of customers monthly income under 40000.' %(data.loc[data['MonthlyIncome']&lt;=40000].shape[0]*100/data.shape[0]))#月收入绝大部分集中在40000以下，可画出对应的月收入的分布sns.distplot(data.loc[data['MonthlyIncome']&lt;=40000,'MonthlyIncome'], bins=80, label='MonthlyIncome dist', kde=False) 99.69% of customers monthly income under 40000. &lt;matplotlib.axes._subplots.AxesSubplot at 0x107f14048&gt; 1sns.distplot(data['age'], bins=30, label='age dist', kde=True) &lt;matplotlib.axes._subplots.AxesSubplot at 0x10e075978&gt; 特征工程123456#训练集与测试集划分from sklearn.model_selection import train_test_splity = data['SeriousDlqin2yrs']X = data.iloc[:,1:]X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=10) 123456789101112131415161718192021222324252627282930# 定义最优分箱函数import scipy.stats as statsdef mono_bin(Y, X, n = 20): r = 0 total_bad=Y.sum() total_good=Y.count()-total_bad #print(total_bad, total_good) while np.abs(r) &lt; 1: d1 = pd.DataFrame(&#123;\"X\": X, \"Y\": Y, \"Bucket\": pd.qcut(X, n)&#125;) d2 = d1.groupby('Bucket', as_index = True) r, p = stats.spearmanr(d2.mean().X, d2.mean().Y) n = n - 1 d3 = pd.DataFrame(d2.X.min(), columns = ['min']) d3['min']=d2.min().X d3['max'] = d2.max().X d3['bad'] = d2.sum().Y d3['good'] = d2.count().Y-d3['bad'] d3['bad_rate'] = d2.mean().Y d3['woe']=np.log(((d2.count().Y-d2.sum().Y)/total_good)/(d2.sum().Y/total_bad)) iv = ((d3['good']/total_good - d3['bad']/total_bad)*d3['woe']).sum() d4 = (d3.sort_index(by = 'min')).reset_index(drop=True) woe = list(d4['woe'].round(3)) cut=[] cut.append(float('-inf')) for i in range(1, n+1): qua = X.quantile(i / (n+1)) cut.append(round(qua, 4)) cut.append(float('inf')) return d4, iv, cut, woe 12#举例，将‘age’最优分箱mono_bin(y, data['age'], n=10) ( min max bad good bad_rate woe 0 21 33 1816 14471 0.111500 -0.561845 1 34 40 1664 16073 0.093815 -0.369439 2 41 45 1360 14683 0.084772 -0.258150 3 46 49 1209 13619 0.081535 -0.215683 4 50 54 1298 16516 0.072864 -0.093851 5 55 59 913 15757 0.054769 0.210948 6 60 64 690 15923 0.041534 0.501473 7 65 71 414 14194 0.028341 0.897353 8 72 99 341 14403 0.023128 1.105954, 0.2414021266070617, [-inf, 33.0, 40.0, 45.0, 49.0, 54.0, 59.0, 64.0, 71.0, inf], [-0.562, -0.369, -0.258, -0.216, -0.094, 0.211, 0.501, 0.897, 1.106]) 123456#每个变量的个数，从而确定连续变量与分类变量var_lst=data.columns[1:]var_num=&#123;&#125;for var in var_lst: var_num[var]=len(data[var].unique())var_num {&#39;RevolvingUtilizationOfUnsecuredLines&#39;: 122950, &#39;age&#39;: 79, &#39;NumberOfTime30-59DaysPastDueNotWorse&#39;: 14, &#39;DebtRatio&#39;: 114065, &#39;MonthlyIncome&#39;: 13592, &#39;NumberOfOpenCreditLinesAndLoans&#39;: 58, &#39;NumberOfTimes90DaysLate&#39;: 17, &#39;NumberRealEstateLoansOrLines&#39;: 28, &#39;NumberOfTime60-89DaysPastDueNotWorse&#39;: 11, &#39;NumberOfDependents&#39;: 13} 12345#将四个连续变量最优分箱x1_df, x1_iv, x1_cut, x1_woe = mono_bin( y_train, X_train['RevolvingUtilizationOfUnsecuredLines'], n=10)x2_df, x2_iv, x2_cut, x2_woe = mono_bin( y_train, X_train['age'], n=10)x4_df, x4_iv, x4_cut, x4_woe = mono_bin( y_train, X_train['DebtRatio'], n=10)x5_df, x5_iv, x5_cut, x5_woe = mono_bin( y_train, X_train['MonthlyIncome'], n=10) 123456789101112131415161718#不能最优分箱的变量则进行手动分箱，WOE计算函数def woe_value(Y, X, cut): total_bad=Y.sum() total_good=Y.count()-total_bad d1 = pd.DataFrame(&#123;\"X\": X, \"Y\": Y, \"Bucket\": cut&#125;) d2 = d1.groupby('Bucket', as_index = True) d3 = pd.DataFrame(d2.X.min(), columns = ['min']) d3['min']=d2.min().X d3['max'] = d2.max().X d3['bad'] = d2.sum().Y d3['good'] = d2.count().Y-d3['bad'] d3['bad_rate'] = d2.mean().Y d3['woe']=np.log(((d2.count().Y-d2.sum().Y)/total_good)/(d2.sum().Y/total_bad)) iv = ((d3['good']/total_good - d3['bad']/total_bad)*d3['woe']).sum() d4 = (d3.sort_index(by = 'min')).reset_index(drop=True) woe = list(d4['woe'].round(3)) return d4, iv, woe 1234567891011121314151617181920x3_cut = [-np.Inf, 0, 1, 3, 5, np.Inf]x6_cut = [-np.Inf, 1, 2, 3, 5, np.Inf]x7_cut = [-np.Inf, 0, 1, 3, 5, np.Inf]x8_cut = [-np.Inf, 0, 1, 2, 3, np.Inf]x9_cut = [-np.Inf, 0, 1, 3, np.Inf]x10_cut = [-np.Inf, 0, 1, 2, 3, 5, np.Inf]x3_bin = pd.cut(X_train['NumberOfTime30-59DaysPastDueNotWorse'], bins= x3_cut)x6_bin = pd.cut(X_train['NumberOfOpenCreditLinesAndLoans'], bins= x6_cut)x7_bin = pd.cut(X_train['NumberOfTimes90DaysLate'], bins=x7_cut)x8_bin = pd.cut(X_train['NumberRealEstateLoansOrLines'], bins=x8_cut)x9_bin = pd.cut(X_train['NumberOfTime60-89DaysPastDueNotWorse'], bins=x9_cut)x10_bin = pd.cut(X_train['NumberOfDependents'], bins=x10_cut)x3_df, x3_iv, x3_woe = woe_value(y_train, X_train['NumberOfTime30-59DaysPastDueNotWorse'], x3_bin)x6_df, x6_iv, x6_woe = woe_value(y_train, X_train['NumberOfOpenCreditLinesAndLoans'], x6_bin)x7_df, x7_iv, x7_woe = woe_value(y_train, X_train['NumberOfTimes90DaysLate'], x7_bin)x8_df, x8_iv, x8_woe = woe_value(y_train, X_train['NumberRealEstateLoansOrLines'], x8_bin)x9_df, x9_iv, x9_woe = woe_value(y_train, X_train['NumberOfTime60-89DaysPastDueNotWorse'], x9_bin)x10_df, x10_iv, x10_woe = woe_value(y_train, X_train['NumberOfDependents'], x10_bin) 12345678910#相关性分析corr = data.corr()xticks = ['x'+str(i) for i in range(12)]yticks = list(data.columns)fig = plt.figure()ax1 = fig.add_subplot(111)sns.heatmap(corr, cmap='GnBu', annot=True, ax= ax1, annot_kws=&#123;'size':6, 'color':'red'&#125;)ax1.set_xticklabels(xticks, rotation=0, fontsize=10)ax1.set_yticklabels(yticks, rotation=0, fontsize=10)plt.show() 123456#从相关系数热力图可以看出，自变量之间的线性相关性比较弱#画出每个变量的IV值iv = [eval('x'+str(i)+'_iv') for i in range(1,11)]index=['x'+str(i) for i in range(1,11)]sns.barplot(x=index, y=iv)plt.ylabel('IV') Text(0,0.5,&#39;IV&#39;) 12345678910111213141516171819#选x1, x2, x3, x7, x9， IV&gt;0.2高预测性#WOE编码x1= 'RevolvingUtilizationOfUnsecuredLines'x2= 'age'x3= 'NumberOfTime30-59DaysPastDueNotWorse'x7= 'NumberOfTimes90DaysLate'x9= 'NumberOfTime60-89DaysPastDueNotWorse'#定义WOE编码函数def woe_trans(data, var, woe, cut): woe_name = var+'_woe' for i in range(len(woe)): if i == 0: data.loc[(data[var]&lt;=cut[i+1]), woe_name] = woe[i] elif ((i&gt;0) and (i&lt;=(len(woe)-2))): data.loc[((data[var]&lt;=cut[i+1]) &amp; (data[var]&gt;cut[i])), woe_name] = woe[i] else: data.loc[(data[var]&gt;cut[i]), woe_name] = woe[i] return data 12for i in [1,2,3,7,9]: X_train = woe_trans(X_train, eval('x'+str(i)), eval('x'+str(i)+'_woe'), eval('x'+str(i)+'_cut')) 12#选取WOE编码之后的列作为训练数据集X_train = X_train.iloc[:, -5:] 4. 模型开发123456#建立逻辑回归模型import statsmodels.api as smX1=sm.add_constant(X_train)logit=sm.Logit(y_train, X1)result=logit.fit()print(result.summary()) Optimization terminated successfully. Current function value: 0.185840 Iterations 8 Logit Regression Results ============================================================================== Dep. Variable: SeriousDlqin2yrs No. Observations: 101740 Model: Logit Df Residuals: 101734 Method: MLE Df Model: 5 Date: Thu, 20 Sep 2018 Pseudo R-squ.: 0.2382 Time: 21:57:47 Log-Likelihood: -18907. converged: True LL-Null: -24820. LLR p-value: 0.000 ============================================================================================================ coef std err z P&gt;|z| [0.025 0.975] ------------------------------------------------------------------------------------------------------------ const -2.6195 0.015 -171.961 0.000 -2.649 -2.590 RevolvingUtilizationOfUnsecuredLines_woe -0.6441 0.016 -41.387 0.000 -0.675 -0.614 age_woe -0.4992 0.033 -15.294 0.000 -0.563 -0.435 NumberOfTime30-59DaysPastDueNotWorse_woe -0.5455 0.016 -34.470 0.000 -0.577 -0.515 NumberOfTimes90DaysLate_woe -0.5683 0.014 -41.833 0.000 -0.595 -0.542 NumberOfTime60-89DaysPastDueNotWorse_woe -0.4019 0.017 -23.032 0.000 -0.436 -0.368 ============================================================================================================ 5. 模型评估1234#对测试集进行WOE编码for i in [1,2,3,7,9]: X_test = woe_trans(X_test, eval('x'+str(i)), eval('x'+str(i)+'_woe'), eval('x'+str(i)+'_cut'))X_test = X_test.iloc[:, -5:] 123456789101112131415#绘制ROC曲线，计算AUCfrom sklearn import metricsX2=sm.add_constant(X_test)y_pred = result.predict(X2)fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)auc = metrics.auc(fpr, tpr)plt.plot(fpr, tpr, 'b', label='AUC=%.2f' %auc)plt.legend(loc='lower right')plt.plot([0,1], [0,1], 'r--')plt.xlim([0,1])plt.ylim([0,1])plt.xlabel('FPR')plt.ylabel('TPR')plt.show() 6. 评分系统建立12345678910#分数计算函数PDO=20base=600#all_woe是某一个体所有相关变量woe编码值构成的序列#total_score = base- PDO*(all_woe.dot(coef))/np.log(2)factor= -PDO/np.log(2)coef = result.paramsdef get_score(coef, woe, factor): scores=[round(coef*woe[i]*factor, 0) for i in range(len(woe))] return scores 1234567#计算各因子每个区间对应的分数x1_scores=get_score(coef[1], x1_woe, factor)print(x1_scores)x2_scores=get_score(coef[2], x2_woe, factor)x3_scores=get_score(coef[3], x3_woe, factor)x7_scores=get_score(coef[4], x7_woe, factor)x9_scores=get_score(coef[5], x9_woe, factor) [24.0, 23.0, 5.0, -20.0] 12data.head()datacopy=data 1data=datacopy 123for i in [1,2,3,7,9]: data = woe_trans(data, eval('x'+str(i)), eval('x'+str(i)+'_woe'), eval('x'+str(i)+'_cut'))data = data.iloc[:, -5:] 1234#对整个data进行打分计算data_c=sm.add_constant(data) data['score']=500 + round(data_c.dot(coef)*factor, 0) data.head(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } RevolvingUtilizationOfUnsecuredLines_woe age_woe NumberOfTime30-59DaysPastDueNotWorse_woe NumberOfTimes90DaysLate_woe NumberOfTime60-89DaysPastDueNotWorse_woe score 0 -1.097 -0.264 -1.720 0.373 0.267 534.0 1 -1.097 -0.371 0.515 0.373 0.267 567.0 2 -1.097 -0.371 -0.878 -1.969 0.267 507.0 3 0.284 -0.564 0.515 0.373 0.267 590.0 4 -1.097 -0.184 -0.878 0.373 0.267 548.0 5 0.284 1.094 0.515 0.373 0.267 614.0 6 0.284 0.216 0.515 0.373 0.267 601.0 7 -1.097 -0.371 0.515 0.373 0.267 567.0 9 0.284 0.216 0.515 0.373 0.267 601.0 10 -1.097 -0.564 0.515 0.373 0.267 564.0","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://yoursite.com/tags/Machine-Learning/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-02-15T08:09:15.599Z","updated":"2019-02-15T08:09:15.599Z","comments":true,"path":"2019/02/15/hello-world/","link":"","permalink":"http://yoursite.com/2019/02/15/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[],"keywords":[]},{"title":"K-Means","slug":"k-Means","date":"2019-02-15T05:30:45.000Z","updated":"2019-02-15T08:09:15.599Z","comments":true,"path":"2019/02/15/k-Means/","link":"","permalink":"http://yoursite.com/2019/02/15/k-Means/","excerpt":"","text":"电影评分的 k 均值聚类假设你是 Netflix 的一名数据分析师，你想要根据用户对不同电影的评分研究用户在电影品位上的相似和不同之处。了解这些评分对用户电影推荐系统有帮助吗？我们来研究下这方面的数据。 我们将使用的数据来自精彩的 MovieLens 用户评分数据集。我们稍后将在 notebook 中查看每个电影评分，先看看不同类型之间的评分比较情况。 数据集概述该数据集有两个文件。我们将这两个文件导入 pandas dataframe 中： 123456789import pandas as pdimport matplotlib.pyplot as pltimport numpy as npfrom scipy.sparse import csr_matriximport helper# Import the Movies datasetmovies = pd.read_csv('ml-latest-small/movies.csv')movies.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } movieId title genres 0 1 Toy Story (1995) Adventure|Animation|Children|Comedy|Fantasy 1 2 Jumanji (1995) Adventure|Children|Fantasy 2 3 Grumpier Old Men (1995) Comedy|Romance 3 4 Waiting to Exhale (1995) Comedy|Drama|Romance 4 5 Father of the Bride Part II (1995) Comedy 123# Import the ratings datasetratings = pd.read_csv('ml-latest-small/ratings.csv')ratings.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } userId movieId rating timestamp 0 1 31 2.5 1260759144 1 1 1029 3.0 1260759179 2 1 1061 3.0 1260759182 3 1 1129 2.0 1260759185 4 1 1172 4.0 1260759205 现在我们已经知道数据集的结构，每个表格中有多少条记录。 1print('The dataset contains: ', len(ratings), ' ratings of ', len(movies), ' movies.') The dataset contains: 100004 ratings of 9125 movies. 爱情片与科幻片我们先查看一小部分用户，并看看他们喜欢什么类型的电影。我们将大部分数据预处理过程都隐藏在了辅助函数中，并重点研究聚类概念。在完成此 notebook 后，建议你快速浏览下 helper.py，了解这些辅助函数是如何实现的。 1234# Calculate the average rating of romance and scifi moviesgenre_ratings = helper.get_genre_ratings(ratings, movies, ['Romance', 'Sci-Fi'], ['avg_romance_rating', 'avg_scifi_rating'])genre_ratings.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } avg_romance_rating avg_scifi_rating userId 1 3.50 2.40 2 3.59 3.80 3 3.65 3.14 4 4.50 4.26 5 4.08 4.00 函数 get_genre_ratings 计算了每位用户对所有爱情片和科幻片的平均评分。我们对数据集稍微进行偏倚，删除同时喜欢科幻片和爱情片的用户，使聚类能够将他们定义为更喜欢其中一种类型。 1234biased_dataset = helper.bias_genre_rating_dataset(genre_ratings, 3.2, 2.5)print( \"Number of records: \", len(biased_dataset))biased_dataset.head() Number of records: 183 .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } userId avg_romance_rating avg_scifi_rating 0 1 3.50 2.40 1 3 3.65 3.14 2 6 2.90 2.75 3 7 2.93 3.36 4 12 2.89 2.62 可以看出我们有 183 位用户，对于每位用户，我们都得出了他们对看过的爱情片和科幻片的平均评分。 我们来绘制该数据集： 123%matplotlib inlinehelper.draw_scatterplot(biased_dataset['avg_scifi_rating'],'Avg scifi rating', biased_dataset['avg_romance_rating'], 'Avg romance rating') 我们可以在此样本中看到明显的偏差（我们故意创建的）。如果使用 k 均值将样本分成两组，效果如何？ 12# Let's turn our dataset into a listX = biased_dataset[['avg_scifi_rating','avg_romance_rating']].values 导入 KMeans 通过 n_clusters = 2 准备 KMeans 将数据集 X 传递给 KMeans 的 fit_predict 方法，并将聚类标签放入 predictions 1234567891011# TODO: Import KMeansfrom sklearn.cluster import KMeans# TODO: Create an instance of KMeans to find two clusterskmeans_1 = KMeans(n_clusters = 2)# TODO: use fit_predict to cluster the datasetpredictions = kmeans_1.fit_predict(X)# Plothelper.draw_clusters(biased_dataset, predictions) 可以看出分组的依据主要是每个人对爱情片的评分高低。如果爱情片的平均评分超过 3 星，则属于第一组，否则属于另一组。 如果分成三组，会发生什么？ 123456789# TODO: Create an instance of KMeans to find three clusterskmeans_2 = KMeans(n_clusters = 3)# TODO: use fit_predict to cluster the datasetpredictions_2 = kmeans_2.fit_predict(X)# Plothelper.draw_clusters(biased_dataset, predictions_2) 现在平均科幻片评分开始起作用了，分组情况如下所示： 喜欢爱情片但是不喜欢科幻片的用户 喜欢科幻片但是不喜欢爱情片的用户 即喜欢科幻片又喜欢爱情片的用户 再添加一组 12345678# TODO: Create an instance of KMeans to find four clusterskmeans_3 = KMeans(n_clusters = 4)# TODO: use fit_predict to cluster the datasetpredictions_3 = kmeans_3.fit_predict(X)# Plothelper.draw_clusters(biased_dataset, predictions_3) 可以看出将数据集分成的聚类越多，每个聚类中用户的兴趣就相互之间越相似。 选择 K我们可以将数据点拆分为任何数量的聚类。对于此数据集来说，正确的聚类数量是多少？ 可以通过多种方式选择聚类 k。我们将研究一种简单的方式，叫做“肘部方法”。肘部方法会绘制 k 的上升值与使用该 k 值计算的总误差分布情况。 如何计算总误差？一种方法是计算平方误差。假设我们要计算 k=2 时的误差。有两个聚类，每个聚类有一个“图心”点。对于数据集中的每个点，我们将其坐标减去所属聚类的图心。然后将差值结果取平方（以便消除负值），并对结果求和。这样就可以获得每个点的误差值。如果将这些误差值求和，就会获得 k=2 时所有点的总误差。 现在的一个任务是对每个 k（介于 1 到数据集中的元素数量之间）执行相同的操作。 123456# Choose the range of k values to test.# We added a stride of 5 to improve performance. We don't need to calculate the error for every k valuepossible_k_values = range(2, len(X)+1, 5)# Calculate error values for all k values we're interested inerrors_per_k = [helper.clustering_errors(k, X) for k in possible_k_values] 12# Optional: Look at the values of K vs the silhouette score of running K-means with that value of klist(zip(possible_k_values, errors_per_k)) [(2, 0.35588178764728251), (7, 0.37324118163771741), (12, 0.35650856326047475), (17, 0.3741137698024623), (22, 0.37718217339438476), (27, 0.36071909992215945), (32, 0.37104279808464452), (37, 0.3649882241766923), (42, 0.36895091450195883), (47, 0.37696003940733186), (52, 0.38716548900081571), (57, 0.35079775582937778), (62, 0.34916584233387205), (67, 0.34839937724907), (72, 0.34907390154971468), (77, 0.34837739216196456), (82, 0.3309353056966266), (87, 0.34005916910201761), (92, 0.32494553685658306), (97, 0.32418331059507227), (102, 0.31329160485165003), (107, 0.29407239955320186), (112, 0.27366896911138017), (117, 0.28906341363336779), (122, 0.27342563040040624), (127, 0.25219179857975438), (132, 0.25320773897416415), (137, 0.2412264569953621), (142, 0.21855949198498667), (147, 0.19924498428850082), (152, 0.18722856283659275), (157, 0.16447514022082693), (162, 0.14697529680439808), (167, 0.12609539969216882), (172, 0.096865005870864829), (177, 0.064230120163174503), (182, 0.054644808743169397)] 123456789101112131415# Plot the each value of K vs. the silhouette score at that valuefig, ax = plt.subplots(figsize=(16, 6))ax.set_xlabel('K - number of clusters')ax.set_ylabel('Silhouette Score (higher is better)')ax.plot(possible_k_values, errors_per_k)# Ticks and gridxticks = np.arange(min(possible_k_values), max(possible_k_values)+1, 5.0)ax.set_xticks(xticks, minor=False)ax.set_xticks(xticks, minor=True)ax.xaxis.grid(True, which='both')yticks = np.arange(round(min(errors_per_k), 2), max(errors_per_k), .05)ax.set_yticks(yticks, minor=False)ax.set_yticks(yticks, minor=True)ax.yaxis.grid(True, which='both') 看了该图后发现，合适的 k 值包括 7、22、27、32 等（每次运行时稍微不同）。聚类 (k) 数量超过该范围将开始导致糟糕的聚类情况（根据轮廓分数） 我会选择 k=7，因为更容易可视化： 12345678# TODO: Create an instance of KMeans to find seven clusterskmeans_4 = KMeans(n_clusters=7)# TODO: use fit_predict to cluster the datasetpredictions_4 = kmeans_4.fit_predict(X)# plothelper.draw_clusters(biased_dataset, predictions_4, cmap='Accent') 注意：当你尝试绘制更大的 k 值（超过 10）时，需要确保你的绘制库没有对聚类重复使用相同的颜色。对于此图，我们需要使用 matplotlib colormap ‘Accent’，因为其他色图要么颜色之间的对比度不强烈，要么在超过 8 个或 10 个聚类后会重复利用某些颜色。 再加入动作片类型到目前为止，我们只查看了用户如何对爱情片和科幻片进行评分。我们再添加另一种类型，看看加入动作片类型后效果如何。 现在数据集如下所示： 1234567biased_dataset_3_genres = helper.get_genre_ratings(ratings, movies, ['Romance', 'Sci-Fi', 'Action'], ['avg_romance_rating', 'avg_scifi_rating', 'avg_action_rating'])biased_dataset_3_genres = helper.bias_genre_rating_dataset(biased_dataset_3_genres, 3.2, 2.5).dropna()print( \"Number of records: \", len(biased_dataset_3_genres))biased_dataset_3_genres.head() Number of records: 183 .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } userId avg_romance_rating avg_scifi_rating avg_action_rating 0 1 3.50 2.40 2.80 1 3 3.65 3.14 3.47 2 6 2.90 2.75 3.27 3 7 2.93 3.36 3.29 4 12 2.89 2.62 3.21 123X_with_action = biased_dataset_3_genres[['avg_scifi_rating', 'avg_romance_rating', 'avg_action_rating']].values 12345678# TODO: Create an instance of KMeans to find seven clusterskmeans_5 = KMeans(n_clusters=7)# TODO: use fit_predict to cluster the datasetpredictions_5 = kmeans_5.fit_predict(X_with_action)# plothelper.draw_clusters_3d(biased_dataset_3_genres, predictions_5) 我们依然分别用 x 轴和 y 轴表示科幻片和爱情片。并用点的大小大致表示动作片评分情况（更大的点表示平均评分超过 3 颗星，更小的点表示不超过 3 颗星 ）。 可以看出添加类型后，用户的聚类分布发生了变化。为 k 均值提供的数据越多，每组中用户之间的兴趣越相似。但是如果继续这么绘制，我们将无法可视化二维或三维之外的情形。在下个部分，我们将使用另一种图表，看看多达 50 个维度的聚类情况。 电影级别的聚类现在我们已经知道 k 均值会如何根据用户的类型品位对用户进行聚类，我们再进一步分析，看看用户对单个影片的评分情况。为此，我们将数据集构建成 userId 与用户对每部电影的评分形式。例如，我们来看看以下数据集子集： 123456# Merge the two tables then pivot so we have Users X Movies dataframeratings_title = pd.merge(ratings, movies[['movieId', 'title']], on='movieId' )user_movie_ratings = pd.pivot_table(ratings_title, index='userId', columns= 'title', values='rating')print('dataset dimensions: ', user_movie_ratings.shape, '\\n\\nSubset example:')user_movie_ratings.iloc[:6, :10] dataset dimensions: (671, 9064) Subset example: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } title \"Great Performances\" Cats (1998) $9.99 (2008) 'Hellboy': The Seeds of Creation (2004) 'Neath the Arizona Skies (1934) 'Round Midnight (1986) 'Salem's Lot (2004) 'Til There Was You (1997) 'burbs, The (1989) 'night Mother (1986) (500) Days of Summer (2009) userId 1 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 5 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 6 NaN NaN NaN NaN NaN NaN NaN 4.0 NaN NaN NaN 值的优势表明了第一个问题。大多数用户没有看过大部分电影，并且没有为这些电影评分。这种数据集称为“稀疏”数据集，因为只有少数单元格有值。 为了解决这一问题，我们按照获得评分次数最多的电影和对电影评分次数最多的用户排序。这样可以形成更“密集”的区域，使我们能够查看数据集的顶部数据。 如果我们要选择获得评分次数最多的电影和对电影评分次数最多的用户，则如下所示： 123456n_movies = 30n_users = 18most_rated_movies_users_selection = helper.sort_by_rating_density(user_movie_ratings, n_movies, n_users)print('dataset dimensions: ', most_rated_movies_users_selection.shape)most_rated_movies_users_selection.head() dataset dimensions: (18, 30) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } title Forrest Gump (1994) Pulp Fiction (1994) Shawshank Redemption, The (1994) Silence of the Lambs, The (1991) Star Wars: Episode IV - A New Hope (1977) Jurassic Park (1993) Matrix, The (1999) Toy Story (1995) Schindler's List (1993) Terminator 2: Judgment Day (1991) ... Dances with Wolves (1990) Fight Club (1999) Usual Suspects, The (1995) Seven (a.k.a. Se7en) (1995) Lion King, The (1994) Godfather, The (1972) Lord of the Rings: The Fellowship of the Ring, The (2001) Apollo 13 (1995) True Lies (1994) Twelve Monkeys (a.k.a. 12 Monkeys) (1995) 29 5.0 5.0 5.0 4.0 4.0 4.0 3.0 4.0 5.0 4.0 ... 5.0 4.0 5.0 4.0 3.0 5.0 3.0 5.0 4.0 2.0 508 4.0 5.0 4.0 4.0 5.0 3.0 4.5 3.0 5.0 2.0 ... 5.0 4.0 5.0 4.0 3.5 5.0 4.5 3.0 2.0 4.0 14 1.0 5.0 2.0 5.0 5.0 3.0 5.0 2.0 4.0 4.0 ... 3.0 5.0 5.0 5.0 4.0 5.0 5.0 3.0 4.0 4.0 72 5.0 5.0 5.0 4.5 4.5 4.0 4.5 5.0 5.0 3.0 ... 4.5 5.0 5.0 5.0 5.0 5.0 5.0 3.5 3.0 5.0 653 4.0 5.0 5.0 4.5 5.0 4.5 5.0 5.0 5.0 5.0 ... 4.5 5.0 5.0 4.5 5.0 4.5 5.0 5.0 4.0 5.0 5 rows × 30 columns 这样更好分析。我们还需要指定一个可视化这些评分的良好方式，以便在查看更庞大的子集时能够直观地识别这些评分（稍后变成聚类）。 我们使用颜色代替评分数字： 1helper.draw_movies_heatmap(most_rated_movies_users_selection) 每列表示一部电影。每行表示一位用户。单元格的颜色根据图表右侧的刻度表示用户对该电影的评分情况。 注意到某些单元格是白色吗？表示相应用户没有对该电影进行评分。在现实中进行聚类时就会遇到这种问题。与一开始经过整理的示例不同，现实中的数据集经常比较稀疏，数据集中的部分单元格没有值。这样的话，直接根据电影评分对用户进行聚类不太方便，因为 k 均值通常不喜欢缺失值。 为了提高性能，我们将仅使用 1000 部电影的评分（数据集中一共有 9000 部以上）。 12user_movie_ratings = pd.pivot_table(ratings_title, index='userId', columns= 'title', values='rating')most_rated_movies_1k = helper.get_most_rated_movies(user_movie_ratings, 1000) 为了使 sklearn 对像这样缺少值的数据集运行 k 均值聚类，我们首先需要将其转型为稀疏 csr 矩阵类型（如 SciPi 库中所定义）。 要从 pandas dataframe 转换为稀疏矩阵，我们需要先转换为 SparseDataFrame，然后使用 pandas 的 to_coo() 方法进行转换。 注意：只有较新版本的 pandas 具有to_coo()。如果你在下个单元格中遇到问题，确保你的 pandas 是最新版本。 1sparse_ratings = csr_matrix(pd.SparseDataFrame(most_rated_movies_1k).to_coo()) 我们来聚类吧！对于 k 均值，我们需要指定 k，即聚类数量。我们随意地尝试 k=20（选择 k 的更佳方式如上述肘部方法所示。但是，该方法需要一定的运行时间。): 12# 20 clusterspredictions = KMeans(n_clusters=20, algorithm='full').fit_predict(sparse_ratings) 为了可视化其中一些聚类，我们需要将每个聚类绘制成热图： 12345max_users = 70max_movies = 50clustered = pd.concat([most_rated_movies_1k.reset_index(), pd.DataFrame(&#123;'group':predictions&#125;)], axis=1)helper.draw_movie_clusters(clustered, max_users, max_movies) cluster # 7 # of users in cluster: 276. # of users in plot: 70 cluster # 16 # of users in cluster: 64. # of users in plot: 64 cluster # 0 # of users in cluster: 26. # of users in plot: 26 cluster # 2 # of users in cluster: 72. # of users in plot: 70 cluster # 6 # of users in cluster: 17. # of users in plot: 17 cluster # 3 # of users in cluster: 37. # of users in plot: 37 cluster # 11 # of users in cluster: 12. # of users in plot: 12 cluster # 18 # of users in cluster: 35. # of users in plot: 35 cluster # 9 # of users in cluster: 55. # of users in plot: 55 cluster # 8 # of users in cluster: 27. # of users in plot: 27 cluster # 15 # of users in cluster: 15. # of users in plot: 15 需要注意以下几个事项： 聚类中的评分越相似，你在该聚类中就越能发现颜色相似的垂直线。 在聚类中发现了非常有趣的规律： 某些聚类比其他聚类更稀疏，其中的用户可能比其他聚类中的用户看的电影更少，评分的电影也更少。 某些聚类主要是黄色，汇聚了非常喜欢特定类型电影的用户。其他聚类主要是绿色或海蓝色，表示这些用户都认为某些电影可以评 2-3 颗星。 注意每个聚类中的电影有何变化。图表对数据进行了过滤，仅显示评分最多的电影，然后按照平均评分排序。 能找到《指环王》在每个聚类中位于哪个位置吗？《星球大战》呢？ 很容易发现具有相似颜色的水平线，表示评分变化不大的用户。这可能是 Netflix 从基于星级的评分切换到喜欢/不喜欢评分的原因之一。四颗星评分对不同的人来说，含义不同。 我们在可视化聚类时，采取了一些措施（过滤/排序/切片）。因为这种数据集比较“稀疏”，大多数单元格没有值（因为大部分用户没有看过大部分电影）。 预测我们选择一个聚类和一位特定的用户，看看该聚类可以使我们执行哪些实用的操作。 首先选择一个聚类： 12345678910# TODO: Pick a cluster ID from the clusters abovecluster_number = 11# Let's filter to only see the region of the dataset with the most number of values n_users = 75n_movies = 300cluster = clustered[clustered.group == cluster_number].drop(['index', 'group'], axis=1)cluster = helper.sort_by_rating_density(cluster, n_movies, n_users)helper.draw_movies_heatmap(cluster, axis_labels=False) 聚类中的实际评分如下所示： 1cluster.fillna('').head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Amadeus (1984) Annie Hall (1977) One Flew Over the Cuckoo's Nest (1975) Fargo (1996) Cool Hand Luke (1967) Chinatown (1974) North by Northwest (1959) Citizen Kane (1941) Wizard of Oz, The (1939) Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981) ... Sense and Sensibility (1995) Top Gun (1986) Flashdance (1983) Jerry Maguire (1996) Superman (1978) Abyss, The (1989) Devil in a Blue Dress (1995) Beetlejuice (1988) Dial M for Murder (1954) Broken Arrow (1996) 0 5.0 4.0 4.0 5 4 4 4 5 4 ... 3 3 3 4 3 1 4.0 4.0 4.0 4 5 5 3 5 4 3 ... 2 3 2 4 2 3 2 5.0 4.0 5.0 5 5 5 5 5 5 5 ... 3 4 5 4 8 2.0 5.0 2.0 5 3 5 3 4 5 3 ... 4.5 2 4 3 3 3 10 3.0 4.0 3.0 4 5 4 4 4 5 ... 5 4 3 2 5 rows × 300 columns 从表格中选择一个空白单元格。因为用户没有对该电影评分，所以是空白状态。能够预测她是否喜欢该电影吗？因为该用户属于似乎具有相似品位的用户聚类，我们可以计算该电影在此聚类中的平均评分，结果可以作为她是否喜欢该电影的合理预测依据。 12345# TODO: Fill in the name of the column/movie. e.g. 'Forrest Gump (1994)'# Pick a movie from the table above since we're looking at a subsetmovie_name = 'Forrest Gump (1994)'cluster[movie_name].mean() 3.6666666666666665 这就是我们关于她会如何对该电影进行评分的预测。 推荐我们回顾下上一步的操作。我们使用 k 均值根据用户的评分对用户进行聚类。这样就形成了具有相似评分的用户聚类，因此通常具有相似的电影品位。基于这一点，当某个用户对某部电影没有评分时，我们对该聚类中所有其他用户的评分取平均值，该平均值就是我们猜测该用户对该电影的喜欢程度。 根据这一逻辑，如果我们计算该聚类中每部电影的平均分数，就可以判断该“品位聚类”对数据集中每部电影的喜欢程度。 12# The average rating of 20 movies as rated by the users in the clustercluster.mean().head(20) Amadeus (1984) 3.833333 Annie Hall (1977) 4.291667 One Flew Over the Cuckoo&#39;s Nest (1975) 4.208333 Fargo (1996) 4.454545 Cool Hand Luke (1967) 4.636364 Chinatown (1974) 4.454545 North by Northwest (1959) 4.409091 Citizen Kane (1941) 4.681818 Wizard of Oz, The (1939) 4.500000 Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981) 4.272727 Butch Cassidy and the Sundance Kid (1969) 4.045455 Star Wars: Episode V - The Empire Strikes Back (1980) 4.090909 Groundhog Day (1993) 3.727273 Gone with the Wind (1939) 4.272727 It&#39;s a Wonderful Life (1946) 4.272727 2001: A Space Odyssey (1968) 4.272727 Shawshank Redemption, The (1994) 4.363636 Philadelphia Story, The (1940) 4.409091 Bonnie and Clyde (1967) 4.150000 To Kill a Mockingbird (1962) 4.400000 dtype: float64 这对我们来说变得非常实用，因为现在我们可以使用它作为推荐引擎，使用户能够发现他们可能喜欢的电影。 当用户登录我们的应用时，现在我们可以向他们显示符合他们的兴趣品位的电影。推荐方式是选择聚类中该用户尚未评分的最高评分的电影。 12345678910111213141516# TODO: Pick a user ID from the dataset# Look at the table above outputted by the command \"cluster.fillna('').head()\" # and pick one of the user ids (the first column in the table)user_id = 11# Get all this user's ratingsuser_2_ratings = cluster.loc[user_id, :]# Which movies did they not rate? (We don't want to recommend movies they've already rated)user_2_unrated_movies = user_2_ratings[user_2_ratings.isnull()]# What are the ratings of these movies the user did not rate?avg_ratings = pd.concat([user_2_unrated_movies, cluster.mean()], axis=1, join='inner').loc[:,0]# Let's sort by rating so the highest rated movies are presented firstavg_ratings.sort_values(ascending=False)[:20] Remains of the Day, The (1993) 4.666667 Saving Private Ryan (1998) 4.642857 African Queen, The (1951) 4.625000 Lone Star (1996) 4.600000 Godfather: Part II, The (1974) 4.500000 Singin&#39; in the Rain (1952) 4.500000 My Cousin Vinny (1992) 4.500000 Raising Arizona (1987) 4.500000 Fargo (1996) 4.454545 Rain Man (1988) 4.400000 Full Metal Jacket (1987) 4.400000 Sense and Sensibility (1995) 4.375000 Fried Green Tomatoes (1991) 4.333333 Room with a View, A (1986) 4.300000 It&#39;s a Wonderful Life (1946) 4.272727 Dial M for Murder (1954) 4.250000 Laura (1944) 4.250000 American Graffiti (1973) 4.250000 Much Ado About Nothing (1993) 4.250000 Ordinary People (1980) 4.250000 Name: 0, dtype: float64 这些是向用户推荐的前 20 部电影！ 练习： 如果聚类中有一部电影只有一个评分，评分是 5 颗星。该电影在该聚类中的平均评分是多少？这会对我们的简单推荐引擎有何影响？你会如何调整推荐系统，以解决这一问题？ 关于协同过滤的更多信息 这是一个简单的推荐引擎，展示了“协同过滤”的最基本概念。有很多可以改进该引擎的启发法和方法。为了推动在这一领域的发展，Netflix 设立了 Netflix 奖项 ，他们会向对 Netflix 的推荐算法做出最大改进的算法奖励 1,000,000 美元。 在 2009 年，“BellKor’s Pragmatic Chaos”团队获得了这一奖项。这篇论文介绍了他们采用的方式，其中包含大量方法。 Netflix 最终并没有使用这个荣获 1,000,000 美元奖励的算法，因为他们采用了流式传输的方式，并产生了比电影评分要庞大得多的数据集——用户搜索了哪些内容？用户在此会话中试看了哪些其他电影？他们是否先看了一部电影，然后切换到了其他电影？这些新的数据点可以提供比评分本身更多的线索。 深入研究 该 notebook 显示了用户级推荐系统。我们实际上可以使用几乎一样的代码进行商品级推荐。例如亚马逊的“购买（评价或喜欢）此商品的客户也购买了（评价了或喜欢）以下商品：” 。我们可以在应用的每个电影页面显示这种推荐。为此，我们只需将数据集转置为“电影 X 用户”形状，然后根据评分之间的联系对电影（而不是用户）进行聚类。 我们从数据集 Movie Lens 中抽取了最小的子集，只包含 100,000 个评分。如果你想深入了解电影评分数据，可以查看他们的完整数据集，其中包含 2400 万个评分。","categories":[],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://yoursite.com/tags/Machine-Learning/"}],"keywords":[]},{"title":"CNN网络图像识别","slug":"My-First-Post","date":"2019-02-15T05:05:16.000Z","updated":"2019-02-15T10:19:04.357Z","comments":true,"path":"2019/02/15/My-First-Post/","link":"","permalink":"http://yoursite.com/2019/02/15/My-First-Post/","excerpt":"","text":"简介本文使用keras(2.1.4)——其他版本有坑. 网络框架搭建CNN网络，对cifar10数据集进行图像识别，cifar10是一种自带label的图像数据集，数据集种类十分丰富可以很好的检验网络性能，话不多说直接进入正题 第一步获取数据集通过keras可以直接下载cifar10数据集(数据集比较大可能需要一些时间)1234import keras#使用cifar10数据集from keras.datasets import cifar10(x_train, y_train), (x_test, y_test) = cifar10.load_data() 展示前24张图片观察数据集的部分样本别问为什么，要有一个程序员的严谨！！严谨！！严谨！！(重要的事说3遍)1234567import numpy as npimport matplotlib.pyplot as pltfig = plt.figure(figsize=(20,5))for i in range(36): ax = fig.add_subplot(3, 12, i + 1, xticks=[], yticks=[]) ax.imshow(np.squeeze(x_train[i])) 所有数据集除以255重构图像因为图像单个像素中最大值为255，将其除以255是将每一个像素缩放到0-1之间，类似于标准化12x_train = x_train.astype('float32')/255x_test = x_test.astype('float32')/255 将数据分解为测试集、训练集、验证集123456789101112131415161718from keras.utils import np_utils# 将标签转化为one-hotnum_classes = len(np.unique(y_train))y_train = keras.utils.to_categorical(y_train, num_classes)y_test = keras.utils.to_categorical(y_test, num_classes)# 将数据分解为训练集和测试集(x_train, x_valid) = x_train[5000:], x_train[:5000](y_train, y_valid) = y_train[5000:], y_train[:5000]# 输出训练集形状print('x_train shape:', x_train.shape)# 输出每一个集合的长度print(x_train.shape[0], 'train samples')print(x_test.shape[0], 'test samples')print(x_valid.shape[0], 'validation samples') 开始构建卷积神经网络12345678910111213141516171819202122232425from keras.models import Sequentialfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout#初始化网络类型，选择顺序网络model = Sequential()#添加卷积层，使用same填充，relu激活model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(32, 32, 3)))#添加池化层model.add(MaxPooling2D(pool_size=2))model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))model.add(MaxPooling2D(pool_size=2))model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))model.add(MaxPooling2D(pool_size=2))#舍弃部分神经元，避免过拟合model.add(Dropout(0.3))#数据扁平化model.add(Flatten())model.add(Dense(500, activation='relu'))model.add(Dropout(0.4))model.add(Dense(10, activation='softmax'))#模型确认model.summary()#模型启动，定义损失函数，优化器，评分标准model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) 模型训练开始！心疼一波没有GPU的小伙伴。。。12345678from keras.callbacks import ModelCheckpoint #训练模型checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, save_best_only=True)hist = model.fit(x_train, y_train, batch_size=32, epochs=100, validation_data=(x_valid, y_valid), callbacks=[checkpointer], verbose=2, shuffle=True) 测试集预测终于到了激动人心的时刻，想不想知道自己搭建的模型的性能? 等着吧！12345# 获取训练集预测y_hat = model.predict(x_test)# 定义文本标签--来源:(source: https://www.cs.toronto.edu/~kriz/cifar.html)cifar10_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'] 结果展示！！！！！123456789# 展示样本训练结果fig = plt.figure(figsize=(20, 8))for i, idx in enumerate(np.random.choice(x_test.shape[0], size=32, replace=False)): ax = fig.add_subplot(4, 8, i + 1, xticks=[], yticks=[]) ax.imshow(np.squeeze(x_test[idx])) pred_idx = np.argmax(y_hat[idx]) true_idx = np.argmax(y_test[idx]) ax.set_title(\"&#123;&#125; (&#123;&#125;)\".format(cifar10_labels[pred_idx], cifar10_labels[true_idx]), color=(\"green\" if pred_idx == true_idx else \"red\")) 感言:说实话图像识别的发展是一个很漫长的过程，通过结果可以发现有时候我们确实有点为难机器了，不信你们自己看看那训练结果。。 有些图片你自己都不知道是什么东西。。 还有一点 感谢各位的支持 ！拜拜👋！ 还没完。 没有GPU的小伙伴可以去亚马逊申请免费的GPU服务器后 嘿嘿😁最后像提供数据集的前辈们致敬！","categories":[],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"}],"keywords":[]}]}