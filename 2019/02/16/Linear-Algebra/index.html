<!DOCTYPE HTML>
<html lang="null">
<head><meta name="generator" content="Hexo 3.8.0">
    <!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta http-equiv="Cache-Control" content="no-siteapp">
    <meta http-equiv="Cache-Control" content="no-transform">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable" content="单身程序员的小窝">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    <meta name="browsermode" content="application">
    <meta name="screen-orientation" content="portrait">
    <link rel="dns-prefetch" href="http://yoursite.com">
    <!--SEO-->

    <meta name="keywords" content="Machine Learning">


    <meta name="description" content="线性代数：机器学习背后的优化原理线性代数作为数学的一个分支，广泛应用于科学和工程中，掌握好线性代数对于理解和从事机器学习算法相关工作是很有必要的，尤其对于深度学习算法而言。因此，这个项目会从浅入...">



<meta name="robots" content="all">
<meta name="google" content="all">
<meta name="googlebot" content="all">
<meta name="verify" content="all">

    <!--Title-->


<title>Linear Algebra | 单身程序员的小窝</title>


    <link rel="alternate" href="/atom.xml" title="单身程序员的小窝" type="application/atom+xml">


    <link rel="icon" href="/favicon.ico">

    



<link rel="stylesheet" href="/css/bootstrap.min.css?rev=3.3.7">
<link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/style.css?rev=@@hash">




    
	<div class="hide">
		<script type="text/javascript">
			var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan class='cnzz_stat_icon_1263868967 hide' %3E%3Cscript%20src%3D%22https%3A%2F%2Fs95.cnzz.com%2Fz_stat.php%3Fweb_id%3D1272564536%22%3E%3C%2Fscript%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s19.cnzz.com/z_stat.php%3Fid%3D1263868967%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
		</script>
	</div>




    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?4cc1f2d8f3067386cc5cdb626a202900";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>



    

</head>


</html>
<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<div style="ie">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->

<body>
    <header class="main-header" style="background-image:url(http://snippet.shenliyang.com/img/banner.jpg)">
    <div class="main-header-box">
        <a class="header-avatar" href="/" title="Jindong">
            <img src="/img/avatar.jpg" alt="logo头像" class="img-responsive center-block">
        </a>
        <div class="branding">
        	<!--<h2 class="text-hide">Snippet主题,从未如此简单有趣</h2>-->
            
                <h2> 活成了一个快乐的小×× </h2>
            
    	</div>
    </div>
</header>
    <nav class="main-navigation">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="navbar-header"><span class="nav-toggle-button collapsed pull-right" data-toggle="collapse" data-target="#main-menu" id="mnav">
                    <span class="sr-only"></span>
                        <i class="fa fa-bars"></i>
                    </span>
                    <a class="navbar-brand" href="http://yoursite.com">单身程序员的小窝</a>
                </div>
                <div class="collapse navbar-collapse" id="main-menu">
                    <ul class="menu">
                        
                            <li role="presentation" class="text-center">
                                <a href="/"><i class="fa "></i>Home</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/机器学习/"><i class="fa "></i>MachineLearning</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/深度学习/"><i class="fa "></i>DeepLearning</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/强化学习/"><i class="fa "></i>Reinforcement learning</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/tools/"><i class="fa "></i>Tools</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/archives/"><i class="fa "></i>History</a>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</nav>
    <section class="content-wrap">
        <div class="container">
            <div class="row">
                <main class="col-md-8 main-content m-post">
                    <p id="process"></p>
<article class="post">
    <div class="post-head">
        <h1 id="Linear Algebra">
            
	            Linear Algebra
            
        </h1>
        <div class="post-meta">
    
        <span class="categories-meta fa-wrap">
            <i class="fa fa-folder-open-o"></i>
            <a class="category-link" href="/categories/机器学习/">机器学习</a>
        </span>
    

    
        <span class="fa-wrap">
            <i class="fa fa-tags"></i>
            <span class="tags-meta">
                
                    <a class="tag-link" href="/tags/Machine-Learning/">Machine Learning</a>
                
            </span>
        </span>
    

    
        
        <span class="fa-wrap">
            <i class="fa fa-clock-o"></i>
            <span class="date-meta">2019/02/16</span>
        </span>
        
    
</div>
            
            
    </div>
    
    <div class="post-body post-content">
        <h1 id="线性代数：机器学习背后的优化原理"><a href="#线性代数：机器学习背后的优化原理" class="headerlink" title="线性代数：机器学习背后的优化原理"></a>线性代数：机器学习背后的优化原理</h1><p>线性代数作为数学的一个分支，广泛应用于科学和工程中，掌握好线性代数对于理解和从事机器学习算法相关工作是很有必要的，尤其对于深度学习算法而言。因此，这个项目会从浅入深更好的帮助你学习与积累一些跟人工智能强相关的线性代数的知识。</p>
<p>本项目内容理论知识部分参考<a href="https://book.douban.com/subject/27087503/" target="_blank" rel="noopener">《DeepLearning》又名花书</a>第二章，希望大家支持正版购买图书。</p>
<p>若项目中的题目有困难没完成也没关系，我们鼓励你带着问题提交项目，评审人会给予你诸多帮助。</p>
<p>所有选做题都可以不做，不影响项目通过。如果你做了，那么项目评审会帮你批改，也会因为选做部分做错而判定为不通过。</p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们将讲解常用的线性代数知识，而学员需使用numpy来实现这些知识点（当然也可以自己写算法实现），还需要使用matplotlib完成规定图像习题，当然，本项目用到的python代码(或numpy的使用)课程中并未完全教授，所以需要学员对相应操作进行学习与查询，这在我们往后的人工智能学习之旅中是必不可少的一个技能，请大家珍惜此项目的练习机会。</p>
<p>当然，这里提供官方的<a href="https://docs.scipy.org/doc/numpy/user/quickstart.html#" target="_blank" rel="noopener">numpy Quickstart</a>来帮助你更好的完成项目。</p>
<p>本项目还需要使用LaTeX公式，以下两个链接供学习与使用：</p>
<p><a href="https://www.authorea.com/users/77723/articles/110898-how-to-write-mathematical-equations-expressions-and-symbols-with-latex-a-cheatsheet" target="_blank" rel="noopener">Latex cheatsheet</a></p>
<p><a href="http://www.personal.ceu.hu/tex/cookbook.html#inline" target="_blank" rel="noopener">aTeX Cookbook</a></p>
<p>首先，导入你所需的软件包。一般我们建议在工程开头导入<strong>所有</strong>需要的软件包。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> import相关库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<h2 id="1、标量，向量，矩阵，张量"><a href="#1、标量，向量，矩阵，张量" class="headerlink" title="1、标量，向量，矩阵，张量"></a>1、标量，向量，矩阵，张量</h2><p><strong>首先，让我们回顾下基本的定义：</strong></p>
<ul>
<li><p>标量（scalar）：形式而言，一个标量是一个单独的数，常用斜体的小写变量名称来表示。<em>v</em></p>
</li>
<li><p>向量（vector）：形式而言，一个向量是一列有序数，常用粗体的小写变量名称表示<strong>v</strong>，或者上面标记剪头$\vec{v}$ </p>
</li>
<li><p>矩阵（matrix）：形式而言，一个矩阵是一个二维数组，常用大写变量名称表示A，表示内部的元素则会使用$A_{i,j}$</p>
</li>
<li><p>张量（tensor）：形式而言，一个张量是一个多维数组，常用粗体的大写字母变量名称表示<strong>T</strong>，表示内部的元素则会使用$A_{i,j,z}$ 等等</p>
</li>
</ul>
<p>用图片直观的显示区别如下<br><img src="/2019/02/16/Linear-Algebra/diff.png" width="500"></p>
<p><strong>接下来让我们回顾下基本的运算：</strong></p>
<ul>
<li><p>加法<br><img src="/2019/02/16/Linear-Algebra/add.png" width="500"></p>
</li>
<li><p>标量乘法<br><img src="/2019/02/16/Linear-Algebra/scmu.png" width="400"></p>
</li>
<li><p>转置<br><img src="/2019/02/16/Linear-Algebra/trans.png" width="370"></p>
</li>
<li><p>矩阵向量乘法（内积，人工智能中常见的拼写：matrix product 或者 dot product）<br><img src="/2019/02/16/Linear-Algebra/mul.png" width="570"></p>
</li>
</ul>
<p><strong>线性方程组：</strong></p>
<p>由矩阵乘法也演变出了我们最常见的线性方程组，已知矩阵与未知向量的乘积，等于另一个已知向量，通过此方程组可求解那个未知向量，一般写为x，具体如下表示。<br>等式左侧可以这么来理解：<br><img src="/2019/02/16/Linear-Algebra/axb.png" width="400"><br>列为具体的矩阵来看：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
    A_{1,1} & A_{1,2} & \cdots & A_{1,n} \\\\
    A_{2,1} & A_{2,2} & \cdots & A_{2,n} \\\\
    \cdots & \cdots & \cdots & \cdots \\\\
    A_{m,1} & A_{m,2} & \cdots & A_{m,n}
\end{bmatrix}
\times
\begin{bmatrix}
    x_1 \\\\
    x_2 \\\\
    \cdots \\\\
    x_n
\end{bmatrix}
=
\begin{bmatrix}
    b_1 \\\\
    b_2 \\\\
    \cdots \\\\
    b_m
\end{bmatrix}</script><p>或者更简单的表示为</p>
<script type="math/tex; mode=display">Ax=b</script><p>既然有未知数，那么自然需要求解未知数，而我们的未知数需要满足所有方程，也不是一直都有解的，下面来列我们二维矩阵所组成的方程解的情况,若两条线平行不存在焦点，那么说明没有一个$x_1$, $x_2$同时满足两个方程，则此方程组无解，同理，若相交，则有一个解，若完全相等，则有无穷个解。<br><img src="/2019/02/16/Linear-Algebra/axbsolu.png" width="570"></p>
<h3 id="1-1、基本运算并绘图"><a href="#1-1、基本运算并绘图" class="headerlink" title="1.1、基本运算并绘图"></a>1.1、基本运算并绘图</h3><p>例题 $\vec{v}$ + $\vec{w}$</p>
<p>$\hspace{1cm}\vec{v} = \begin{bmatrix} 1\ 1\end{bmatrix}$</p>
<p>$\hspace{1cm}\vec{w} = \begin{bmatrix} -2\ 2\end{bmatrix}$</p>
<p>结果需要先使用numpy计算向量运算结果，并用LaTeX公式表示：</p>
<p>$\hspace{1cm}\vec{v}+\vec{w} = \begin{bmatrix} -1\ 3\end{bmatrix}$</p>
<p>并使用matlibplot绘制出(图表颜色样式不要求)</p>
<p><img src="/2019/02/16/Linear-Algebra/add_e.png" width="300"></p>
<h4 id="1-1-1"><a href="#1-1-1" class="headerlink" title="1.1.1"></a>1.1.1</h4><p><strong>根据上面例题展示，计算并绘制  $2\vec{v}$ - $\vec{w}$  的结果</strong></p>
<p>$\hspace{1cm}\vec{v} = \begin{bmatrix} 4\ 1\end{bmatrix}$</p>
<p>$\hspace{1cm}\vec{w} = \begin{bmatrix} -1\ 2\end{bmatrix}$</p>
<p>$\hspace{1cm}\vec{v}+\vec{w} = \begin{bmatrix} -1\ 3\end{bmatrix}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.1.1 TODO：</span></span><br><span class="line">a = <span class="number">2</span></span><br><span class="line">v = np.array([<span class="number">4</span>,<span class="number">1</span>])</span><br><span class="line">av = v*a</span><br><span class="line">w = np.array([<span class="number">-1</span>,<span class="number">2</span>])</span><br><span class="line">v_2_w = <span class="number">2</span> * v - w</span><br><span class="line"></span><br><span class="line">ax = plt.axes()</span><br><span class="line">ax.arrow(<span class="number">0</span>, <span class="number">0</span>, *av, color=<span class="string">'b'</span>, linewidth=<span class="number">2.0</span>, head_width=<span class="number">0.20</span>, head_length=<span class="number">0.25</span>)</span><br><span class="line">ax.arrow(v_2_w[<span class="number">0</span>], v_2_w[<span class="number">1</span>], *w, color=<span class="string">'r'</span>, linewidth=<span class="number">2.0</span>, head_width=<span class="number">0.20</span>, head_length=<span class="number">0.25</span>)</span><br><span class="line">ax.arrow(<span class="number">0</span>, <span class="number">0</span>, *v_2_w, color=<span class="string">'k'</span>, linewidth=<span class="number">2.0</span>, head_width=<span class="number">0.20</span>, head_length=<span class="number">0.25</span>)</span><br><span class="line">plt.xlim(<span class="number">-1</span>,<span class="number">10</span>)</span><br><span class="line">major_xticks = np.arange(<span class="number">-1</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">plt.ylim(<span class="number">-1</span>,<span class="number">4</span>)</span><br><span class="line">major_yticks = np.arange(<span class="number">-1</span>, <span class="number">10</span>)</span><br><span class="line">ax.set_yticks(major_yticks)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.grid(b=<span class="keyword">True</span>, which=<span class="string">'major'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/16/Linear-Algebra/output_6_0.png" alt="png"></p>
<script type="math/tex; mode=display">
\begin{bmatrix}
    4\\\\
    1
\end{bmatrix}
\times
\begin{bmatrix}
    2
\end{bmatrix}-
\begin{bmatrix}
    -1\\\\
    2
\end{bmatrix}
=
\begin{bmatrix}
    9\\\\
    0
\end{bmatrix}</script><p>例题，方程组求解：</p>
<script type="math/tex; mode=display">
\begin{cases}
y = 2x + 1\\\\
y = 6x - 2
\end{cases}</script><p>用matplotlib绘制图表（图表样式不要求）<br><img src="/2019/02/16/Linear-Algebra/2equ_solu.png" width="300"><br>由上可知此方程组有且仅有一个解</p>
<p>需使用numpy（或自写算法）计算该解的结果,并用LaTeX公式表示出来(结果可以用小数或者分数展示)</p>
<script type="math/tex; mode=display">
\begin{cases}
x = \frac{3}{4} \\\\
y = \frac{5}{2}
\end{cases}</script><h4 id="1-1-2"><a href="#1-1-2" class="headerlink" title="1.1.2"></a>1.1.2</h4><p><strong>根据上面例题展示，绘制方程组，说明是否有解是否为唯一解，若有解需计算出方程组的解</strong></p>
<script type="math/tex; mode=display">
\begin{cases}
y = 2x + 1\\\\
y = \frac{1}{10}x+6
\end{cases}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.1.2 TODO</span></span><br><span class="line">X = np.mat([[<span class="number">-1</span>,<span class="number">2</span>],[<span class="number">-1</span>,<span class="number">0.1</span>]])</span><br><span class="line">y = np.mat([<span class="number">-1</span>,<span class="number">-6</span>]).T</span><br><span class="line">x = np.arange(<span class="number">-10</span>,<span class="number">10</span>)</span><br><span class="line">plt.plot(<span class="number">2</span>*x+<span class="number">1</span>)</span><br><span class="line">plt.plot(<span class="number">0.1</span>*x+<span class="number">6</span>)</span><br><span class="line">plt.show</span><br><span class="line"></span><br><span class="line">r = np.linalg.solve(X,y)</span><br><span class="line">print(r)</span><br><span class="line"><span class="comment">#有唯一解 函数相交</span></span><br></pre></td></tr></table></figure>
<pre><code>[[6.26315789]
 [2.63157895]]
</code></pre><p><img src="/2019/02/16/Linear-Algebra/output_9_1.png" alt="png"></p>
<h3 id="1-2、说明题"><a href="#1-2、说明题" class="headerlink" title="1.2、说明题"></a>1.2、说明题</h3><h4 id="1-2-1"><a href="#1-2-1" class="headerlink" title="1.2.1"></a>1.2.1</h4><p><strong>使用numpy（或自写算法）说明$(AB)^{\text{T}} = B^\text{T}A^\text{T}$</strong></p>
<p><strong>其中</strong></p>
<script type="math/tex; mode=display">
A=\begin{bmatrix}
    21 & 7 \\\\
    15 & 42 \\\\
    9 & 6
\end{bmatrix}, 
B=\begin{bmatrix}
    4 \\\\
    33
\end{bmatrix}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.2.1 TODO</span></span><br><span class="line">A = np.array([[<span class="number">21</span>,<span class="number">7</span>],[<span class="number">15</span>,<span class="number">42</span>],[<span class="number">9</span>,<span class="number">6</span>]],dtype=int)</span><br><span class="line">B = np.array([<span class="number">4</span>,<span class="number">33</span>],dtype=int)</span><br><span class="line"></span><br><span class="line">display(A.dot(B).T)</span><br><span class="line">display(B.T.dot(A.T))</span><br><span class="line"><span class="comment">#结果证明(AB)T=BTAT</span></span><br></pre></td></tr></table></figure>
<pre><code>array([ 315, 1446,  234])



array([ 315, 1446,  234])
</code></pre><h4 id="1-2-2"><a href="#1-2-2" class="headerlink" title="1.2.2"></a>1.2.2</h4><p><strong>使用numpy（或自写算法）说明  $A ( B + C ) = AB + AC$ </strong></p>
<p><strong>其中</strong></p>
<script type="math/tex; mode=display">
A=\begin{bmatrix}
    9 & 3 \\\\
    8 & 4 \\\\
    7 & 6
\end{bmatrix}, 
B=\begin{bmatrix}
    5 \\\\
    2
\end{bmatrix}, 
C=\begin{bmatrix}
    5 \\\\
    7
\end{bmatrix}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.2.2 TODO</span></span><br><span class="line">A = np.array([[<span class="number">9</span>,<span class="number">3</span>],[<span class="number">8</span>,<span class="number">4</span>],[<span class="number">7</span>,<span class="number">6</span>]])</span><br><span class="line">B = np.array([[<span class="number">5</span>,<span class="number">2</span>]])</span><br><span class="line">C = np.array([[<span class="number">5</span>,<span class="number">7</span>]])</span><br><span class="line"></span><br><span class="line">display(A*(B+C))</span><br><span class="line">display(A*B+A*C)</span><br><span class="line"><span class="comment"># 据结果A(B+C)=AB+AC</span></span><br></pre></td></tr></table></figure>
<pre><code>array([[90, 27],
       [80, 36],
       [70, 54]])



array([[90, 27],
       [80, 36],
       [70, 54]])
</code></pre><h2 id="2、特殊矩阵"><a href="#2、特殊矩阵" class="headerlink" title="2、特殊矩阵"></a>2、特殊矩阵</h2><ul>
<li>单位矩阵</li>
</ul>
<p>如果选取任意一个向量和某矩阵相乘，该向量都不会改变，我们将这种保持n维向量不变的矩阵记为单位矩阵$I_n$</p>
<ul>
<li>逆矩阵</li>
</ul>
<p>如果存在一个矩阵，使$A^{-1} A = I_n$，那么$A^{-1}$就是A的逆矩阵。</p>
<ul>
<li>对角矩阵</li>
</ul>
<p>如果一个矩阵只有主对角线上还有非零元素，其他位置都是零，这个矩阵就是对角矩阵</p>
<ul>
<li>对称矩阵</li>
</ul>
<p>如果一个矩阵的转置是和它自己相等的矩阵，即$A=A^{T}$，那么这个矩阵就是对称矩阵</p>
<ul>
<li>正交矩阵</li>
</ul>
<p>行向量和列向量是分别标准正交(90度)的方阵，即$A^{T}A = AA^{T} = I_n$，又即$A^{-1} = A^{T}$，那么这种方阵就是正交矩阵</p>
<h3 id="2-1、证明题"><a href="#2-1、证明题" class="headerlink" title="2.1、证明题"></a>2.1、证明题</h3><p>通过LaTeX公式，结合上面所述概念，假设$A^{-1}$存在的情况下，证明$Ax=b$的解$x={A}^{-1}{b}$</p>
<p>回答：</p>
<p>因为$A^{-1}$存在 所以 等式两边同乘 $A^{-1}$           $A^{-1}Ax = A^{-1}b$</p>
<p>又因为$A^{-1} A = I_n$  所以 等式为 $I_nx = A^{-1}b$</p>
<p>同时 $I_n$为单位矩阵所以 $x = A^{-1}b$</p>
<p>$a^2$</p>
<h3 id="2-2、-计算题"><a href="#2-2、-计算题" class="headerlink" title="2.2、 计算题"></a>2.2、 计算题</h3><h4 id="2-2-1"><a href="#2-2-1" class="headerlink" title="2.2.1"></a>2.2.1</h4><p>通过numpy计算，再次验证2.1证明题</p>
<script type="math/tex; mode=display">
\begin{cases}
y = 2x + 1\\\\
y = \frac{1}{10}x+6
\end{cases}</script><p>并用LaTeX公式写出$A^{-1}$是多少（小数分数皆可）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.2.1 TODO</span></span><br><span class="line">x = np.mat([[<span class="number">-1</span>,<span class="number">2</span>],[<span class="number">-1</span>,<span class="number">0.1</span>]])</span><br><span class="line">y = np.mat([<span class="number">-1</span>,<span class="number">-6</span>]).T</span><br><span class="line">r = np.linalg.solve(x,y)</span><br></pre></td></tr></table></figure>
<h4 id="2-2-2"><a href="#2-2-2" class="headerlink" title="2.2.2"></a>2.2.2</h4><p>1、请用numpy（或自写算法）实现一个6x6的对角矩阵，矩阵的对角线由3至8（含8）组成。</p>
<p>2、计算第一问生成的对角矩阵与向量$[6,7,1,2,5,9]^{T}$的乘积</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.2.2 TODO</span></span><br><span class="line">diagonal = np.diag(np.arange(<span class="number">3</span>,<span class="number">9</span>))</span><br><span class="line">vector = np.array([<span class="number">6</span>,<span class="number">7</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">9</span>])</span><br><span class="line">diagonal.dot(vector.T)</span><br></pre></td></tr></table></figure>
<pre><code>array([18, 28,  5, 12, 35, 72])
</code></pre><h2 id="3、迹运算"><a href="#3、迹运算" class="headerlink" title="3、迹运算"></a>3、迹运算</h2><p>迹运算返回的是矩阵对角元素的和，如图所示<br><img src="/2019/02/16/Linear-Algebra/matrix.png" width="360"><br>写成数学公式为：</p>
<script type="math/tex; mode=display">\large Tr(A) = \sum_{i}A_{i,i}</script><p><strong>说明题：</strong></p>
<p>使用numpy验证</p>
<script type="math/tex; mode=display">
\large Tr(ABC) = Tr(CAB) = Tr(BCA)</script><p>其中</p>
<script type="math/tex; mode=display">
A=
\begin{bmatrix}
    7 & 6 \\\\
    29 & 3
\end{bmatrix}</script><script type="math/tex; mode=display">
B=
\begin{bmatrix}
    2 & -8 \\\\
    9 & 10
\end{bmatrix}</script><script type="math/tex; mode=display">
C=
\begin{bmatrix}
    2 & 17 \\\\
    1 & 5
\end{bmatrix}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3 TODO</span></span><br><span class="line">A = np.array([[<span class="number">7</span>,<span class="number">6</span>],[<span class="number">29</span>,<span class="number">3</span>]])</span><br><span class="line">B = np.array([[<span class="number">2</span>,<span class="number">-8</span>],[<span class="number">9</span>,<span class="number">10</span>]])</span><br><span class="line">C = np.array([[<span class="number">2</span>,<span class="number">17</span>],[<span class="number">1</span>,<span class="number">5</span>]])</span><br><span class="line">ABC = A.dot(B).dot(C)</span><br><span class="line">CAB = C.dot(A).dot(B)</span><br><span class="line">BCA = B.dot(C).dot(A)</span><br><span class="line">print(<span class="string">'ABC:&#123;0&#125;,CAB:&#123;1&#125;,BCA:&#123;2&#125;'</span>.format(np.trace(ABC),np.trace(CAB),np.trace(BCA)))</span><br></pre></td></tr></table></figure>
<pre><code>ABC:575,CAB:575,BCA:575
</code></pre><h2 id="4、衡量向量以及矩阵的大小：范数与条件数"><a href="#4、衡量向量以及矩阵的大小：范数与条件数" class="headerlink" title="4、衡量向量以及矩阵的大小：范数与条件数"></a>4、衡量向量以及矩阵的大小：范数与条件数</h2><h3 id="范数的定义"><a href="#范数的定义" class="headerlink" title="范数的定义"></a>范数的定义</h3><p>在线性代数等数学分支中，范数（Norm）是一个函数，其给予某向量空间（或矩阵）中的每个向量以长度或称之为大小。对于零向量，其长度为零。直观的说，向量或矩阵的范数越大，则我们可以说这个向量或矩阵也就越大。有时范数有很多更为常见的叫法，如绝对值其实便是一维向量空间中实数或复数的范数，范数的一般化定义：设$p\ge 1$，p-norm用以下来表示</p>
<script type="math/tex; mode=display">\large {\Vert x \Vert}_{p} =  \lgroup {\sum_{i}{\vert x_i \vert}^p }\rgroup ^{\frac{1}{p}}</script><p>此处，当p=1时，我们称之曼哈顿范数(Manhattan Norm)。其来源是曼哈顿的出租车司机在四四方方的曼哈顿街道中从一点到另一点所需要走过的距离。也即我们所要讨论的L1范数。其表示某个向量中所有元素绝对值的和。 而当p=2时，则是我们最为常见的Euclidean norm。也称为Euclidean distance，中文叫欧几里得范数，也即我们要讨论的L2范数，他也经常被用来衡量向量的大小。 而当p=0时，严格的说此时p已不算是范数了，L0范数是指向量中非0的元素的个数，但很多人仍然称之为L0范数（Zero norm零范数）。 这三个范数有很多非常有意思的特征，尤其是在机器学习中的正则化（Regularization）以及稀疏编码（Sparse Coding）有非常有趣的应用，这个在进阶课程可以做更深入的了解。</p>
<p><strong>L0 范数</strong></p>
<script type="math/tex; mode=display">\large \Vert x \Vert = \sqrt[0]{\sum_i x_i^0} = \#(i|x_i \neq0)</script><p><strong>L1 范数</strong></p>
<script type="math/tex; mode=display">\large {\Vert x \Vert}_{1} =  \lgroup {\sum_{i}{\vert x_i \vert} }\rgroup</script><p><strong>L2 范数</strong></p>
<script type="math/tex; mode=display">\large {\Vert x \Vert}_{2} =  \lgroup {\sum_{i}{\vert x_i \vert}^2 }\rgroup ^{\frac{1}{2}}</script><p>另外这里还存在特例：<br> 当 $ p -&gt; \infty $ 时，我们称之为 $ L^{\infty} $范数，也被称为“maximum norm（max范数）”，这个范数表示向量中具有最大幅度的元素的绝对值：</p>
<script type="math/tex; mode=display">\large {\Vert x \Vert}^{\infty} =  \max_{i}{\vert x_i \vert}</script><p><a href="http://t.cn/RINHvvt" target="_blank" rel="noopener">以上资料部分参考wiki</a></p>
<h3 id="4-1、计算向量的范数"><a href="#4-1、计算向量的范数" class="headerlink" title="4.1、计算向量的范数"></a>4.1、计算向量的范数</h3><p>编写一个函数来计算一下向量的各种范数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO 实现这里向量范数计算的函数，要求可以计算p = 0,1,2,3 ... 无穷 情况下的范数</span></span><br><span class="line"></span><br><span class="line"><span class="string">""" 计算向量的范数</span></span><br><span class="line"><span class="string">    参数</span></span><br><span class="line"><span class="string">        x: 向量 numpy数组 或者list数组</span></span><br><span class="line"><span class="string">        p: 范数的阶，int型整数或者None</span></span><br><span class="line"><span class="string">        infty: 是否计算max范数，bool型变量，True的时候表示计算max范数，False的时候计算p范数</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    返回</span></span><br><span class="line"><span class="string">        向量的范数，float类型数值</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    hint:</span></span><br><span class="line"><span class="string">        1.你需要首先判断infty是True or False, 然后判断p 是否为零</span></span><br><span class="line"><span class="string">        2.注意int类型变量在计算时候需要规整为float类型</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_Norm</span><span class="params">(x, p = <span class="number">2</span>, infty = False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> infty == <span class="keyword">False</span>:</span><br><span class="line">        answer = np.linalg.norm(x,float(p))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        answer = np.linalg.norm(x,np.Inf)</span><br><span class="line">    <span class="keyword">return</span> answer</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%run -i -e test.py LinearRegressionTestCase.test_calc_Norm</span><br></pre></td></tr></table></figure>
<pre><code>.
----------------------------------------------------------------------
Ran 1 test in 0.004s

OK
</code></pre><h3 id="4-2、计算矩阵的范数"><a href="#4-2、计算矩阵的范数" class="headerlink" title="4.2、计算矩阵的范数"></a>4.2、计算矩阵的范数</h3><p>我们也需要衡量矩阵的大小，对于矩阵大小的衡量在很多优化问题中是非常重要的。而在深度学习中，最常见的做法是使用Frobenius 范数(Frobenius norm)，也称作矩阵的F范数，其定义如下：</p>
<script type="math/tex; mode=display">\large {\Vert A \Vert}_{F} =  \sqrt {\sum_{i,j}{\vert A_{i,j} \vert}^2 }</script><p>我们这里继续来计算一下F范数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO 实现这里矩阵Frobenius范数计算的函数</span></span><br><span class="line"></span><br><span class="line"><span class="string">""" 计算向量的范数</span></span><br><span class="line"><span class="string">    参数</span></span><br><span class="line"><span class="string">        A: 给定的任意二维矩阵 list或者numpy数组形式</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    返回</span></span><br><span class="line"><span class="string">        矩阵的Frobenius范数，float类型数值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_Frobenius_Norm</span><span class="params">(A)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.linalg.norm(A,<span class="string">'fro'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%run -i -e test.py LinearRegressionTestCase.test_calc_Frobenius_Norm</span><br></pre></td></tr></table></figure>
<pre><code>.
----------------------------------------------------------------------
Ran 1 test in 0.001s

OK
</code></pre><h3 id="4-3、计算矩阵的条件数"><a href="#4-3、计算矩阵的条件数" class="headerlink" title="4.3、计算矩阵的条件数"></a>4.3、计算矩阵的条件数</h3><p>矩阵的条件数(condition number)是矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，我们这里为了简化条件，这里只考虑矩阵是奇异矩阵的时候，如何计算以及理解条件数(condition number):</p>
<p>当矩阵A为奇异矩阵的时候，condition number为无限大；当矩阵A非奇异的时候，我们定义condition number如下：</p>
<script type="math/tex; mode=display">\large \kappa{(A)} =  {\Vert A \Vert}_F {\Vert A^{-1} \Vert}_F</script><p><a href="https://zh.wikipedia.org/wiki/%E9%9D%9E%E5%A5%87%E5%BC%82%E6%96%B9%E9%98%B5" target="_blank" rel="noopener">奇异矩阵，非奇异矩阵</a></p>
<p>计算矩阵的条件数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" 计算矩阵的条件数</span></span><br><span class="line"><span class="string">    参数</span></span><br><span class="line"><span class="string">        A: 给定的任意二维矩阵 list或者numpy数组形式</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    返回</span></span><br><span class="line"><span class="string">        矩阵的condition number,</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_Condition_Number</span><span class="params">(A)</span>:</span></span><br><span class="line">    A = np.mat(A)</span><br><span class="line">    <span class="keyword">return</span> np.dot(np.linalg.norm(A,<span class="string">'fro'</span>),np.linalg.norm(A.I,<span class="string">'fro'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%run -i -e test.py LinearRegressionTestCase.test_calc_Condition_Number</span><br></pre></td></tr></table></figure>
<pre><code>/opt/anaconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py:68: PendingDeprecationWarning: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray.
  return matrix(data, dtype=dtype, copy=False)
.
----------------------------------------------------------------------
Ran 1 test in 0.017s

OK
</code></pre><h3 id="选做-4-4、条件数的理解与应用"><a href="#选做-4-4、条件数的理解与应用" class="headerlink" title="(选做)4.4、条件数的理解与应用"></a>(选做)4.4、条件数的理解与应用</h3><p>a. 有如下两个2*2的非奇异矩阵A和B:</p>
<p>$ A = \begin{bmatrix}<br>     1   &amp;2 \<br>     3   &amp;4 \<br>\end{bmatrix} $ </p>
<p>$ B = \begin{bmatrix}<br>     1   &amp;2 \<br>     2   &amp;4.0001 \<br>\end{bmatrix}<br>$</p>
<p>计算condition number(A), condition number(B);</p>
<p>b. 根据上面构造的矩阵A,B分别计算线性系统方程组的解$w$:</p>
<p>   A $ \begin{bmatrix}w<em>{a1}\w</em>{a2}\ \end{bmatrix} $ = $ \begin{bmatrix}1\2\ \end{bmatrix} $, </p>
<p>   B $ \begin{bmatrix}w<em>{b1}\w</em>{b2}\ \end{bmatrix} $ = $ \begin{bmatrix}1\2\ \end{bmatrix} $,</p>
<p>   A $ \begin{bmatrix}w<em>{a1}\w</em>{a2}\ \end{bmatrix} $ = $ \begin{bmatrix}{1.0001}\{2.0001}\ \end{bmatrix} $, </p>
<p>   B $ \begin{bmatrix}w<em>{b1}\w</em>{b2}\ \end{bmatrix} $ = $ \begin{bmatrix}{1.0001}\{2.0001}\ \end{bmatrix} $.</p>
<p>c. 计算完成之后，比较condition number大小与线性系统稳定性之间的关系，并且给出规律性的总结；</p>
<p>d. <strong>阅读与思考</strong>: 考虑更为通用的一种情况，我们计算一个典型的线性回归系统: </p>
<script type="math/tex; mode=display">Xw = b</script><p>可以简单推导得出其闭式解为：$ w=(X^TX)^{−1}X^Tb $ ，如果 $X^TX$可逆</p>
<p>推导过程： </p>
<p>1.等式两边乘以$X^T$</p>
<script type="math/tex; mode=display">X^TXw = X^Tb</script><p>2.等式两边乘以$(X^TX)^{-1}$</p>
<script type="math/tex; mode=display">(X^TX)^{-1}X^TXw = (X^TX)^{−1}X^Tb</script><p>3.因为$A^{-1}A = I$，两边约去即可得：</p>
<script type="math/tex; mode=display">w=(X^TX)^{−1}X^Tb</script><p>当我们需要拟合的数据X满足数据量远远小于特征数目的时候，也就是X矩阵的行数 &lt;&lt; X矩阵的列数的时候，因为$X^TX$不是奇异矩阵，此时方程组不存在闭式解；那么我们该如何重新构造$X^TX$，使得该闭式解成立？</p>
<p>hint1. 单位矩阵的condition number是最低的，是最为稳定的；</p>
<p>hint2. 如果要使得该系统存在闭式解，那么就必须使得求逆运算是可以进行的，也就是说重新构造的$X^TX$必须是可逆的方阵；</p>
<p>hint3. 重新构造的方式可以是在$X^TX$基础上进行加或者减或者乘除相关矩阵的操作；</p>
<p>一种可行的方式就是：</p>
<script type="math/tex; mode=display">w = (X^TX+\lambda I)^{−1}X^Tb</script><p>实际上我们最为常用的<a href="http://scikit-learn.org/stable/modules/linear_model.html" target="_blank" rel="noopener">Ridge Regression</a>和 L2范数以及condition number之间某种程度上是可以相互推导的：</p>
<p>首先，Ridge Regression的损失函数为：</p>
<script type="math/tex; mode=display">J_w = min({\Vert Xw -y \Vert}^2 + \alpha {\Vert w \Vert}^2)</script><p>我们展开这个损失函数：</p>
<script type="math/tex; mode=display">{\Vert Xw -y \Vert}^2 + \alpha {\Vert w \Vert}^2  =  (Xw -y)^T (Xw-y) + \alpha w^Tw</script><p>由于这里是一个凸函数，我们令导数等于零，即为最小值的解，求导可得：</p>
<script type="math/tex; mode=display">X^T (Xw-y) + \alpha w = 0</script><p>整理即可得到：</p>
<script type="math/tex; mode=display">w = (X^TX+\lambda I)^{−1}X^Tb</script><h2 id="5、SVD"><a href="#5、SVD" class="headerlink" title="5、SVD"></a>5、SVD</h2><p><a href="https://en.wikipedia.org/wiki/Singular-value_decomposition" target="_blank" rel="noopener">SVD</a>是Singular value decomposition的缩写，称为奇异值分解，是分解矩阵的一种方式，会将矩阵分解为奇异向量（singular vector）和奇异值（singular value），分解的意义其实很明确，就是想将一个很大很复杂的矩阵，用更小更简单的几个子矩阵的相乘来表示，这些小矩阵描述的是矩阵的重要的特性。</p>
<p>那么SVD具体的数学表达是什么呢？</p>
<p>假设有一个矩阵C，我们可以将矩阵C分解为三个矩阵的乘积：<br><img src="/2019/02/16/Linear-Algebra/svd.png" width="480"></p>
<script type="math/tex; mode=display">\large C = UDV^{T}</script><p>如果C是一个m x n的矩阵，那么U是一个m x m的矩阵，D是一个m x n的矩阵，V是一个n x n的矩阵，这些小矩阵并不是普普通通的矩阵，U和V都定义为正交矩阵，而D定义为对角矩阵。</p>
<p>SVD最常用的做法就是用来进行特征的降维以及矩阵的低秩重构，例如这里分别取矩阵U、D、VT的前k列，如图示中的白色部分，然后重新计算新的C矩阵，即为k维度下的矩阵重构，这种方法被广泛应用于自然语言处理<a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis" target="_blank" rel="noopener">LSA</a>、推荐系统<a href="https://tech.meituan.com/deep_understanding_of_ffm_principles_and_practices.html" target="_blank" rel="noopener">SVD++,FM,FFM</a>等领域，如有兴趣可以继续参考链接相关资料。<br><img src="/2019/02/16/Linear-Algebra/svd_decompostion.png" width="480"></p>
<p>具体计算UDV的算法不是我们这个项目的关键，我们只需使用numpy得出结果即可，下面的习题，将会带你体会SVD的某一应用场景。</p>
<p>提示：我们会需要使用<a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.linalg.html" target="_blank" rel="noopener">numpy.linalg</a>相关函数。</p>
<h3 id="5-1、使用numpy去计算任意矩阵的奇异值分解："><a href="#5-1、使用numpy去计算任意矩阵的奇异值分解：" class="headerlink" title="5.1、使用numpy去计算任意矩阵的奇异值分解："></a>5.1、使用numpy去计算任意矩阵的奇异值分解：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" 计算任意矩阵的奇异值分解</span></span><br><span class="line"><span class="string">    参数</span></span><br><span class="line"><span class="string">        A: 给定的任意二维矩阵 list或者numpy数组形式 </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    返回</span></span><br><span class="line"><span class="string">        使用numpy.linalg相关函数，直接返回分解之后的矩阵U,D,V</span></span><br><span class="line"><span class="string">        （可以尝试一下使用np.shape一下分解出来的U，D，VT，会发现维度跟我们上面讲解所描述的不同，</span></span><br><span class="line"><span class="string">        暂时不用管他直接返回np求解出的U，D，VT即可）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_svd</span><span class="params">(A)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.linalg.svd(A)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%run -i -e test.py LinearRegressionTestCase.test_calc_svd</span><br></pre></td></tr></table></figure>
<pre><code>.
----------------------------------------------------------------------
Ran 1 test in 0.004s

OK
</code></pre><h3 id="选做-5-2、利用奇异值分解对矩阵进行降维"><a href="#选做-5-2、利用奇异值分解对矩阵进行降维" class="headerlink" title="(选做) 5.2、利用奇异值分解对矩阵进行降维"></a>(选做) 5.2、利用奇异值分解对矩阵进行降维</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TODO 利用SVD进行对于矩阵进行降维</span></span><br><span class="line"></span><br><span class="line"><span class="string">""" 利用SVD进行对于矩阵进行降维</span></span><br><span class="line"><span class="string">    参数</span></span><br><span class="line"><span class="string">        A: 给定的任意二维矩阵 list或者numpy数组形式 shape为(m,n)</span></span><br><span class="line"><span class="string">        topk: 降维的维度 (m,n) -&gt; (m,topk)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    返回</span></span><br><span class="line"><span class="string">        降维后的矩阵 (m, topk)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    hint</span></span><br><span class="line"><span class="string">    1. 对角矩阵D存在一个较为明显的特性，就是D的对角线元素是递减的，这些元素实际上是衡量了所分解的矩阵U,V的列向量的重要性</span></span><br><span class="line"><span class="string">    2. 因此我们常说的svd降维就是利用选取的前topk大的对角线矩阵元素进行构造新的降维矩阵</span></span><br><span class="line"><span class="string">    3. U的按照前topk截取的列向量 * topk截取的对角矩阵 即为新的降维后的矩阵</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_svd_decompostion</span><span class="params">(A, topk = <span class="number">2</span>)</span>:</span></span><br><span class="line">    U,Sig,V = np.linalg.svd(A)</span><br><span class="line">    Sig[<span class="number">1</span>] = topk</span><br><span class="line">    <span class="keyword">return</span> Sig</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%run -i -e test.py LinearRegressionTestCase.test_calc_svd_decompostion</span><br></pre></td></tr></table></figure>
<pre><code>.
----------------------------------------------------------------------
Ran 1 test in 0.004s

OK
</code></pre><h3 id="选做-5-3、利用奇异值分解对矩阵进行降维后重构"><a href="#选做-5-3、利用奇异值分解对矩阵进行降维后重构" class="headerlink" title="(选做) 5.3、利用奇异值分解对矩阵进行降维后重构"></a>(选做) 5.3、利用奇异值分解对矩阵进行降维后重构</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" 利用SVD进行对于矩阵进行降维</span></span><br><span class="line"><span class="string">    参数</span></span><br><span class="line"><span class="string">        A: 给定的任意二维矩阵 list或者numpy数组形式 shape为(m,n)</span></span><br><span class="line"><span class="string">        topk: 降维的维度 (m,n) -&gt; (m,topk)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    返回</span></span><br><span class="line"><span class="string">        降维重构后的矩阵 (m, n)</span></span><br><span class="line"><span class="string">    hint</span></span><br><span class="line"><span class="string">        这里除了降维矩阵外，另外一个较为常见的应用就是对矩阵进行重构，具体的做法类似前面的思路</span></span><br><span class="line"><span class="string">        1. 选取对应的U，D，V的topk向量</span></span><br><span class="line"><span class="string">        2. U的按照前topk截取的列向量 * topk截取的对角矩阵 * V^T按照前topk截取的行向量(注意这里是V的转置,因为分解得到的是V^T)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_svd_reconsitution</span><span class="params">(A, topk = <span class="number">2</span>)</span>:</span></span><br><span class="line">    A = np.array(A)</span><br><span class="line">    U,Sig,V = np.linalg.svd(A)</span><br><span class="line">    Sig[<span class="number">1</span>] = topk</span><br><span class="line">    </span><br><span class="line">    S = np.zeros((<span class="number">5</span>,<span class="number">8</span>))</span><br><span class="line">    S[:<span class="number">5</span>,:<span class="number">5</span>] = np.diag(Sig)</span><br><span class="line">    A_conv = np.dot(np.dot(A.T,U),S)</span><br><span class="line">    A = np.dot(np.dot(U,S),V)</span><br><span class="line">    <span class="keyword">return</span> A</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%run -i -e test.py LinearRegressionTestCase.test_calc_svd_reconsitution</span><br></pre></td></tr></table></figure>
<pre><code>.
----------------------------------------------------------------------
Ran 1 test in 0.001s

OK
</code></pre><h3 id="选做-5-4、计算不同降维大小重构矩阵的Frobenius范数损失"><a href="#选做-5-4、计算不同降维大小重构矩阵的Frobenius范数损失" class="headerlink" title="(选做) 5.4、计算不同降维大小重构矩阵的Frobenius范数损失"></a>(选做) 5.4、计算不同降维大小重构矩阵的Frobenius范数损失</h3><p>定义矩阵$A$以及使用SVD降维（降维大小为k)分解后的重构矩阵$A_k$，则这里的F范数损失定义如下：</p>
<script type="math/tex; mode=display">\Large Loss_{F} = {\Vert A - A_k \Vert}_F</script><p>这里需要编码求出对于给定的矩阵A 分别在不同的降维幅度下重构后的F范数损失，并且作出损失大小随着降维大小的变化图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 不要修改这里！</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston  </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline </span><br><span class="line">A = load_boston()[<span class="string">'data'</span>]  <span class="comment"># 载入boston house 数据集</span></span><br><span class="line">print(A.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(506, 13)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">loss_hist = []</span><br><span class="line"><span class="keyword">for</span> topk <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">13</span>):</span><br><span class="line">    <span class="comment"># 5.4 TODO </span></span><br><span class="line">    <span class="comment">### 1.计算相应的SVD topk降维后的重构矩阵，需实现calc_svd_reconsitution</span></span><br><span class="line">    <span class="comment">### 2.计算对应的F范数损失，并存储loss放入loss_hist列表中</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 画出F损失随着降维大小的变化图</span></span><br><span class="line"><span class="comment">### x坐标为对应的降维大小，y坐标为对应的F损失</span></span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">13</span>),loss_hist,<span class="string">'r--'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'decomposition size'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'F Loss'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>  File &quot;&lt;ipython-input-24-309fc4d50cf4&gt;&quot;, line 10
    plt.plot(range(1,13),loss_hist,&#39;r&#39;)
      ^
IndentationError: expected an indented block
</code></pre><h3 id="5-5、SVD的有趣应用"><a href="#5-5、SVD的有趣应用" class="headerlink" title="5.5、SVD的有趣应用"></a>5.5、SVD的有趣应用</h3><p>为了这个习题我准备了两张图，参见项目文件夹下的test_girl.jpg和test_boy.jpeg，自选一张，你需要</p>
<ul>
<li>需要使用 <code>PIL</code> 加载你所选择的图像（<a href="https://pillow.readthedocs.io/en/latest/reference/Image.html" target="_blank" rel="noopener">文档</a>）,所以记得导入需要的包（模块）</li>
<li>使用Image的<a href="https://pillow.readthedocs.io/en/latest/reference/Image.html#PIL.Image.Image.convert" target="_blank" rel="noopener">convert方法</a>将图像变为灰度图</li>
<li>将convert后的结果转换成np.array,需用到<a href="https://pillow.readthedocs.io/en/latest/reference/Image.html#PIL.Image.Image.getdata" target="_blank" rel="noopener">Image.getdata方法</a>来读取图片每个pixel的数据，特别注意一下，对于彩色的图来说，即使我们转为了灰度图，但每一个pixel还是由RGB三个维度组成，所以在getdata时，band需要设定为某一个颜色index，比如band = 0，这样只用R这个维度的数据。用这个方法来保证图片的每个pixel只占有一个单元的空间。</li>
<li>因为我们转np.array时破坏了原有图形的样子，变成了一个一维数据，我们要将转换后的np.array恢复到图片应有的size，转换后，可以shape确认下是否与最开始转出的灰度图的size一致，注意图的size是（宽，高），而宽对应array.shape的应该是列，而高对应的是行，别弄反了。</li>
<li>使用上方实现的calc_svd函数计算上一步计算出的np.array数据，赋值给变量：U,D,VT</li>
<li>打印出U,D,VT的shape形状，尤其注意观察D的shape</li>
<li>在U，VT，D变量成功实现的情况下，运行测试程序看效果</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5.5 TODO</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">im = Image.open(<span class="string">'test_girl.jpg'</span>).convert(<span class="string">'LA'</span>)</span><br><span class="line">im_array = np.array(im.getdata(band=<span class="number">0</span>))</span><br><span class="line">im_array = im_array.reshape(im.size[<span class="number">1</span>],im.size[<span class="number">0</span>])</span><br><span class="line">U,D,VT = calc_svd(im_array)</span><br><span class="line">display(U.shape)</span><br><span class="line">display(D.shape)</span><br><span class="line">display(VT.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(600, 600)



(600,)



(750, 750)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#请在U，D，V变量完成的情况下调用此测试程序，不要修改此处</span></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> i,topk <span class="keyword">in</span> enumerate([<span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">50</span>]):</span><br><span class="line">    reconstimg = np.matrix(U[:, :topk]) * np.diag(D[:topk]) * np.matrix(VT[:topk, :])</span><br><span class="line">    plt.subplot(<span class="number">231</span>+i)</span><br><span class="line">    plt.imshow(reconstimg, cmap=<span class="string">'gray'</span>)</span><br><span class="line">    title = <span class="string">"n = %s"</span> % ((i+<span class="number">1</span>)*<span class="number">5</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/16/Linear-Algebra/output_51_0.png" alt="png"></p>
<p>相关继续深入学习的资料：</p>
<ol>
<li><a href="http://freemind.pluskid.org/series/mlopt/" target="_blank" rel="noopener">机器学习与优化</a></li>
<li><a href="https://www.zhihu.com/question/40043805/answer/138429562" target="_blank" rel="noopener">PCA与SVD的区别</a></li>
<li><a href="https://www.cnblogs.com/pinard/p/6251584.html" target="_blank" rel="noopener">SVD在降维中的应用</a></li>
<li><a href="https://blog.csdn.net/pipisorry/article/details/42560331" target="_blank" rel="noopener">SVD在自然语言处理中的应用</a></li>
<li><a href="https://tech.meituan.com/deep_understanding_of_ffm_principles_and_practices.html" target="_blank" rel="noopener">SVD在推荐系统中的应用</a></li>
<li><a href="https://web.stanford.edu/~hastie/ElemStatLearn//" target="_blank" rel="noopener">《Elements of Statistical Learning》Trevor Hastie, Robert Tibshirani, and Jerome Friedman</a></li>
</ol>

    </div>
    
    <div class="post-footer">
        <div>
            
                转载声明：商业转载请联系作者获得授权,非商业转载请注明出处 © <a href target="_blank">Lucifer</a>
            
        </div>
        <div>
            
        </div>
    </div>
</article>

<div class="article-nav prev-next-wrap clearfix">
    
        <a href="/2019/02/16/Explore-Movie-Dataset/" class="pre-post btn btn-default" title="Explore Movie Dataset">
            <i class="fa fa-angle-left fa-fw"></i><span class="hidden-lg">上一篇</span>
            <span class="hidden-xs">Explore Movie Dataset</span>
        </a>
    
    
        <a href="/2019/02/16/image-classifier-project/" class="next-post btn btn-default" title="image classifier project">
            <span class="hidden-lg">下一篇</span>
            <span class="hidden-xs">image classifier project</span><i class="fa fa-angle-right fa-fw"></i>
        </a>
    
</div>


    <div id="comments">
        
	
<div id="lv-container" data-id="city" data-uid="MTAyMC8zMzA1MS85NjEz">
  <script type="text/javascript">
     (function(d, s) {
         var j, e = d.getElementsByTagName(s)[0];
         if (typeof LivereTower === 'function') { return; }
         j = d.createElement(s);
         j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
         j.async = true;
         e.parentNode.insertBefore(j, e);
     })(document, 'script');
  </script>
</div>


    </div>





                </main>
                
                    <aside id="article-toc" role="navigation" class="col-md-4">
    <div class="widget">
        <h3 class="title">Table of Contents</h3>
        
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#线性代数：机器学习背后的优化原理"><span class="toc-text">线性代数：机器学习背后的优化原理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#准备工作"><span class="toc-text">准备工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1、标量，向量，矩阵，张量"><span class="toc-text">1、标量，向量，矩阵，张量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1、基本运算并绘图"><span class="toc-text">1.1、基本运算并绘图</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-1"><span class="toc-text">1.1.1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-2"><span class="toc-text">1.1.2</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2、说明题"><span class="toc-text">1.2、说明题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-1"><span class="toc-text">1.2.1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-2"><span class="toc-text">1.2.2</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、特殊矩阵"><span class="toc-text">2、特殊矩阵</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1、证明题"><span class="toc-text">2.1、证明题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2、-计算题"><span class="toc-text">2.2、 计算题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-1"><span class="toc-text">2.2.1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2"><span class="toc-text">2.2.2</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、迹运算"><span class="toc-text">3、迹运算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4、衡量向量以及矩阵的大小：范数与条件数"><span class="toc-text">4、衡量向量以及矩阵的大小：范数与条件数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#范数的定义"><span class="toc-text">范数的定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1、计算向量的范数"><span class="toc-text">4.1、计算向量的范数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2、计算矩阵的范数"><span class="toc-text">4.2、计算矩阵的范数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3、计算矩阵的条件数"><span class="toc-text">4.3、计算矩阵的条件数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#选做-4-4、条件数的理解与应用"><span class="toc-text">(选做)4.4、条件数的理解与应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5、SVD"><span class="toc-text">5、SVD</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1、使用numpy去计算任意矩阵的奇异值分解："><span class="toc-text">5.1、使用numpy去计算任意矩阵的奇异值分解：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#选做-5-2、利用奇异值分解对矩阵进行降维"><span class="toc-text">(选做) 5.2、利用奇异值分解对矩阵进行降维</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#选做-5-3、利用奇异值分解对矩阵进行降维后重构"><span class="toc-text">(选做) 5.3、利用奇异值分解对矩阵进行降维后重构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#选做-5-4、计算不同降维大小重构矩阵的Frobenius范数损失"><span class="toc-text">(选做) 5.4、计算不同降维大小重构矩阵的Frobenius范数损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5、SVD的有趣应用"><span class="toc-text">5.5、SVD的有趣应用</span></a></li></ol></li></ol></li></ol>
        
    </div>
</aside>

                
            </div>
        </div>
    </section>
    <footer class="main-footer">
    <div class="container">
        <div class="row">
        </div>
    </div>
</footer>

<a id="back-to-top" class="icon-btn hide">
	<i class="fa fa-chevron-up"></i>
</a>




    <div class="copyright">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="busuanzi">
    
        Total:
        <strong id="busuanzi_value_site_pv">
            <i class="fa fa-spinner fa-spin"></i>
        </strong>
        &nbsp; | &nbsp;
        Visitors:
        <strong id="busuanzi_value_site_uv">
            <i class="fa fa-spinner fa-spin"></i>
        </strong>
    
</div>

            </div>
            <div class="col-sm-12">
                <span>Copyright &copy; 2018
                </span> |
                <span>
                    Powered by <a href="//hexo.io" class="copyright-links" target="_blank" rel="nofollow">Hexo</a>
                </span> |
                <span>
                    Theme by <a href="//github.com/shenliyang/hexo-theme-snippet.git" class="copyright-links" target="_blank" rel="nofollow">Snippet</a>
                </span>
            </div>
        </div>
    </div>
</div>




    <script src="/assets/tagcanvas.min.js?rev=2.9"></script>
    <script>
        var tagOption = {
            textColour: '#444', // 字体颜色
            outlineMethod: 'block', // 选中模式
            outlineColour: '#FFDAB9', // 选中模式的颜色
            interval: 30 || 30, // 动画帧之间的时间间隔，值越大，转动幅度越大
            textHeight: 13,
            outlineRadius: 3,
            freezeActive: true || '', // 选中的标签是否继续滚动
            frontSelect: true || '', // 不选标签云后部的标签
            initial: [0.1, -0.1],
            depth: 0.5,
            decel: 0.95,
            maxSpeed: 0.03,
            reverse: true || '', // 是否反向触发
            fadeIn: 500, // 进入动画时间
            wheelZoom: false || '' // 是否启用鼠标滚轮
        }
        TagCanvas.Start('tag-cloud-3d','',tagOption);
    </script>



    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<script src="/js/app.js?rev=@@hash"></script>

</body>
</html>