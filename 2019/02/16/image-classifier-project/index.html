<!DOCTYPE HTML>
<html lang="null">
<head><meta name="generator" content="Hexo 3.8.0">
    <!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta http-equiv="Cache-Control" content="no-siteapp">
    <meta http-equiv="Cache-Control" content="no-transform">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable" content="单身程序员的小窝">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    <meta name="browsermode" content="application">
    <meta name="screen-orientation" content="portrait">
    <link rel="dns-prefetch" href="http://yoursite.com">
    <!--SEO-->

    <meta name="keywords" content="Deep Learning">


    <meta name="description" content="开发 AI 应用未来，AI 算法在日常生活中的应用将越来越广泛。例如，你可能想要在智能手机应用中包含图像分类器。为此，在整个应用架构中，你将使用一个用成百上千个图像训练过的深度学习模型。未来的软...">



<meta name="robots" content="all">
<meta name="google" content="all">
<meta name="googlebot" content="all">
<meta name="verify" content="all">

    <!--Title-->


<title>image classifier project | 单身程序员的小窝</title>


    <link rel="alternate" href="/atom.xml" title="单身程序员的小窝" type="application/atom+xml">


    <link rel="icon" href="/favicon.ico">

    



<link rel="stylesheet" href="/css/bootstrap.min.css?rev=3.3.7">
<link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/style.css?rev=@@hash">




    
	<div class="hide">
		<script type="text/javascript">
			var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan class='cnzz_stat_icon_1263868967 hide' %3E%3Cscript%20src%3D%22https%3A%2F%2Fs95.cnzz.com%2Fz_stat.php%3Fweb_id%3D1272564536%22%3E%3C%2Fscript%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s19.cnzz.com/z_stat.php%3Fid%3D1263868967%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
		</script>
	</div>






    

</head>


</html>
<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<div style="ie">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->

<body>
    <header class="main-header" style="background-image:url(http://snippet.shenliyang.com/img/banner2.jpg)">
    <div class="main-header-box">
        <a class="header-avatar" href="/" title="Jindong">
            <img src="/img/avatar.jpg" alt="logo头像" class="img-responsive center-block">
        </a>
        <div class="branding">
        	<!--<h2 class="text-hide">Snippet主题,从未如此简单有趣</h2>-->
            
                <h2> 科技让复杂的世界更简单! </h2>
            
    	</div>
    </div>
</header>
    <nav class="main-navigation">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="navbar-header"><span class="nav-toggle-button collapsed pull-right" data-toggle="collapse" data-target="#main-menu" id="mnav">
                    <span class="sr-only"></span>
                        <i class="fa fa-bars"></i>
                    </span>
                    <a class="navbar-brand" href="http://yoursite.com">单身程序员的小窝</a>
                </div>
                <div class="collapse navbar-collapse" id="main-menu">
                    <ul class="menu">
                        
                            <li role="presentation" class="text-center">
                                <a href="/"><i class="fa "></i>Home</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/机器学习/"><i class="fa "></i>MachineLearning</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/深度学习/"><i class="fa "></i>DeepLearning</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/tools/"><i class="fa "></i>Tools</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/archives/"><i class="fa "></i>History</a>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</nav>
    <section class="content-wrap">
        <div class="container">
            <div class="row">
                <main class="col-md-8 main-content m-post">
                    <p id="process"></p>
<article class="post">
    <div class="post-head">
        <h1 id="image classifier project">
            
	            image classifier project
            
        </h1>
        <div class="post-meta">
    
        <span class="categories-meta fa-wrap">
            <i class="fa fa-folder-open-o"></i>
            <a class="category-link" href="/categories/深度学习/">深度学习</a>
        </span>
    

    
        <span class="fa-wrap">
            <i class="fa fa-tags"></i>
            <span class="tags-meta">
                
                    <a class="tag-link" href="/tags/Deep-Learning/">Deep Learning</a>
                
            </span>
        </span>
    

    
        
        <span class="fa-wrap">
            <i class="fa fa-clock-o"></i>
            <span class="date-meta">2019/02/16</span>
        </span>
        
    
</div>
            
            
    </div>
    
    <div class="post-body post-content">
        <h1 id="开发-AI-应用"><a href="#开发-AI-应用" class="headerlink" title="开发 AI 应用"></a>开发 AI 应用</h1><p>未来，AI 算法在日常生活中的应用将越来越广泛。例如，你可能想要在智能手机应用中包含图像分类器。为此，在整个应用架构中，你将使用一个用成百上千个图像训练过的深度学习模型。未来的软件开发很大一部分将是使用这些模型作为应用的常用部分。</p>
<p>在此项目中，你将训练一个图像分类器来识别不同的花卉品种。可以想象有这么一款手机应用，当你对着花卉拍摄时，它能够告诉你这朵花的名称。在实际操作中，你会训练此分类器，然后导出它以用在你的应用中。我们将使用<a href="http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html" target="_blank" rel="noopener">此数据集</a>，其中包含 102 个花卉类别。你可以在下面查看几个示例。<br><img src="/2019/02/16/image-classifier-project/Flowers.png" alt="png"></p>
<p>该项目分为多个步骤：</p>
<ul>
<li>加载和预处理图像数据集</li>
<li>用数据集训练图像分类器</li>
<li>使用训练的分类器预测图像内容</li>
</ul>
<p>我们将指导你完成每一步，你将用 Python 实现这些步骤。</p>
<p>完成此项目后，你将拥有一个可以用任何带标签图像的数据集进行训练的应用。你的网络将学习花卉，并成为一个命令行应用。但是，你对新技能的应用取决于你的想象力和构建数据集的精力。例如，想象有一款应用能够拍摄汽车，告诉你汽车的制造商和型号，然后查询关于该汽车的信息。构建你自己的数据集并开发一款新型应用吧。</p>
<p>首先，导入你所需的软件包。建议在代码开头导入所有软件包。当你创建此 notebook 时，如果发现你需要导入某个软件包，确保在开头导入该软件包。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Imports here</span></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'retina'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms, models</span><br></pre></td></tr></table></figure>
<h2 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h2><p>在此项目中，你将使用 <code>torchvision</code> 加载数据（<a href="http://pytorch.org/docs/master/torchvision/transforms.html#" target="_blank" rel="noopener">文档</a>）。数据应该和此 notebook 一起包含在内，否则你可以<a href="https://s3.amazonaws.com/content.udacity-data.com/nd089/flower_data.tar.gz" target="_blank" rel="noopener">在此处下载数据</a>。数据集分成了三部分：训练集、验证集和测试集。对于训练集，你需要变换数据，例如随机缩放、剪裁和翻转。这样有助于网络泛化，并带来更好的效果。你还需要确保将输入数据的大小调整为 224x224 像素，因为预训练的网络需要这么做。</p>
<p>验证集和测试集用于衡量模型对尚未见过的数据的预测效果。对此步骤，你不需要进行任何缩放或旋转变换，但是需要将图像剪裁到合适的大小。</p>
<p>对于所有三个数据集，你都需要将均值和标准差标准化到网络期望的结果。均值为 <code>[0.485, 0.456, 0.406]</code>，标准差为 <code>[0.229, 0.224, 0.225]</code>。这样使得每个颜色通道的值位于 -1 到 1 之间，而不是 0 到 1 之间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_dir = <span class="string">'train'</span></span><br><span class="line">valid_dir = <span class="string">'valid'</span></span><br><span class="line">test_dir = <span class="string">'test'</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Define your transforms for the training, validation, and testing sets</span></span><br><span class="line">data_dir = <span class="string">'flowers/'</span></span><br><span class="line">data_transforms = transforms.Compose([</span><br><span class="line">    transforms.RandomRotation(<span class="number">30</span>),</span><br><span class="line">    transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                        [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">])</span><br><span class="line">test_transforms = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">255</span>),</span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                        [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Load the datasets with ImageFolder</span></span><br><span class="line">image_datasets = datasets.ImageFolder(data_dir + train_dir, transform=data_transforms)</span><br><span class="line">image_datasets_test = datasets.ImageFolder(data_dir + test_dir, transform=test_transforms)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Using the image datasets and the trainforms, define the dataloaders</span></span><br><span class="line">dataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=<span class="number">64</span>, shuffle=<span class="keyword">True</span>)</span><br><span class="line">test_dataloaders = torch.utils.data.DataLoader(image_datasets_test, batch_size=<span class="number">32</span>)</span><br></pre></td></tr></table></figure>
<h3 id="标签映射"><a href="#标签映射" class="headerlink" title="标签映射"></a>标签映射</h3><p>你还需要加载从类别标签到类别名称的映射。你可以在文件 <code>cat_to_name.json</code> 中找到此映射。它是一个 JSON 对象，可以使用 <a href="https://docs.python.org/2/library/json.html" target="_blank" rel="noopener"><code>json</code> 模块</a>读取它。这样可以获得一个从整数编码的类别到实际花卉名称的映射字典。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'cat_to_name.json'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    cat_to_name = json.load(f)</span><br></pre></td></tr></table></figure>
<h1 id="构建和训练分类器"><a href="#构建和训练分类器" class="headerlink" title="构建和训练分类器"></a>构建和训练分类器</h1><p>数据准备好后，就开始构建和训练分类器了。和往常一样，你应该使用 <code>torchvision.models</code> 中的某个预训练模型获取图像特征。使用这些特征构建和训练新的前馈分类器。</p>
<p>这部分将由你来完成。如果你想与他人讨论这部分，欢迎与你的同学讨论！你还可以在论坛上提问或在工作时间内咨询我们的课程经理和助教导师。</p>
<p>请参阅<a href="https://review.udacity.com/#!/rubrics/1663/view" target="_blank" rel="noopener">审阅标准</a>，了解如何成功地完成此部分。你需要执行以下操作：</p>
<ul>
<li>加载<a href="http://pytorch.org/docs/master/torchvision/models.html" target="_blank" rel="noopener">预训练的网络</a>（如果你需要一个起点，推荐使用 VGG 网络，它简单易用）</li>
<li>使用 ReLU 激活函数和丢弃定义新的未训练前馈网络作为分类器</li>
<li>使用反向传播训练分类器层，并使用预训练的网络获取特征</li>
<li>跟踪验证集的损失和准确率，以确定最佳超参数</li>
</ul>
<p>我们在下面为你留了一个空的单元格，但是你可以使用多个单元格。建议将问题拆分为更小的部分，并单独运行。检查确保每部分都达到预期效果，然后再完成下个部分。你可能会发现，当你实现每部分时，可能需要回去修改之前的代码，这很正常！</p>
<p>训练时，确保仅更新前馈网络的权重。如果一切构建正确的话，验证准确率应该能够超过 70%。确保尝试不同的超参数（学习速率、分类器中的单元、周期等），寻找最佳模型。保存这些超参数并用作项目下个部分的默认值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Build and train your network</span></span><br><span class="line">model = models.vgg16(pretrained=<span class="keyword">True</span>)</span><br><span class="line">model</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">classifier = nn.Sequential(OrderedDict([</span><br><span class="line">    (<span class="string">'fc1'</span>, nn.Linear(<span class="number">25088</span>,<span class="number">1024</span>)),</span><br><span class="line">    (<span class="string">'relu'</span>, nn.ReLU()),</span><br><span class="line">    (<span class="string">'fc2'</span>, nn.Linear(<span class="number">1024</span>,<span class="number">102</span>)),</span><br><span class="line">    (<span class="string">'output'</span>, nn.LogSoftmax(dim=<span class="number">1</span>))</span><br><span class="line">]))</span><br><span class="line"></span><br><span class="line">model.classifier = classifier</span><br><span class="line"></span><br><span class="line">criterion = nn.NLLLoss()</span><br><span class="line">optimizer = optim.Adam(model.classifier.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Downloading: &quot;https://download.pytorch.org/models/vgg16-397923af.pth&quot; to /root/.torch/models/vgg16-397923af.pth
100%|██████████| 553433881/553433881 [00:34&lt;00:00, 16147158.40it/s]
</code></pre><h2 id="测试网络"><a href="#测试网络" class="headerlink" title="测试网络"></a>测试网络</h2><p>建议使用网络在训练或验证过程中从未见过的测试数据测试训练的网络。这样，可以很好地判断模型预测全新图像的效果。用网络预测测试图像，并测量准确率，就像验证过程一样。如果模型训练良好的话，你应该能够达到大约 70% 的准确率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Do validation on the test set</span></span><br><span class="line">epochs = <span class="number">3</span></span><br><span class="line">print_every = <span class="number">40</span></span><br><span class="line">steps = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">model.to(<span class="string">'cuda'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    running_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> li, (images, labels) <span class="keyword">in</span> enumerate(dataloaders):</span><br><span class="line">        steps += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        images, labels = images.to(<span class="string">'cuda'</span>), labels.to(<span class="string">'cuda'</span>)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        outputs = model.forward(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> print_every % steps == <span class="number">0</span>:</span><br><span class="line">            model.eval()</span><br><span class="line">            accuracy = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> ii, (images, labels) <span class="keyword">in</span> enumerate(test_dataloaders):</span><br><span class="line">                </span><br><span class="line">                images, labels = images.to(<span class="string">'cuda'</span>), labels.to(<span class="string">'cuda'</span>)</span><br><span class="line">                predicted = model(images)</span><br><span class="line">                equality = (labels == predicted.max(<span class="number">1</span>)[<span class="number">1</span>])</span><br><span class="line">                accuracy += equality.type_as(torch.FloatTensor()).mean()</span><br><span class="line">            print(<span class="string">"Epoch: &#123;&#125;/&#123;&#125;"</span>.format(i+<span class="number">1</span>, epochs),</span><br><span class="line">                  <span class="string">"Loss: &#123;:.4f&#125;"</span>.format(running_loss/print_every),</span><br><span class="line">                  <span class="string">"Test accuracy: &#123;:.4f&#125;"</span>.format(accuracy/(ii+<span class="number">1</span>)))</span><br><span class="line">            model.train()</span><br><span class="line">            steps = <span class="number">0</span></span><br><span class="line">            running_loss = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<pre><code>Epoch: 1/3 Loss: 0.1159 Test accuracy: 0.0613
Epoch: 1/3 Loss: 0.1666 Test accuracy: 0.0481
Epoch: 1/3 Loss: 0.1464 Test accuracy: 0.1146
Epoch: 1/3 Loss: 0.1557 Test accuracy: 0.1070
Epoch: 1/3 Loss: 0.1142 Test accuracy: 0.1154
Epoch: 1/3 Loss: 0.1290 Test accuracy: 0.1619
Epoch: 1/3 Loss: 0.1171 Test accuracy: 0.2007
Epoch: 1/3 Loss: 0.1080 Test accuracy: 0.1863
Epoch: 1/3 Loss: 0.1133 Test accuracy: 0.2224
Epoch: 1/3 Loss: 0.1019 Test accuracy: 0.2841
Epoch: 1/3 Loss: 0.0866 Test accuracy: 0.3153
Epoch: 1/3 Loss: 0.0825 Test accuracy: 0.3322
Epoch: 1/3 Loss: 0.0917 Test accuracy: 0.3346
Epoch: 1/3 Loss: 0.0864 Test accuracy: 0.3531
Epoch: 1/3 Loss: 0.0782 Test accuracy: 0.3771
Epoch: 1/3 Loss: 0.0754 Test accuracy: 0.3847
Epoch: 1/3 Loss: 0.0702 Test accuracy: 0.3875
Epoch: 1/3 Loss: 0.0790 Test accuracy: 0.3971
Epoch: 1/3 Loss: 0.0797 Test accuracy: 0.4152
Epoch: 1/3 Loss: 0.0707 Test accuracy: 0.4292
Epoch: 1/3 Loss: 0.0616 Test accuracy: 0.4248
Epoch: 1/3 Loss: 0.0624 Test accuracy: 0.4429
Epoch: 1/3 Loss: 0.0766 Test accuracy: 0.4593
Epoch: 1/3 Loss: 0.0594 Test accuracy: 0.4701
Epoch: 1/3 Loss: 0.0612 Test accuracy: 0.4953
Epoch: 1/3 Loss: 0.0680 Test accuracy: 0.4989
Epoch: 1/3 Loss: 0.0605 Test accuracy: 0.5077
Epoch: 1/3 Loss: 0.0618 Test accuracy: 0.5153
Epoch: 1/3 Loss: 0.0606 Test accuracy: 0.5157
Epoch: 1/3 Loss: 0.0542 Test accuracy: 0.5493
Epoch: 1/3 Loss: 0.0562 Test accuracy: 0.5385
Epoch: 1/3 Loss: 0.0484 Test accuracy: 0.5389
Epoch: 1/3 Loss: 0.0390 Test accuracy: 0.5638
Epoch: 1/3 Loss: 0.0559 Test accuracy: 0.5786
Epoch: 1/3 Loss: 0.0445 Test accuracy: 0.5758
Epoch: 1/3 Loss: 0.0487 Test accuracy: 0.5682
Epoch: 1/3 Loss: 0.0436 Test accuracy: 0.5819
Epoch: 1/3 Loss: 0.0566 Test accuracy: 0.5731
Epoch: 1/3 Loss: 0.0488 Test accuracy: 0.5879
Epoch: 1/3 Loss: 0.0469 Test accuracy: 0.6104
Epoch: 1/3 Loss: 0.0519 Test accuracy: 0.6116
Epoch: 1/3 Loss: 0.0413 Test accuracy: 0.6144
Epoch: 1/3 Loss: 0.0532 Test accuracy: 0.6300
Epoch: 1/3 Loss: 0.0380 Test accuracy: 0.6480
Epoch: 1/3 Loss: 0.0592 Test accuracy: 0.6673
Epoch: 1/3 Loss: 0.0497 Test accuracy: 0.6805
Epoch: 1/3 Loss: 0.0449 Test accuracy: 0.6925
Epoch: 1/3 Loss: 0.0509 Test accuracy: 0.6969
Epoch: 1/3 Loss: 0.0479 Test accuracy: 0.6933
Epoch: 1/3 Loss: 0.0409 Test accuracy: 0.6912
Epoch: 1/3 Loss: 0.0470 Test accuracy: 0.6952
Epoch: 1/3 Loss: 0.0369 Test accuracy: 0.6832
Epoch: 1/3 Loss: 0.0432 Test accuracy: 0.6696
Epoch: 1/3 Loss: 0.0372 Test accuracy: 0.6821
Epoch: 1/3 Loss: 0.0347 Test accuracy: 0.6781
Epoch: 1/3 Loss: 0.0302 Test accuracy: 0.6777
Epoch: 1/3 Loss: 0.0433 Test accuracy: 0.6757
Epoch: 1/3 Loss: 0.0433 Test accuracy: 0.6869
Epoch: 1/3 Loss: 0.0454 Test accuracy: 0.7022
Epoch: 1/3 Loss: 0.0380 Test accuracy: 0.7130
Epoch: 1/3 Loss: 0.0339 Test accuracy: 0.7154
Epoch: 1/3 Loss: 0.0346 Test accuracy: 0.7094
Epoch: 1/3 Loss: 0.0343 Test accuracy: 0.6914
Epoch: 1/3 Loss: 0.0469 Test accuracy: 0.6998
Epoch: 1/3 Loss: 0.0324 Test accuracy: 0.7118
Epoch: 1/3 Loss: 0.0347 Test accuracy: 0.7130
Epoch: 1/3 Loss: 0.0314 Test accuracy: 0.7174
Epoch: 1/3 Loss: 0.0408 Test accuracy: 0.7222
Epoch: 1/3 Loss: 0.0355 Test accuracy: 0.7382
Epoch: 1/3 Loss: 0.0421 Test accuracy: 0.7603
Epoch: 1/3 Loss: 0.0414 Test accuracy: 0.7747
Epoch: 1/3 Loss: 0.0332 Test accuracy: 0.7771
Epoch: 1/3 Loss: 0.0318 Test accuracy: 0.7755
Epoch: 1/3 Loss: 0.0281 Test accuracy: 0.7666
Epoch: 1/3 Loss: 0.0430 Test accuracy: 0.7718
Epoch: 1/3 Loss: 0.0384 Test accuracy: 0.7690
Epoch: 1/3 Loss: 0.0350 Test accuracy: 0.7718
Epoch: 1/3 Loss: 0.0353 Test accuracy: 0.7582
Epoch: 1/3 Loss: 0.0390 Test accuracy: 0.7618
Epoch: 1/3 Loss: 0.0271 Test accuracy: 0.7522
Epoch: 1/3 Loss: 0.0253 Test accuracy: 0.7594
Epoch: 1/3 Loss: 0.0417 Test accuracy: 0.7622
Epoch: 1/3 Loss: 0.0258 Test accuracy: 0.7590
Epoch: 1/3 Loss: 0.0358 Test accuracy: 0.7590
Epoch: 1/3 Loss: 0.0314 Test accuracy: 0.7566
Epoch: 1/3 Loss: 0.0260 Test accuracy: 0.7386
Epoch: 1/3 Loss: 0.0384 Test accuracy: 0.7278
Epoch: 1/3 Loss: 0.0281 Test accuracy: 0.7218
Epoch: 1/3 Loss: 0.0323 Test accuracy: 0.7126
Epoch: 1/3 Loss: 0.0287 Test accuracy: 0.7077
Epoch: 1/3 Loss: 0.0287 Test accuracy: 0.7142
Epoch: 1/3 Loss: 0.0206 Test accuracy: 0.7355
Epoch: 1/3 Loss: 0.0277 Test accuracy: 0.7523
Epoch: 1/3 Loss: 0.0314 Test accuracy: 0.7667
Epoch: 1/3 Loss: 0.0339 Test accuracy: 0.7823
Epoch: 1/3 Loss: 0.0322 Test accuracy: 0.7967
Epoch: 1/3 Loss: 0.0248 Test accuracy: 0.8064
Epoch: 1/3 Loss: 0.0267 Test accuracy: 0.7995
Epoch: 1/3 Loss: 0.0368 Test accuracy: 0.7815
Epoch: 1/3 Loss: 0.0378 Test accuracy: 0.7747
Epoch: 1/3 Loss: 0.0312 Test accuracy: 0.7567
Epoch: 1/3 Loss: 0.0221 Test accuracy: 0.7519
Epoch: 1/3 Loss: 0.0260 Test accuracy: 0.7386
Epoch: 2/3 Loss: 0.0254 Test accuracy: 0.7378
Epoch: 2/3 Loss: 0.0358 Test accuracy: 0.7514
Epoch: 2/3 Loss: 0.0297 Test accuracy: 0.7574
Epoch: 2/3 Loss: 0.0246 Test accuracy: 0.7510
Epoch: 2/3 Loss: 0.0277 Test accuracy: 0.7602
Epoch: 2/3 Loss: 0.0300 Test accuracy: 0.7723
Epoch: 2/3 Loss: 0.0331 Test accuracy: 0.7803
Epoch: 2/3 Loss: 0.0217 Test accuracy: 0.7739
Epoch: 2/3 Loss: 0.0230 Test accuracy: 0.7727
Epoch: 2/3 Loss: 0.0229 Test accuracy: 0.7655
Epoch: 2/3 Loss: 0.0195 Test accuracy: 0.7583
Epoch: 2/3 Loss: 0.0281 Test accuracy: 0.7571
Epoch: 2/3 Loss: 0.0306 Test accuracy: 0.7655
Epoch: 2/3 Loss: 0.0194 Test accuracy: 0.7819
Epoch: 2/3 Loss: 0.0213 Test accuracy: 0.7916
Epoch: 2/3 Loss: 0.0231 Test accuracy: 0.7976
Epoch: 2/3 Loss: 0.0173 Test accuracy: 0.7916
Epoch: 2/3 Loss: 0.0161 Test accuracy: 0.7856
Epoch: 2/3 Loss: 0.0144 Test accuracy: 0.7771
Epoch: 2/3 Loss: 0.0289 Test accuracy: 0.7739
Epoch: 2/3 Loss: 0.0204 Test accuracy: 0.7695
Epoch: 2/3 Loss: 0.0260 Test accuracy: 0.7815
Epoch: 2/3 Loss: 0.0271 Test accuracy: 0.8031
Epoch: 2/3 Loss: 0.0237 Test accuracy: 0.8120
Epoch: 2/3 Loss: 0.0228 Test accuracy: 0.8084
Epoch: 2/3 Loss: 0.0238 Test accuracy: 0.8084
Epoch: 2/3 Loss: 0.0208 Test accuracy: 0.7980
Epoch: 2/3 Loss: 0.0237 Test accuracy: 0.7919
Epoch: 2/3 Loss: 0.0181 Test accuracy: 0.7976
Epoch: 2/3 Loss: 0.0361 Test accuracy: 0.7996
Epoch: 2/3 Loss: 0.0228 Test accuracy: 0.7828
Epoch: 2/3 Loss: 0.0252 Test accuracy: 0.7852
Epoch: 2/3 Loss: 0.0246 Test accuracy: 0.7880
Epoch: 2/3 Loss: 0.0157 Test accuracy: 0.7856
Epoch: 2/3 Loss: 0.0221 Test accuracy: 0.7847
Epoch: 2/3 Loss: 0.0241 Test accuracy: 0.7919
Epoch: 2/3 Loss: 0.0220 Test accuracy: 0.7959
Epoch: 2/3 Loss: 0.0158 Test accuracy: 0.8055
Epoch: 2/3 Loss: 0.0233 Test accuracy: 0.8164
Epoch: 2/3 Loss: 0.0267 Test accuracy: 0.8304
Epoch: 2/3 Loss: 0.0331 Test accuracy: 0.8324
Epoch: 2/3 Loss: 0.0232 Test accuracy: 0.8312
Epoch: 2/3 Loss: 0.0169 Test accuracy: 0.8132
Epoch: 2/3 Loss: 0.0201 Test accuracy: 0.8036
Epoch: 2/3 Loss: 0.0197 Test accuracy: 0.7827
Epoch: 2/3 Loss: 0.0181 Test accuracy: 0.7899
Epoch: 2/3 Loss: 0.0277 Test accuracy: 0.8043
Epoch: 2/3 Loss: 0.0124 Test accuracy: 0.8176
Epoch: 2/3 Loss: 0.0211 Test accuracy: 0.8236
Epoch: 2/3 Loss: 0.0203 Test accuracy: 0.8284
Epoch: 2/3 Loss: 0.0177 Test accuracy: 0.8357
Epoch: 2/3 Loss: 0.0196 Test accuracy: 0.8272
Epoch: 2/3 Loss: 0.0251 Test accuracy: 0.8236
Epoch: 2/3 Loss: 0.0212 Test accuracy: 0.8140
Epoch: 2/3 Loss: 0.0180 Test accuracy: 0.8056
Epoch: 2/3 Loss: 0.0213 Test accuracy: 0.7984
Epoch: 2/3 Loss: 0.0292 Test accuracy: 0.7964
Epoch: 2/3 Loss: 0.0195 Test accuracy: 0.8060
Epoch: 2/3 Loss: 0.0317 Test accuracy: 0.8036
Epoch: 2/3 Loss: 0.0245 Test accuracy: 0.8168
Epoch: 2/3 Loss: 0.0229 Test accuracy: 0.8144
Epoch: 2/3 Loss: 0.0170 Test accuracy: 0.8168
Epoch: 2/3 Loss: 0.0243 Test accuracy: 0.8048
Epoch: 2/3 Loss: 0.0174 Test accuracy: 0.8068
Epoch: 2/3 Loss: 0.0205 Test accuracy: 0.8008
Epoch: 2/3 Loss: 0.0250 Test accuracy: 0.8056
Epoch: 2/3 Loss: 0.0168 Test accuracy: 0.8104
Epoch: 2/3 Loss: 0.0174 Test accuracy: 0.8116
Epoch: 2/3 Loss: 0.0151 Test accuracy: 0.8104
Epoch: 2/3 Loss: 0.0247 Test accuracy: 0.8056
Epoch: 2/3 Loss: 0.0254 Test accuracy: 0.8092
Epoch: 2/3 Loss: 0.0323 Test accuracy: 0.8272
Epoch: 2/3 Loss: 0.0241 Test accuracy: 0.8332
Epoch: 2/3 Loss: 0.0187 Test accuracy: 0.8369
Epoch: 2/3 Loss: 0.0249 Test accuracy: 0.8372
Epoch: 2/3 Loss: 0.0256 Test accuracy: 0.8396
Epoch: 2/3 Loss: 0.0196 Test accuracy: 0.8280
Epoch: 2/3 Loss: 0.0201 Test accuracy: 0.8280
Epoch: 2/3 Loss: 0.0243 Test accuracy: 0.8324
Epoch: 2/3 Loss: 0.0303 Test accuracy: 0.8280
Epoch: 2/3 Loss: 0.0196 Test accuracy: 0.8300
Epoch: 2/3 Loss: 0.0188 Test accuracy: 0.8132
Epoch: 2/3 Loss: 0.0207 Test accuracy: 0.8152
Epoch: 2/3 Loss: 0.0300 Test accuracy: 0.8068
Epoch: 2/3 Loss: 0.0179 Test accuracy: 0.8080
Epoch: 2/3 Loss: 0.0353 Test accuracy: 0.8068
Epoch: 2/3 Loss: 0.0162 Test accuracy: 0.8236
Epoch: 2/3 Loss: 0.0130 Test accuracy: 0.8256
Epoch: 2/3 Loss: 0.0252 Test accuracy: 0.8256
Epoch: 2/3 Loss: 0.0199 Test accuracy: 0.8284
Epoch: 2/3 Loss: 0.0217 Test accuracy: 0.8360
Epoch: 2/3 Loss: 0.0212 Test accuracy: 0.8492
Epoch: 2/3 Loss: 0.0222 Test accuracy: 0.8516
Epoch: 2/3 Loss: 0.0216 Test accuracy: 0.8492
Epoch: 2/3 Loss: 0.0164 Test accuracy: 0.8528
Epoch: 2/3 Loss: 0.0171 Test accuracy: 0.8504
Epoch: 2/3 Loss: 0.0170 Test accuracy: 0.8464
Epoch: 2/3 Loss: 0.0197 Test accuracy: 0.8464
Epoch: 2/3 Loss: 0.0249 Test accuracy: 0.8404
Epoch: 2/3 Loss: 0.0329 Test accuracy: 0.8348
Epoch: 3/3 Loss: 0.0149 Test accuracy: 0.8364
Epoch: 3/3 Loss: 0.0172 Test accuracy: 0.8316
Epoch: 3/3 Loss: 0.0240 Test accuracy: 0.8292
Epoch: 3/3 Loss: 0.0231 Test accuracy: 0.8228
Epoch: 3/3 Loss: 0.0250 Test accuracy: 0.8312
Epoch: 3/3 Loss: 0.0110 Test accuracy: 0.8312
Epoch: 3/3 Loss: 0.0179 Test accuracy: 0.8300
Epoch: 3/3 Loss: 0.0157 Test accuracy: 0.8296
Epoch: 3/3 Loss: 0.0201 Test accuracy: 0.8345
Epoch: 3/3 Loss: 0.0181 Test accuracy: 0.8308
Epoch: 3/3 Loss: 0.0270 Test accuracy: 0.8284
Epoch: 3/3 Loss: 0.0233 Test accuracy: 0.8264
Epoch: 3/3 Loss: 0.0161 Test accuracy: 0.8268
Epoch: 3/3 Loss: 0.0189 Test accuracy: 0.8172
Epoch: 3/3 Loss: 0.0272 Test accuracy: 0.8152
Epoch: 3/3 Loss: 0.0241 Test accuracy: 0.8107
Epoch: 3/3 Loss: 0.0175 Test accuracy: 0.8119
Epoch: 3/3 Loss: 0.0187 Test accuracy: 0.8167
Epoch: 3/3 Loss: 0.0137 Test accuracy: 0.8320
Epoch: 3/3 Loss: 0.0210 Test accuracy: 0.8344
Epoch: 3/3 Loss: 0.0156 Test accuracy: 0.8400
Epoch: 3/3 Loss: 0.0179 Test accuracy: 0.8380
Epoch: 3/3 Loss: 0.0126 Test accuracy: 0.8320
Epoch: 3/3 Loss: 0.0169 Test accuracy: 0.8236
Epoch: 3/3 Loss: 0.0161 Test accuracy: 0.8304
Epoch: 3/3 Loss: 0.0160 Test accuracy: 0.8376
Epoch: 3/3 Loss: 0.0187 Test accuracy: 0.8412
Epoch: 3/3 Loss: 0.0231 Test accuracy: 0.8448
Epoch: 3/3 Loss: 0.0206 Test accuracy: 0.8393
Epoch: 3/3 Loss: 0.0248 Test accuracy: 0.8332
Epoch: 3/3 Loss: 0.0169 Test accuracy: 0.8429
Epoch: 3/3 Loss: 0.0158 Test accuracy: 0.8364
Epoch: 3/3 Loss: 0.0172 Test accuracy: 0.8320
Epoch: 3/3 Loss: 0.0199 Test accuracy: 0.8320
Epoch: 3/3 Loss: 0.0167 Test accuracy: 0.8352
Epoch: 3/3 Loss: 0.0220 Test accuracy: 0.8340
Epoch: 3/3 Loss: 0.0151 Test accuracy: 0.8316
Epoch: 3/3 Loss: 0.0198 Test accuracy: 0.8316
Epoch: 3/3 Loss: 0.0119 Test accuracy: 0.8376
Epoch: 3/3 Loss: 0.0177 Test accuracy: 0.8352
Epoch: 3/3 Loss: 0.0188 Test accuracy: 0.8308
Epoch: 3/3 Loss: 0.0187 Test accuracy: 0.8332
Epoch: 3/3 Loss: 0.0193 Test accuracy: 0.8360
Epoch: 3/3 Loss: 0.0135 Test accuracy: 0.8356
Epoch: 3/3 Loss: 0.0173 Test accuracy: 0.8380
Epoch: 3/3 Loss: 0.0154 Test accuracy: 0.8356
Epoch: 3/3 Loss: 0.0187 Test accuracy: 0.8340
Epoch: 3/3 Loss: 0.0203 Test accuracy: 0.8360
Epoch: 3/3 Loss: 0.0242 Test accuracy: 0.8396
Epoch: 3/3 Loss: 0.0175 Test accuracy: 0.8260
Epoch: 3/3 Loss: 0.0140 Test accuracy: 0.8212
Epoch: 3/3 Loss: 0.0127 Test accuracy: 0.8116
Epoch: 3/3 Loss: 0.0303 Test accuracy: 0.8056
Epoch: 3/3 Loss: 0.0125 Test accuracy: 0.7996
Epoch: 3/3 Loss: 0.0148 Test accuracy: 0.8020
Epoch: 3/3 Loss: 0.0168 Test accuracy: 0.8080
Epoch: 3/3 Loss: 0.0129 Test accuracy: 0.8120
Epoch: 3/3 Loss: 0.0202 Test accuracy: 0.8156
Epoch: 3/3 Loss: 0.0230 Test accuracy: 0.8144
Epoch: 3/3 Loss: 0.0111 Test accuracy: 0.8180
Epoch: 3/3 Loss: 0.0242 Test accuracy: 0.8184
Epoch: 3/3 Loss: 0.0158 Test accuracy: 0.8256
Epoch: 3/3 Loss: 0.0155 Test accuracy: 0.8184
Epoch: 3/3 Loss: 0.0133 Test accuracy: 0.8228
Epoch: 3/3 Loss: 0.0142 Test accuracy: 0.8216
Epoch: 3/3 Loss: 0.0189 Test accuracy: 0.8300
Epoch: 3/3 Loss: 0.0184 Test accuracy: 0.8481
Epoch: 3/3 Loss: 0.0163 Test accuracy: 0.8505
Epoch: 3/3 Loss: 0.0066 Test accuracy: 0.8444
Epoch: 3/3 Loss: 0.0141 Test accuracy: 0.8364
Epoch: 3/3 Loss: 0.0192 Test accuracy: 0.8260
Epoch: 3/3 Loss: 0.0196 Test accuracy: 0.8248
Epoch: 3/3 Loss: 0.0177 Test accuracy: 0.8264
Epoch: 3/3 Loss: 0.0183 Test accuracy: 0.8227
Epoch: 3/3 Loss: 0.0220 Test accuracy: 0.8315
Epoch: 3/3 Loss: 0.0182 Test accuracy: 0.8391
Epoch: 3/3 Loss: 0.0240 Test accuracy: 0.8391
Epoch: 3/3 Loss: 0.0275 Test accuracy: 0.8503
Epoch: 3/3 Loss: 0.0233 Test accuracy: 0.8463
Epoch: 3/3 Loss: 0.0228 Test accuracy: 0.8463
Epoch: 3/3 Loss: 0.0137 Test accuracy: 0.8379
Epoch: 3/3 Loss: 0.0231 Test accuracy: 0.8376
Epoch: 3/3 Loss: 0.0134 Test accuracy: 0.8308
Epoch: 3/3 Loss: 0.0292 Test accuracy: 0.8324
Epoch: 3/3 Loss: 0.0212 Test accuracy: 0.8389
Epoch: 3/3 Loss: 0.0101 Test accuracy: 0.8405
Epoch: 3/3 Loss: 0.0216 Test accuracy: 0.8372
Epoch: 3/3 Loss: 0.0202 Test accuracy: 0.8396
Epoch: 3/3 Loss: 0.0227 Test accuracy: 0.8420
Epoch: 3/3 Loss: 0.0194 Test accuracy: 0.8400
Epoch: 3/3 Loss: 0.0221 Test accuracy: 0.8400
Epoch: 3/3 Loss: 0.0159 Test accuracy: 0.8420
Epoch: 3/3 Loss: 0.0198 Test accuracy: 0.8444
Epoch: 3/3 Loss: 0.0214 Test accuracy: 0.8372
Epoch: 3/3 Loss: 0.0144 Test accuracy: 0.8324
Epoch: 3/3 Loss: 0.0148 Test accuracy: 0.8228
Epoch: 3/3 Loss: 0.0187 Test accuracy: 0.8296
Epoch: 3/3 Loss: 0.0175 Test accuracy: 0.8312
Epoch: 3/3 Loss: 0.0175 Test accuracy: 0.8300
Epoch: 3/3 Loss: 0.0280 Test accuracy: 0.8340
Epoch: 3/3 Loss: 0.0187 Test accuracy: 0.8412
Epoch: 3/3 Loss: 0.0215 Test accuracy: 0.8501
Epoch: 3/3 Loss: 0.0249 Test accuracy: 0.8537
</code></pre><h2 id="保存检查点"><a href="#保存检查点" class="headerlink" title="保存检查点"></a>保存检查点</h2><p>训练好网络后，保存模型，以便稍后加载它并进行预测。你可能还需要保存其他内容，例如从类别到索引的映射，索引是从某个图像数据集中获取的：<code>image_datasets[&#39;train&#39;].class_to_idx</code>。你可以将其作为属性附加到模型上，这样稍后推理会更轻松。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">注意，稍后你需要完全重新构建模型，以便用模型进行推理。确保在检查点中包含你所需的任何信息。如果你想加载模型并继续训练，则需要保存周期数量和优化器状态 `optimizer.state_dict`。你可能需要在下面的下个部分使用训练的模型，因此建议立即保存它。</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">```python</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Save the checkpoint </span></span><br><span class="line"><span class="comment"># model_dict = image_datasets.class_to_idx</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">'checkpoint.pth'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="加载检查点"><a href="#加载检查点" class="headerlink" title="加载检查点"></a>加载检查点</h2><p>此刻，建议写一个可以加载检查点并重新构建模型的函数。这样的话，你可以回到此项目并继续完善它，而不用重新训练网络。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Write a function that loads a checkpoint and rebuilds the model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_checkpoint</span><span class="params">(filepath)</span>:</span></span><br><span class="line">    model_res = torch.load(filepath)</span><br><span class="line">    model.load_state_dict(model_res)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = load_checkpoint(<span class="string">'checkpoint.pth'</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<pre><code>VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (fc1): Linear(in_features=25088, out_features=1024, bias=True)
    (relu): ReLU()
    (fc2): Linear(in_features=1024, out_features=102, bias=True)
    (output): LogSoftmax()
  )
)
</code></pre><h1 id="类别推理"><a href="#类别推理" class="headerlink" title="类别推理"></a>类别推理</h1><p>现在，你需要写一个使用训练的网络进行推理的函数。即你将向网络中传入一个图像，并预测图像中的花卉类别。写一个叫做 <code>predict</code> 的函数，该函数会接受图像和模型，然后返回概率在前 $K$ 的类别及其概率。应该如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(image_path, model, topk=<span class="number">5</span>)</span>:</span></span><br><span class="line">    <span class="string">''' Predict the class (or classes) of an image using a trained deep learning model.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    model.eval()</span><br><span class="line">    </span><br><span class="line">    dataiter = iter(valid_dataloaders)</span><br><span class="line">    images, labels = dataiter.next()</span><br><span class="line">    outputs = model(Variable(images))</span><br><span class="line">    </span><br><span class="line">    _,predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> _,predicted</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the code to predict the class from an image file</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">image_path = data_dir + valid_dir</span><br><span class="line">probs, classes = predict(image_path, model)</span><br><span class="line">print(probs)</span><br><span class="line">print(classes)</span><br><span class="line"><span class="comment"># &gt; [ 0.01558163  0.01541934  0.01452626  0.01443549  0.01407339]</span></span><br><span class="line"><span class="comment"># &gt; ['70', '3', '45', '62', '55']</span></span><br></pre></td></tr></table></figure>
<pre><code>tensor([-5.6533e-01, -1.0756e+00, -3.3086e-02, -6.0575e-01, -1.6227e+00,
        -1.2889e+00, -9.1778e-01, -1.0155e-01, -9.0205e-03, -2.6388e-02,
        -1.1171e-01, -2.5127e-05, -8.6765e-06, -1.2372e-05, -3.3515e-03,
        -4.3414e-04, -2.2719e-02, -3.1610e-02, -1.4661e-01, -6.8172e-03,
        -3.0326e-03, -4.6300e-04, -5.8895e-01, -7.0555e-03, -4.4683e-03,
        -1.2083e-06, -7.6587e-02, -2.2083e-05, -7.6721e-05, -5.5256e-02,
        -9.8522e-01, -1.8399e-01])
tensor([  0,  95,   0,  27,  99,   0,  49,   0,  94,   1,   1,   1,
          2,   2,   2,   2,   2,   2,   3,   3,   3,   3,   5,   4,
          4,   4,   4,   4,   4,   5,  40,   5])
</code></pre><p>首先，你需要处理输入图像，使其可以用于你的网络。</p>
<h2 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h2><p>你需要使用 <code>PIL</code> 加载图像（<a href="https://pillow.readthedocs.io/en/latest/reference/Image.html" target="_blank" rel="noopener">文档</a>）。建议写一个函数来处理图像，使图像可以作为模型的输入。该函数应该按照训练的相同方式处理图像。</p>
<p>首先，调整图像大小，使最小的边为 256 像素，并保持宽高比。为此，可以使用 <a href="http://pillow.readthedocs.io/en/3.1.x/reference/Image.html#PIL.Image.Image.thumbnail" target="_blank" rel="noopener"><code>thumbnail</code></a> 或 <a href="http://pillow.readthedocs.io/en/3.1.x/reference/Image.html#PIL.Image.Image.thumbnail" target="_blank" rel="noopener"><code>resize</code></a> 方法。然后，你需要从图像的中心裁剪出 224x224 的部分。</p>
<p>图像的颜色通道通常编码为整数 0-255，但是该模型要求值为浮点数 0-1。你需要变换值。使用 Numpy 数组最简单，你可以从 PIL 图像中获取，例如 <code>np_image = np.array(pil_image)</code>。</p>
<p>和之前一样，网络要求图像按照特定的方式标准化。均值应标准化为 <code>[0.485, 0.456, 0.406]</code>，标准差应标准化为 <code>[0.229, 0.224, 0.225]</code>。你需要用每个颜色通道减去均值，然后除以标准差。</p>
<p>最后，PyTorch 要求颜色通道为第一个维度，但是在 PIL 图像和 Numpy 数组中是第三个维度。你可以使用 <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ndarray.transpose.html" target="_blank" rel="noopener"><code>ndarray.transpose</code></a>对维度重新排序。颜色通道必须是第一个维度，并保持另外两个维度的顺序。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_image</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="string">''' Scales, crops, and normalizes a PIL image for a PyTorch model,</span></span><br><span class="line"><span class="string">        returns an Numpy array</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    valid_transforms = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">255</span>),</span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                        [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">])</span><br><span class="line">    image_datasets_valid = datasets.ImageFolder(data_dir + valid_dir, transform=valid_transforms)</span><br><span class="line">    </span><br><span class="line">    valid_dataloaders = torch.utils.data.DataLoader(image_datasets_valid, batch_size=<span class="number">32</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Process a PIL image for use in a PyTorch model</span></span><br></pre></td></tr></table></figure>
<p>要检查你的项目，可以使用以下函数来转换 PyTorch 张量并将其显示在  notebook 中。如果 <code>process_image</code> 函数可行，用该函数运行输出应该会返回原始图像（但是剪裁掉的部分除外）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(image, ax=None, title=None)</span>:</span></span><br><span class="line">    <span class="string">"""Imshow for Tensor."""</span></span><br><span class="line">    <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        fig, ax = plt.subplots()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># PyTorch tensors assume the color channel is the first dimension</span></span><br><span class="line">    <span class="comment"># but matplotlib assumes is the third dimension</span></span><br><span class="line">    image = image.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Undo preprocessing</span></span><br><span class="line">    mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">    std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    image = std * image + mean</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Image needs to be clipped between 0 and 1 or it looks like noise when displayed</span></span><br><span class="line">    image = np.clip(image, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    ax.imshow(image)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ax</span><br></pre></td></tr></table></figure>
<h2 id="类别预测"><a href="#类别预测" class="headerlink" title="类别预测"></a>类别预测</h2><p>可以获得格式正确的图像后 </p>
<p>要获得前 $K$ 个值，在张量中使用 <a href="http://pytorch.org/docs/master/torch.html#torch.topk" target="_blank" rel="noopener"><code>x.topk(k)</code></a>。该函数会返回前 <code>k</code> 个概率和对应的类别索引。你需要使用  <code>class_to_idx</code>（希望你将其添加到了模型中）将这些索引转换为实际类别标签，或者从用来加载数据的<a href="https://pytorch.org/docs/master/torchvision/datasets.html?highlight=imagefolder#torchvision.datasets.ImageFolder" target="_blank" rel="noopener"> <code>ImageFolder</code></a>进行转换。确保颠倒字典</p>
<p>同样，此方法应该接受图像路径和模型检查点，并返回概率和类别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">probs, classes = predict(image_path, model)</span><br><span class="line">print(probs)</span><br><span class="line">print(classes)</span><br><span class="line">&gt; [ <span class="number">0.01558163</span>  <span class="number">0.01541934</span>  <span class="number">0.01452626</span>  <span class="number">0.01443549</span>  <span class="number">0.01407339</span>]</span><br><span class="line">&gt; [<span class="string">'70'</span>, <span class="string">'3'</span>, <span class="string">'45'</span>, <span class="string">'62'</span>, <span class="string">'55'</span>]</span><br></pre></td></tr></table></figure>
<h2 id="检查运行状况"><a href="#检查运行状况" class="headerlink" title="检查运行状况"></a>检查运行状况</h2><p>你已经可以使用训练的模型做出预测，现在检查模型的性能如何。即使测试准确率很高，始终有必要检查是否存在明显的错误。使用 <code>matplotlib</code> 将前 5 个类别的概率以及输入图像绘制为条形图，应该如下所示：</p>
<p><img src="/2019/02/16/image-classifier-project/inference_example.png" width="300px"></p>
<p>你可以使用 <code>cat_to_name.json</code> 文件（应该之前已经在 notebook 中加载该文件）将类别整数编码转换为实际花卉名称。要将 PyTorch 张量显示为图像，请使用定义如下的 <code>imshow</code> 函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Display an image along with the top 5 classes</span></span><br></pre></td></tr></table></figure>

    </div>
    
    <div class="post-footer">
        <div>
            
                转载声明：商业转载请联系作者获得授权,非商业转载请注明出处 © <a href target="_blank">Lucifer</a>
            
        </div>
        <div>
            
        </div>
    </div>
</article>

<div class="article-nav prev-next-wrap clearfix">
    
        <a href="/2019/02/16/Linear-Algebra/" class="pre-post btn btn-default" title="Linear Algebra">
            <i class="fa fa-angle-left fa-fw"></i><span class="hidden-lg">上一篇</span>
            <span class="hidden-xs">Linear Algebra</span>
        </a>
    
    
        <a href="/2019/02/16/蒙特卡洛/" class="next-post btn btn-default" title="强化学习(二) 蒙特卡洛">
            <span class="hidden-lg">下一篇</span>
            <span class="hidden-xs">强化学习(二) 蒙特卡洛</span><i class="fa fa-angle-right fa-fw"></i>
        </a>
    
</div>






                </main>
                
                    <aside id="article-toc" role="navigation" class="col-md-4">
    <div class="widget">
        <h3 class="title">Table of Contents</h3>
        
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#开发-AI-应用"><span class="toc-text">开发 AI 应用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#加载数据"><span class="toc-text">加载数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#标签映射"><span class="toc-text">标签映射</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#构建和训练分类器"><span class="toc-text">构建和训练分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#测试网络"><span class="toc-text">测试网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#保存检查点"><span class="toc-text">保存检查点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#加载检查点"><span class="toc-text">加载检查点</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#类别推理"><span class="toc-text">类别推理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#图像处理"><span class="toc-text">图像处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#类别预测"><span class="toc-text">类别预测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#检查运行状况"><span class="toc-text">检查运行状况</span></a></li></ol></li></ol>
        
    </div>
</aside>

                
            </div>
        </div>
    </section>
    <footer class="main-footer">
    <div class="container">
        <div class="row">
        </div>
    </div>
</footer>

<a id="back-to-top" class="icon-btn hide">
	<i class="fa fa-chevron-up"></i>
</a>




    <div class="copyright">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="busuanzi">
    
        Total:
        <strong id="busuanzi_value_site_pv">
            <i class="fa fa-spinner fa-spin"></i>
        </strong>
        &nbsp; | &nbsp;
        Visitors:
        <strong id="busuanzi_value_site_uv">
            <i class="fa fa-spinner fa-spin"></i>
        </strong>
    
</div>

            </div>
            <div class="col-sm-12">
                <span>Copyright &copy; 2017
                </span> |
                <span>
                    Powered by <a href="//hexo.io" class="copyright-links" target="_blank" rel="nofollow">Hexo</a>
                </span> |
                <span>
                    Theme by <a href="//github.com/shenliyang/hexo-theme-snippet.git" class="copyright-links" target="_blank" rel="nofollow">Snippet</a>
                </span>
            </div>
        </div>
    </div>
</div>






    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<script src="/js/app.js?rev=@@hash"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>